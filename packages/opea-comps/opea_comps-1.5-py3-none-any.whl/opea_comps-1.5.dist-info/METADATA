Metadata-Version: 2.4
Name: opea-comps
Version: 1.5
Summary: Generative AI components
Home-page: https://github.com/opea-project/GenAIComps
Author: Intel DCAI Software
Author-email: liang1.lv@intel.com, haihao.shen@intel.com, suyue.chen@intel.com
License: Apache 2.0
Keywords: GenAI
Classifier: Intended Audience :: Science/Research
Classifier: Programming Language :: Python :: 3
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: License :: OSI Approved :: Apache Software License
Requires-Python: >=3.8.0
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: aiofiles==24.1.0
Requires-Dist: aiohappyeyeballs==2.6.1
Requires-Dist: aiohttp==3.12.15
Requires-Dist: aiosignal==1.4.0
Requires-Dist: annotated-doc==0.0.4
Requires-Dist: annotated-types==0.7.0
Requires-Dist: anyio==4.10.0
Requires-Dist: async-timeout==5.0.1; python_full_version < "3.11.3"
Requires-Dist: attrs==25.3.0
Requires-Dist: cachetools==5.5.2
Requires-Dist: certifi==2025.8.3
Requires-Dist: cffi==2.0.0; platform_python_implementation != "PyPy"
Requires-Dist: charset-normalizer==3.4.3
Requires-Dist: click==8.2.1
Requires-Dist: colorama==0.4.6; sys_platform == "win32"
Requires-Dist: cryptography==46.0.3
Requires-Dist: dataclasses-json==0.6.7
Requires-Dist: dnspython==2.7.0
Requires-Dist: docarray==0.41.0
Requires-Dist: docx2txt==0.9
Requires-Dist: durationpy==0.10
Requires-Dist: fastapi==0.124.0
Requires-Dist: frozenlist==1.7.0
Requires-Dist: google-auth==2.40.3
Requires-Dist: googleapis-common-protos==1.70.0
Requires-Dist: greenlet==3.2.4; (python_full_version < "3.14" and platform_machine == "AMD64") or (python_full_version < "3.14" and platform_machine == "WIN32") or (python_full_version < "3.14" and platform_machine == "aarch64") or (python_full_version < "3.14" and platform_machine == "amd64") or (python_full_version < "3.14" and platform_machine == "ppc64le") or (python_full_version < "3.14" and platform_machine == "win32") or (python_full_version < "3.14" and platform_machine == "x86_64")
Requires-Dist: grpcio==1.74.0
Requires-Dist: h11==0.16.0
Requires-Dist: httpcore==1.0.9
Requires-Dist: httpx==0.28.1
Requires-Dist: httpx-sse==0.4.1
Requires-Dist: idna==3.10
Requires-Dist: importlib-metadata==8.7.0
Requires-Dist: jsonpatch==1.33
Requires-Dist: jsonpointer==3.0.0
Requires-Dist: jsonschema==4.25.1
Requires-Dist: jsonschema-specifications==2025.4.1
Requires-Dist: kubernetes==33.1.0
Requires-Dist: langchain==0.3.27
Requires-Dist: langchain-community==0.3.27
Requires-Dist: langchain-core==0.3.80
Requires-Dist: langchain-text-splitters==0.3.9
Requires-Dist: langsmith==0.4.14
Requires-Dist: markdown-it-py==4.0.0
Requires-Dist: marshmallow==3.26.1
Requires-Dist: mcp==1.23.0
Requires-Dist: mdurl==0.1.2
Requires-Dist: motor==3.7.1
Requires-Dist: multidict==6.6.4
Requires-Dist: mypy-extensions==1.1.0
Requires-Dist: numpy==2.3.2
Requires-Dist: oauthlib==3.3.1
Requires-Dist: opentelemetry-api==1.36.0
Requires-Dist: opentelemetry-exporter-otlp==1.36.0
Requires-Dist: opentelemetry-exporter-otlp-proto-common==1.36.0
Requires-Dist: opentelemetry-exporter-otlp-proto-grpc==1.36.0
Requires-Dist: opentelemetry-exporter-otlp-proto-http==1.36.0
Requires-Dist: opentelemetry-proto==1.36.0
Requires-Dist: opentelemetry-sdk==1.36.0
Requires-Dist: opentelemetry-semantic-conventions==0.57b0
Requires-Dist: orjson==3.11.2
Requires-Dist: packaging==25.0
Requires-Dist: pillow==11.3.0
Requires-Dist: prometheus-client==0.22.1
Requires-Dist: prometheus-fastapi-instrumentator==7.1.0
Requires-Dist: propcache==0.3.2
Requires-Dist: protobuf==6.32.0
Requires-Dist: pyasn1==0.6.1
Requires-Dist: pyasn1-modules==0.4.2
Requires-Dist: pycparser==2.23; implementation_name != "PyPy" and platform_python_implementation != "PyPy"
Requires-Dist: pydantic==2.11.7
Requires-Dist: pydantic-core==2.33.2
Requires-Dist: pydantic-settings==2.10.1
Requires-Dist: pygments==2.19.2
Requires-Dist: pyjwt==2.10.1
Requires-Dist: pymongo==4.14.0
Requires-Dist: pypdf==6.4.0
Requires-Dist: python-arango==8.2.2
Requires-Dist: python-arango-async==1.0.2
Requires-Dist: python-dateutil==2.9.0.post0
Requires-Dist: python-dotenv==1.1.1
Requires-Dist: python-multipart==0.0.20
Requires-Dist: pywin32==311; sys_platform == "win32"
Requires-Dist: pyyaml==6.0.2
Requires-Dist: redis==6.4.0
Requires-Dist: referencing==0.36.2
Requires-Dist: requests==2.32.5
Requires-Dist: requests-oauthlib==2.0.0
Requires-Dist: requests-toolbelt==1.0.0
Requires-Dist: rich==14.1.0
Requires-Dist: rpds-py==0.27.0
Requires-Dist: rsa==4.9.1
Requires-Dist: setuptools==80.9.0
Requires-Dist: shortuuid==1.0.13
Requires-Dist: six==1.17.0
Requires-Dist: sniffio==1.3.1
Requires-Dist: sqlalchemy==2.0.43
Requires-Dist: sse-starlette==3.0.2
Requires-Dist: starlette==0.49.1
Requires-Dist: tenacity==9.1.2
Requires-Dist: types-requests==2.32.4.20250809
Requires-Dist: typing-extensions==4.14.1
Requires-Dist: typing-inspect==0.9.0
Requires-Dist: typing-inspection==0.4.1
Requires-Dist: urllib3==2.6.0
Requires-Dist: uvicorn==0.35.0
Requires-Dist: websocket-client==1.8.0
Requires-Dist: yarl==1.20.1
Requires-Dist: zipp==3.23.0
Requires-Dist: zstandard==0.24.0
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: license
Dynamic: license-file
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# Generative AI Components (GenAIComps)

**Build Enterprise-grade Generative AI Applications with Microservice Architecture**

This initiative empowers the development of high-quality Generative AI applications for enterprises via microservices, simplifying the scaling and deployment process for production. It abstracts away infrastructure complexities, facilitating the seamless development and deployment of Enterprise AI services.

## GenAIComps

GenAIComps provides a suite of microservices, leveraging a service composer to assemble a mega-service tailored for real-world Enterprise AI applications. All the microservices are containerized, allowing cloud native deployment. Check out how the microservices are used in [GenAIExamples](https://github.com/opea-project/GenAIExamples)
or [Getting Start with OPEA](https://opea-project.github.io/latest/getting-started/README.html) to deploy the ChatQnA application from OPEA GenAIExamples across multiple cloud platforms.

![Architecture](https://i.imgur.com/r5J0i8j.png)

### Installation

- Install from Pypi

```bash
pip install opea-comps
```

- Build from Source

```bash
git clone https://github.com/opea-project/GenAIComps
cd GenAIComps
pip install -e .
```

## MicroService

`Microservices` are akin to building blocks, offering the fundamental services for constructing `RAG (Retrieval-Augmented Generation)` and other Enterprise AI applications.

Each `Microservice` is designed to perform a specific function or task within the application architecture. By breaking down the system into smaller, self-contained services, `Microservices` promote modularity, flexibility, and scalability.

This modular approach allows developers to independently develop, deploy, and scale individual components of the application, making it easier to maintain and evolve over time. Additionally, `Microservices` facilitate fault isolation, as issues in one service are less likely to impact the entire system.

The initially supported `Microservices` are described in the below table. More `Microservices` are on the way.

| MicroService                                      | Framework                                                                      | Model                                                                                                   | Serving                                                         | HW     | Description                          |
| ------------------------------------------------- | ------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------- | ------ | ------------------------------------ |
| [Embedding](./comps/embeddings/README.md)         | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)                                   | [TEI-Gaudi](https://github.com/huggingface/tei-gaudi)           | Gaudi2 | Embedding on Gaudi2                  |
| [Embedding](./comps/embeddings/README.md)         | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)                                   | [TEI](https://github.com/huggingface/text-embeddings-inference) | Xeon   | Embedding on Xeon CPU                |
| [Retriever](./comps/retrievers/README.md)         | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)                                   | [TEI](https://github.com/huggingface/text-embeddings-inference) | Xeon   | Retriever on Xeon CPU                |
| [Reranking](./comps/rerankings/src/README.md)     | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)                                 | [TEI-Gaudi](https://github.com/huggingface/tei-gaudi)           | Gaudi2 | Reranking on Gaudi2                  |
| [Reranking](./comps/rerankings/src/README.md)     | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)                                 | [TEI](https://github.com/huggingface/text-embeddings-inference) | Xeon   | Reranking on Xeon CPU                |
| [ASR](./comps/asr/src/README.md)                  | NA                                                                             | [openai/whisper-small](https://huggingface.co/openai/whisper-small)                                     | NA                                                              | Gaudi2 | Audio-Speech-Recognition on Gaudi2   |
| [ASR](./comps/asr/src/README.md)                  | NA                                                                             | [openai/whisper-small](https://huggingface.co/openai/whisper-small)                                     | NA                                                              | Xeon   | Audio-Speech-Recognition on Xeon CPU |
| [TTS](./comps/tts/src/README.md)                  | NA                                                                             | [microsoft/speecht5_tts](https://huggingface.co/microsoft/speecht5_tts)                                 | NA                                                              | Gaudi2 | Text-To-Speech on Gaudi2             |
| [TTS](./comps/tts/src/README.md)                  | NA                                                                             | [microsoft/speecht5_tts](https://huggingface.co/microsoft/speecht5_tts)                                 | NA                                                              | Xeon   | Text-To-Speech on Xeon CPU           |
| [Dataprep](./comps/dataprep/README.md)            | [Qdrant](https://qdrant.tech/)                                                 | [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) | NA                                                              | Gaudi2 | Dataprep on Gaudi2                   |
| [Dataprep](./comps/dataprep/README.md)            | [Qdrant](https://qdrant.tech/)                                                 | [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) | NA                                                              | Xeon   | Dataprep on Xeon CPU                 |
| [Dataprep](./comps/dataprep/README.md)            | [Redis](https://redis.io/)                                                     | [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)                                   | NA                                                              | Gaudi2 | Dataprep on Gaudi2                   |
| [Dataprep](./comps/dataprep/README.md)            | [Redis](https://redis.io/)                                                     | [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)                                   | NA                                                              | Xeon   | Dataprep on Xeon CPU                 |
| [LLM](./comps/llms/src/text-generation/README.md) | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [Intel/neural-chat-7b-v3-3](https://huggingface.co/Intel/neural-chat-7b-v3-3)                           | [TGI Gaudi](https://github.com/huggingface/tgi-gaudi)           | Gaudi2 | LLM on Gaudi2                        |
| [LLM](./comps/llms/src/text-generation/README.md) | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [Intel/neural-chat-7b-v3-3](https://huggingface.co/Intel/neural-chat-7b-v3-3)                           | [TGI](https://github.com/huggingface/text-generation-inference) | Xeon   | LLM on Xeon CPU                      |
| [LLM](./comps/llms/src/text-generation/README.md) | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [Intel/neural-chat-7b-v3-3](https://huggingface.co/Intel/neural-chat-7b-v3-3)                           | [Ray Serve](https://github.com/ray-project/ray)                 | Gaudi2 | LLM on Gaudi2                        |
| [LLM](./comps/llms/src/text-generation/README.md) | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [Intel/neural-chat-7b-v3-3](https://huggingface.co/Intel/neural-chat-7b-v3-3)                           | [Ray Serve](https://github.com/ray-project/ray)                 | Xeon   | LLM on Xeon CPU                      |
| [LLM](./comps/llms/src/text-generation/README.md) | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [Intel/neural-chat-7b-v3-3](https://huggingface.co/Intel/neural-chat-7b-v3-3)                           | [vLLM](https://github.com/vllm-project/vllm/)                   | Gaudi2 | LLM on Gaudi2                        |
| [LLM](./comps/llms/src/text-generation/README.md) | [LangChain](https://www.langchain.com)/[LlamaIndex](https://www.llamaindex.ai) | [Intel/neural-chat-7b-v3-3](https://huggingface.co/Intel/neural-chat-7b-v3-3)                           | [vLLM](https://github.com/vllm-project/vllm/)                   | Xeon   | LLM on Xeon CPU                      |

A `Microservices` can be created by using the decorator `register_microservice`. Taking the `embedding microservice` as an example:

```python
from comps import register_microservice, EmbedDoc, ServiceType, TextDoc


@register_microservice(
    name="opea_service@embedding_tgi_gaudi",
    service_type=ServiceType.EMBEDDING,
    endpoint="/v1/embeddings",
    host="0.0.0.0",
    port=6000,
    input_datatype=TextDoc,
    output_datatype=EmbedDoc,
)
def embedding(input: TextDoc) -> EmbedDoc:
    embed_vector = embeddings.embed_query(input.text)
    res = EmbedDoc(text=input.text, embedding=embed_vector)
    return res
```

## MegaService

A `Megaservice` is a higher-level architectural construct composed of one or more `Microservices`, providing the capability to assemble end-to-end applications. Unlike individual `Microservices`, which focus on specific tasks or functions, a `Megaservice` orchestrates multiple `Microservices` to deliver a comprehensive solution.

`Megaservices` encapsulate complex business logic and workflow orchestration, coordinating the interactions between various `Microservices` to fulfill specific application requirements. This approach enables the creation of modular yet integrated applications, where each `Microservice` contributes to the overall functionality of the `Megaservice`.

Here is a simple example of building `Megaservice`:

```python
from comps import MicroService, ServiceOrchestrator

EMBEDDING_SERVICE_HOST_IP = os.getenv("EMBEDDING_SERVICE_HOST_IP", "0.0.0.0")
EMBEDDING_SERVICE_PORT = os.getenv("EMBEDDING_SERVICE_PORT", 6000)
LLM_SERVICE_HOST_IP = os.getenv("LLM_SERVICE_HOST_IP", "0.0.0.0")
LLM_SERVICE_PORT = os.getenv("LLM_SERVICE_PORT", 9000)


class ExampleService:
    def __init__(self, host="0.0.0.0", port=8000):
        self.host = host
        self.port = port
        self.megaservice = ServiceOrchestrator()

    def add_remote_service(self):
        embedding = MicroService(
            name="embedding",
            host=EMBEDDING_SERVICE_HOST_IP,
            port=EMBEDDING_SERVICE_PORT,
            endpoint="/v1/embeddings",
            use_remote_service=True,
            service_type=ServiceType.EMBEDDING,
        )
        llm = MicroService(
            name="llm",
            host=LLM_SERVICE_HOST_IP,
            port=LLM_SERVICE_PORT,
            endpoint="/v1/chat/completions",
            use_remote_service=True,
            service_type=ServiceType.LLM,
        )
        self.megaservice.add(embedding).add(llm)
        self.megaservice.flow_to(embedding, llm)
```

self.gateway = ChatQnAGateway(megaservice=self.megaservice, host="0.0.0.0", port=self.port)

````

## Check Mega/Micro Service health status and version number

Use the command below to check Mega/Micro Service status.

```bash
curl http://${your_ip}:${service_port}/v1/health_check\
  -X GET \
  -H 'Content-Type: application/json'
````

Users should get output like below example if Mega/Micro Service works correctly.

```bash
{"Service Title":"ChatQnAGateway/MicroService","Version":"1.0","Service Description":"OPEA Microservice Infrastructure"}
```

## Contributing to OPEA

Welcome to the OPEA open-source community! We are thrilled to have you here and excited about the potential contributions you can bring to the OPEA platform. Whether you are fixing bugs, adding new GenAI components, improving documentation, or sharing your unique use cases, your contributions are invaluable.

Together, we can make OPEA the go-to platform for enterprise AI solutions. Let's work together to push the boundaries of what's possible and create a future where AI is accessible, efficient, and impactful for everyone.

Please check the [Contributing Guidelines](https://github.com/opea-project/docs/tree/main/community/CONTRIBUTING.md) for a detailed guide on how to contribute a GenAI example and all the ways you can contribute!

Thank you for being a part of this journey. We can't wait to see what we can achieve together!

### uv pip compile usage

To update the existing requirements files, follow the steps below:

1. Update `requirements.in` file with the dependencies you want to add or modify.
2. Install `uv` package with `pip install uv`, suggest to work in the same python version used by your dockerfile.
3. Edit and run `freeze_dependency.sh` to update the `requirements*.txt` file.

To add a new requirements file, create a new `requirements.in` file and an empty `requirements.txt` or `requirements-cpu.txt` or `requirements-gpu.txt`, then follow the same steps above.

## Additional Content

- [Code of Conduct](https://github.com/opea-project/docs/tree/main/community/CODE_OF_CONDUCT.md)
- [Security Policy](https://github.com/opea-project/docs/tree/main/community/SECURITY.md)
- [Legal Information](LEGAL_INFORMATION.md)
