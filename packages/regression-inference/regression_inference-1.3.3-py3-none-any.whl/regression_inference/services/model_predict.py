import numpy as np    
from scipy.stats import t as t_dist, norm

try:
    import cupy as cp
    from ..utils import cuda_conversion
    CUDA = True
except ImportError:
    CUDA = False
    pass


PROB_CLIP_MIN = 1e-15
PROB_CLIP_MAX = 1 - 1e-15

def predict(model, X, alpha, return_table):

    X = np.asarray(X, dtype=float)


    '''
    GPU check for OrdinalRegression()
    '''
    if CUDA:
        if hasattr(model, 'cuda'):
            cuda_conversion.to_numpy(model)


    '''
    Predictor for LogisticRegression()
    '''
    def sigmoid(z):
        return np.where(
            z >= 0,
            1 / (1 + np.exp(-z)),
            np.exp(z) / (1 + np.exp(z))
    )


    '''
    Predictor for MultinomialRegression()
    '''
    def softmax(Z: np.ndarray) -> np.ndarray:

        Z_stable = Z - np.max(Z, axis=1, keepdims=True)

        exp_Z = np.exp(np.clip(Z_stable, -700, 700))

        return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)
    
    
    '''
    Predictor for OrdinalRegression()
    '''
    def predict_prob(X, beta, alpha, n_classes):
            
        n = X.shape[0]
        J = len(alpha)
        cumulative = np.zeros((n, J))

        for j in range(J):
            eta = alpha[j] - X @ beta
            cumulative[:, j] = 1 / (1 + np.exp(-np.clip(eta, -700, 700)))

        categorical_pr = np.zeros((n, n_classes))
        categorical_pr[:, 0] = cumulative[:, 0]

        for j in range(1, J):
            categorical_pr[:, j] = cumulative[:, j] - cumulative[:, j-1]

        categorical_pr[:, J] = 1 - cumulative[:, J-1]
        categorical_pr = np.clip(categorical_pr, PROB_CLIP_MIN, PROB_CLIP_MAX)
        categorical_pr /= categorical_pr.sum(axis=1, keepdims=True)
        return categorical_pr



    '''
    Return only Prediction as integer
    '''
    if not return_table:

        return (

            X @ model.coefficients + model.intercept
            if model.model_type == "linear" else

            sigmoid(X @ model.coefficients + model.intercept)
            if model.model_type == "logit" else

            softmax(
                np.column_stack([
                    np.zeros(X.shape[0]),
                    np.asarray(X, dtype=float) @ model.coefficients + model.intercept
                ])
            )
            if model.model_type == "logit_multinomial" else

            predict_prob(
                np.atleast_2d(X),
                model.coefficients,
                model.alpha_cutpoints,
                model.n_classes
            )
            if model.model_type == "logit_ordinal" else

            ValueError(f"Unknown model type: {model.model_type}")
        )



    '''
    Tabular Predictions shared configurations
    '''

    if not model.model_type == "logit_ordinal":

        prediction_features = {
                name: f'{value_at.item():.2f}'
                for name, value_at in zip(model.feature_names[1:], X[0])
        }

    X = (
        np.hstack([np.ones((X.shape[0], 1)), X])
        if not model.model_type == "logit_ordinal" else
        np.atleast_2d(X)
    )

    if model.model_type in ["linear", "logit"]:
        prediction = X @ model.theta
        se_prediction = (np.sqrt((X @ model.variance_coefficient @ X.T)).item())



    '''
    Tabular Least Squares Predictions
    '''

    if model.model_type == "linear":
                
        t_critical = t_dist.ppf(1 - alpha/2, model.degrees_freedom)

        ci_low, ci_high = (
            (prediction - t_critical * se_prediction),
            (prediction + t_critical * se_prediction)
        )

        t_stat = (
            prediction / se_prediction
            if se_prediction > 0
            else np.inf
        )
        
        p = 2 * (1 - t_dist.cdf(abs(t_stat), model.degrees_freedom))

        return ({
            "features": [prediction_features],
            "prediction": [np.round(prediction.item(), 4)],
            "std_error": [np.round(se_prediction,4)],
            "t_statistic": [np.round(t_stat.item(),4)],
            "P>|t|": [f"{p.item():.3f}"],
            f"ci_low_{alpha}": [np.round(ci_low.item(), 4)],
            f"ci_high_{alpha}": [np.round(ci_high.item(), 4)],
        })
    


    '''
    Tabular base Logit Predictions
    '''

    if model.model_type == "logit":

        prediction_prob = sigmoid(prediction)

        z_critical = norm.ppf(1 - alpha/2)

        ci_low_z, ci_high_z = (
            (prediction - z_critical * se_prediction),
            (prediction + z_critical * se_prediction)
        )

        ci_low_prob, ci_high_prob = (
            sigmoid(ci_low_z),
            sigmoid(ci_high_z)
        )

        z_stat = (
            prediction / se_prediction
            if se_prediction > 0
            else np.inf
        )

        p = (
            2 * (1 - norm.cdf(abs(z_stat)))
        )

        return ({
            "features": [prediction_features],
            "prediction_prob": [np.round(prediction_prob.item(), 4)],
            "prediction_class": [int(prediction_prob.item() >= 0.5)],
            "std_error": [np.round(se_prediction, 4)],
            "z_statistic": [np.round(z_stat.item(), 4)],
            "P>|z|": [f"{p.item():.3f}"],
            f"ci_low_{alpha}": [np.round(ci_low_prob.item(), 4)],
            f"ci_high_{alpha}": [np.round(ci_high_prob.item(), 4)],
        })




    '''
    Tabular Multinomial Logit Predictions
    '''

    if model.model_type == "logit_multinomial":

        multinomial_classes, prediction_probs, prediction_linear, std_errors, z_statistics, p_values, ci_lows, ci_highs = (
            [],[],[],[],[],[],[],[]
        )
        
        x = X[0]

        p, J = model.theta.shape

        z_crit = norm.ppf(1 - alpha / 2)

        ''' 'J' Linear Predictors -> Add Reference Class -> Convert To Probabilities '''

        prediction = x @ model.theta        
        eta_full = np.r_[0.0, prediction]     
        prediction_prob = softmax(eta_full[None, :])[0]
        pred_class = int(np.argmax(prediction_prob))

        '''Initial inference for class Y = 0'''

        p0, pj = (
            prediction_prob[0],  prediction_prob[1:]   
        )

        g0 = np.zeros(p * J)

        for j in range(J):

            g0[ j*p : (j+1)*p ] = -p0 * pj[j] * x

        var_p0 = (
            g0 @ model.xtWx_inv @ g0
        )

        se_p0 = (
            np.sqrt(var_p0)
        )

        ci_low_p0, ci_high_p0 = (
            max(0.0, p0 - z_crit * se_p0),
            min(1.0, p0 + z_crit * se_p0)
        )
  
        multinomial_classes.append(int(model.y_classes[0]))
        prediction_probs.append(np.round(p0, 4))
        prediction_linear.append(0.0)
        std_errors.append(np.round(se_p0, 4))
        z_statistics.append(None)       
        p_values.append(None)
        ci_lows.append(np.round(ci_low_p0, 4))
        ci_highs.append(np.round(ci_high_p0, 4))

        for j in range(J):

            '''Compute inference for J reference classes'''

            idx = slice(j*p, (j+1)*p)
            Vj = model.xtWx_inv[idx, idx]  # Cov block

            se_eta = np.sqrt(x @ Vj @ x)

            ''' Log odds relative to reference class'''

            ci_low_eta, ci_high_eta = (     
                prediction[j] - z_crit * se_eta,
                prediction[j] + z_crit * se_eta
            )

            eta_low, eta_high = (
                eta_full.copy(),
                eta_full.copy()
            )

            eta_low[j+1], eta_high[j+1] = (
                ci_low_eta,
                ci_high_eta
            )

            p_low, p_high = (
                softmax(eta_low[None, :])[0][j+1],
                softmax(eta_high[None, :])[0][j+1]
            )

            z_stat = (
                prediction[j] / se_eta
                if se_eta > 0 else
                np.inf
            )
            
            p_val = (
                2 * (1 - norm.cdf(abs(z_stat)))
            )

            multinomial_classes.append(int(model.y_classes[j+1]))
            prediction_probs.append(np.round(prediction_prob[j+1], 4))
            prediction_linear.append(np.round(prediction[j], 4))
            std_errors.append(np.round(se_eta, 4))
            z_statistics.append(np.round(z_stat, 4))
            p_values.append(f"{p_val:.3f}")
            ci_lows.append(np.round(p_low, 4))
            ci_highs.append(np.round(p_high, 4))

        return {
            "features": str(prediction_features),  
            "prediction_linear": [prediction_linear],
            "prediction_class": pred_class,
            "prediction_prob": [prediction_probs],
            "std_error": [std_errors],
            "z_statistic": [z_statistics],
            "P>|z|": [p_values],
            f"ci_low_{alpha}": [ci_lows],
            f"ci_high_{alpha}": [ci_highs],
        }

      
    



    '''
    Tabular Ordinal Logit Predictions
    '''
    if model.model_type == "logit_ordinal":

        prediction_features = {
                name: f'{value_at.item():.2f}'
                for name, value_at in zip(model.feature_names, X[0])
        }

        probs = predict_prob(
            X,
            model.coefficients,
            model.alpha_cutpoints,
            model.n_classes
        )

        results = []

        for i in range(X.shape[0]):

            p_i = probs[i]

            pred_class = (
                int(np.argmax(p_i))
            )

            expected = (
                float(np.dot(p_i, np.arange(model.n_classes)))
            )

            cumulative = np.cumsum(p_i)

            x_i = X[i]

            J = len(model.alpha_cutpoints)

            p = len(model.coefficients)

            def probability_gradient(x, beta, alpha, n_classes):

                grad = np.zeros((n_classes, p + J))
                F = np.zeros(J)
                dF_dbeta = np.zeros(J)
                dF_dalpha = np.zeros((J, J))

                for j in range(J):
                
                    eta = alpha[j] - x @ beta

                    F[j] = (
                        1 / (1 + np.exp(-np.clip(eta, -700, 700)))
                    )

                    f = F[j] * (1 - F[j]) # derivative of logistic

                    dF_dbeta[j] = -f  
                    dF_dalpha[j, j] = f 


                grad[0, :p] = dF_dbeta[0] * x
                grad[0, p] = dF_dalpha[0, 0]

                for j in range(1, J):
                
                    grad[j, :p] = (dF_dbeta[j] - dF_dbeta[j-1]) * x
                    grad[j, p+j-1] = -dF_dalpha[j-1, j-1]
                    grad[j, p+j] = dF_dalpha[j, j]

                grad[J, :p] = -dF_dbeta[J-1] * x
                grad[J, p+J-1] = -dF_dalpha[J-1, J-1]

                return grad
            

            grad_p = (
                probability_gradient(x_i, model.coefficients, model.alpha_cutpoints, model.n_classes)
            )

            var_p = (
                grad_p @ model.xtWx_inv @ grad_p.T
            )

            se_p = (
                np.sqrt(np.maximum(np.diag(var_p), 1e-20))
            )

            z_stats = (
                np.where(se_p > 0, p_i / se_p, np.inf)
            )

            p_values = (
                2 * (1 - norm.cdf(np.abs(z_stats)))
            )

            z_critical = (
                norm.ppf(1 - alpha/2)
            )

            ci_low, ci_high = (
                np.clip(p_i - z_critical * se_p, 0, 1),
                np.clip(p_i + z_critical * se_p, 0, 1)
            )

            results.append({
                "features": [prediction_features],
                "prediction_class": pred_class,
                #"prediction_ex": round(expected, 4),
                "cumulative_probabilities": np.round(cumulative, 4).tolist(),
                "prediction_prob": np.round(p_i, 4).tolist(),
                "std_error": np.round(se_p, 4).tolist(),
                "z_statistic": np.round(z_stats, 4).tolist(),
                "P>|z|": [f"{p:.3f}" for p in p_values],
                f"ci_low_{alpha}": np.round(ci_low, 4).tolist(),
                f"ci_high_{alpha}": np.round(ci_high, 4).tolist(),
            })

        return results
    
