import re
import pytest
from shekar.normalizer import Normalizer


@pytest.fixture
def normalizer():
    return Normalizer()


def test_normalize_informal_verbs(normalizer):
    assert normalizer("Ø¯Ø§Ø±Ù… Ø¯Ø±Ø³ Ù…ÛŒØ®ÙˆÙ†Ù…!") == "Ø¯Ø§Ø±Ù… Ø¯Ø±Ø³ Ù…ÛŒâ€ŒØ®ÙˆÙ†Ù…!"
    assert normalizer("Ù†Ù…ÛŒ ØªÙˆÙ†Ù… Ø¨Ù‡Øª Ø¨Ú¯Ù… Ú†ÛŒ Ù…ÛŒØ´Ù‡!") == "Ù†Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ø¨Ù‡Øª Ø¨Ú¯Ù… Ú†ÛŒ Ù…ÛŒâ€ŒØ´Ù‡!"
    assert (
        normalizer("Ù…ÛŒ Ø¯ÙˆÙ†ÛŒ Ú©Ù‡ Ù†Ù…ÛŒØ®Ø§Ø³ØªÙ… Ù†Ø§Ø±Ø§Ø­ØªØª Ú©Ù†Ù….")
        == "Ù…ÛŒâ€ŒØ¯ÙˆÙ†ÛŒ Ú©Ù‡ Ù†Ù…ÛŒâ€ŒØ®Ø§Ø³ØªÙ… Ù†Ø§Ø±Ø§Ø­ØªØª Ú©Ù†Ù…."
    )


def test_normalize_basic_spacing_and_quotes(normalizer):
    # Exercises punctuation spacing, ZWNJ for "Ù…ÛŒ + verb", and Persian quotes
    inp = "Ù†Ø§ØµØ± Ú¯ÙØª:Â«Ù…Ù† Ù…ÛŒØ±ÙˆÙ….Â»  \u200c ğŸ‰ she+kar@she-kar.io"
    out = normalizer.normalize(inp)
    # Email should be removed (mask="")
    assert "@" not in out
    # Emoji removed
    assert "ğŸ‰" not in out
    # Space after colon and before opening quote
    assert "Ú¯ÙØª:" in out and "Ú¯ÙØª: Â«" in out
    # ZWNJ in "Ù…ÛŒâ€ŒØ±ÙˆÙ…"
    assert "Ù…ÛŒâ€ŒØ±ÙˆÙ…" in out
    # Balanced Persian quotes around sentence
    assert "Â«" in out and "Â»" in out

    input_text = "Ø¨Ù†ÛŒØ§Ù†    Ú¯Ø°Ø§Ø± Ù‡Ø§ÛŒ Ø®Ø§Ù†Ù‡ Ù‡Ø§ÛŒÙ…Ø§Ù†"
    expected_output = "Ø¨Ù†ÛŒØ§Ù†â€ŒÚ¯Ø°Ø§Ø±Ù‡Ø§ÛŒ Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒÙ…Ø§Ù†"
    assert normalizer(input_text) == expected_output

    input_text = "Â«ÙØ§Ø±Ø³ÛŒ Ø´ÙÚ©ÙØ± Ø§Ø³ØªÂ» Ù†Ø§Ù… Ø¯Ø§Ø³ØªØ§Ù† ÚªÙˆØªØ§Ù‡ Ø·Ù†Ø²    Ø¢Ù…ÛØ²ÛŒ Ø§Ø² Ù…Ø­Ù…Ø¯ Ø¹Ù„ÛŒ Ø¬Ù…Ø§Ù„Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ù€Ø²Ø§Ø¯Ù‡  Ù…ÛŒ   Ø¨Ø§Ø´Ø¯ Ú©Ù‡ Ø¯Ø± Ø³Ø§Ù„ 1921 Ù…Ù†ØªØ´Ø±  Ø´Ø¯Ù‡ Ø§Ø³Øª Ùˆ Ø¢ØºØ§Ø²   Ú±Ø± ØªØ­ÙˆÙ„ Ø¨Ø²Ø±Ú¯ÛŒ Ø¯Ø± Ø§Ø¯ÙØ¨ÛØ§Øª Ù…Ø¹Ø§ØµØ± Ø§ÛŒØ±Ø§Ù† ğŸ‡®ğŸ‡· Ø¨Ûƒ Ø´Ù…Ø§Ø± Ù…ÛŒØ±ÙˆØ¯."
    expected_output = "Â«ÙØ§Ø±Ø³ÛŒ Ø´Ú©Ø± Ø§Ø³ØªÂ» Ù†Ø§Ù… Ø¯Ø§Ø³ØªØ§Ù† Ú©ÙˆØªØ§Ù‡ Ø·Ù†Ø²Ø¢Ù…ÛŒØ²ÛŒ Ø§Ø² Ù…Ø­Ù…Ø¯â€ŒØ¹Ù„ÛŒ Ø¬Ù…Ø§Ù„Ø²Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ Ú©Ù‡ Ø¯Ø± Ø³Ø§Ù„ Û±Û¹Û²Û± Ù…Ù†ØªØ´Ø± Ø´Ø¯Ù‡â€ŒØ§Ø³Øª Ùˆ Ø¢ØºØ§Ø²Ú¯Ø± ØªØ­ÙˆÙ„ Ø¨Ø²Ø±Ú¯ÛŒ Ø¯Ø± Ø§Ø¯Ø¨ÛŒØ§Øª Ù…Ø¹Ø§ØµØ± Ø§ÛŒØ±Ø§Ù† Ø¨Ù‡ Ø´Ù…Ø§Ø± Ù…ÛŒâ€ŒØ±ÙˆØ¯."
    assert normalizer(input_text) == expected_output


def test_email_and_url_masking(normalizer):
    inp = "ØªÙ…Ø§Ø³: user@example.com Ùˆ ÙˆØ¨Ú¯Ø§Ù‡: https://example.com/page"
    out = normalizer.normalize(inp)
    # Both should be masked to empty by default
    assert "@" not in out
    assert "http" not in out
    # No leftover double spaces from masking
    assert "  " not in out


def test_diacritic_and_digit_normalization(normalizer):
    inp = "ÙØ§Ø±Ø³ÛŒ Ø´ÙÚ©ÙØ± Ø§Ø³Øª 1234 Ùˆ Ù¡Ù¢Ù£"
    out = normalizer.normalize(inp)
    # Diacritics removed from "Ø´ÙÚ©ÙØ±"
    assert "Ø´ÙÚ©ÙØ±" not in out
    assert "Ø´Ú©Ø±" in out
    # Western and Arabic-Indic digits normalized to Persian
    assert "1234" not in out and "Ù¡Ù¢Ù£" not in out
    assert "Û±Û²Û³Û´" in out or "Û±Û²Û³" in out


def test_arabic_unicode_normalizer(normalizer):
    # Arabic Yeh/Kaf should be mapped to Persian forms
    inp = "ÙƒØªØ§Ø¨ Ùˆ Ù‡ÙˆÙŠØª Ø¨Ø§ Ùƒ ÙŠ"
    out = normalizer.normalize(inp)
    assert "Ùƒ" not in out and "ÙŠ" not in out
    # Expect Persian Kaf/Ye present
    assert "Ú©" in out or "Ú©ØªØ§Ø¨" in out
    assert "ÛŒ" in out or "Ù‡ÙˆÛŒØª" in out


def test_repeated_letter_filter(normalizer):
    # Collapses repeated letters like "Ø¹Ø§Ø§Ø§Ø§Ù„ÛŒ"
    inp = "Ø¹Ø§Ø§Ø§Ø§Ù„ÛŒ Ø¨ÙˆÙˆÙˆÙˆØ¯!!!"
    out = normalizer.normalize(inp)
    # No triple or more repeats remain
    assert re.search(r"(.)\1\1", out) is None


def test_html_tag_filter(normalizer):
    inp = "<p>Ø³Ù„Ø§Ù…</p> <a href='#'>Ø¯Ù†ÛŒØ§</a>"
    out = normalizer.normalize(inp)
    # Tags removed but content preserved
    assert "<" not in out and ">" not in out
    assert "Ø³Ù„Ø§Ù…" in out and "Ø¯Ù†ÛŒØ§" in out


def test_emoji_filter(normalizer):
    inp = "Ø³Ù„Ø§Ù… ğŸŒğŸ‡®ğŸ‡·ğŸ˜Š"
    out = normalizer.normalize(inp)
    for ch in "ğŸŒğŸ‡®ğŸ‡·ğŸ˜Š":
        assert ch not in out


def test_spacing_normalizer_variants(normalizer):
    # Common Persian spacing and ZWNJ cases
    cases = [
        ("Ù…ÛŒØ±ÙˆÙ…", "Ù…ÛŒâ€ŒØ±ÙˆÙ…"),
        ("Ù…ÛŒ Ø±ÙˆÙ…", "Ù…ÛŒâ€ŒØ±ÙˆÙ…"),
        ("Ù†Ù…ÛŒ Ø¯Ø§Ù†Ù…", "Ù†Ù…ÛŒâ€ŒØ¯Ø§Ù†Ù…"),
        ("Ú¯ÙØªÙ‡ Ø§Ø³Øª", "Ú¯ÙØªÙ‡â€ŒØ§Ø³Øª"),
    ]
    outs = [normalizer.normalize(s) for s, _ in cases]
    for (_, expected), out in zip(cases, outs):
        assert expected in out


def test_iterable_input_list(normalizer):
    texts = ["Ù…ÛŒ Ø±ÙˆÙ…", "Ø³Ù„Ø§Ù…ğŸ˜Š", "user@mail.com"]
    outs = normalizer.normalize(texts)
    # Pipeline may return a generator or list; convert to list
    outs = list(outs)
    assert len(outs) == 3
    assert "Ù…ÛŒâ€ŒØ±ÙˆÙ…" in outs[0]
    assert "ğŸ˜Š" not in outs[1]
    assert "@" not in outs[2]


def test_idempotence_on_normal_text(normalizer):
    text = "ÙØ§Ø±Ø³ÛŒ Ø´Ú©Ø± Ø§Ø³Øª."
    once = normalizer.normalize(text)
    twice = normalizer.normalize(once)
    assert once == twice


def test_empty_string(normalizer):
    assert normalizer.normalize("") == ""


def test_custom_steps_override_identity():
    # When steps=[], Normalizer should act like identity pass-through
    n = Normalizer(steps=[])
    text = "Ù…ØªÙ† ØªØ³ØªÛŒ Ø¨Ø§ Ø§ÛŒÙ…ÛŒÙ„ test@example.com Ùˆ ğŸ˜€"
    out = n.normalize(text)
    assert out == text  # no transformation


def test_normalize_method_alias(normalizer):
    # Ensure __call__ and normalize give the same result
    s = "Ù…ÛŒ Ø±ÙˆÙ…"
    assert normalizer.normalize(s) == normalizer(s)


def test_ya_normalizer_joda(normalizer):
    assert normalizer("Ø®Ø§Ù†Û€ Ù…Ø§") == "Ø®Ø§Ù†Ù‡â€ŒÛŒ Ù…Ø§"
    assert normalizer("Ø®Ø§Ù†Ù‡â€ŒÛŒ Ù…Ø§") == "Ø®Ø§Ù†Ù‡â€ŒÛŒ Ù…Ø§"
    assert normalizer("Ø®Ø§Ù†Ù‡ ÛŒ Ù…Ø§") == "Ø®Ø§Ù†Ù‡â€ŒÛŒ Ù…Ø§"
