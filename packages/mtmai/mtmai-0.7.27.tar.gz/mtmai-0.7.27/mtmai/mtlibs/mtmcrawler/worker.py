import asyncio
import json

from sqlalchemy.ext.asyncio import AsyncEngine

from mtmai.core.logging import get_logger
from mtmai.mtlibs.mq.pq_queue import AsyncPGMQueue
from mtmai.mtlibs.mtmcrawler.mtmcrawler import MTCrawlerPageParams, _fetch_page_html
from mtmlib.queue.queue import Message

logger = get_logger()

mtm_crawl_queue = "mtm-crawl-queue"


class CrawlWorker:
    def __init__(self, mq: AsyncPGMQueue, engine: AsyncEngine):
        self.mq = mq
        self.db = engine
        self.is_running = False

    async def start(self):
        # logger.info("ğŸ•·ï¸ ğŸŸ¢ Start MTM crawler worker ")
        self.is_running = True

        asyncio.create_task(self._pull_messages())

    async def stop(self):
        logger.info("ğŸ•·ï¸ ğŸ›‘ Stop MTM crawler worker ")
        self.is_running = False

    async def _pull_messages(self):
        logger.info(f"ğŸ•·ï¸ ğŸŸ¢ pull_messages from {mtm_crawl_queue}")
        await self.mq.create_queue(queue=mtm_crawl_queue)
        while self.is_running:
            try:
                msg = await self.mq.read(queue=mtm_crawl_queue)
                if msg:
                    await self._handle_message(msg)
                else:
                    await asyncio.sleep(1)
            except Exception as e:
                logger.error(f"ğŸ•·ï¸ ğŸ”´ pull_messages error: {e}")
                await asyncio.sleep(1)

    async def _handle_message(self, msg: Message):
        # logger.info(f"è¯»å–åˆ°æ¶ˆæ¯é˜Ÿåˆ—çš„æ¶ˆæ¯ : {msg.msg_id}")
        params = MTCrawlerPageParams(**json.loads(msg.message))
        # logger.info(f"å¤„ç†æ¶ˆæ¯é˜Ÿåˆ—çš„æ¶ˆæ¯ : {params.url}")
        await self.crawl_page(params)
        await self.mq.ack(mtm_crawl_queue, msg.msg_id)
        logger.debug(f"å¤„ç†æ¶ˆæ¯é˜Ÿå®Œæˆ{msg.msg_id}")

    async def enqueue_url(self, site_id: str, url: str):
        params = MTCrawlerPageParams(site_id=site_id, url=url, current_depth=0)
        message = json.dumps(params.model_dump())
        await self.mq.send(mtm_crawl_queue, message)
        logger.info(f"å‘é€æ¶ˆæ¯é˜Ÿåˆ—çš„æ¶ˆæ¯ : {params.url} å®Œæˆ")

    async def crawl_page(self, params: MTCrawlerPageParams):
        """
        çˆ¬å–ä¸€ä¸ªé¡µé¢å¹¶å»ºç«‹é¡µé¢ç´¢å¼•
        """
        html = await _fetch_page_html(params.url)
        if html is None:
            return None
        logger.info(f"crawl_page: {len(html)}, {params.url}")

        # page_item = MTCrawlPage(
        #     site_id=params.site_id,
        #     url=params.url,
        #     depth=params.current_depth,
        #     title="",
        #     description="",
        #     keywords="",
        #     author="fake_author",
        #     copyright="fake_copyright",
        # )
        # async with AsyncSession(self.db) as session:
        #     await session.add(page_item)
        #     await session.commit()
        # return update_item
        return None
