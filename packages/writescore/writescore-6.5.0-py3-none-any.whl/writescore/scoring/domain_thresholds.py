"""
Domain-specific thresholds for hierarchical structure analysis.

Based on deep research (Perplexity Deep Research 2025):
- Academic papers: Higher variance acceptable due to IMRaD flexibility
- Technical docs: Lower variance expected due to template constraints
- Business docs: Moderate variance reflecting template + content balance
- Tutorial content: Lower variance due to step-by-step uniformity

Research sources:
- Academic CV thresholds: arxiv.org/abs/2509.18880, pmc.ncbi.nlm.nih.gov/articles/PMC11231544
- Technical docs patterns: graphapp.ai/blog/technical-documentation-template
- Business docs: venngage.com/blog/white-paper-examples
- Multi-level weighting: aclanthology.org/2025.naacl-long.446.pdf
"""

from dataclasses import dataclass
from enum import Enum
from typing import Dict, Optional, Tuple


class DocumentDomain(Enum):
    """Document type classification for threshold application."""

    ACADEMIC = "academic"
    TECHNICAL = "technical"
    BUSINESS = "business"
    TUTORIAL = "tutorial"
    GENERAL = "general"  # Default fallback


@dataclass
class HierarchyThresholds:
    """Thresholds for a specific heading level."""

    # Coefficient of Variation (CV) thresholds
    cv_human_min: float  # Minimum CV typically seen in human writing
    cv_ai_max: float  # Maximum CV typically seen in AI writing
    cv_threshold: float  # Decision boundary (50% confidence point)

    # Scoring parameters
    max_score: float  # Maximum points for this level

    def __post_init__(self):
        """Validate threshold consistency."""
        if self.cv_threshold < self.cv_ai_max or self.cv_threshold > self.cv_human_min:
            # Threshold should be in the overlap region
            pass  # Allow flexibility for now


@dataclass
class DomainConfig:
    """Complete configuration for a document domain."""

    name: str
    description: str

    # Thresholds for each heading level
    h2_section_length: HierarchyThresholds
    h3_subsection_count: HierarchyThresholds
    h4_subsection_count: HierarchyThresholds

    # Multi-level weights (should sum to 1.0)
    weight_h2: float
    weight_h3: float
    weight_h4: float

    def __post_init__(self):
        """Validate weight sum."""
        total_weight = self.weight_h2 + self.weight_h3 + self.weight_h4
        if abs(total_weight - 1.0) > 0.001:
            raise ValueError(f"Weights must sum to 1.0, got {total_weight}")


# ============================================================================
# DOMAIN-SPECIFIC CONFIGURATIONS (Research-Backed)
# ============================================================================

ACADEMIC_CONFIG = DomainConfig(
    name="academic",
    description="Academic papers (IMRaD format, research articles)",
    # H2 Section Length CV (Introduction, Methods, Results, Discussion)
    # Research: Human 0.35-0.55, AI 0.12-0.28, Threshold ~0.30-0.35
    h2_section_length=HierarchyThresholds(
        cv_human_min=0.35, cv_ai_max=0.28, cv_threshold=0.32, max_score=10.0
    ),
    # H3 Subsection Count CV (subsections per major section)
    # Research: Human 0.50-0.75, AI 0.15-0.35, Threshold ~0.40
    h3_subsection_count=HierarchyThresholds(
        cv_human_min=0.50, cv_ai_max=0.35, cv_threshold=0.42, max_score=8.0
    ),
    # H4 Subsection Count CV (deeper nesting)
    # Research: Human 0.45+, AI 0.15-0.30, Threshold ~0.35
    h4_subsection_count=HierarchyThresholds(
        cv_human_min=0.45, cv_ai_max=0.30, cv_threshold=0.37, max_score=6.0
    ),
    # Multi-level weights (research-backed: emphasize H2 organizational decisions)
    weight_h2=0.50,
    weight_h3=0.35,
    weight_h4=0.15,
)

TECHNICAL_CONFIG = DomainConfig(
    name="technical",
    description="Technical documentation (API docs, manuals, guides)",
    # H2 Section Length CV (standardized section templates)
    # Research: Human 0.08-0.18, AI 0.04-0.12, Threshold ~0.12-0.15
    h2_section_length=HierarchyThresholds(
        cv_human_min=0.08, cv_ai_max=0.12, cv_threshold=0.13, max_score=10.0
    ),
    # H3 Subsection Count CV (more variation at H3 level)
    # Research: Human 0.15-0.30, AI 0.05-0.15, Threshold ~0.18
    h3_subsection_count=HierarchyThresholds(
        cv_human_min=0.15, cv_ai_max=0.15, cv_threshold=0.18, max_score=8.0
    ),
    # H4 Subsection Count CV
    h4_subsection_count=HierarchyThresholds(
        cv_human_min=0.20, cv_ai_max=0.12, cv_threshold=0.16, max_score=6.0
    ),
    # Multi-level weights (emphasize H3 where variation emerges)
    weight_h2=0.30,
    weight_h3=0.50,
    weight_h4=0.20,
)

BUSINESS_CONFIG = DomainConfig(
    name="business",
    description="Business documents (reports, proposals, white papers)",
    # H2 Section Length CV (moderate standardization)
    # Research: Human 0.25-0.45, AI 0.10-0.22, Threshold ~0.25-0.30
    h2_section_length=HierarchyThresholds(
        cv_human_min=0.25, cv_ai_max=0.22, cv_threshold=0.27, max_score=10.0
    ),
    # H3 Subsection Count CV
    # Research: Human 0.40-0.65, AI 0.15-0.30, Threshold ~0.32
    h3_subsection_count=HierarchyThresholds(
        cv_human_min=0.40, cv_ai_max=0.30, cv_threshold=0.35, max_score=8.0
    ),
    # H4 Subsection Count CV
    h4_subsection_count=HierarchyThresholds(
        cv_human_min=0.35, cv_ai_max=0.20, cv_threshold=0.28, max_score=6.0
    ),
    # Multi-level weights (balanced across levels)
    weight_h2=0.40,
    weight_h3=0.40,
    weight_h4=0.20,
)

TUTORIAL_CONFIG = DomainConfig(
    name="tutorial",
    description="Tutorial and educational content (how-to guides, courses)",
    # H2 Section Length CV (step-based structure)
    # Research: Human 0.15-0.35, AI 0.08-0.18, Threshold ~0.20
    h2_section_length=HierarchyThresholds(
        cv_human_min=0.15, cv_ai_max=0.18, cv_threshold=0.20, max_score=10.0
    ),
    # H3 Subsection Count CV (consistent subsection structure)
    # Research: Human 0.10-0.25, AI 0.05-0.15, Threshold ~0.15
    h3_subsection_count=HierarchyThresholds(
        cv_human_min=0.10, cv_ai_max=0.15, cv_threshold=0.15, max_score=8.0
    ),
    # H4 Subsection Count CV
    h4_subsection_count=HierarchyThresholds(
        cv_human_min=0.15, cv_ai_max=0.10, cv_threshold=0.12, max_score=6.0
    ),
    # Multi-level weights (balanced, structural signal weaker in tutorials)
    weight_h2=0.40,
    weight_h3=0.40,
    weight_h4=0.20,
)

GENERAL_CONFIG = DomainConfig(
    name="general",
    description="General content (fallback when domain unknown)",
    # Use moderate thresholds between academic and business
    h2_section_length=HierarchyThresholds(
        cv_human_min=0.30, cv_ai_max=0.25, cv_threshold=0.30, max_score=10.0
    ),
    h3_subsection_count=HierarchyThresholds(
        cv_human_min=0.45, cv_ai_max=0.30, cv_threshold=0.38, max_score=8.0
    ),
    h4_subsection_count=HierarchyThresholds(
        cv_human_min=0.40, cv_ai_max=0.25, cv_threshold=0.32, max_score=6.0
    ),
    weight_h2=0.45,
    weight_h3=0.35,
    weight_h4=0.20,
)

# Domain registry
DOMAIN_CONFIGS: Dict[DocumentDomain, DomainConfig] = {
    DocumentDomain.ACADEMIC: ACADEMIC_CONFIG,
    DocumentDomain.TECHNICAL: TECHNICAL_CONFIG,
    DocumentDomain.BUSINESS: BUSINESS_CONFIG,
    DocumentDomain.TUTORIAL: TUTORIAL_CONFIG,
    DocumentDomain.GENERAL: GENERAL_CONFIG,
}


def get_domain_config(domain: DocumentDomain) -> DomainConfig:
    """Get configuration for a specific domain."""
    return DOMAIN_CONFIGS.get(domain, GENERAL_CONFIG)


def calculate_cv_score(cv: float, thresholds: HierarchyThresholds) -> Tuple[float, str]:
    """
    Calculate score and assessment for a CV value using sigmoid function.

    Uses logistic function: P(Human|CV) = 1 / (1 + e^(-Î²(CV - threshold)))

    Args:
        cv: Coefficient of variation value
        thresholds: Domain-specific thresholds

    Returns:
        (score, assessment) where score is 0-max_score and assessment is label
    """
    import math

    # Sigmoid steepness parameter (higher = sharper transition)
    # Use domain-specific steepness based on overlap between human/AI distributions
    overlap = thresholds.cv_human_min - thresholds.cv_ai_max
    if overlap > 0.15:
        beta = 20.0  # Sharp transition (clear separation)
    elif overlap > 0.08:
        beta = 12.0  # Moderate transition
    else:
        beta = 6.0  # Gradual transition (substantial overlap)

    # Calculate probability of human authorship
    exponent = -beta * (cv - thresholds.cv_threshold)
    prob_human = 1.0 / (1.0 + math.exp(exponent))

    # Map to score (0 to max_score)
    score = prob_human * thresholds.max_score

    # Assessment labels
    if prob_human >= 0.85:
        assessment = "EXCELLENT"
    elif prob_human >= 0.65:
        assessment = "GOOD"
    elif prob_human >= 0.40:
        assessment = "FAIR"
    elif prob_human >= 0.20:
        assessment = "POOR"
    else:
        assessment = "VERY_POOR"

    return (round(score, 1), assessment)


def calculate_combined_structure_score(
    section_length_cv: Optional[float] = None,
    h3_subsection_cv: Optional[float] = None,
    h4_subsection_cv: Optional[float] = None,
    domain: DocumentDomain = DocumentDomain.GENERAL,
) -> Dict:
    """
    Calculate multi-level combined structural score using domain-specific thresholds.

    This function implements hierarchical structural analysis across three levels:
    - H2 Section Length CV: Variance in word counts across major sections
    - H3 Subsection CV: Variance in H3 counts under each H2
    - H4 Subsection CV: Variance in H4 counts under each H3

    Research basis: Multi-level weighting provides 15.5% improvement in detection
    accuracy over single-level analysis (Deep Research 2025).

    Args:
        section_length_cv: Coefficient of variation for H2 section lengths
        h3_subsection_cv: Coefficient of variation for H3 counts under H2s
        h4_subsection_cv: Coefficient of variation for H4 counts under H3s
        domain: Document domain (academic, technical, business, tutorial, general)

    Returns:
        {
            'combined_score': float (0-24 max),
            'combined_assessment': str (EXCELLENT/GOOD/FAIR/POOR/VERY_POOR),
            'domain': str,
            'breakdown': {
                'h2_score': float,
                'h2_assessment': str,
                'h2_weight': float,
                'h3_score': float,
                'h3_assessment': str,
                'h3_weight': float,
                'h4_score': float,
                'h4_assessment': str,
                'h4_weight': float
            },
            'prob_human': float (0-1)
        }
    """
    config = get_domain_config(domain)

    # Calculate individual scores for each level
    # Use neutral score (50% of max) for insufficient data (None values)
    if section_length_cv is not None:
        h2_score, h2_assessment = calculate_cv_score(section_length_cv, config.h2_section_length)
    else:
        h2_score = config.h2_section_length.max_score * 0.5
        h2_assessment = "INSUFFICIENT_DATA"

    if h3_subsection_cv is not None:
        h3_score, h3_assessment = calculate_cv_score(h3_subsection_cv, config.h3_subsection_count)
    else:
        h3_score = config.h3_subsection_count.max_score * 0.5
        h3_assessment = "INSUFFICIENT_DATA"

    if h4_subsection_cv is not None:
        h4_score, h4_assessment = calculate_cv_score(h4_subsection_cv, config.h4_subsection_count)
    else:
        h4_score = config.h4_subsection_count.max_score * 0.5
        h4_assessment = "INSUFFICIENT_DATA"

    # Apply domain-specific weights
    weighted_h2 = h2_score * config.weight_h2
    weighted_h3 = h3_score * config.weight_h3
    weighted_h4 = h4_score * config.weight_h4

    # Combined score (max 24 points: 10 + 8 + 6)
    combined_score = weighted_h2 + weighted_h3 + weighted_h4

    # Calculate overall probability of human authorship
    max_possible = (
        config.h2_section_length.max_score * config.weight_h2
        + config.h3_subsection_count.max_score * config.weight_h3
        + config.h4_subsection_count.max_score * config.weight_h4
    )

    prob_human = combined_score / max_possible if max_possible > 0 else 0.0

    # Combined assessment
    if prob_human >= 0.85:
        combined_assessment = "EXCELLENT"
    elif prob_human >= 0.65:
        combined_assessment = "GOOD"
    elif prob_human >= 0.40:
        combined_assessment = "FAIR"
    elif prob_human >= 0.20:
        combined_assessment = "POOR"
    else:
        combined_assessment = "VERY_POOR"

    return {
        "combined_score": round(combined_score, 1),
        "combined_assessment": combined_assessment,
        "domain": domain.value,
        "breakdown": {
            "h2_score": h2_score,
            "h2_assessment": h2_assessment,
            "h2_weight": config.weight_h2,
            "h3_score": h3_score,
            "h3_assessment": h3_assessment,
            "h3_weight": config.weight_h3,
            "h4_score": h4_score,
            "h4_assessment": h4_assessment,
            "h4_weight": config.weight_h4,
        },
        "prob_human": round(prob_human, 3),
    }
