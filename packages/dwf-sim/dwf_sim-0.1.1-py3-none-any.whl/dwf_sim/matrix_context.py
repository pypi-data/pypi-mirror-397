"""Matrix context generation and management."""

import csv
import hashlib
import json
import logging
import re
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union, Any

logger = logging.getLogger(__name__)

class MatrixContext:
    """
    A helper class for accessing matrix context data generated by build_matrix.py.
    Provides convenience functions to navigate DoBSeq matrices safely.
    """
    
    def __init__(self, ctx: dict):
        self._ctx = ctx
        
        # Sort pools by index for consistent ordering
        self._rows = sorted(ctx["pools"]["rows"], key=lambda r: r["index"])
        self._cols = sorted(ctx["pools"]["columns"], key=lambda c: c["index"])
        
        # Pool ID lists (ordered)
        self.row_pools: List[str] = [r["pool_id"] for r in self._rows]
        self.col_pools: List[str] = [c["pool_id"] for c in self._cols]
        self.all_pools: List[str] = self.row_pools + self.col_pools
        
        # Index lookups
        self.row_index_by_pool: Dict[str, int] = {r["pool_id"]: r["index"] for r in self._rows}
        self.col_index_by_pool: Dict[str, int] = {c["pool_id"]: c["index"] for c in self._cols}
        
        # Label lookups
        self.row_label_by_pool: Dict[str, str] = {r["pool_id"]: r["label"] for r in self._rows}
        self.col_label_by_pool: Dict[str, str] = {c["pool_id"]: c["label"] for c in self._cols}
        
        # Combined dimension lookup
        self._dim_by_pool: Dict[str, str] = {
            **{p: "row" for p in self.row_pools},
            **{p: "column" for p in self.col_pools}
        }
        
        # Label lists (ordered)
        self.row_labels: List[str] = [r["label"] for r in self._rows]
        self.col_labels: List[str] = [c["label"] for c in self._cols]
        
        # Cell lookups - single source of truth
        cells = ctx["matrix"]["cells"]
        self.cells: Dict[Tuple[str, str], Dict[str, Any]] = {
            (c["row_pool_id"], c["col_pool_id"]): c for c in cells
        }
        
        # Alternative access methods
        self.by_alias: Dict[str, Dict[str, Any]] = {
            c["alias"]: c for c in cells
        }
        self.by_sample: Dict[str, Dict[str, Any]] = {
            c["sample_id"]: c for c in cells if c.get("sample_id")
        }
    
    @classmethod
    def load(cls, path: Union[Path, str]) -> "MatrixContext":
        """Load matrix context from JSON file."""
        with open(path, "r") as fh:
            data = json.load(fh)
        return cls(data)
    
    @property
    def shape(self) -> Tuple[int, int]:
        """Return matrix shape as (n_rows, n_cols)."""
        return (self._ctx["matrix"]["n_rows"], self._ctx["matrix"]["n_cols"])
    
    @property
    def n_rows(self) -> int:
        """Number of rows in the matrix."""
        return self._ctx["matrix"]["n_rows"]
    
    @property
    def n_cols(self) -> int:
        """Number of columns in the matrix."""
        return self._ctx["matrix"]["n_cols"]
    
    @property
    def layout_hash(self) -> str:
        """Get the layout hash for this matrix configuration."""
        return self._ctx["layout_hash"]
    
    def pool_dim(self, pool_id: str) -> str:
        """Get dimension ('row' or 'column') for a pool ID."""
        if pool_id not in self._dim_by_pool:
            raise ValueError(f"Unknown pool_id: {pool_id}")
        return self._dim_by_pool[pool_id]
    
    def pool_index(self, pool_id: str) -> int:
        """Get the 1-based index for a pool ID."""
        if pool_id in self.row_index_by_pool:
            return self.row_index_by_pool[pool_id]
        elif pool_id in self.col_index_by_pool:
            return self.col_index_by_pool[pool_id]
        else:
            raise ValueError(f"Unknown pool_id: {pool_id}")
    
    def pool_label(self, pool_id: str) -> str:
        """Get the formatted label for a pool ID."""
        if pool_id in self.row_label_by_pool:
            return self.row_label_by_pool[pool_id]
        elif pool_id in self.col_label_by_pool:
            return self.col_label_by_pool[pool_id]
        else:
            raise ValueError(f"Unknown pool_id: {pool_id}")
    
    def cell(self, row_pool: str, col_pool: str) -> Optional[Dict[str, Any]]:
        """Get cell data for the intersection of row and column pools."""
        return self.cells.get((row_pool, col_pool))
    
    def cell_from_alias(self, alias: str) -> Optional[Dict[str, Any]]:
        """Get cell data from cell alias (e.g., 'A01')."""
        return self.by_alias.get(alias)
    
    def cell_from_sample(self, sample_id: str) -> Optional[Dict[str, Any]]:
        """Get cell data from sample ID."""
        return self.by_sample.get(sample_id)
    
    def cell_label(self, row_pool_id: str, col_pool_id: str) -> Optional[str]:
        """Get the cell label (e.g., 'A01') for pool intersection."""
        cell = self.cell(row_pool_id, col_pool_id)
        return cell["cell_label"] if cell else None
    
    def get_samples(self) -> List[str]:
        """Get all sample IDs in the matrix."""
        return list(self.by_alias.keys())
    
    def __repr__(self) -> str:
        return (f"MatrixContext({self.n_rows}Ã—{self.n_cols}, "
                f"{len(self.get_samples())} samples)")

def read_pools(pools_path: Path) -> Tuple[List[str], List[str]]:
    rows, cols = [], []
    with pools_path.open() as fh:
        reader = csv.reader(fh, delimiter="\t")
        first = next(reader, None)
        if first is None:
            raise ValueError("Pools TSV is empty.")
        records = [first]
        for r in reader:
            records.append(r)
        for rec in records:
            if len(rec) < 2:
                raise ValueError(f"Bad pools row (need 2 columns): {rec}")
            pool_id, dim = rec[0].strip(), rec[1].strip().lower()
            if dim not in {"row", "column"}:
                raise ValueError(f"Invalid dimension '{dim}' for pool '{pool_id}'")
            (rows if dim == "row" else cols).append(pool_id)
    # Check uniqueness
    all_ids = rows + cols
    if len(all_ids) != len(set(all_ids)):
        dup = [p for p in set(all_ids) if all_ids.count(p) > 1]
        raise ValueError(f"Duplicate pool_id(s) in pools TSV: {dup}")
    if not rows or not cols:
        raise ValueError("Need at least one row pool and one column pool.")
    return rows, cols

def read_decode(decode_path: Optional[Path]) -> Dict[Tuple[str,str], str]:
    mapping = {}
    if not decode_path: 
        return mapping
    with decode_path.open() as fh:
        reader = csv.reader(fh, delimiter="\t")

        first = next(reader, None)
        if first is None:
            return mapping

        records = [first]
        for r in reader:
            records.append(r)
        for rec in records:
            if len(rec) < 3:
                raise ValueError(f"Bad decode row (need 3 columns): {rec}")
            sample_id, rpid, cpid = rec[0].strip(), rec[1].strip(), rec[2].strip()
            key = (rpid, cpid)
            if key in mapping and mapping[key] != sample_id:
                raise ValueError(f"Decode conflict for {key}: '{mapping[key]}' vs '{sample_id}'")
            mapping[key] = sample_id
    return mapping

def column_label_from_index(idx: int) -> str:
    """Create excel style column label 1->A, 26->Z, 27->AA"""
    label = []
    n = idx
    while n > 0:
        n -= 1
        label.append(chr(65 + (n % 26)))
        n //= 26
    return "".join(reversed(label))

def layout_hash(rows_ordered: List[str], cols_ordered: List[str]) -> str:
    m = hashlib.sha256()
    m.update(b"rows\0")
    [m.update((r+"\0").encode()) for r in rows_ordered]
    m.update(b"cols\0")
    [m.update((c+"\0").encode()) for c in cols_ordered]
    return m.hexdigest()[:16]

def column_index_from_label(label: str) -> int:
    """
    Convert Excel-style column label back to index: A->1, Z->26, AA->27, etc.
    Reverse of column_label_from_index.
    """
    index = 0
    for char in label:
        index = index * 26 + (ord(char) - ord('A') + 1)
    return index

def natural_sort(pool_id: str):
    parts = pool_id.rsplit('_', 1)
    prefix, suffix = parts

    if suffix.isdigit():
        return (prefix.lower(), int(suffix))
    else:
        return (prefix.lower(), column_index_from_label(suffix))
            

def build_context(rows: List[str],
                  cols: List[str],
                  decode: Dict[Tuple[str,str], str],
                  pad_width: Optional[int],
                  prefix: bool=False) -> dict:
    # Force deterministic ordering
    if prefix:
        rows_ord = sorted(rows, key=natural_sort)
        cols_ord = sorted(cols, key=natural_sort)
    else:
        rows_ord = sorted(rows, key=str.lower)
        cols_ord = sorted(cols, key=str.lower)

    # Indices and labels
    row_index = {pid: i for i, pid in enumerate(rows_ord)}
    col_index = {pid: i for i, pid in enumerate(cols_ord)}
    col_label = {pid: column_label_from_index(col_index[pid] +1 ) for pid in cols_ord}
    n_rows = len(rows_ord)
    width = pad_width if pad_width is not None and pad_width >= 0 else len(str(n_rows))
    row_label = {pid: str(row_index[pid] + 1).zfill(width) for pid in rows_ord}

    # Pools section
    pools_rows = [{"pool_id": pid, "index": row_index[pid], "label": row_label[pid]} for pid in rows_ord]
    pools_cols = [{"pool_id": pid, "index": col_index[pid], "label": col_label[pid]} for pid in cols_ord]

    # Cells (Cartesian product, ie. all combinations)
    cells = []
    for rpid in rows_ord:
        for cpid in cols_ord:
            ri, ci = row_index[rpid], col_index[cpid]
            rl, cl = row_label[rpid], col_label[cpid]
            cell = {
                "row_pool_id": rpid,
                "col_pool_id": cpid,
                "row_index": ri,
                "col_index": ci,
                "row_label": rl,
                "col_label": cl,
                "cell_label": f"{cl}{rl}",
                "alias": f"{cl}{rl}",
                "sample_id": decode.get((rpid, cpid), None),
            }
            cells.append(cell)

    ctx = {
        "schema_version": "1.0",
        "layout_hash": layout_hash(rows_ord, cols_ord),
        "pools": {
            "rows": pools_rows,
            "columns": pools_cols,
        },
        "matrix": {
            "n_rows": len(rows_ord),
            "n_cols": len(cols_ord),
            "row_label_width": width,
            "cells": cells,
        },
    }
    return ctx

def write_json(ctx: dict, path: Path):
    with path.open("w") as fh:
        json.dump(ctx, fh, indent=2, sort_keys=False)

def write_tsv(ctx: dict, path: Path):
    fields = [
        "row_pool_id","col_pool_id",
        "row_index","col_index",
        "row_label","col_label",
        "cell_label","alias","sample_id"
    ]
    with path.open("w", newline="") as fh:
        w = csv.writer(fh, delimiter="\t")
        w.writerow(fields)
        for cell in ctx["matrix"]["cells"]:
            w.writerow([
                cell["row_pool_id"], cell["col_pool_id"],
                cell["row_index"], cell["col_index"],
                cell["row_label"], cell["col_label"],
                cell["cell_label"], cell["alias"], cell.get("sample_id")
            ])

def get_context(sampletable:Path, output_dir:Path, decodetable:Path = "", pad_width:int = "auto"):
    rows, cols = read_pools(sampletable)
    decode = read_decode(decodetable) if decodetable else {}
    pad_width = None if str(pad_width).lower() == "auto" else int(pad_width)

    for (rpid, cpid) in decode.keys():
        if rpid not in rows:
            raise ValueError(f"Decode references unknown row_pool_id '{rpid}'")
        if cpid not in cols:
            raise ValueError(f"Decode references unknown col_pool_id '{cpid}'")

    ctx = build_context(rows, cols, decode, pad_width)
    write_json(ctx, output_dir / "matrix_context.json")
    write_tsv(ctx, output_dir / "matrix_context.tsv")
    mc = MatrixContext(ctx)
    return mc

def generate_pools(matrix_size: int, pool_prefix: str = "P") -> Tuple[List[str], List[str]]:
    """
    Generate pool IDs for a square matrix.
    
    Args:
        matrix_size: Dimension of square matrix (N x N)
        pool_prefix: Prefix for pool IDs (default: "P")
    
    Returns:
        Tuple of (row_pools, col_pools)
    """
    rows = [f"{pool_prefix}_{i+1}" for i in range(matrix_size)]
    cols = [f"{pool_prefix}_{column_label_from_index(i+1)}" for i in range(matrix_size)]
    return rows, cols


def write_pooltable(ctx: dict, output_dir: Path, output_path: Path) -> None:
    """
    Write pooltable.tsv with pool_id, dimension, and predefined fastq paths.
    
    Args:
        ctx: Matrix context dictionary
        output_dir: Base output directory (for constructing pool paths)
        output_path: Path to output pooltable.tsv
    """
    logger.info(f"Writing pooltable: {output_path}")
    
    pools_dir = output_dir / "pools"
    
    with output_path.open("w", newline="") as fh:
        writer = csv.writer(fh, delimiter="\t")
        
        # Write row pools
        for row_pool in ctx["pools"]["rows"]:
            pool_id = row_pool["pool_id"]
            writer.writerow([
                pool_id,
                "row",
                str(pools_dir / f"{pool_id}_1.fq.gz"),
                str(pools_dir / f"{pool_id}_2.fq.gz"),
            ])
        
        # Write column pools
        for col_pool in ctx["pools"]["columns"]:
            pool_id = col_pool["pool_id"]
            writer.writerow([
                pool_id,
                "column",
                str(pools_dir / f"{pool_id}_1.fq.gz"),
                str(pools_dir / f"{pool_id}_2.fq.gz"),
            ])

def write_decodetable(ctx: dict, output_path: Path) -> None:
    """Write decodetable.tsv with sample_id, row_pool_id, col_pool_id."""
    logger.info(f"Writing decodetable: {output_path}")
    with output_path.open("w", newline="") as fh:
        writer = csv.writer(fh, delimiter="\t")
        for cell in ctx["matrix"]["cells"]:
            writer.writerow([
                cell["sample_id"],
                cell["row_pool_id"],
                cell["col_pool_id"],
            ])

def create_matrix_context(
    matrix_size: int,
    output_dir: Path,
    pool_prefix: str = "P",
    pad_width: Optional[int] = None,
) -> dict:
    """
    Generate matrix context from scratch and write all output files.
    
    Args:
        matrix_size: Dimension of square matrix (N x N)
        output_dir: Directory for output files
        pool_prefix: Not used, kept for compatibility
        pad_width: Zero-padding width for row labels (auto if None)
    
    Returns:
        Matrix context dictionary
    
    Output files:
        - pooltable.tsv (with predefined pool paths)
        - decodetable.tsv
        - matrix_context.json
        - matrix_context.tsv
    """
    logger.info(f"Generating {matrix_size}x{matrix_size} matrix context")
    
    # Generate pool IDs (numeric for rows, letters for columns)
    rows, cols = generate_pools(matrix_size, pool_prefix)
    
    # Create decode mapping (use cell aliases as sample IDs)
    decode = {}
    
    # Build context using existing function
    ctx = build_context(rows, cols, decode, pad_width, prefix=True)
    
    # Update sample_id in cells to use alias
    for cell in ctx["matrix"]["cells"]:
        cell["sample_id"] = cell["alias"]
    
    # Write all output files
    output_dir.mkdir(parents=True, exist_ok=True)
    write_pooltable(ctx, output_dir, output_dir / "pooltable.tsv")
    write_decodetable(ctx, output_dir / "decodetable.tsv")
    write_json(ctx, output_dir / "matrix_context.json")
    write_tsv(ctx, output_dir / "matrix_context.tsv")
    
    logger.info(f"Matrix context created with {len(ctx['matrix']['cells'])} individuals")
    
    return ctx