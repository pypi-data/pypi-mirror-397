from typing import Dict, Any, List
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image
import numpy as np

class Qwen3VLSemanticValidator:
    """Qwen3-VLè¯­ä¹‰æ ¡éªŒæ¨¡å—"""
    
    def __init__(self, model_name: str = "Qwen/Qwen2-VL-7B-Instruct"):
        """
        åˆå§‹åŒ–Qwen3-VLè¯­ä¹‰æ ¡éªŒæ¨¡å—
        
        Args:
            model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
        """
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # åŠ è½½æ¨¡å‹
        self.load_model()
    
    def load_model(self):
        """åŠ è½½Qwen3-VLæ¨¡å‹"""
        try:
            print(f"ğŸš€ åŠ è½½Qwen3-VLæ¨¡å‹: {self.model_name}")
            print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {self.device}")
            
            # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.bfloat16,
                device_map="auto"
            )
            
            print("âœ… Qwen3-VLæ¨¡å‹åŠ è½½æˆåŠŸ!")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½Qwen3-VLæ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def validate_sign_sentence(self, sentence: str, image: np.ndarray = None) -> Dict[str, Any]:
        """
        è¯­ä¹‰æ ¡éªŒæ‰‹è¯­å¥å­
        
        Args:
            sentence: ç”Ÿæˆçš„æ‰‹è¯­å¥å­
            image: å¯é€‰çš„è¾“å…¥å›¾åƒ
            
        Returns:
            dict: è¯­ä¹‰æ ¡éªŒç»“æœ
        """
        try:
            if not self.model or not self.tokenizer:
                return {
                    "success": False,
                    "message": "Qwen3-VLæ¨¡å‹æœªåŠ è½½",
                    "is_valid": False,
                    "confidence": 0.0,
                    "suggestion": ""
                }
            
            # æ„å»ºæç¤ºè¯
            prompt = f"è¯·æ£€æŸ¥ä»¥ä¸‹æ‰‹è¯­è¯†åˆ«ç»“æœçš„è¯­ä¹‰æ˜¯å¦é€šé¡ºã€åˆç†:\n\n{sentence}\n\nè¯·å›ç­”ï¼š\n1. æ˜¯å¦è¯­ä¹‰é€šé¡ºï¼Ÿï¼ˆæ˜¯/å¦ï¼‰\n2. ç½®ä¿¡åº¦è¯„åˆ†ï¼ˆ0-100ï¼‰\n3. å¦‚æœæœ‰é—®é¢˜ï¼Œè¯·ç»™å‡ºä¿®æ­£å»ºè®®"
            
            messages = [
                {
                    "role": "user",
                    "content": prompt
                }
            ]
            
            # æ¨¡å‹æ¨ç†
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.device)
            
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=512
            )
            generated_ids = [
                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            
            response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            # è§£æå“åº”
            result = self._parse_response(response)
            
            return {
                "success": True,
                "message": "è¯­ä¹‰æ ¡éªŒæˆåŠŸ",
                "is_valid": result["is_valid"],
                "confidence": result["confidence"],
                "suggestion": result["suggestion"],
                "raw_response": response
            }
            
        except Exception as e:
            print(f"âŒ è¯­ä¹‰æ ¡éªŒå¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"è¯­ä¹‰æ ¡éªŒå¤±è´¥: {str(e)}",
                "is_valid": False,
                "confidence": 0.0,
                "suggestion": ""
            }
    
    def _parse_response(self, response: str) -> Dict[str, Any]:
        """
        è§£ææ¨¡å‹å“åº”
        
        Args:
            response: æ¨¡å‹å“åº”æ–‡æœ¬
            
        Returns:
            dict: è§£æç»“æœ
        """
        # é»˜è®¤ç»“æœ
        result = {
            "is_valid": False,
            "confidence": 50,
            "suggestion": ""
        }
        
        try:
            # è§£ææ˜¯å¦è¯­ä¹‰é€šé¡º
            if "æ˜¯" in response.split("\n")[0]:
                result["is_valid"] = True
            
            # è§£æç½®ä¿¡åº¦è¯„åˆ†
            for line in response.split("\n"):
                if "ç½®ä¿¡åº¦" in line or "è¯„åˆ†" in line:
                    # æå–æ•°å­—
                    confidence = int(''.join(filter(str.isdigit, line)))
                    result["confidence"] = min(max(confidence, 0), 100) / 100.0
                    break
            
            # è§£æä¿®æ­£å»ºè®®
            if "å»ºè®®" in response:
                suggestion_start = response.find("å»ºè®®")
                if suggestion_start != -1:
                    result["suggestion"] = response[suggestion_start:].strip()
            
        except Exception as e:
            print(f"âŒ è§£æå“åº”å¤±è´¥: {e}")
        
        return result
    
    def validate_with_image(self, sentence: str, image: np.ndarray) -> Dict[str, Any]:
        """
        ç»“åˆå›¾åƒè¿›è¡Œè¯­ä¹‰æ ¡éªŒ
        
        Args:
            sentence: ç”Ÿæˆçš„æ‰‹è¯­å¥å­
            image: è¾“å…¥å›¾åƒ
            
        Returns:
            dict: è¯­ä¹‰æ ¡éªŒç»“æœ
        """
        try:
            if not self.model or not self.tokenizer:
                return {
                    "success": False,
                    "message": "Qwen3-VLæ¨¡å‹æœªåŠ è½½",
                    "is_valid": False,
                    "confidence": 0.0,
                    "suggestion": ""
                }
            
            # å°†numpyæ•°ç»„è½¬æ¢ä¸ºPILå›¾åƒ
            pil_image = Image.fromarray(image)
            
            # æ„å»ºæç¤ºè¯
            prompt = f"è¯·è§‚å¯Ÿä»¥ä¸‹å›¾åƒï¼Œå¹¶æ£€æŸ¥å›¾åƒå†…å®¹ä¸æ‰‹è¯­è¯†åˆ«ç»“æœæ˜¯å¦åŒ¹é…ï¼š\n\næ‰‹è¯­è¯†åˆ«ç»“æœï¼š{sentence}\n\nè¯·å›ç­”ï¼š\n1. å›¾åƒå†…å®¹ä¸è¯†åˆ«ç»“æœæ˜¯å¦åŒ¹é…ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰\n2. åŒ¹é…åº¦è¯„åˆ†ï¼ˆ0-100ï¼‰\n3. å¦‚æœæœ‰é—®é¢˜ï¼Œè¯·ç»™å‡ºä¿®æ­£å»ºè®®"
            
            # æ¨¡å‹æ¨ç†
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image",
                            "image": pil_image
                        },
                        {
                            "type": "text",
                            "text": prompt
                        }
                    ]
                }
            ]
            
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.device)
            
            generated_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=512
            )
            generated_ids = [
                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            
            response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            # è§£æå“åº”
            result = self._parse_response(response)
            
            return {
                "success": True,
                "message": "å›¾åƒè¯­ä¹‰æ ¡éªŒæˆåŠŸ",
                "is_valid": result["is_valid"],
                "confidence": result["confidence"],
                "suggestion": result["suggestion"],
                "raw_response": response
            }
            
        except Exception as e:
            print(f"âŒ å›¾åƒè¯­ä¹‰æ ¡éªŒå¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"å›¾åƒè¯­ä¹‰æ ¡éªŒå¤±è´¥: {str(e)}",
                "is_valid": False,
                "confidence": 0.0,
                "suggestion": ""
            }
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            dict: æ¨¡å‹ä¿¡æ¯
        """
        return {
            "model_name": self.model_name,
            "model_type": "qwen3-vl",
            "device": self.device,
            "model_loaded": self.model is not None
        }
    
    def close(self):
        """
        é‡Šæ”¾æ¨¡å‹èµ„æº
        """
        try:
            if hasattr(self, 'model') and self.model is not None:
                del self.model
                self.model = None
            if hasattr(self, 'tokenizer') and self.tokenizer is not None:
                del self.tokenizer
                self.tokenizer = None
            print("âœ… Qwen3-VLæ¨¡å‹èµ„æºå·²é‡Šæ”¾")
        except Exception as e:
            print(f"âŒ é‡Šæ”¾Qwen3-VLæ¨¡å‹èµ„æºå¤±è´¥: {e}")
    
    def __del__(self):
        """
        ææ„å‡½æ•°
        """
        self.close()