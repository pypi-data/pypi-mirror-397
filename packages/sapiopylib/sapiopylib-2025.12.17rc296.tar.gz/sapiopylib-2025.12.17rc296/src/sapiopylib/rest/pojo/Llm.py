from __future__ import annotations

from enum import Enum
from typing import Any

# FR-53789 - Created classes.
class LlmMessageRole(Enum):
    USER = "USER"
    SYSTEM = "SYSTEM"
    ASSISTANT = "ASSISTANT"


class LlmMessage:
    """
    A message part of an LLM conversation.
    """
    llm_role: LlmMessageRole
    content: str

    def __init__(self, llm_role: LlmMessageRole, content: str):
        """
        Initialize the LlmMessage.

        :param llm_role: The role of the message sender (e.g., USER, ASSISTANT, SYSTEM).
        :param content: The content of the message.
        """
        self.llm_role = llm_role
        self.content = content

    def to_json(self) -> dict[str, Any]:
        return {
            'llmRole': self.llm_role.name,
            'content': self.content
        }

    @staticmethod
    def from_json(json_dct: dict[str, Any]) -> LlmMessage:
        role_str = json_dct.get('llmRole')
        role = LlmMessageRole[role_str] if role_str else None
        content = json_dct.get('content')
        return LlmMessage(role, content)


class LlmChatInput:
    """
    Input for sending a chat message to an LLM.
    """
    structured_output_schema: str | None
    system: str | None
    temperature: float | None
    max_output_tokens: int | None
    messages: list[LlmMessage] | None
    user_message: str | None

    def __init__(self, structured_output_schema: str | None = None, system: str | None = None,
                 temperature: float | None = None, max_output_tokens: int | None = None,
                 messages: list[LlmMessage] | None = None, user_message: str | None = None):
        """
        Initialize the LlmChatInput.

        :param structured_output_schema: A JSON schema string to enforce structured output from the LLM.
        :param system: The system prompt to set context/behavior for the LLM.
        :param temperature: Sampling temperature to use, between 0 and 2. Higher values mean the model will take more risks.
        :param max_output_tokens: The maximum number of tokens to generate in the completion.
        :param messages: Ordered list of prior messages to include in the chat history.
        :param user_message: The new user message to append to the chat history.
        """
        self.structured_output_schema = structured_output_schema
        self.system = system
        self.temperature = temperature
        self.max_output_tokens = max_output_tokens
        self.messages = messages
        self.user_message = user_message

    def to_json(self) -> dict[str, Any]:
        ret = {}
        if self.structured_output_schema is not None:
            ret['structuredOutputSchema'] = self.structured_output_schema
        if self.system is not None:
            ret['system'] = self.system
        if self.temperature is not None:
            ret['temperature'] = self.temperature
        if self.max_output_tokens is not None:
            ret['maxOutputTokens'] = self.max_output_tokens
        if self.messages is not None:
            ret['messages'] = [x.to_json() for x in self.messages]
        if self.user_message is not None:
            ret['userMessage'] = self.user_message
        return ret

    @staticmethod
    def from_json(json_dct: dict[str, Any]) -> LlmChatInput | None:
        if json_dct is None:
            return None
        messages_json = json_dct.get('messages')
        messages = [LlmMessage.from_json(x) for x in messages_json] if messages_json else None
        return LlmChatInput(
            structured_output_schema=json_dct.get('structuredOutputSchema'),
            system=json_dct.get('system'),
            temperature=json_dct.get('temperature'),
            max_output_tokens=json_dct.get('maxOutputTokens'),
            messages=messages,
            user_message=json_dct.get('userMessage')
        )


class LlmContent:
    """
    A content block within an LLM response.
    """
    text: str

    def __init__(self, text: str):
        """
        Initialize the LlmContent.

        :param text: The text content.
        """
        self.text = text

    def to_json(self) -> dict[str, Any]:
        return {'text': self.text}

    @staticmethod
    def from_json(json_dct: dict[str, Any]) -> LlmContent | None:
        if json_dct is None:
            return None
        return LlmContent(json_dct.get('text'))


class LlmChatOutput:
    """
    Output response from an LLM chat interaction.
    """
    chat_type: str
    content: list[LlmContent]
    error_string: str | None

    def __init__(self, chat_type: str, content: list[LlmContent], error_string: str | None = None):
        """
        Initialize the LlmChatOutput.

        :param chat_type: The type of the response.
        :param content: List of content generated by the LLM.
        :param error_string: Error message if the request failed or encountered issues.
        """
        self.chat_type = chat_type
        self.content = content
        self.error_string = error_string

    def to_json(self) -> dict[str, Any]:
        return {
            'type': self.chat_type,
            'content': [x.to_json() for x in self.content],
            'errorString': self.error_string
        }

    @staticmethod
    def from_json(json_dct: dict[str, Any]) -> LlmChatOutput | None:
        if json_dct is None:
            return None
        content_json = json_dct.get('content')
        content = [LlmContent.from_json(x) for x in content_json] if content_json else []
        return LlmChatOutput(
            chat_type=json_dct.get('type'),
            content=content,
            error_string=json_dct.get('errorString')
        )
