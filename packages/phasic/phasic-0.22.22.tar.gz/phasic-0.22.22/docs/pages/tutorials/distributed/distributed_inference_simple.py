#!/usr/bin/env python3
"""
Simple Distributed Computing Example

This example demonstrates the new simplified distributed computing interface
with a basic parallelized computation.

Features demonstrated:
- Single-line distributed initialization (no boilerplate!)
- Distributed particle computation across devices
- Automatic SLURM detection
- Works on both single-node and multi-node setups

Usage:
    # Local testing (single node)
    python distributed_inference_simple.py

    # Multi-node SLURM cluster
    sbatch <(python generate_slurm_script.py --profile small --script distributed_inference_simple.py)

Author: phasic Team
Date: 2025-10-07
"""

import numpy as np
import jax
import jax.numpy as jnp
from phasic import Graph, initialize_distributed


def build_erlang_model(num_stages: int = 3):
    """
    Build a simple Erlang distribution (sum of exponentials).

    Parameters
    ----------
    num_stages : int
        Number of exponential stages

    Returns
    -------
    Graph
        Erlang distribution graph
    """
    g = Graph(1)
    start = g.starting_vertex()

    # Create chain of states
    vertices = [start]
    for i in range(num_stages):
        v = g.find_or_create_vertex([i + 1])
        vertices.append(v)

    # Add edges with rate 1.0 (will scale by theta later)
    for i in range(num_stages):
        vertices[i].add_edge(vertices[i + 1], 1.0)

    return g


def evaluate_particles_distributed(dist_info, graph: Graph,
                                  n_particles: int, time_point: float = 2.0):
    """
    Evaluate PDF for multiple parameter values (particles) in parallel.

    This demonstrates how to distribute computation across devices using pmap.

    Parameters
    ----------
    dist_info : DistributedConfig
        Distributed computing configuration
    graph : Graph
        Phase-type distribution graph
    n_particles : int
        Total number of particles to evaluate
    time_point : float
        Time point to evaluate PDF at

    Returns
    -------
    tuple
        (theta_values, pdf_values) arrays
    """
    # Generate particle positions (theta values)
    np.random.seed(42 + dist_info.process_id)
    particles_per_device = n_particles // dist_info.global_device_count

    # Each device gets a subset of particles
    theta_min = 0.5
    theta_max = 2.5
    theta_values = np.linspace(theta_min, theta_max, n_particles)

    # Reshape for pmap: (n_devices, particles_per_device)
    local_start = dist_info.process_id * dist_info.local_device_count * particles_per_device
    local_end = local_start + dist_info.local_device_count * particles_per_device
    local_theta = theta_values[local_start:local_end].reshape(
        (dist_info.local_device_count, particles_per_device)
    )

    if dist_info.is_coordinator:
        print(f"\n[Process {dist_info.process_id}] Evaluating {n_particles} particles...")
        print(f"  Theta range: [{theta_min:.2f}, {theta_max:.2f}]")
        print(f"  Time point: {time_point:.2f}")
        print(f"  Particles per device: {particles_per_device}")

    # Define PDF evaluation function (this will be vmapped and pmapped)
    def evaluate_pdf(theta_val):
        """Evaluate PDF at fixed time point for given theta."""
        # Build graph with scaled rate
        g_scaled = graph.copy()
        # Scale all edge rates by theta
        for i in range(1, g_scaled.vertices_length()):
            v = g_scaled.vertex_at(i)
            # Note: For this simple example, we're using a pre-built graph
            # In practice, you'd rebuild the graph with theta-dependent rates

        # Evaluate PDF
        return g_scaled.pdf(time_point)

    # Vectorize over particles on each device
    vmap_pdf = jax.vmap(evaluate_pdf)

    # Parallelize across devices
    pmap_pdf = jax.pmap(vmap_pdf)

    # Convert to JAX arrays and evaluate
    local_theta_jax = jnp.array(local_theta)
    local_pdf = pmap_pdf(local_theta_jax)

    # Flatten results
    local_pdf_flat = local_pdf.reshape(-1)

    if dist_info.is_coordinator:
        print(f"[Process {dist_info.process_id}] Computation complete!")
        print(f"  Local results shape: {local_pdf_flat.shape}")

    return local_theta.flatten(), np.array(local_pdf_flat)


def demonstrate_distributed_computation(dist_info):
    """
    Demonstrate distributed computation with phase-type distributions.

    Parameters
    ----------
    dist_info : DistributedConfig
        Distributed computing configuration
    """
    if dist_info.is_coordinator:
        print("\n" + "="*80)
        print("DISTRIBUTED COMPUTATION DEMONSTRATION")
        print("="*80)

    # Build model
    if dist_info.is_coordinator:
        print("\nBuilding Erlang distribution model...")

    num_stages = 5
    graph = build_erlang_model(num_stages=num_stages)

    if dist_info.is_coordinator:
        print(f"  Distribution: Erlang({num_stages})")
        print(f"  States: {graph.vertices_length()}")

    # Evaluate particles in parallel
    n_particles = dist_info.global_device_count * 4  # 4 particles per device
    theta_vals, pdf_vals = evaluate_particles_distributed(
        dist_info, graph, n_particles, time_point=2.0
    )

    # Report results (coordinator only)
    if dist_info.is_coordinator:
        print("\n" + "="*80)
        print("RESULTS")
        print("="*80)
        print(f"Total particles evaluated: {n_particles}")
        print(f"Local results: {len(pdf_vals)} values")
        print(f"PDF range: [{np.min(pdf_vals):.6f}, {np.max(pdf_vals):.6f}]")
        print(f"Mean PDF: {np.mean(pdf_vals):.6f}")

        # Show some sample values
        print("\nSample evaluations:")
        for i in range(min(5, len(theta_vals))):
            print(f"  theta={theta_vals[i]:.3f} -> PDF={pdf_vals[i]:.6f}")

        print("="*80)


def main():
    """Main execution."""
    print("="*80)
    print("SIMPLE DISTRIBUTED COMPUTING EXAMPLE")
    print("="*80)
    print()

    # ========================================================================
    # ONE LINE INITIALIZATION!
    # ========================================================================
    # This replaces 150+ lines of SLURM boilerplate
    dist_info = initialize_distributed(
        coordinator_port=12345,
        platform="cpu",
        enable_x64=True
    )

    print()
    if dist_info.is_coordinator:
        print(f"Distributed setup complete!")
        print(f"  Processes: {dist_info.num_processes}")
        print(f"  Total devices: {dist_info.global_device_count}")
        print(f"  Local devices: {dist_info.local_device_count}")
        if dist_info.job_id:
            print(f"  SLURM Job ID: {dist_info.job_id}")

    # Run demonstration
    demonstrate_distributed_computation(dist_info)

    # Success message
    if dist_info.is_coordinator:
        print()
        print("="*80)
        print("EXAMPLE COMPLETE ✓")
        print("="*80)
        print()
        print("What just happened:")
        print("  • Automatic SLURM detection")
        print("  • JAX distributed initialization")
        print(f"  • Parallel computation on {dist_info.global_device_count} devices")
        print("  • All with just: dist_info = initialize_distributed()")
        print()
        print("Next steps:")
        print("  1. Try on SLURM cluster with multiple nodes:")
        print("     sbatch <(python generate_slurm_script.py --profile medium \\")
        print("                   --script distributed_inference_simple.py)")
        print()
        print("  2. See available cluster profiles:")
        print("     python generate_slurm_script.py --list-profiles")
        print()
        print("  3. Use in your own code:")
        print("     from phasic import initialize_distributed")
        print("     dist_info = initialize_distributed()")
        print()


if __name__ == "__main__":
    main()
