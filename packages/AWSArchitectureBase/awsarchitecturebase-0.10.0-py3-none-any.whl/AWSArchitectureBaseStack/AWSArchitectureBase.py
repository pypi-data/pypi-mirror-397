"""
AWS Architecture Base Class
==========================

This module provides a base class for AWS infrastructure deployments
using CDKTF (Cloud Development Kit for Terraform) with Python.

The base class includes common functionality for:
    * Architecture flags management
    * S3 bucket operations and naming utilities
    * Terraform backend setup
    * Pre/post deployment script handling
    * Archetype integration
    * Common AWS resource patterns

:author: Generated by Claude Code
:version: 1.0.0
:license: MIT
"""

import os
import json
from enum import Enum
from constructs import Construct
from cdktf import TerraformStack, TerraformOutput, S3Backend

# AWS Provider and Resources
from cdktf_cdktf_provider_aws.provider import AwsProvider

# Random Provider
from cdktf_cdktf_provider_random.provider import RandomProvider

# Null Provider
from cdktf_cdktf_provider_null.provider import NullProvider
from cdktf_cdktf_provider_null.resource import Resource as NullResource

# Import modular components
from . import utils, s3, security_hub
from . import ftr_compliance
from .iam import IAMService



class AWSArchitectureBase(TerraformStack):
    """
    Base class for AWS infrastructure stacks.

    Provides common functionality for AWS deployments including:
        * Provider initialization (AWS, Random, Null)
        * S3 bucket management and naming utilities
        * Terraform backend configuration
        * Architecture flags management
        * Pre/post deployment script handling
        * Resource registry management

    :param scope: The construct scope
    :param id: The construct ID
    :param kwargs: Configuration parameters including region, profile, flags, etc.
    """

    # Class-level resource registry
    resources = {}

    # Default post-apply scripts executed after deployment
    default_post_apply_scripts = []

    @staticmethod
    def get_architecture_flags():
        """
        Get the ArchitectureFlags enum for configuration.

        :returns: ArchitectureFlags enum class
        :rtype: type[ArchitectureFlags]
        """
        from .ArchitectureFlags import ArchitectureFlags
        return ArchitectureFlags

    @staticmethod
    def get_archetype(product, app, tier, organization, region):
        """
        Get the BuzzerboyArchetype instance for advanced configuration.

        :param product: Product name
        :param app: Application name
        :param tier: Environment tier (dev, staging, prod)
        :param organization: Organization name
        :param region: AWS region
        :returns: BuzzerboyArchetype instance
        :rtype: BuzzerboyArchetype

        .. note::
           This method requires the BuzzerboyArchetypeStack module to be available.
        """
        from BuzzerboyArchetypeStack.BuzzerboyArchetype import BuzzerboyArchetype

        return BuzzerboyArchetype(product=product, app=app, tier=tier, organization=organization, region=region)

    @staticmethod
    def get_default_ftr_compliance_config(project_name):
        """
        Get default FTR Compliance configuration.
        
        This is a convenience method that delegates to the base package's
        ftr_compliance_config module. Users can customize the returned configuration
        before passing it to the stack.
        
        :param project_name: Project name for naming (e.g., "product-app-tier")
        :type project_name: str
        :return: FTR compliance configuration dictionary
        :rtype: dict
        
        Example:
            >>> # Get default config from base package
            >>> config = AWSArchitectureBase.get_default_ftr_compliance_config("my-app-prod")
            >>> # Customize as needed
            >>> config['inspector']['enable_ec2'] = True
            >>> # Pass to stack
            >>> stack = AWSArchitectureBase(
            ...     scope, "my-stack",
            ...     ftr_compliance_config=config,
            ...     ...
            ... )
        """
        from . import ftr_compliance_config
        return ftr_compliance_config.get_default_ftr_compliance_config(project_name)

    def __init__(self, scope, id, **kwargs):
        """
        Initialize the AWS Architecture Base.

        :param scope: The construct scope
        :param id: Unique identifier for this stack
        :param kwargs: Configuration parameters

        **Configuration Parameters:**

        :param region: AWS region (default: "us-east-1")
        :param environment: Environment name (default: "dev")
        :param project_name: Project identifier (default: "aws-architecture-app")
        :param profile: AWS profile to use (default: "default")
        :param flags: List of ArchitectureFlags to modify behavior
        :param postApplyScripts: List of shell commands to execute after deployment
        :param ftr_compliance_config: Dictionary containing FTR compliance service configurations.
            If not provided or empty, uses default from get_default_ftr_compliance_config()
        
        **FTR Compliance Configuration:**
        
        Pass a dictionary with service-specific configurations (optional):
        
        .. code-block:: python
        
            ftr_compliance_config = {
                'security_hub': {
                    'enable_standards': True,          # Enable compliance standards
                    'auto_enable_controls': True,      # Auto-enable new controls
                    'standards': [                      # Custom standards (optional)
                        'aws-foundational-security-best-practices/v/1.0.0',
                        'cis-aws-foundations-benchmark/v/1.4.0'
                    ]
                },
                'config': {
                    's3_bucket_name': 'my-config-bucket',  # S3 bucket for Config data
                    'recorder_name': 'default',             # Recorder name
                    'channel_name': 'default',              # Channel name
                    'enable_recorder': True,                # Start recording
                    'enable_rules': True,                   # Enable compliance rules
                    'rules': ['S3_BUCKET_VERSIONING_ENABLED'],  # Custom rules (optional)
                    's3_key_prefix': 'config',              # S3 prefix
                    'sns_topic_arn': None                   # SNS topic (optional)
                },
                'backup': {
                    # AWS Backup configuration
                    'enable_backup': False,
                    'vault_name': None,
                    'retention_days': 35
                },
                'inspector': {
                    # AWS Inspector configuration
                    'enable_scanning': False,
                    'enable_ec2': False,
                    'enable_ecr': False
                },
                'systems_manager': {
                    # AWS Systems Manager configuration
                    'enable_patching': False,
                    'operating_system': 'AMAZON_LINUX_2'
                },
                'cloudtrail': {
                    # AWS CloudTrail configuration
                    'enable_logging': False,
                    'trail_name': None,
                    'enable_log_file_validation': True
                },
                'notifications': {
                    # AWS SNS/SES configuration
                    'topic_name': None,
                    'email_addresses': []
                }
            }
        
        .. note::
           If ftr_compliance_config is not provided, the default configuration from
           get_default_ftr_compliance_config() will be used automatically.
           All service configurations are optional with safe defaults.
           Enable services via flags, configure via ftr_compliance_config.
        """
        super().__init__(scope, id)

        # ===== Base Configuration =====
        self.region = kwargs.get("region", "us-east-1")
        self.environment = kwargs.get("environment", "dev")
        self.project_name = kwargs.get("project_name", "aws-architecture-app")
        self.profile = kwargs.get("profile", "default")
        self.flags = kwargs.get("flags", [])
        self.post_apply_scripts = kwargs.get("postApplyScripts", []) or []
        
        # ===== FTR Compliance Configuration =====
        # Use default config if not provided or empty
        self.ftr_compliance_config = kwargs.get("ftr_compliance_config") or AWSArchitectureBase.get_default_ftr_compliance_config(self.project_name)

        # ===== Storage Configuration =====
        self.state_bucket_name = kwargs.get(
            "state_bucket_name", utils.naming.properize_string(f"{self.region}-{self.project_name}-tfstate")
        )

        # ===== Internal State =====
        self.secrets = {}
        self.post_terraform_messages = []
        self._post_plan_guidance: list[str] = []

        # ===== Base Infrastructure Setup =====
        self._initialize_providers()
        
        # Setup S3 backend automatically 
        s3_resources = s3.create_s3_resources(
            scope=self,
            project_name=self.project_name,
            environment=self.environment,
            region=self.region,
            profile=self.profile,
            state_bucket_name=self.state_bucket_name
        )
        self.state_bucket_name = s3_resources['state_bucket_name']
        self.resources['s3'] = s3_resources
        
        # Setup FTR compliance services if enabled
        ftr_resources = ftr_compliance.setup_ftr_compliance(stack=self, flags=self.flags)
        if ftr_resources:
            self.resources.update(ftr_resources)

        # ===== Helper Services =====
        self.iam = IAMService(stack=self)

    def _initialize_providers(self):
        """Initialize all required Terraform providers."""
        # Main AWS provider for the specified region
        aws = AwsProvider(self, "aws", region=self.region, profile=self.profile)
        self.resources["aws"] = aws

        # Random provider for password generation
        RandomProvider(self, "random")
        self.resources["random"] = RandomProvider

        # Null provider for post-deployment scripts
        NullProvider(self, "null")
        self.resources["null"] = NullProvider

    def has_flag(self, flag):
        """
        Check if a specific architecture flag is set.

        :param flag: Flag to check (from ArchitectureFlags enum)
        :type flag: str
        :returns: True if flag is set, False otherwise
        :rtype: bool

        Example:
            >>> if stack.has_flag(ArchitectureFlags.SKIP_DATABASE.value):
            ...     print("Database creation skipped")
        """
        return flag in self.flags

    def get_extra_secret_env(self, env_var_name="SECRET_STRING"):
        """
        Load additional secrets from environment variable.

        Attempts to load and parse a JSON string from the specified environment variable.
        Any valid JSON key-value pairs are added to the secrets dictionary if they
        don't already exist.

        :param env_var_name: Name of the environment variable containing JSON secrets (default: "SECRET_STRING")
        :type env_var_name: str
        :raises: No exceptions - silently continues if JSON parsing fails

        **Environment Variable Format:**

        .. code-block:: json

            {
                "custom_key": "custom_value",
                "api_token": "secret_token_value"
            }

        .. note::
           This is a backward compatibility wrapper. Use utils.secrets.parse_secrets_from_env() directly.
        """
        extra_secrets = utils.secrets.parse_secrets_from_env(env_var_name) or {}
        for key, value in extra_secrets.items():
            if key not in self.secrets:
                self.secrets[key] = value

    def execute_post_apply_scripts(self, dependencies=None):
        """
        Execute post-apply scripts after infrastructure deployment.

        Creates null resources with local-exec provisioners to run shell scripts
        after all other infrastructure resources are created.

        :param dependencies: List of resources that scripts should depend on (optional)
        :type dependencies: list
        
        **Script Execution:**

        * Each script runs as a separate null resource
        * Scripts execute in the order they appear in the list
        * Failures in scripts don't prevent deployment completion
        * All scripts depend on core infrastructure being ready

        **Error Handling:**

        * Scripts use "on_failure: continue" to prevent deployment failures
        * Failed scripts are logged but don't halt the deployment process
        * Manual intervention may be required if critical scripts fail

        .. note::
           Post-apply scripts can be provided via the postApplyScripts parameter
           during stack initialization. If no scripts are provided, this method
           returns without creating any resources.

        .. warning::
           Scripts have access to the local environment where Terraform runs.
           Ensure scripts are safe and don't expose sensitive information.

        Example:
            >>> stack = MyArchitecture(
            ...     app, "my-stack",
            ...     postApplyScripts=[
            ...         "echo 'Deployment completed successfully'",
            ...         "curl -X POST https://api.example.com/notify",
            ...         "python /path/to/setup_script.py"
            ...     ]
            ... )
            >>> # Later, in your resource creation method:
            >>> stack.execute_post_apply_scripts(dependencies=[stack.my_resource])
        """
        if not self.post_apply_scripts:
            return

        # Use provided dependencies or empty list
        deps = dependencies or []

        for index, script in enumerate(self.post_apply_scripts):
            # Create a unique resource name for each script
            resource_name = f"post_apply_script_{index + 1}"
            
            # Create null resource with local-exec provisioner
            NullResource(
                self,
                resource_name,
                depends_on=deps,
                provisioners=[
                    {
                        "type": "local-exec",
                        "command": script,
                        "on_failure": "continue",  # Don't fail deployment on script failure
                    }
                ],
            )