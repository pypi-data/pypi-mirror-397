"""
msgmodel.providers.openai
~~~~~~~~~~~~~~~~~~~~~~~~~

OpenAI API provider implementation.

ZERO DATA RETENTION (ZDR) - ENFORCED:
- The X-OpenAI-No-Store header is ALWAYS sent with all requests.
- This header instructs OpenAI not to store inputs and outputs for service improvements.
- ZDR is non-negotiable and cannot be disabled.
- See: https://platform.openai.com/docs/guides/zero-data-retention

FILE UPLOADS:
- All file uploads are via inline base64-encoding in prompts (no Files API)
- Files are limited to practical API size constraints (~15-20MB)
- This approach provides better privacy and stateless operation
"""

import json
import base64
import logging
from typing import Optional, Dict, Any, Iterator, List, Callable

import requests

from ..config import OpenAIConfig, OPENAI_URL
from ..exceptions import APIError, ProviderError, StreamingError

logger = logging.getLogger(__name__)

# MIME type constants
MIME_TYPE_JSON = "application/json"


class OpenAIProvider:
    """
    OpenAI API provider for making LLM requests.
    
    Handles API calls and response parsing for OpenAI models.
    All file uploads use inline base64-encoding for privacy and statelessness.
    """
    
    def __init__(self, api_key: str, config: Optional[OpenAIConfig] = None):
        """
        Initialize the OpenAI provider.
        
        Args:
            api_key: OpenAI API key
            config: Optional configuration (uses defaults if not provided)
        """
        self.api_key = api_key
        self.config = config or OpenAIConfig()
    
    def _build_headers(self) -> Dict[str, str]:
        """
        Build HTTP headers for OpenAI API requests.
        
        ENFORCES Zero Data Retention (ZDR) by always including the X-OpenAI-No-Store header.
        This header instructs OpenAI not to store inputs and outputs for service improvements.
        
        ZDR is non-negotiable and cannot be disabled.
        
        Returns:
            Dictionary of HTTP headers with ZDR enforced
        """
        headers: Dict[str, str] = {
            "Content-Type": MIME_TYPE_JSON,
            "Authorization": f"Bearer {self.api_key}",
            "X-OpenAI-No-Store": "true"  # â† ALWAYS enforced, no option to disable
        }
        
        return headers
    
    def _build_content(
        self,
        prompt: str,
        file_data: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """Build the content array for the API request."""
        content: List[Dict[str, Any]] = []
        
        if file_data:
            mime_type = file_data["mime_type"]
            encoded_data = file_data.get("data", "")
            filename = file_data.get("filename", "input.bin")
            
            if mime_type.startswith("image/"):
                content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:{mime_type};base64,{encoded_data}"
                    }
                })
            elif mime_type.startswith("text/"):
                try:
                    decoded_text = base64.b64decode(encoded_data).decode("utf-8", errors="ignore")
                except Exception:
                    decoded_text = ""
                if decoded_text.strip():
                    content.append({
                        "type": "text",
                        "text": f"(Contents of {filename}):\n\n{decoded_text}"
                    })
            else:
                content.append({
                    "type": "text",
                    "text": (
                        f"[Note: A file named '{filename}' with MIME type '{mime_type}' "
                        f"was provided. You may not be able to read it directly, but you "
                        f"can still respond based on the description and prompt.]"
                    )
                })
        
        content.append({
            "type": "text",
            "text": prompt
        })
        
        return content
    
    def _supports_max_completion_tokens(self, model_name: str) -> bool:
        """
        Check if model supports max_completion_tokens (modern OpenAI standard).
        
        v3.2.1 Enhancement: Prefer max_completion_tokens for all models except known legacy ones.
        This ensures compatibility with GPT-5, GPT-6, and future models automatically.
        
        Args:
            model_name: The model identifier
            
        Returns:
            True if model uses max_completion_tokens (default for new models),
            False only for known legacy models that require max_tokens
        """
        # Models that ONLY support max_tokens (legacy, pre-GPT-4o era)
        # Check these first before checking prefixes
        legacy_exact_matches = [
            "gpt-3.5-turbo",
            "gpt-4",
        ]
        
        # If exact match to legacy model, use max_tokens
        if model_name in legacy_exact_matches:
            return False
        
        # Check for legacy models with version suffixes
        # gpt-3.5-turbo-0613, gpt-4-0613, etc.
        legacy_prefixes_with_version = [
            "gpt-3.5-turbo-",
            "gpt-4-0",  # gpt-4-0613, gpt-4-0125-preview, etc. (but NOT gpt-4-turbo)
        ]
        
        for legacy_prefix in legacy_prefixes_with_version:
            if model_name.startswith(legacy_prefix):
                # Make sure gpt-4-turbo is not caught by "gpt-4-0" check
                if legacy_prefix == "gpt-4-0" and "turbo" in model_name:
                    continue
                return False
        
        # All other models use max_completion_tokens:
        # - GPT-4o (all versions): gpt-4o, gpt-4o-mini, gpt-4o-2024-*
        # - GPT-4-turbo (all versions): gpt-4-turbo, gpt-4-turbo-preview
        # - GPT-5 and future models
        # This future-proofs the implementation
        return True
    
    def _build_payload(
        self,
        prompt: str,
        system_instruction: Optional[str] = None,
        file_data: Optional[Dict[str, Any]] = None,
        stream: bool = False
    ) -> Dict[str, Any]:
        """Build the API request payload for OpenAI Chat Completions API."""
        content = self._build_content(prompt, file_data)
        
        # Build messages array with system message first (if provided)
        messages: List[Dict[str, Any]] = []
        if system_instruction:
            messages.append({"role": "system", "content": system_instruction})
        messages.append({"role": "user", "content": content})
        
        payload: Dict[str, Any] = {
            "model": self.config.model,
            "messages": messages,
            "temperature": self.config.temperature,
            "top_p": self.config.top_p,
        }
        
        # Use appropriate max tokens parameter based on model version
        # v3.2.1: Support both max_tokens (legacy) and max_completion_tokens (GPT-4o+)
        if self._supports_max_completion_tokens(self.config.model):
            payload["max_completion_tokens"] = self.config.max_tokens
        else:
            payload["max_tokens"] = self.config.max_tokens
        
        if stream:
            payload["stream"] = True
        
        return payload
    
    def query(
        self,
        prompt: str,
        system_instruction: Optional[str] = None,
        file_data: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Make a non-streaming API call to OpenAI.
        
        With store_data=False (default), this request opts into Zero Data Retention (ZDR)
        via the X-OpenAI-No-Store header, ensuring OpenAI does not store this interaction
        for service improvements.
        
        Args:
            prompt: The user prompt
            system_instruction: Optional system instruction
            file_data: Optional file data dict
            
        Returns:
            The API response as a dictionary
            
        Raises:
            APIError: If the API call fails
        """
        payload = self._build_payload(prompt, system_instruction, file_data)
        headers = self._build_headers()
        
        try:
            response = requests.post(
                OPENAI_URL,
                headers=headers,
                data=json.dumps(payload)
            )
        except requests.RequestException as e:
            raise APIError(f"Request failed: {e}")
        
        if not response.ok:
            raise APIError(
                f"OpenAI API error: {response.text}",
                status_code=response.status_code,
                response_text=response.text
            )
        
        return response.json()
    
    def stream(
        self,
        prompt: str,
        system_instruction: Optional[str] = None,
        file_data: Optional[Dict[str, Any]] = None,
        timeout: float = 300,
        on_chunk: Optional[Callable[[str], bool]] = None
    ) -> Iterator[str]:
        """
        Make a streaming API call to OpenAI.
        
        With store_data=False (default), this request opts into Zero Data Retention (ZDR)
        via the X-OpenAI-No-Store header, ensuring OpenAI does not store this interaction
        for service improvements.
        
        v3.2.1 Enhancement: Adds timeout support and optional abort callback.
        
        Args:
            prompt: The user prompt
            system_instruction: Optional system instruction
            file_data: Optional file data dict
            timeout: Timeout in seconds for the streaming connection (default: 300s/5min)
            on_chunk: Optional callback that receives each chunk. Return False to abort stream.
            
        Yields:
            Text chunks as they arrive
            
        Raises:
            APIError: If the API call fails
            StreamingError: If streaming fails or timeout occurs
        """
        payload = self._build_payload(prompt, system_instruction, file_data, stream=True)
        headers = self._build_headers()
        
        try:
            response = requests.post(
                OPENAI_URL,
                headers=headers,
                data=json.dumps(payload),
                stream=True,
                timeout=timeout
            )
        except requests.Timeout:
            raise StreamingError(f"OpenAI streaming request timed out after {timeout} seconds")
        except requests.RequestException as e:
            raise APIError(f"Request failed: {e}")
        
        if not response.ok:
            raise APIError(
                f"OpenAI API error: {response.text}",
                status_code=response.status_code,
                response_text=response.text
            )
        
        chunks_received = 0
        try:
            for line in response.iter_lines():
                if line:
                    line_text = line.decode("utf-8")
                    if line_text.startswith("data: "):
                        data = line_text[6:]
                        if data == "[DONE]":
                            break
                        try:
                            chunk = json.loads(data)
                            # Extract text from OpenAI Chat Completions streaming response
                            # Format: {"choices": [{"delta": {"content": "..."}}], ...}
                            if "choices" in chunk and isinstance(chunk["choices"], list):
                                for choice in chunk["choices"]:
                                    if isinstance(choice, dict):
                                        delta = choice.get("delta", {})
                                        if isinstance(delta, dict):
                                            text = delta.get("content", "")
                                            if text:
                                                chunks_received += 1
                                                # v3.2.1: Support abort callback
                                                if on_chunk is not None:
                                                    should_continue = on_chunk(text)
                                                    if should_continue is False:
                                                        return
                                                yield text
                        except json.JSONDecodeError:
                            continue
            
            if chunks_received == 0:
                logger.error("No text chunks extracted from streaming response. Response format may not match OpenAI Chat Completions delta structure or stream may have ended prematurely.")
        except Exception as e:
            raise StreamingError(f"Streaming interrupted: {e}")
    
    @staticmethod
    def extract_text(response: Dict[str, Any]) -> str:
        """
        Extract text from an OpenAI Chat Completions response.
        
        Args:
            response: The raw API response
            
        Returns:
            Extracted text content
        """
        # OpenAI Chat Completions response format:
        # {"choices": [{"message": {"content": "..."}}], ...}
        if "choices" in response and isinstance(response["choices"], list):
            for choice in response["choices"]:
                if isinstance(choice, dict):
                    message = choice.get("message", {})
                    if isinstance(message, dict):
                        content = message.get("content", "")
                        if content:
                            return content
        
        return ""
