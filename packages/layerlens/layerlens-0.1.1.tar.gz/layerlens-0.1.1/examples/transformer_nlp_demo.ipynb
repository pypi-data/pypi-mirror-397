{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2fae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract attention patterns from BERT\n",
    "def extract_attention(model, encoded_input):\n",
    "    \"\"\"Extract attention weights from the model.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Forward pass with output_attentions=True\n",
    "        outputs = model(**encoded_input, output_attentions=True)\n",
    "        \n",
    "        # Get attention weights (tuple of tensors, one per layer)\n",
    "        attentions = outputs.attentions\n",
    "        \n",
    "        return attentions\n",
    "\n",
    "# Extract attention weights for our sample\n",
    "attention_weights = extract_attention(model, sample_input)\n",
    "\n",
    "print(f\"Extracted attention weights from {len(attention_weights)} layers\")\n",
    "print(f\"Shape of attention weights for first layer: {attention_weights[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039d7abf",
   "metadata": {},
   "source": [
    "## 6. Visualize attention patterns for a specific layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95082e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a layer to visualize (e.g., the first layer)\n",
    "layer_idx = 0\n",
    "layer_attention = attention_weights[layer_idx][0].detach().numpy()\n",
    "\n",
    "# Get the tokens for visualization\n",
    "tokens = tokenizer.convert_ids_to_tokens(sample_input['input_ids'][0])\n",
    "\n",
    "# Plot the attention map\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(layer_attention[0], cmap='viridis')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.title(f'Attention Map for Layer {layer_idx}, Head 0')\n",
    "\n",
    "# Label the axes with tokens (limit to first 15 tokens for readability)\n",
    "max_tokens = min(15, len(tokens))\n",
    "plt.xticks(range(max_tokens), tokens[:max_tokens], rotation=90)\n",
    "plt.yticks(range(max_tokens), tokens[:max_tokens])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a51e75d",
   "metadata": {},
   "source": [
    "## 7. Compare attention patterns across different heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e540d92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare attention patterns across different heads\n",
    "n_heads = layer_attention.shape[0]\n",
    "fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    if i < n_heads:\n",
    "        ax.imshow(layer_attention[i][:max_tokens, :max_tokens], cmap='viridis')\n",
    "        ax.set_title(f'Head {i}')\n",
    "        \n",
    "        # Only show token labels for the first head to avoid clutter\n",
    "        if i == 0:\n",
    "            ax.set_xticks(range(max_tokens))\n",
    "            ax.set_yticks(range(max_tokens))\n",
    "            ax.set_xticklabels(tokens[:max_tokens], rotation=90)\n",
    "            ax.set_yticklabels(tokens[:max_tokens])\n",
    "        else:\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f'Attention Patterns Across Different Heads in Layer {layer_idx}', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a61ef",
   "metadata": {},
   "source": [
    "## 8. Build a surrogate model for the classifier layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10625c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need more samples to build a reliable surrogate\n",
    "# Let's generate a few more sentences for our analysis\n",
    "additional_texts = [\n",
    "    \"This movie exceeded all my expectations.\",\n",
    "    \"I was disappointed by the quality of the product.\",\n",
    "    \"The service was satisfactory, could be better.\",\n",
    "    \"An amazing experience from start to finish!\",\n",
    "    \"I regret purchasing this item, it broke quickly.\"\n",
    "]\n",
    "\n",
    "# Tokenize all texts\n",
    "all_texts = sample_texts + additional_texts\n",
    "all_encoded = []\n",
    "\n",
    "for text in all_texts:\n",
    "    encoded = tokenizer(\n",
    "        text, \n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    all_encoded.append(encoded)\n",
    "\n",
    "# Extract pooler output and predictions for all samples\n",
    "from layerlens.core.layer_extractor import LayerExtractor\n",
    "\n",
    "# Create a custom extractor function for BERT\n",
    "def extract_bert_outputs(model, inputs_list):\n",
    "    \"\"\"Extract outputs from BERT layers.\"\"\"\n",
    "    outputs = {}\n",
    "    pooler_outputs = []\n",
    "    predictions = []\n",
    "    \n",
    "    # Process each input\n",
    "    for encoded_input in inputs_list:\n",
    "        with torch.no_grad():\n",
    "            # Get pooler output\n",
    "            pooler_output = model.bert.pooler(\n",
    "                model.bert(**encoded_input).last_hidden_state\n",
    "            )\n",
    "            pooler_outputs.append(pooler_output.numpy())\n",
    "            \n",
    "            # Get predictions\n",
    "            logits = model.classifier(pooler_output)\n",
    "            predictions.append(logits.numpy())\n",
    "    \n",
    "    # Combine results\n",
    "    outputs['bert.pooler'] = np.vstack(pooler_outputs)\n",
    "    outputs['classifier'] = np.vstack(predictions)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Extract outputs\n",
    "layer_outputs = extract_bert_outputs(model, all_encoded)\n",
    "\n",
    "# Build surrogate for the classifier layer\n",
    "from layerlens.core.surrogate_builder import SurrogateBuilder\n",
    "\n",
    "surrogate_builder = SurrogateBuilder(surrogate_type='linear')\n",
    "classifier_surrogate = surrogate_builder.fit(\n",
    "    'classifier',\n",
    "    layer_outputs['bert.pooler'],\n",
    "    layer_outputs['classifier']\n",
    ")\n",
    "\n",
    "print(\"Built surrogate model for the classifier layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bc6940",
   "metadata": {},
   "source": [
    "## 9. Analyze feature importance from the surrogate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7111bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from the surrogate model\n",
    "if hasattr(classifier_surrogate, 'coef_'):\n",
    "    # For linear models, we can look at the coefficients\n",
    "    # We'll focus on the positive sentiment class (index 1)\n",
    "    positive_coefs = classifier_surrogate.coef_[1]\n",
    "    \n",
    "    # Plot top feature importances\n",
    "    from layerlens.utils.plot_utils import plot_feature_importance\n",
    "    \n",
    "    importance_fig = plot_feature_importance(\n",
    "        np.abs(positive_coefs),  # Use absolute value of coefficients\n",
    "        feature_names=None,\n",
    "        top_n=20\n",
    "    )\n",
    "    plt.title('Feature Importance for Positive Sentiment')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3153c65",
   "metadata": {},
   "source": [
    "## 10. Analyze token contributions to the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5218c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simplified token attribution analysis\n",
    "def analyze_token_importance(model, tokenizer, text, layer_idx=11):\n",
    "    \"\"\"Analyze which tokens contribute most to the prediction.\"\"\"\n",
    "    # Tokenize the text\n",
    "    encoded = tokenizer(\n",
    "        text, \n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Extract attention weights\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded, output_attentions=True)\n",
    "        attentions = outputs.attentions\n",
    "        \n",
    "    # Get last layer attention\n",
    "    last_layer_attention = attentions[layer_idx][0].mean(dim=0).detach().numpy()\n",
    "    \n",
    "    # Get the [CLS] token attention (how much each token influences the classification)\n",
    "    cls_attention = last_layer_attention[0]\n",
    "    \n",
    "    # Get tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n",
    "    \n",
    "    # Create a dictionary of token importances\n",
    "    token_importance = {}\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token not in ['[PAD]', '[CLS]', '[SEP]']:\n",
    "            token_importance[token] = cls_attention[i]\n",
    "    \n",
    "    return token_importance, tokens, cls_attention\n",
    "\n",
    "# Analyze our first sample text\n",
    "token_importance, tokens, cls_attention = analyze_token_importance(\n",
    "    model, tokenizer, sample_texts[0]\n",
    ")\n",
    "\n",
    "# Visualize token importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(cls_attention)), cls_attention)\n",
    "plt.xticks(range(len(cls_attention)), tokens, rotation=90)\n",
    "plt.title('Token Importance based on Attention to [CLS] Token')\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Attention Weight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top important tokens\n",
    "print(\"Top 10 important tokens:\")\n",
    "sorted_tokens = sorted(token_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "for token, importance in sorted_tokens[:10]:\n",
    "    print(f\"{token}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f650d5",
   "metadata": {},
   "source": [
    "## 11. Compare different text samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78068d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions and explanations across different samples\n",
    "predictions = []\n",
    "top_tokens = []\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    # Get prediction\n",
    "    encoded = tokenizer(\n",
    "        text, \n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "    \n",
    "    # Get token importance\n",
    "    token_importance, _, _ = analyze_token_importance(model, tokenizer, text)\n",
    "    top_tokens_for_sample = sorted(token_importance.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    \n",
    "    predictions.append(probabilities[0][1].item())  # Probability of positive sentiment\n",
    "    top_tokens.append(top_tokens_for_sample)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(range(len(sample_texts)), predictions)\n",
    "\n",
    "# Color-code by sentiment\n",
    "for i, prob in enumerate(predictions):\n",
    "    if prob > 0.5:\n",
    "        bars[i].set_color('green')\n",
    "    else:\n",
    "        bars[i].set_color('red')\n",
    "\n",
    "plt.xticks(range(len(sample_texts)), [f\"Sample {i+1}\" for i in range(len(sample_texts))])\n",
    "plt.ylabel('Positive Sentiment Probability')\n",
    "plt.title('Sentiment Predictions Across Samples')\n",
    "plt.axhline(y=0.5, color='black', linestyle='--')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "for i, prob in enumerate(predictions):\n",
    "    plt.text(i, prob + 0.05, f\"{prob:.2f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display top tokens for each sample\n",
    "for i, tokens in enumerate(top_tokens):\n",
    "    print(f\"\\nSample {i+1}: {sample_texts[i]}\")\n",
    "    print(\"Top influential tokens:\")\n",
    "    for token, importance in tokens:\n",
    "        print(f\"  {token}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1ff58b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to use LayerLens to explain a transformer-based NLP model (BERT). We've:\n",
    "\n",
    "1. Extracted and visualized attention patterns from different layers and heads\n",
    "2. Built a surrogate model to explain the classifier layer\n",
    "3. Analyzed feature importance for sentiment predictions\n",
    "4. Identified which tokens contribute most to predictions\n",
    "\n",
    "LayerLens helps us understand how transformer models process text and make predictions by providing layer-by-layer insights into their behavior."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
