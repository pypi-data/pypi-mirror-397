{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74662910",
   "metadata": {},
   "source": [
    "# LayerLens MNIST Demo\n",
    "\n",
    "This notebook demonstrates how to use LayerLens to explain a CNN model trained on the MNIST dataset.\n",
    "\n",
    "LayerLens provides comprehensive layer-by-layer explainability for deep learning models, including:\n",
    "- Layer activation extraction and visualization\n",
    "- Surrogate model construction for interpretability\n",
    "- Feature importance analysis\n",
    "- Interactive dashboards for exploration\n",
    "- Model monitoring and drift detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee10e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Import LayerLens\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))  # Add parent directory to path\n",
    "\n",
    "import layerlens as ll\n",
    "from layerlens.utils.data_utils import preprocess_data\n",
    "from layerlens.utils.model_utils import predict_with_model\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"LayerLens version: {ll.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460e04be",
   "metadata": {},
   "source": [
    "## 1. Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c3a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Reshape for CNN input (add channel dimension)\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54066aab",
   "metadata": {},
   "source": [
    "## 2. Create a simple CNN model for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2345a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), name='conv1'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool1'),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', name='conv2'),\n",
    "    layers.MaxPooling2D((2, 2), name='pool2'),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', name='conv3'),\n",
    "    layers.Flatten(name='flatten'),\n",
    "    layers.Dense(64, activation='relu', name='dense1'),\n",
    "    layers.Dense(10, activation='softmax', name='output')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5699e1",
   "metadata": {},
   "source": [
    "## 3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e7edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (or load pre-trained weights if available)\n",
    "try:\n",
    "    model.load_weights('mnist_cnn.h5')\n",
    "    print(\"Loaded pre-trained weights\")\n",
    "except:\n",
    "    print(\"Training the model...\")\n",
    "    history = model.fit(x_train[:1000], y_train[:1000], \n",
    "                       epochs=3, batch_size=32, \n",
    "                       validation_split=0.2, verbose=1)\n",
    "    model.save_weights('mnist_cnn.h5')\n",
    "    print(\"Model training completed and weights saved\")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test[:100], y_test[:100], verbose=0)\n",
    "print(f'Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727fec2f",
   "metadata": {},
   "source": [
    "## 4. Use LayerLens to explain the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68775764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LayerLens explainer\n",
    "print(\"Creating LayerLens explainer...\")\n",
    "\n",
    "# Build the model first to ensure it's properly initialized\n",
    "# For Keras 3.x, predict() is more reliable than calling the model directly\n",
    "print(\"Building model...\")\n",
    "_ = model.predict(x_test[:1], verbose=0)\n",
    "print(\"✓ Model built successfully\")\n",
    "\n",
    "explainer = ll.Explainer(model, surrogate_type='tree')\n",
    "print(\"✓ Explainer created successfully\")\n",
    "\n",
    "# Select a sample image to explain\n",
    "sample_idx = 42\n",
    "sample_image = x_test[sample_idx:sample_idx+1]\n",
    "sample_label = np.argmax(y_test[sample_idx])\n",
    "\n",
    "# Display the sample image\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(sample_image[0, :, :, 0], cmap='gray')\n",
    "plt.title(f\"Actual Digit: {sample_label}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Make prediction with the model\n",
    "prediction = model.predict(sample_image, verbose=0)\n",
    "predicted_class = np.argmax(prediction)\n",
    "confidence = np.max(prediction)\n",
    "\n",
    "print(f\"Model prediction: {predicted_class} (confidence: {confidence:.3f})\")\n",
    "\n",
    "# Generate explanations for multiple samples to build better surrogates\n",
    "print(\"Extracting layer outputs for explanation...\")\n",
    "n_explain_samples = 50\n",
    "explain_indices = np.random.choice(len(x_test), n_explain_samples, replace=False)\n",
    "explain_data = x_test[explain_indices]\n",
    "\n",
    "# Generate explanations\n",
    "explanations = explainer.explain(explain_data)\n",
    "\n",
    "print(f\"Generated explanations for model layers\")\n",
    "if hasattr(explanations, 'layer_explanations'):\n",
    "    print(f\"Available layers: {list(explanations.layer_explanations.keys())}\")\n",
    "else:\n",
    "    print(f\"Explanation keys: {list(explanations.keys()) if isinstance(explanations, dict) else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909b0505",
   "metadata": {},
   "source": [
    "## 5. Visualize layer activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e36f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract layer outputs for visualization\n",
    "print(\"Extracting layer outputs...\")\n",
    "from layerlens.core.layer_extractor import LayerExtractor\n",
    "\n",
    "extractor = LayerExtractor(model)\n",
    "layer_outputs = extractor.extract(sample_image)\n",
    "\n",
    "print(f\"Extracted outputs for layers: {list(layer_outputs.keys())}\")\n",
    "\n",
    "# Visualize activations for the first convolutional layer\n",
    "print(\"Generating activation heatmap...\")\n",
    "from layerlens.visualization.heatmap_generator import generate_activation_heatmap\n",
    "\n",
    "if 'conv1' in layer_outputs:\n",
    "    conv1_activations = layer_outputs['conv1']\n",
    "    print(f\"Conv1 activation shape: {conv1_activations.shape}\")\n",
    "    \n",
    "    # Generate heatmap\n",
    "    conv1_heatmap = generate_activation_heatmap(conv1_activations, 'conv1')\n",
    "    \n",
    "    # Display using matplotlib since plotly might not be available\n",
    "    print(\"Activation heatmap generated successfully\")\n",
    "    \n",
    "    # Show a simplified visualization\n",
    "    if len(conv1_activations.shape) == 4:\n",
    "        # Take first sample and first few channels\n",
    "        sample_activations = conv1_activations[0, :, :, :8]  # First 8 channels\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "        for i in range(8):\n",
    "            row, col = i // 4, i % 4\n",
    "            axes[row, col].imshow(sample_activations[:, :, i], cmap='viridis')\n",
    "            axes[row, col].set_title(f'Channel {i}')\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        plt.suptitle('Conv1 Layer Activations (First 8 Channels)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Conv1 layer not found in outputs\")\n",
    "\n",
    "# Clean up\n",
    "extractor.remove_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fdafa1",
   "metadata": {},
   "source": [
    "## 6. Build surrogate models for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab29211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build surrogate models for interpretability\n",
    "print(\"Building surrogate models...\")\n",
    "from layerlens.core.surrogate_builder import SurrogateBuilder\n",
    "\n",
    "surrogate_builder = SurrogateBuilder(surrogate_type='tree', max_samples=100)\n",
    "\n",
    "# Get a sample for training surrogates\n",
    "n_samples = 100\n",
    "sample_indices = np.random.choice(len(x_test), n_samples, replace=False)\n",
    "sample_data = x_test[sample_indices]\n",
    "\n",
    "# Extract layer outputs for all samples\n",
    "extractor = LayerExtractor(model)\n",
    "all_layer_outputs = extractor.extract(sample_data)\n",
    "\n",
    "print(f\"Available layers for surrogate modeling: {list(all_layer_outputs.keys())}\")\n",
    "\n",
    "# Build surrogate for the first dense layer if available\n",
    "dense_layer = None\n",
    "for layer_name in ['dense1', 'dense', 'fc']:\n",
    "    if layer_name in all_layer_outputs:\n",
    "        dense_layer = layer_name\n",
    "        break\n",
    "\n",
    "if dense_layer:\n",
    "    print(f\"Building surrogate for layer: {dense_layer}\")\n",
    "    dense_outputs = all_layer_outputs[dense_layer]\n",
    "    print(f\"Dense layer output shape: {dense_outputs.shape}\")\n",
    "    \n",
    "    # Flatten input data for the surrogate\n",
    "    flat_input_data = sample_data.reshape(n_samples, -1)\n",
    "    \n",
    "    # Build surrogate\n",
    "    dense_surrogate = surrogate_builder.fit(dense_layer, flat_input_data, dense_outputs)\n",
    "    \n",
    "    # Evaluate the surrogate\n",
    "    print(\"Evaluating surrogate model...\")\n",
    "    from layerlens.utils.plot_utils import plot_surrogate_vs_original\n",
    "    \n",
    "    # Get predictions\n",
    "    surrogate_preds = dense_surrogate.predict(flat_input_data)\n",
    "    original_preds = dense_outputs\n",
    "    \n",
    "    # Ensure same shape for comparison\n",
    "    if len(surrogate_preds.shape) != len(original_preds.shape):\n",
    "        if len(original_preds.shape) > 1:\n",
    "            # Take only first output dimension for comparison\n",
    "            original_preds = original_preds[:, 0] if original_preds.shape[1] > 1 else original_preds.flatten()\n",
    "        if len(surrogate_preds.shape) > 1:\n",
    "            surrogate_preds = surrogate_preds[:, 0] if surrogate_preds.shape[1] > 1 else surrogate_preds.flatten()\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig = plot_surrogate_vs_original(\n",
    "        surrogate_preds, original_preds, \n",
    "        title=f\"{dense_layer}: Surrogate vs Original\"\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    # Check fidelity score\n",
    "    if dense_layer in surrogate_builder.fidelity_scores:\n",
    "        fidelity = surrogate_builder.fidelity_scores[dense_layer]\n",
    "        print(f\"Surrogate fidelity score: {fidelity}\")\n",
    "else:\n",
    "    print(\"No dense layer found for surrogate modeling\")\n",
    "\n",
    "# Clean up\n",
    "extractor.remove_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f528bf41",
   "metadata": {},
   "source": [
    "## 7. Analyze feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab4a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance from surrogate models\n",
    "print(\"Analyzing feature importance...\")\n",
    "\n",
    "if 'dense_surrogate' in locals() and dense_surrogate is not None:\n",
    "    # Get feature importances from the surrogate model\n",
    "    if hasattr(dense_surrogate, 'feature_importances_'):\n",
    "        feature_importances = dense_surrogate.feature_importances_\n",
    "        print(f\"Feature importances shape: {feature_importances.shape}\")\n",
    "        \n",
    "        # Reshape importances back to image shape for visualization\n",
    "        if len(feature_importances) == 28*28:  # MNIST flattened\n",
    "            importance_image = feature_importances.reshape(28, 28)\n",
    "            \n",
    "            # Display as heatmap\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(sample_image[0, :, :, 0], cmap='gray')\n",
    "            plt.title('Original Image')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(importance_image, cmap='hot')\n",
    "            plt.title(f'Feature Importance Heatmap\\n({dense_layer})')\n",
    "            plt.colorbar()\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Also plot top features as bar chart\n",
    "        from layerlens.utils.plot_utils import plot_feature_importance\n",
    "        \n",
    "        # Get top 20 most important features\n",
    "        top_indices = np.argsort(feature_importances)[-20:]\n",
    "        top_importances = feature_importances[top_indices]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(range(len(top_importances)), top_importances)\n",
    "        plt.xlabel('Feature Rank (Top 20)')\n",
    "        plt.ylabel('Importance')\n",
    "        plt.title(f'Top 20 Feature Importances ({dense_layer})')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Mean importance: {np.mean(feature_importances):.6f}\")\n",
    "        print(f\"Max importance: {np.max(feature_importances):.6f}\")\n",
    "        print(f\"Number of non-zero importances: {np.sum(feature_importances > 0)}\")\n",
    "    else:\n",
    "        print(\"Feature importances not available for this surrogate model type\")\n",
    "else:\n",
    "    print(\"No surrogate model available for feature importance analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e659dac",
   "metadata": {},
   "source": [
    "## 8. Visualize the model graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8dcbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model architecture\n",
    "print(\"Creating model graph visualization...\")\n",
    "from layerlens.visualization.layer_graph import plot_layer_graph, create_layer_graph_figure\n",
    "\n",
    "try:\n",
    "    # Create the graph visualization\n",
    "    if 'dense_layer' in locals() and dense_layer:\n",
    "        layer_graph = create_layer_graph_figure(model, highlighted_layer=dense_layer)\n",
    "    else:\n",
    "        layer_graph = create_layer_graph_figure(model)\n",
    "    \n",
    "    print(\"Model graph created successfully\")\n",
    "    print(\"Note: Use iplot(layer_graph) in Jupyter with plotly to display interactive graph\")\n",
    "    \n",
    "    # Alternative: Show model summary\n",
    "    print(\"\\nModel Architecture Summary:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Simple text representation of model structure\n",
    "    print(\"\\nLayer Structure:\")\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        print(f\"{i+1}. {layer.name} ({layer.__class__.__name__}) - Output: {layer.output_shape}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error creating graph visualization: {e}\")\n",
    "    print(\"Showing model summary instead:\")\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a6fcf8",
   "metadata": {},
   "source": [
    "## 9. Use the dashboard for interactive exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1638e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for the explanations\n",
    "print(\"LayerLens Explanation Visualizations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show what layers were explained\n",
    "if isinstance(explanations, dict):\n",
    "    layer_names = list(explanations.keys())\n",
    "    print(f\"✓ Generated explanations for {len(layer_names)} layers:\")\n",
    "    for i, name in enumerate(layer_names, 1):\n",
    "        print(f\"  {i}. {name}\")\n",
    "elif hasattr(explanations, 'layer_explanations'):\n",
    "    layer_names = list(explanations.layer_explanations.keys())\n",
    "    print(f\"✓ Generated explanations for {len(layer_names)} layers:\")\n",
    "    for i, name in enumerate(layer_names, 1):\n",
    "        print(f\"  {i}. {name}\")\n",
    "else:\n",
    "    layer_names = []\n",
    "    print(\"No layer explanations found\")\n",
    "\n",
    "# Create a simple visualization of the layer structure\n",
    "if layer_names:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Layer Output Shapes:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    from layerlens.core.layer_extractor import LayerExtractor\n",
    "    extractor = LayerExtractor(model)\n",
    "    sample_outputs = extractor.extract(x_test[:1])\n",
    "    \n",
    "    for layer_name in layer_names:\n",
    "        if layer_name in sample_outputs:\n",
    "            shape = sample_outputs[layer_name].shape\n",
    "            print(f\"{layer_name:15s} -> {str(shape):30s}\")\n",
    "    \n",
    "    extractor.remove_hooks()\n",
    "    \n",
    "    # Visualize the network architecture\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Model Architecture:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, layer in enumerate(model.layers):\n",
    "        layer_type = layer.__class__.__name__\n",
    "        output_shape = str(layer.output_shape)\n",
    "        print(f\"{i+1}. {layer.name:12s} ({layer_type:15s}) -> {output_shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Launching Interactive Dashboard...\")\n",
    "print(\"=\" * 50)\n",
    "print(\"# from layerlens.visualization.dashboard import show_dashboard\")\n",
    "print(\"# show_dashboard(explanations, port=8050)\")\n",
    "from layerlens.visualization.dashboard import show_dashboard\n",
    "show_dashboard(explanations, port=8050)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1458b234",
   "metadata": {},
   "source": [
    "## 10. Explore feature flow through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10e8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate monitoring capabilities\n",
    "print(\"LayerLens Monitoring Demo:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Set up drift detection\n",
    "from layerlens.monitoring.drift_detector import DriftDetector\n",
    "\n",
    "# Use a subset of training data as reference\n",
    "reference_data = x_train[:100]\n",
    "\n",
    "print(\"Setting up drift detector...\")\n",
    "drift_detector = DriftDetector(reference_data, sensitivity=0.05)\n",
    "\n",
    "# Simulate new data (with slight modifications to trigger drift)\n",
    "new_data = x_test[:50] + 0.1 * np.random.normal(0, 0.1, x_test[:50].shape)\n",
    "\n",
    "print(\"Detecting drift in new data...\")\n",
    "drift_results = drift_detector.detect(new_data)\n",
    "\n",
    "print(\"\\nDrift Detection Results:\")\n",
    "for key, result in drift_results.items():\n",
    "    if isinstance(result, dict):\n",
    "        drift_detected = result.get('drift_detected', False)\n",
    "        print(f\"Layer {key}: {'DRIFT DETECTED' if drift_detected else 'No drift'}\")\n",
    "        if 'mean_drift' in result:\n",
    "            print(f\"  - Mean drift: {result['mean_drift']:.4f}\")\n",
    "        if 'distribution_drift' in result:\n",
    "            print(f\"  - Distribution drift: {result['distribution_drift']:.4f}\")\n",
    "\n",
    "# Demonstrate alert system\n",
    "print(\"\\n\" + \"=\" * 30)\n",
    "print(\"Alert System Demo:\")\n",
    "from layerlens.monitoring.alert_system import AlertSystem\n",
    "\n",
    "alert_system = AlertSystem()\n",
    "\n",
    "# Configure alert system for logging\n",
    "alert_system.configure({\n",
    "    'channels': ['log'],\n",
    "    'email': {\n",
    "        'to': 'admin@example.com',\n",
    "        'from': 'alerts@layerlens.ai'\n",
    "    }\n",
    "})\n",
    "\n",
    "# Send a test alert\n",
    "alert_system.send_alert(\n",
    "    \"Model Drift Detection\",\n",
    "    \"Drift detected in model layers. Please review the model performance.\",\n",
    "    severity='warning'\n",
    ")\n",
    "\n",
    "print(\"Alert sent successfully!\")\n",
    "print(\"Check the logs for alert details.\")\n",
    "\n",
    "# Show alert history\n",
    "history = alert_system.get_alert_history()\n",
    "print(f\"\\nTotal alerts in history: {len(history)}\")\n",
    "if history:\n",
    "    latest = history[-1]\n",
    "    print(f\"Latest alert: {latest['title']} at {latest['timestamp']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76cc0a6",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "In this demo notebook, we've explored the key capabilities of LayerLens:\n",
    "\n",
    "### What We Demonstrated:\n",
    "1. **Model Training**: Trained a CNN on MNIST dataset\n",
    "2. **Layer Extraction**: Extracted intermediate layer outputs and activations\n",
    "3. **Surrogate Models**: Built interpretable surrogate models for complex layers\n",
    "4. **Feature Importance**: Analyzed which input features matter most for decisions\n",
    "5. **Visualization**: Created activation heatmaps and model structure graphs\n",
    "6. **Monitoring**: Set up drift detection and alert systems\n",
    "\n",
    "### Key LayerLens Features:\n",
    "- ✅ **Layer-by-Layer Analysis**: Understand how each layer contributes to predictions\n",
    "- ✅ **Surrogate Model Building**: Create interpretable models that approximate layer behavior\n",
    "- ✅ **Visual Explanations**: Generate heatmaps, importance plots, and interactive dashboards\n",
    "- ✅ **Production Monitoring**: Detect drift and model degradation in real-time\n",
    "- ✅ **Framework Agnostic**: Works with PyTorch, TensorFlow, and other frameworks\n",
    "\n",
    "### Production Usage:\n",
    "```python\n",
    "# Quick start for production use\n",
    "import layerlens as ll\n",
    "\n",
    "# 1. Create explainer\n",
    "explainer = ll.Explainer(your_model)\n",
    "\n",
    "# 2. Generate explanations\n",
    "explanations = explainer.explain(your_data)\n",
    "\n",
    "# 3. Monitor in production\n",
    "monitor = ll.monitor(your_model, reference_data)\n",
    "drift_results = monitor.detect(new_data)\n",
    "\n",
    "# 4. Launch dashboard\n",
    "ll.visualize(explanations, visualization_type='dashboard')\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "- Try LayerLens with your own models and datasets\n",
    "- Explore the interactive dashboard for deeper analysis\n",
    "- Set up monitoring for production models\n",
    "- Integrate with your ML pipeline for continuous explainability\n",
    "\n",
    "LayerLens provides a comprehensive toolkit for understanding, explaining, and monitoring deep learning models at the layer level!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
