import sys, json, io
import pickle, base64, importlib, numpy as np
from collections import OrderedDict
import os
from contextlib import contextmanager

func_name = "<func_name>"
module_name = "<module_name>"
is_lake_system = <is_lake_system>
params = json.loads('<params>')
data_partition_column_indices = <partition_cols_indices>
data_partition_column_types = <partition_cols_types>
model_file_prefix = "<model_file_prefix>" # Needed in case of lake system for writing model to /tmp

DELIMITER = '\t'

@contextmanager
def suppress_stderr():
    """
    Function to suppress the warnings(lake systems treats warnings as errors).
    """
    with open(os.devnull, "w") as devnull:
        old_stderr = sys.stderr
        sys.stderr = devnull
        try:
            yield
        finally:
            sys.stderr = old_stderr

## On Lake system warnings raised by script are treated as a errors.
## Hence, to suppress it putting the under suppress_stderr().
with suppress_stderr():
    def convert_to_type(val, typee):
        if typee == 'int':
            return int(val) if val != "" else np.nan
        if typee == 'float':
            if isinstance(val, str):
                val = val.replace(' ', '')
            return float(val) if val != "" else np.nan
        if typee == 'bool':
            return eval(val) if val != "" else None
        return str(val) if val != "" else None

    if not is_lake_system:
        db = sys.argv[0].split("/")[1]

    data_present = False
    data_partition_column_values = []

    while 1:
        try:
            line = input()
            if line == '':  # Exit if user provides blank line
                break
            else:
                data_present = True
                values = line.split(DELIMITER)
                if not data_partition_column_values:
                    # Partition column values is same for all rows. Hence, only read once.
                    for i, val in enumerate(data_partition_column_indices): # Only partition columns are
                        data_partition_column_values.append(
                            convert_to_type(values[val], typee=data_partition_column_types[i])
                            )

                    # Prepare the corresponding model file name and extract model.
                    partition_join = "_".join([str(x) for x in data_partition_column_values])
                    # Replace '-' with '_' because partition_columns can be negative containing '-'.
                    partition_join = partition_join.replace("-", "_")

                    train_set = params.get("train_set") # Gets file name prefix.
                    model_file_path = f"{train_set}_{partition_join}"\
                        if is_lake_system else \
                        f"./{db}/{train_set}_{partition_join}"

                    with open(model_file_path, "rb") as fp:
                        params["train_set"] = pickle.loads(fp.read())

                    valid_sets = params.get("valid_sets", None) # Gets file names prefix.
                    if valid_sets:
                        params["valid_sets"] = []
                        for valid_set in valid_sets:
                            model_file_path = f"{valid_set}_{partition_join}"\
                                if is_lake_system else \
                                f"./{db}/{valid_set}_{partition_join}"
                            with open(model_file_path, "rb") as fp:
                                params["valid_sets"].append(pickle.load(fp))

        except EOFError:  # Exit if reached EOF or CTRL-D
            break

    if not data_present:
        sys.exit(0)

    # Handle callbacks.
    rec_eval = None
    if "callbacks" in params and params["callbacks"] is not None:
        callbacks = params["callbacks"]
        callbacks = [callbacks] if not isinstance(callbacks, list) else callbacks
        for i, callback in enumerate(callbacks):
            c_module_name = callback["module"]
            c_func_name = callback["func_name"]
            c_kwargs = callback["kwargs"]
            c_module = importlib.import_module(c_module_name)
            if c_func_name == "record_evaluation":
                # record_evaluation function takes empty dict. If the argument has elements in the
                # dict, they will be deleted as per the documentation from lightgbm as described below:
                # eval_result (dict) -
                #   Dictionary used to store all evaluation results of all validation sets. This should
                #   be initialized outside of your call to record_evaluation() and should be empty. Any
                #   initial contents of the dictionary will be deleted.
                rec_eval = {}
                callbacks[i] = getattr(c_module, c_func_name)(rec_eval)
            else:
                callbacks[i] = getattr(c_module, c_func_name)(**c_kwargs)

        params["callbacks"] = callbacks

    module_ = importlib.import_module(module_name)

    ### LightGBM training is giving some meaningful console output like this:
    ### Hence, capturing it to show to the user.

    # [LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000190 seconds.
    # You can set `force_row_wise=true` to remove the overhead.
    # And if memory is not enough, you can set `force_col_wise=true`.
    # [LightGBM] [Info] Total Bins 136
    # [LightGBM] [Info] Number of data points in the train set: 97, number of used features: 4
    # [LightGBM] [Info] Start training from score 0.556701
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [1]	valid_0's l2: 0.219637	valid_1's l2: 0.219637
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [2]	valid_0's l2: 0.196525	valid_1's l2: 0.196525
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [3]	valid_0's l2: 0.178462	valid_1's l2: 0.178462
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [4]	valid_0's l2: 0.162887	valid_1's l2: 0.162887
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [5]	valid_0's l2: 0.150271	valid_1's l2: 0.150271
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [6]	valid_0's l2: 0.140219	valid_1's l2: 0.140219
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [7]	valid_0's l2: 0.131697	valid_1's l2: 0.131697
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [8]	valid_0's l2: 0.124056	valid_1's l2: 0.124056
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [9]	valid_0's l2: 0.117944	valid_1's l2: 0.117944
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [10]	valid_0's l2: 0.11263	valid_1's l2: 0.11263
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [11]	valid_0's l2: 0.105228	valid_1's l2: 0.105228
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [12]	valid_0's l2: 0.0981571	valid_1's l2: 0.0981571
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [13]	valid_0's l2: 0.0924294	valid_1's l2: 0.0924294
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [14]	valid_0's l2: 0.0877899	valid_1's l2: 0.0877899
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [15]	valid_0's l2: 0.084032	valid_1's l2: 0.084032
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [16]	valid_0's l2: 0.080988	valid_1's l2: 0.080988
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [17]	valid_0's l2: 0.0785224	valid_1's l2: 0.0785224
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [18]	valid_0's l2: 0.0765253	valid_1's l2: 0.0765253
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [19]	valid_0's l2: 0.0750803	valid_1's l2: 0.0750803
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [20]	valid_0's l2: 0.0738915	valid_1's l2: 0.0738915
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [21]	valid_0's l2: 0.07288	valid_1's l2: 0.07288
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [22]	valid_0's l2: 0.0718676	valid_1's l2: 0.0718676
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [23]	valid_0's l2: 0.0706037	valid_1's l2: 0.0706037
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [24]	valid_0's l2: 0.0695799	valid_1's l2: 0.0695799
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [25]	valid_0's l2: 0.0687507	valid_1's l2: 0.0687507
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [26]	valid_0's l2: 0.0680819	valid_1's l2: 0.0680819
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [27]	valid_0's l2: 0.0674077	valid_1's l2: 0.0674077
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [28]	valid_0's l2: 0.0665111	valid_1's l2: 0.0665111
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [29]	valid_0's l2: 0.0659656	valid_1's l2: 0.0659656
    # [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    # [30]	valid_0's l2: 0.0652665	valid_1's l2: 0.0652665
    result = ""
    stdout = None
    try:
        stdout = sys.stdout
        new_stdout = io.StringIO()
        sys.stdout = new_stdout
        trained_model = getattr(module_, func_name)(**params)
        result = new_stdout.getvalue()
    except Exception:
        raise
    finally:
        sys.stdout = stdout

    model_str = pickle.dumps(trained_model)
    console_output_str = result.encode()

    if is_lake_system:
        model_file_path = f"/tmp/{model_file_prefix}_{partition_join}.pickle"
        model_console_output_path = f"/tmp/{model_file_prefix}_{partition_join}_console_output.pickle"

        # Write to file in Vantage, to be used in predict/scoring.
        with open(model_file_path, "wb") as fp:
            fp.write(model_str)

        with open(model_console_output_path, "wb") as fpc:
            fpc.write(console_output_str)


    model_data = model_file_path if is_lake_system else base64.b64encode(model_str)
    console_output = model_console_output_path if is_lake_system else base64.b64encode(console_output_str)

    output_data = [model_data, console_output]

    if rec_eval is not None:
        rec_eval = pickle.dumps(rec_eval)
        if is_lake_system:
            rec_eval_file_path = f"/tmp/{model_file_prefix}_{partition_join}_rec_eval.pickle"

            with open(rec_eval_file_path, "wb") as fp:
                fp.write(rec_eval)

        rec_eval = rec_eval_file_path if is_lake_system else base64.b64encode(rec_eval)

        output_data.append(rec_eval)

    print(*(data_partition_column_values + output_data), sep=DELIMITER)
