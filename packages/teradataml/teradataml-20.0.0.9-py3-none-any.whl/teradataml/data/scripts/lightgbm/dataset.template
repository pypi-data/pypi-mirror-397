import pandas as pd
import pickle
import json
import numpy as np
import ast
import sys
from collections import OrderedDict
import base64
from importlib import import_module
import sys
import os
from contextlib import contextmanager

DELIMITER = "\t"

@contextmanager
def suppress_stderr():
    """
    Function to suppress the warnings(lake systems treats warnings as errors).
    """
    with open(os.devnull, "w") as devnull:
        old_stderr = sys.stderr
        sys.stderr = devnull
        try:
            yield
        finally:
            sys.stderr = old_stderr

## On Lake system warnings raised by script are treated as a errors.
## Hence, to suppress it putting the under suppress_stderr().
with suppress_stderr():
    def convert_to_type(val, typee):
        if typee == 'int':
            return int(val) if val != "" else np.nan
        if typee == 'float':
            if isinstance(val, str):
                val = val.replace(' ', '')
            return float(val) if val != "" else np.nan
        if typee == 'bool':
            return eval(val) if val != "" else None
        return str(val) if val != "" else None

    def splitter(strr, delim=",", convert_to="str"):
        """
        Split the string based on delimiter and convert to the type specified.
        """
        if strr == "None":
            return []
        return [convert_to_type(i, convert_to) for i in strr.split(delim)]


    is_lake_system = eval(sys.argv[2])
    model_file_prefix = sys.argv[1]
    if not is_lake_system:
        db = sys.argv[0].split("/")[1]

    ### Start of data related arguments processing
    data_partition_column_values = []
    data_present = False
    partition_join = ""
    model = None

    # Data related arguments information of indices and types.
    data_args_indices_types = OrderedDict()

    func_name = <func_name>
    module_name = <module_name>
    class_name = <class_name>
    all_col_names = <all_col_names>
    all_col_types = <types_of_data_cols>
    data_partition_column_indices = <partition_cols_indices>
    data_partition_column_types = [all_col_types[idx] for idx in data_partition_column_indices]

    # Data related arguments values - prepare dictionary and populate data later.
    data_args_values = {}

    data_args_info_str = <data_args_info_str>

    for data_arg in data_args_info_str.split("--"):
        _arg_name, _indices, _types = data_arg.split("-")
        _indices = splitter(_indices, convert_to="int")
        types = [type_ for idx, type_ in enumerate(all_col_types) if idx in _indices]

        data_args_indices_types[_arg_name] = {"indices": _indices, "types": types}
        data_args_values[_arg_name] = [] # Keeping empty for each data arg name and populate data later.

    ### End of data related arguments processing


    ### Start of other arguments processing
    params = json.loads('<params>')
    ### End of other arguments processing


    # Read data - columns information is passed as command line argument and stored in
    # data_args_indices_types dictionary.
    while 1:
        try:
            line = input()
            if line == '':  # Exit if user provides blank line
                break
            else:
                data_present = True
                values = line.split(DELIMITER)
                if not data_partition_column_values:
                    # Partition column values is same for all rows. Hence, only read once.
                    for i, val in enumerate(data_partition_column_indices):
                        data_partition_column_values.append(
                            convert_to_type(values[val], typee=data_partition_column_types[i])
                            )

                    # Prepare the corresponding model file name and extract model.
                    partition_join = "_".join([str(x) for x in data_partition_column_values])
                    # Replace '-' with '_' as '-' because partition_columns can be negative.
                    partition_join = partition_join.replace("-", "_")

                    model_file_path = f"{model_file_prefix}_{partition_join}"\
                        if is_lake_system else \
                        f"./{db}/{model_file_prefix}_{partition_join}"

                # Prepare data dictionary containing only arguments related to data.
                for arg_name in data_args_values:
                    data_indices = data_args_indices_types[arg_name]["indices"]
                    types = data_args_indices_types[arg_name]["types"]
                    cur_row = []
                    for idx, data_idx in enumerate(data_indices):
                        cur_row.append(convert_to_type(values[data_idx], types[idx]))
                    data_args_values[arg_name].append(cur_row)
        except EOFError:  # Exit if reached EOF or CTRL-D
            break

    if not data_present:
        sys.exit(0)

    for key, value in data_args_values.items():
        col_names = [all_col_names[idx] for idx in data_args_indices_types[key]["indices"]]
        data_args_values[key] = pd.DataFrame(value, columns=col_names)

    # If reference argument (is a Dataset object) present in params, then it contains
    # the prefix of the file path which contains the reference Dataset object.
    if "reference" in params.keys() and params["reference"] is not None:
        reference_dataset_file_prefix = params["reference"]
        reference_arg_file_path = f"{reference_dataset_file_prefix}_{partition_join}"\
                        if is_lake_system else \
                        f"./{db}/{reference_dataset_file_prefix}_{partition_join}"
        with open(reference_arg_file_path, "rb") as f:
            params["reference"] = pickle.load(f)

    if not func_name:
        # Create DataSet object if no function of Dataset class is called.
        lib = import_module(module_name)
        class_instance = getattr(lib, class_name)
        obj = class_instance(**{**data_args_values, **params})
    else:
        # If function of Dataset object is called, then call the function on model object.
        with open(model_file_path, "rb") as fp:
            model = pickle.loads(fp.read())

        if not model:
            sys.exit("Model file is not installed in Vantage.")

        obj = getattr(model, func_name)(**{**data_args_values, **params})

    model_str = pickle.dumps(obj)

    if is_lake_system:
        model_file_path = f"/tmp/{model_file_prefix}_{partition_join}.pickle"

    # Save DataSet object to binary file
    with open(model_file_path, "wb") as f:
        f.write(model_str)

    model_data = model_file_path if is_lake_system else base64.b64encode(model_str)

    print(*(data_partition_column_values + [model_data]), sep=DELIMITER)