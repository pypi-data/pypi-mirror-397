import sys, json, io
import pickle, base64, importlib, numpy as np
from collections import OrderedDict
import os
from contextlib import contextmanager

DELIMITER = '\t'

func_name = <func_name>
params = json.loads('<params>')
is_lake_system = <is_lake_system>
model_file_prefix = <model_file_prefix>

@contextmanager
def suppress_stderr():
    """
    Function to suppress the warnings(lake systems treats warnings as errors).
    """
    with open(os.devnull, "w") as devnull:
        old_stderr = sys.stderr
        sys.stderr = devnull
        try:
            yield
        finally:
            sys.stderr = old_stderr

## On Lake system warnings raised by script are treated as a errors.
## Hence, to suppress it putting the under suppress_stderr().
with suppress_stderr():
    def convert_to_type(val, typee):
        if typee == 'int':
            return int(val) if val != "" else np.nan
        if typee == 'float':
            if isinstance(val, str):
                val = val.replace(' ', '')
            return float(val) if val != "" else np.nan
        if typee == 'bool':
            return eval(val) if val != "" else None
        return str(val) if val != "" else None

    def splitter(strr, delim=",", convert_to="str"):
        """
        Split the string based on delimiter and convert to the type specified.
        """
        if strr == "None":
            return []
        return [convert_to_type(i, convert_to) for i in strr.split(delim)]


    if not is_lake_system:
        db = sys.argv[0].split("/")[1]

    data_partition_column_indices = <partition_cols_indices>
    data_column_types = <types_of_data_cols>

    data_partition_column_types = [data_column_types[idx] for idx in data_partition_column_indices]

    # Data related arguments information of indices and types.
    data_args_indices_types = OrderedDict()

    # Data related arguments values - prepare dictionary and populate data later.
    data_args_values = {}

    data_args_info_str = <data_args_info_str>
    for data_arg in data_args_info_str.split("--"):
        arg_name, indices, types = data_arg.split("-")
        indices = splitter(indices, convert_to="int")
        types = splitter(types)

        data_args_indices_types[arg_name] = {"indices": indices, "types": types}
        data_args_values[arg_name] = [] # Keeping empty for each data arg name and populate data later.

    data_partition_column_values = []
    data_present = False

    model = None

    # Read data - columns information is passed as command line argument and stored in
    # data_args_indices_types dictionary.
    while 1:
        try:
            line = input()
            if line == '':  # Exit if user provides blank line
                break
            else:
                data_present = True
                values = line.split(DELIMITER)
                if not data_partition_column_values:
                    # Partition column values is same for all rows. Hence, only read once.
                    for i, val in enumerate(data_partition_column_indices):
                        data_partition_column_values.append(
                            convert_to_type(values[val], typee=data_partition_column_types[i])
                            )

                    # Prepare the corresponding model file name and extract model.
                    partition_join = "_".join([str(x) for x in data_partition_column_values])
                    # Replace '-' with '_' as '-' because partition_columns can be negative.
                    partition_join = partition_join.replace("-", "_")


                    model_file_path = f"{model_file_prefix}_{partition_join}"\
                        if is_lake_system else \
                        f"./{db}/{model_file_prefix}_{partition_join}"

                    with open(model_file_path, "rb") as fp:
                        model = pickle.loads(fp.read())

                    if model is None:
                        sys.exit("Model file is not installed in Vantage.")

                # Prepare data dictionary containing only arguments related to data.
                for arg_name in data_args_values:
                    data_indices = data_args_indices_types[arg_name]["indices"]
                    types = data_args_indices_types[arg_name]["types"]
                    cur_row = []
                    for idx, data_idx in enumerate(data_indices):
                        cur_row.append(convert_to_type(values[data_idx], types[idx]))
                    data_args_values[arg_name].append(cur_row)
        except EOFError:  # Exit if reached EOF or CTRL-D
            break

    if not data_present:
        sys.exit(0)

    # Handle callbacks.
    rec_eval = None
    if "callbacks" in params and params["callbacks"] is not None:
        callbacks = params["callbacks"]
        callbacks = [callbacks] if not isinstance(callbacks, list) else callbacks
        for i, callback in enumerate(callbacks):
            c_module_name = callback["module"]
            c_func_name = callback["func_name"]
            c_kwargs = callback["kwargs"]
            c_module = importlib.import_module(c_module_name)
            if c_func_name == "record_evaluation":
                # record_evaluation function takes empty dict. If the argument has elements in the
                # dict, they will be deleted as per the documentation from lightgbm as described below:
                # eval_result (dict) -
                #   Dictionary used to store all evaluation results of all validation sets. This should
                #   be initialized outside of your call to record_evaluation() and should be empty. Any
                #   initial contents of the dictionary will be deleted.
                rec_eval = {}
                callbacks[i] = getattr(c_module, c_func_name)(rec_eval)
            else:
                callbacks[i] = getattr(c_module, c_func_name)(**c_kwargs)

        params["callbacks"] = callbacks

    # Update data as numpy arrays.
    for arg_name in data_args_values:
        np_values = np.array(data_args_values[arg_name])
        data_args_values[arg_name] = np_values
        if arg_name in ["y", "sample_weight"]:
            data_args_values[arg_name] = np_values.ravel()

    # Combine all arguments.
    all_args = {**data_args_values, **params}

    result = ""
    stdout = None
    try:
        stdout = sys.stdout
        new_stdout = io.StringIO()
        sys.stdout = new_stdout
        trained_model = getattr(model, func_name)(**all_args)
        result = new_stdout.getvalue()
    except Exception:
        raise
    finally:
        sys.stdout = stdout

    model_data = 0
    if func_name == "fit":
        model_str = pickle.dumps(trained_model)
        console_output_str = result.encode()

        if is_lake_system:
            model_file_path = f"/tmp/{model_file_prefix}_{partition_join}.pickle"
            model_console_output_path = f"/tmp/{model_file_prefix}_{partition_join}_console_output.pickle"

            with open(model_console_output_path, "wb") as fpc:
                fpc.write(console_output_str)

        # Write to file in Vantage, to be used in predict/scoring.
        with open(model_file_path, "wb") as fp:
            fp.write(model_str)

        model_data = model_file_path if is_lake_system else base64.b64encode(model_str)
        console_output = model_console_output_path if is_lake_system else base64.b64encode(console_output_str)

        output_data = [model_data, console_output]

        print(*(data_partition_column_values + output_data), sep=DELIMITER)

    elif func_name == "score":
        model_data = trained_model

        print(*(data_partition_column_values + [model_data]), sep=DELIMITER)
