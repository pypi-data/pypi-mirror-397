import base64
import io
import math
import os
import pickle
import sys
import os
from contextlib import contextmanager
import numpy as np

DELIMITER = '\t'

@contextmanager
def suppress_stderr():
    """
    Function to suppress the warnings(lake systems treats warnings as errors).
    """
    with open(os.devnull, "w") as devnull:
        old_stderr = sys.stderr
        sys.stderr = devnull
        try:
            yield
        finally:
            sys.stderr = old_stderr

## On Lake system warnings raised by script are treated as a errors.
## Hence, to suppress it putting the under suppress_stderr().
with suppress_stderr():
    def get_values_list(values, types):
        ret_vals = []
        for i, val in enumerate(values):
            ret_vals.append(convert_to_type(val, types[i]))
        return ret_vals

    def convert_to_type(val, typee):
        if typee == 'int':
            return int(val) if val != "" else np.nan
        if typee == 'float':
            if isinstance(val, str):
                val = val.replace(' ', '')
            return float(val) if val != "" else np.nan
        if typee == 'bool':
            return eval(val) if val != "" else None
        return str(val) if val != "" else None

    def splitter(strr, delim=",", convert_to="str"):
        """
        Split the string based on delimiter and convert to the type specified.
        """
        if strr == "None":
            return []
        return [convert_to_type(i, convert_to) for i in strr.split(delim)]

    def should_convert(t_val, py_type):
        """
        Function to check type of value and whether value is nan and infinity.
        """
        return not isinstance(t_val, eval(py_type)) and not math.isinf(t_val) and not math.isnan(t_val)

    def convert_value(t_val, py_type):
        """
        Function to convert value to specified python type.
        """
        return convert_to_type(t_val, py_type) if should_convert(t_val, py_type) else t_val

    # Process output returned by sklearn function.
    def get_output_data(trans_values, func_name, n_c_labels, n_out_columns):
        # Converting    sparse matrix to dense array as sparse matrices are NOT
        # supported in Vantage.
        # module_name = model_obj.__module__.split("._")[0]

        # Converting the translated values into corresponding the return column's
        # python type.
        if (return_columns_python_types is None or not isinstance(trans_values, np.ndarray)):
            trans_values_list = trans_values
        else:
            # Conversion.
            trans_values_list = []
            for trans_value in trans_values.tolist():
                if not isinstance(trans_value, list):
                    trans_value = [trans_value]

                converted_list = []
                if len(return_columns_python_types) == len(trans_value):
                    for t_val, py_type in zip(trans_value, return_columns_python_types):
                        converted_list.append(convert_value(t_val, py_type))
                ## transform() is having only 1 python return type, But it actually returns more than 1 column.
                else:
                    for t_val in trans_value:
                        converted_list.append(convert_value(t_val, return_columns_python_types[0]))

                trans_values_list.append(converted_list)

        if type(trans_values_list).__name__ in ["csr_matrix", "csc_matrix"]:
            trans_values_list = trans_values_list.toarray()

        if isinstance(trans_values_list[0], np.ndarray) \
                or isinstance(trans_values_list[0], list) \
                or isinstance(trans_values_list[0], tuple):
            # Here, the value returned by sklearn function is list type.
            opt_list = list(trans_values_list[0])

            if len(opt_list) < n_out_columns:
                # If the output list is less than the required number of columns, append
                # empty strings to the list.
                opt_list += [""] * (n_out_columns - len(opt_list))

            return opt_list

        # Only one element is returned by the function.
        return [trans_values_list[0]]

    # Arguments to the Script
    if len(sys.argv) != 10:
        # 10 arguments command line arguments should be passed to this file.
        # 1: file to be run
        # 2. function name (Eg. predict, fit etc)
        # 3. No of feature columns.
        # 4. No of class labels.
        # 5. Comma separated indices of partition columns.
        # 6. Comma separated types of all the data columns.
        # 7. Model file prefix to generated model file using partition columns.
        # 8. Number of columns to be returned by the sklearn's transform function.
        # 9. Flag to check the system type. True, means Lake, Enterprise otherwise.
        # 10. Python types of returned/transfromed columns.
        sys.exit("10 arguments should be passed to this file - file to be run, function name, "\
                     "no of feature columns, no of class labels, comma separated indices of partition "\
                     "columns, comma separated types of all columns, model file prefix to generate model "\
                     "file using partition columns, number of columns to be returnd by sklearn's "\
                     "transform function, flag to check lake or enterprise and Python types of "\
                     "returned/transfromed columns.")

    is_lake_system = eval(sys.argv[8])
    if not is_lake_system:
        db = sys.argv[0].split("/")[1]
    func_name = sys.argv[1]
    n_f_cols = int(sys.argv[2])
    n_c_labels = int(sys.argv[3])
    data_column_types = splitter(sys.argv[5], delim="--")
    data_partition_column_indices = splitter(sys.argv[4], convert_to="int") # indices are integers.
    model_file_prefix = sys.argv[6]
    # sys.argv[9] will contain a string of python datatypes with '--'
    # separator OR a single datatype OR None in string format.
    ret_col_argv = sys.argv[9]
    if ret_col_argv == "None":
        return_columns_python_types = eval(ret_col_argv)
    else:
        return_columns_python_types = splitter(ret_col_argv, delim="--")

    no_of_output_columns = int(sys.argv[7])

    data_partition_column_types = [data_column_types[idx] for idx in data_partition_column_indices]

    model = None
    data_partition_column_values = []

    all_x_rows = []
    all_y_rows = []

    # Data Format:
    # feature1, feature2, ..., featuren, label1, label2, ... labelk, data_partition_column1, ...,
    # data_partition_columnn.
    # label is optional (it is present when label_exists is not "None")

    model_name = ""
    while 1:
        try:
            line = input()
            if line == '':  # Exit if user provides blank line
                break
            else:
                values = line.split(DELIMITER)
                values = get_values_list(values, data_column_types)
                if not data_partition_column_values:
                    # Partition column values is same for all rows. Hence, only read once.
                    for i, val in enumerate(data_partition_column_indices):
                        data_partition_column_values.append(
                            convert_to_type(values[val], typee=data_partition_column_types[i])
                            )

                    # Prepare the corresponding model file name and extract model.
                    partition_join = "_".join([str(x) for x in data_partition_column_values])
                    # Replace '-' with '_' as '-' because partition_columns can be negative.
                    partition_join = partition_join.replace("-", "_")

                    model_file_path = f"{model_file_prefix}_{partition_join}" \
                        if is_lake_system else \
                        f"./{db}/{model_file_prefix}_{partition_join}"

                    with open(model_file_path, "rb") as fp:
                        model = pickle.loads(fp.read())

                    if not model:
                        sys.exit("Model file is not installed in Vantage.")

                f_ = values[:n_f_cols]
                f__ = np.array([f_])

                if n_c_labels > 0:
                    l_ = values[n_f_cols:n_f_cols+n_c_labels]
                    l__ = np.array([l_])

                if func_name == "refit":
                    # refit() needs all data at once. Hence, read all data at once and call refit().
                    all_x_rows.append(f_)
                    all_y_rows.append(l_)
                    continue

                # Because `predict` function does not accept 'y' as input, we need to handle it separately.
                if n_c_labels > 0 and func_name not in ["predict"]:
                    # Labels are present in last column.
                    trans_values = getattr(model, func_name)(f__, l__, **params)
                else:
                    # If class labels do not exist in data, don't read labels, read just features.
                    trans_values = getattr(model, func_name)(f__, **params)

                result_list = f_
                if n_c_labels > 0 and func_name in ["predict", "decision_function"]:
                    result_list += l_
                result_list += get_output_data(trans_values=trans_values, func_name=func_name,
                                               n_c_labels=n_c_labels, n_out_columns=no_of_output_columns)

                for i, val in enumerate(result_list):
                    if (val is None or (not isinstance(val, str) and (math.isnan(val) or math.isinf(val)))):
                        result_list[i] = ""
                    elif val == False:
                        result_list[i] = 0
                    elif val == True:
                        result_list[i] = 1

                print(*(data_partition_column_values + result_list), sep=DELIMITER)

        except EOFError:  # Exit if reached EOF or CTRL-D
            break


    if func_name == "refit":
        result = ""
        stdout = None
        try:
            stdout = sys.stdout
            new_stdout = io.StringIO()
            sys.stdout = new_stdout
            trained_model = getattr(model, func_name)(all_x_rows, all_y_rows, **params)
            result = new_stdout.getvalue()
        except Exception:
            raise
        finally:
            sys.stdout = stdout

        model_str = pickle.dumps(trained_model)


        if is_lake_system:
            model_file_path = f"/tmp/{model_file_prefix}_{partition_join}.pickle"

        # Write to trained model file in Vantage.
        with open(model_file_path, "wb") as fp:
            fp.write(model_str)

        model_data = model_file_path if is_lake_system else base64.b64encode(model_str)
        console_output = base64.b64encode(result.encode())

        print(*(data_partition_column_values + [model_data, console_output]), sep="..")
