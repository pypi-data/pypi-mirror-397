import sys, json
import pickle, base64, importlib, numpy as np
from collections import OrderedDict
import os
from contextlib import contextmanager

func_name = "<func_name>"
module_name = "<module_name>"
params = json.loads('<params>')

DELIMITER = '\t'

@contextmanager
def suppress_stderr():
    """
    Function to suppress the warnings(lake systems treats warnings as errors).
    """
    with open(os.devnull, "w") as devnull:
        old_stderr = sys.stderr
        sys.stderr = devnull
        try:
            yield
        finally:
            sys.stderr = old_stderr

## On Lake system warnings raised by script are treated as a errors.
## Hence, to suppress it putting the under suppress_stderr().
with suppress_stderr():
    def convert_to_type(val, typee):
        if typee == 'int':
            return int(val) if val != "" else np.nan
        if typee == 'float':
            if isinstance(val, str):
                val = val.replace(' ', '')
            return float(val) if val != "" else np.nan
        if typee == 'bool':
            return eval(val) if val != "" else None
        return str(val) if val != "" else None

    def splitter(strr, delim=",", convert_to="str"):
        """
        Split the string based on delimiter and convert to the type specified.
        """
        if strr == "None":
            return []
        return [convert_to_type(i, convert_to) for i in strr.split(delim)]

    # Arguments to the Script.
    if len(sys.argv) != 3:
        # 3 command line arguments should be passed to this file.
        # 1: file to be run
        # 2. Model file prefix for lake system, None otherwise.
        # 3. Flag to check the system type. True, means Lake, Enterprise otherwise.
        sys.exit("3 arguments command line arguments should be passed: file to be run,"
                 " model file prefix used only for lake system and flag to check lake or enterprise.")

    is_lake_system = eval(sys.argv[2])
    if not is_lake_system:
        db = sys.argv[0].split("/")[1]
    else:
        model_file_prefix = sys.argv[1]

    data_partition_column_indices = <partition_cols_indices>
    data_column_types = <types_of_data_cols>

    data_partition_column_types = [data_column_types[idx] for idx in data_partition_column_indices]

    # Data related arguments information of indices and types.
    data_args_indices_types = OrderedDict()

    # Data related arguments values - prepare dictionary and populate data later.
    data_args_values = {}

    data_args_info_str = <data_args_info_str>
    for data_arg in data_args_info_str.split("--"):
        arg_name, indices, types = data_arg.split("-")
        indices = splitter(indices, convert_to="int")
        types = splitter(types)

        data_args_indices_types[arg_name] = {"indices": indices, "types": types}
        data_args_values[arg_name] = [] # Keeping empty for each data arg name and populate data later.

    data_partition_column_values = []
    data_present = False

    # Read data - columns information is passed as command line argument and stored in
    # data_args_indices_types dictionary.
    while 1:
        try:
            line = input()
            if line == '':  # Exit if user provides blank line
                break
            else:
                data_present = True
                values = line.split(DELIMITER)
                if not data_partition_column_values:
                    # Partition column values is same for all rows. Hence, only read once.
                    for i, val in enumerate(data_partition_column_indices):
                        data_partition_column_values.append(
                            convert_to_type(values[val], typee=data_partition_column_types[i])
                            )

                    # Prepare the corresponding model file name and extract model.
                    partition_join = "_".join([str(x) for x in data_partition_column_values])
                    # Replace '-' with '_' as '-' because partition_columns can be negative.
                    partition_join = partition_join.replace("-", "_")

                # Prepare data dictionary containing only arguments related to data.
                for arg_name in data_args_values:
                    data_indices = data_args_indices_types[arg_name]["indices"]
                    types = data_args_indices_types[arg_name]["types"]
                    cur_row = []
                    for idx, data_idx in enumerate(data_indices):
                        cur_row.append(convert_to_type(values[data_idx], types[idx]))
                    data_args_values[arg_name].append(cur_row)
        except EOFError:  # Exit if reached EOF or CTRL-D
            break

    if not data_present:
        sys.exit(0)

    # Update data as numpy arrays.
    for arg_name in data_args_values:
        np_values = np.array(data_args_values[arg_name])
        data_args_values[arg_name] = np_values

    # Combine all arguments.
    all_args = {**data_args_values, **params}

    module_ = importlib.import_module(module_name)
    sklearn_model = getattr(module_, func_name)(**all_args)

    model_str = pickle.dumps(sklearn_model)

    if is_lake_system:
        model_file_path = f"/tmp/{model_file_prefix}_{partition_join}.pickle"

        # Write to file in Vantage, to be used in predict/scoring.
        with open(model_file_path, "wb") as fp:
            fp.write(model_str)

    model_data = model_file_path if is_lake_system else base64.b64encode(model_str)

    print(*(data_partition_column_values + [model_data]), sep=DELIMITER)
