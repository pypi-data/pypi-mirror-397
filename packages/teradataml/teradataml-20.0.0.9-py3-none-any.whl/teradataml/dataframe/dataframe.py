# -*- coding: utf-8 -*-
"""

Unpublished work.
Copyright (c) 2018 by Teradata Corporation. All rights reserved.
TERADATA CORPORATION CONFIDENTIAL AND TRADE SECRET

Primary Owner: ellen.teradata@teradata.com
Secondary Owner:

This file implements the teradataml dataframe.
A teradataml dataframe maps virtually to teradata tables and views.
"""
import decimal
import inspect
import itertools
import json
import numbers
import re
import random
import sys
import urllib.parse
from collections import OrderedDict
from collections.abc import Iterator
import numpy as np
import pandas as pd
import sqlalchemy
from sqlalchemy import Column
from sqlalchemy.exc import NoSuchColumnError
from sqlalchemy.sql import ClauseElement, literal_column
from datetime import datetime, date
from teradatasql import OperationalError
from teradatasqlalchemy import VARCHAR, types as tdtypes
from teradatasqlalchemy.dialect import dialect as td_dialect
from teradatasqlalchemy.dialect import preparer
from teradatasqlalchemy.types import (BIGINT, BYTEINT, DECIMAL, FLOAT, INTEGER,
                                      PERIOD_TIMESTAMP, SMALLINT, _TDType)

import teradataml.context.context as tdmlctx
from teradataml import GarbageCollector, execute_sql
from teradataml.common.bulk_exposed_utils import \
    _validate_unimplemented_function
from teradataml.common.constants import (AEDConstants, DataFrameTypes, OutputStyle,
                                         PTITableConstants, PythonTypes,
                                         SourceType, SQLConstants,
                                         SQLFunctionConstants,
                                         TableOperatorConstants,
                                         TeradataConstants, TeradataTypes)
from teradataml.common.exceptions import TeradataMlException
from teradataml.common.messagecodes import MessageCodes
from teradataml.common.messages import Messages
from teradataml.common.sqlbundle import SQLBundle
from teradataml.common.utils import UtilFuncs
from teradataml.dataframe.copy_to import copy_to_sql
from teradataml.dataframe.data_transfer import _DataTransferUtils
from teradataml.dataframe.dataframe_utils import DataFrameUtils
from teradataml.dataframe.dataframe_utils import DataFrameUtils as df_utils
from teradataml.dataframe.indexer import _LocationIndexer
from teradataml.dataframe.row import _Row
from teradataml.dataframe.setop import concat
from teradataml.dataframe.sql import _MetaExpression, _SQLColumnExpression
from teradataml.dataframe.sql_functions import case
from teradataml.dataframe.sql_interfaces import ColumnExpression
from teradataml.dataframe.window import Window
from teradataml.dbutils.dbutils import list_td_reserved_keywords
from teradataml.options.configure import configure
from teradataml.options.display import display
from teradataml.plot.plot import _Plot
from teradataml.scriptmgmt.UserEnv import UserEnv
from teradataml.series.series import Series
from teradataml.table_operators.table_operator_util import _TableOperatorUtils
from teradataml.telemetry_utils.queryband import collect_queryband
from teradataml.utils.dtypes import _Dtypes, _ListOf, _TupleOf
from teradataml.utils.validators import _Validators
from teradataml.utils.udt_utils import _get_or_create_internal_array_udt
from teradataml.dataframe.array import Array
# Adding imports at the end to avoid circular imports.
from teradataml.common.aed_utils import AedUtils

# TODO use logger when available on master branch
# logger = teradatapylog.getLogger()


class in_schema:
    """
    Class takes a schema name, a table name and datalake name attributes
    and creates an object that can be passed to DataFrame.
    Note:
        teradataml recommends to use this class to access table(s)/view(s),
        from the database other than the default database.
    """
    def __init__(self, schema_name, table_name, datalake_name=None):
        """
        Constructor for in_schema class.

        PARAMETERS:
            schema_name:
                Required Argument.
                Specifies the schema where the table resides in.
                Types: str

            table_name:
                Required Argument.
                Specifies the table name or view name in Vantage.
                Types: str

            datalake_name:
                Optional Argument.
                Specifies the datalake name.
                Types: str

        EXAMPLES:
            from teradataml.dataframe.dataframe import in_schema, DataFrame

            # Example 1: The following example creates a DataFrame from the
            #            existing Vantage table "dbcinfo" in the non-default
            #            database "dbc" using the in_schema instance.
            df = DataFrame(in_schema("dbc", "dbcinfo"))

            # Example 2: The following example uses from_table() function, existing
            #            Vantage table "dbcinfo" and non-default database "dbc" to
            #            create a teradataml DataFrame.
            df = DataFrame.from_table(in_schema("dbc","dbcinfo"))

            # Example 3: The following example uses "in_schema" object created
            #            with "datalake_name" argument to create DataFrame on OTF table.
            otf_df = DataFrame(in_schema("datalake_db","datalake_table","datalake"))

        """
        self.schema_name = schema_name
        self.table_name = table_name
        self.datalake_name = datalake_name

        awu_matrix = []
        awu_matrix.append(["schema_name", schema_name, False, (str), True])
        awu_matrix.append(["table_name", table_name, False, (str), True])
        awu_matrix.append(["datalake_name", datalake_name, True, (str), True])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

    def __str__(self):
        """
        Returns the string representation of in_schema instance.
        """
        tbl_name = '{}.{}'.format(UtilFuncs._teradata_quote_arg(self.schema_name, "\"", False),
                                  UtilFuncs._teradata_quote_arg(self.table_name, "\"", False))

        if not self.datalake_name:
            return tbl_name

        return '{}.{}'.format(UtilFuncs._teradata_quote_arg(self.datalake_name, "\"", False), tbl_name)


in_schema = in_schema


class DataFrame():
    """
    The teradataml DataFrame enables data manipulation, exploration, and analysis
    on tables, views, and queries on Teradata Vantage.
    """

    def __init__(self, data=None, index=True, index_label=None, query=None, materialize=False, **kwargs):
        """
        Constructor for teradataml DataFrame.

        PARAMETERS:
            data:
                Optional Argument.
                Specifies the input data to create a teradataml DataFrame.
                Notes:
                    * If a dictionary is provided, it must follow the below requirements:
                        * Keys must be strings (column names).
                        * Values must be lists of equal length (column data).
                        * Nested dictionaries are not supported.
                    *  If the first row of an array column contains an empty list, 
                       the column type defaults to ARRAY_VARCHAR with scope 100.
                Types: str OR pandas DataFrame OR in_schema OR numpy array OR list OR dictionary

            index:
                Optional Argument.
                If "data" is a string, then the argument specifies whether to use the index column
                for sorting or not.
                If "data" is a pandas DataFrame, then this argument specifies whether to
                save Pandas DataFrame index as a column or not.
                Default Value: True
                Types: bool

            index_label:
                Optional Argument.
                If "data" is a string, then the argument specifies column(s) used for sorting.
                If "data" is a pandas DataFrame, then the default behavior is applied.
                Note:
                    * Refer to the "index_label" parameter of copy_to_sql() for details on the default behaviour.
                Types: str OR list of str

            query:
                Optional Argument.
                SQL query for this Dataframe. Used by class method from_query.
                Types: str

            materialize:
                Optional Argument.
                Whether to materialize DataFrame or not when created.
                Used by class method from_query.

                You should use materialization, when the query  passed to from_query(),
                is expected to produce non-deterministic results, when it is executed multiple
                times. Using this option will help user to have deterministic results in the
                resulting teradataml DataFrame.
                Default Value: False (No materialization)
                Types: bool

            kwargs:
                table_name:
                    Optional Argument.
                    The table name or view name in Teradata Vantage referenced by this DataFrame.
                    Note:
                        * If "data" and "table_name" are both specified, then the "table_name" argument is ignored.
                    Types: str or in_schema

                primary_index:
                    Optional Argument.
                    Specifies which column(s) to use as primary index for the teradataml DataFrame.
                    Note:
                        * This argument is only applicable when creating a DataFrame from a pandas DataFrame.
                    Types: str OR list of str

                types:
                    Optional Argument.
                    Specifies required data types for requested columns to be saved in Teradata Vantage.
                    Notes:
                        * This argument is not applicable when "data" argument is of type str or in_schema.
                        * Refer to the "types" parameter of copy_to_sql() for more details.
                    Types: dict

                columns:
                    Optional Argument.
                    Specifies the names of the columns to be used in the DataFrame.
                    Notes:
                        * This argument is not applicable when "data" argument is of type str or in_schema.
                        * If "data" is a dictionary and this argument is specified, only the specified columns will be
                          included in the DataFrame if the dictionary contains those keys. If the dictionary does not
                          contain the specified keys, those columns will be added with NaN values.
                    Types: str OR list of str
                
                persist:
                    Optional Argument.
                    Specifies whether to persist the DataFrame.
                    Note:
                        * This argument is only applicable when the "data" argument is of type dict, list or 
                          pandas DataFrame.
                    Default Value: False
                    Types: bool

        EXAMPLES:
            >>> from teradataml.dataframe.dataframe import DataFrame
            >>> import pandas as pd

            # Example 1: Create a teradataml DataFrame from table name.
            >>> df = DataFrame("mytab")

            # Example 2: Create a teradataml DataFrame from view name.
            >>> df = DataFrame("myview")

            # Example 3: Create a teradataml DataFrame using view name without using index column for sorting.
            >>> df = DataFrame("myview", False)

            # Example 4: Create a teradataml DataFrame using table name and consider columns Col1 and Col2
            #            while running DataFrame.head() or DataFrame.tail() methods.
            >>> df = DataFrame("mytab", True, ["Col1", "Col2"])

            # Example 5: Create a teradataml DataFrame from the existing Vantage table "dbcinfo"
            #            in the non-default database "dbc" using the in_schema() object.
            >>> from teradataml.dataframe.dataframe import in_schema
            >>> df = DataFrame(in_schema("dbc", "dbcinfo"))

            # Example 6: Create a teradataml DataFrame from a pandas DataFrame.
            >>> pdf = pd.DataFrame({"col1": [1, 2, 3], "col2": [4, 5, 6]})
            >>> df = DataFrame(pdf)
            >>> df
               col1  col2  index_label
            0     3     6            2
            1     2     5            1
            2     1     4            0

            # Example 7: Create a teradataml DataFrame from a pandas DataFrame without index column.
            >>> pdf = pd.DataFrame({"col1": [1, 2, 3], "col2": [4, 5, 6]})
            >>> df = DataFrame(data=pdf, index=False)
            >>> df
               col1  col2
            0     3     6
            1     2     5
            2     1     4

            # Example 8: Create a teradataml DataFrame from a pandas DataFrame with
            #            index label and primary index as 'id'.
            >>> pdf = pd.DataFrame({"col1": [1, 2, 3], "col2": [4, 5, 6]})
            >>> df = DataFrame(pdf, index=True, index_label='id', primary_index='id')
            >>> df
                col1  col2
            id
            2      3     6
            1      2     5
            0      1     4

            # Example 9: Create a teradataml DataFrame from list of lists.
            >>> df = DataFrame([[1, 2], [3, 4]])
            >>> df
               col_0  col_1  index_label
            0      3      4            1
            1      1      2            0

            # Example 10: Create a teradataml DataFrame from numpy array.
            >>> import numpy as np
            >>> df = DataFrame(np.array([[1, 2], [3, 4]]), index=True, index_label="id")
            >>> df
                col_0  col_1
            id
            1       3      4
            0       1      2

            # Example 11: Create a teradataml DataFrame from a dictionary.
            >>> df = DataFrame({"col1": [1, 2], "col2": [3, 4]}, index=True, index_label="id")
            >>> df
                col1  col2
            id
            1      2     4
            0      1     3

            # Example 12: Create a teradataml DataFrame from list of dictionaries.
            >>> df = DataFrame([{"col1": 1, "col2": 2}, {"col1": 3, "col2": 4}], index=False)
            >>> df
                col1  col2
            0      3     4
            1      1     2

            # Example 13: Create a teradataml DataFrame from list of tuples.
            >>> df = DataFrame([("Alice", 1), ("Bob", 2)])
            >>> df
                  col_0  col_1  index_label
            0     Alice      1            1
            1       Bob      2            0

            # Example 14: Create a teradataml DataFrame from a numpy arrays.
            >>> import numpy as np
            >>> pdf = pd.DataFrame({
            ...     'id': [1, 2],
            ...     'values': [np.array([1, 2, 3]), np.array([4, 5, 6])],
            ...     'tags': [np.array(['a', 'b', 'c']), np.array(['x', 'y', 'z'])]
            ... })
            >>> df = DataFrame(pdf)
            >>> df
               id   values           tags  index_label
            0   2  (4,5,6)  ('x','y','z')            1
            1   1  (1,2,3)  ('a','b','c')            0

        RAISES:
            TeradataMlException - TDMLDF_CREATE_FAIL

        """
        self._table_name = None
        self._query = None
        self._column_names_and_types = None
        self._td_column_names_and_types = None
        self._td_column_names_and_sqlalchemy_types = None
        self._nodeid = None
        self._metaexpr = None
        self._index = index
        self.__index_label = index_label if isinstance(index_label, list) or index_label is None else [index_label]
        # This attribute instructs the _index_label property to query or not query the database
        # to find the index of the table/view for a DataFrame.
        self._index_query_required = True if index_label is None else False
        self._aed_utils = AedUtils()
        self._source_type = None
        self._orderby = None
        self._undropped_index = None
        # This attribute added to add setter for columns property,
        # it is required when setting columns from groupby
        self._columns = None
        # This attribute stores the internal AED query and avoid multiple
        # calls to AED utility function aed_show_query()
        self._aed_query = None
        # This attribute stores the type of query stored in self._aed_query.
        self._is_full_query = None

        # Property to determine if table is an ART table or not.
        self._is_art = None

        # This attribute stores the previous assign arguments in continuous assign calls.
        self._previous_assign_args = None
        # This attribute stores the root DataFrame columns.
        self._root_columns = None

        # Internal argument, when this attribute is set to True, the teradataml DataFrame locks
        # the corresponding row(s) in the underlying table(s) while accessing the data.
        _lock_rows = kwargs.get("_lock_rows", False)

        self._datalake = None
        self._database = None
        self._table = None
        self._otf = False
        self._df_type = None
        self._valid_time_column = None
        self._transaction_time_column = None
        self._eng = kwargs.get("_eng")


        table_name = kwargs.get("table_name", None)
        primary_index = kwargs.get("primary_index", None)
        columns = kwargs.get("columns", None)
        types = kwargs.get("types", None)
        persist = kwargs.get("persist", False)

        # Check if the data is an instance of in_schema or if the data is None
        # and table_name is an instance of in_schema, then assign the table_name,
        # datalake_name and schema_name to the DataFrame object.
        schema_obj = data if isinstance(data, in_schema) else (
                     table_name if data is None and isinstance(table_name, in_schema) else None)

        if schema_obj:
            self._table = schema_obj.table_name
            self._datalake = schema_obj.datalake_name
            self._database = schema_obj.schema_name
            self._otf = True if self._datalake else False

        # Convert schema objects to strings.
        data = str(data) if isinstance(data, in_schema) else data
        table_name = str(table_name) if isinstance(table_name, in_schema) else table_name

        awu_matrix = []
        dtypes = (list, tuple, dict)
        awu_matrix.append(["data", data, True, (str, pd.DataFrame, np.ndarray, dict, _ListOf(dtypes)), True])
        awu_matrix.append(["index", index, True, (bool)])
        awu_matrix.append(["index_label", index_label, True, (str, list)])
        awu_matrix.append(["query", query, True, (str), True])
        awu_matrix.append(["materialize", materialize, True, (bool)])
        awu_matrix.append(["table_name", table_name, True, (str), True])
        awu_matrix.append(["primary_index", primary_index, True, (str, list)])
        awu_matrix.append(["types", types, True, (dict)])
        awu_matrix.append(["columns", columns, True, (str, list), True])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        # Convert columns to list if it is a string.
        if isinstance(columns, str):
            columns = [columns]

        try:
            if table_name is not None or data is not None:

                #   If data is list or numpy array or dictionary, then convert it to a pandas DataFrame.
                if isinstance(data, (list, np.ndarray, dict)):
                    data = pd.DataFrame(data, columns=columns)
                # If the data is a pandas DataFrame, then store the data in a temporary table in Vantage.
                if isinstance(data, pd.DataFrame):
                    # Create a copy of the pandas DataFrame to avoid modifying the original,
                    # because column names will be changed if they are integers.
                    pd_data = data.copy()
                    # If the columns are not of type string, then convert them to string.
                    pd_data.columns = [f"col_{i}" if isinstance(i, int) else i for i in pd_data.columns]

                    # Set the table_name to the name of the table created in the database.
                    table_name = UtilFuncs._generate_temp_table_name(prefix="from_pandas",
                                                                table_type=TeradataConstants.TERADATA_TABLE,
                                                                gc_on_quit=not(persist))

                    copy_to_sql(pd_data, table_name, index=index, index_label=index_label, primary_index=primary_index,
                                types=types)
                # If the data is a string, then set the table_name to the data.
                elif isinstance(data, str):
                    table_name = data

                self._table_name = UtilFuncs._quote_table_names(table_name)
                self._source_type = SourceType.TABLE.value
                self._nodeid = self._aed_utils._aed_table(self._table_name)
            elif query is not None:
                query = query.strip()
                query = query[:-1] if query[-1] == ";" else query

                self._query = query
                self._source_type = SourceType.QUERY.value

                temp_obj_params = {
                    "prefix": "_frmqry_v",
                    "use_default_database": True,
                    "quote": False
                }
                __execute = UtilFuncs._create_view

                if configure.temp_object_type == TeradataConstants.TERADATA_VOLATILE_TABLE:
                    # If user requests to materialize the query, then we should create a
                    # volatile table if user intends to the same instead of view.
                    # Volatile table does not need to be added to the GC.
                    temp_obj_params["table_type"] = TeradataConstants.TERADATA_VOLATILE_TABLE
                    temp_obj_params["gc_on_quit"] = False
                    temp_obj_params["prefix"] = "_frmqry_vt"
                    __execute = UtilFuncs._create_table

                elif materialize:
                    # If user requests to materialize the query, then we should create a
                    # table instead of view and add the same in the GarbageCollector.
                    temp_obj_params["table_type"] = TeradataConstants.TERADATA_TABLE
                    temp_obj_params["gc_on_quit"] = True
                    temp_obj_params["prefix"] = "_frmqry_t"
                    __execute = UtilFuncs._create_table

                temp_table_name = UtilFuncs._generate_temp_table_name(**temp_obj_params)
                self._table_name = temp_table_name
                __execute_params = (self._table_name, self._query)

                if configure.temp_object_type == TeradataConstants.TERADATA_VOLATILE_TABLE:
                    __execute_params = (self._table_name, self._query, True)
                elif configure.temp_object_type == TeradataConstants.TERADATA_VIEW:
                    __execute_params = (self._table_name, self._query, _lock_rows)

                try:
                    __execute(*__execute_params)
                except OperationalError as oe:
                    if "[Error 3707] Syntax error" in str(oe):
                        raise ValueError(Messages.get_message(
                            MessageCodes.FROM_QUERY_SELECT_SUPPORTED).format("Only \"SELECT\" queries are supported."))
                    elif "[Error 3706] Syntax error: ORDER BY is not allowed in subqueries." in str(oe):
                        raise ValueError(Messages.get_message(
                            MessageCodes.FROM_QUERY_SELECT_SUPPORTED).format("ORDER BY is not allowed in the query."))
                    elif "[Error 3706] Syntax error" in str(oe):
                        raise ValueError(Messages.get_message(
                            MessageCodes.FROM_QUERY_SELECT_SUPPORTED).format("Check the syntax."))
                    elif "[Error 7825]" in str(oe):
                        # The UDF/XSP/UDM routine has thrown an SQLException
                        # with an SQL state in the range of 38001-38999 which
                        # is not a syntax error. Hence not a ValueError wrt query string.
                        # Expected when OTF snapshot related query is executed.
                        raise
                    raise ValueError(Messages.get_message(
                        MessageCodes.FROM_QUERY_SELECT_SUPPORTED))

                self._nodeid = self._aed_utils._aed_query(self._query, temp_table_name)
            else:
                if inspect.stack()[1][3] not in ['_from_node', '__init__', 'alias']:
                    raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_CREATE_FAIL),
                                              MessageCodes.TDMLDF_CREATE_FAIL)

            # _get_metaexpr() can be only used if self._table_name is set.
            if table_name or query:
                self._metaexpr = self._get_metaexpr()
                self._get_metadata_from_metaexpr(self._metaexpr)

            self._loc = _LocationIndexer(self)
            self._iloc = _LocationIndexer(self, integer_indexing=True)
            self.__data = None
            self.__data_columns = None
            self._alias = None
            self._plot = None

            self._eda_ui = None

        except TeradataMlException:
            raise
        except ValueError:
            raise
        except Exception as err:
            error_msg = Messages.get_message(MessageCodes.TDMLDF_CREATE_FAIL)
            full_msg = f"{error_msg}\n {str(err)}"
            raise TeradataMlException(full_msg, MessageCodes.TDMLDF_CREATE_FAIL) from None

    @property
    def db_object_name(self):
        """
        DESCRIPTION:
            Get the underlying database object name, on which DataFrame is
            created.

        RETURNS:
            str representing object name of DataFrame

        EXAMPLES:
            >>> load_example_data("dataframe", "sales")
            >>> df = DataFrame('sales')
            >>> df.db_object_name
            '"sales"'
        """
        if self._table_name is not None:
            return self._table_name
        else:
            msg = "Object name is available once DataFrame is materialized. " \
                  "Use DataFrame.materialize() to materialize DataFrame."
            print(msg)

    def alias(self, alias_name):
        """
        DESCRIPTION:
            Method to create an aliased teradataml DataFrame.
            Note:
                * This method is recommended to be used before performing
                  self join using DataFrame's join() API.

        PARAMETERS:
            alias_name:
                Required Argument.
                Specifies the alias name to be assigned to a teradataml DataFrame.
                Types: str

        RETURNS:
            teradataml DataFrame

        EXAMPLES:
            >>> load_example_data("dataframe", "admissions_train")
            >>> df = DataFrame("admissions_train")
            >>> df
               masters   gpa     stats programming  admitted
            id
            13      no  4.00  Advanced      Novice         1
            26     yes  3.57  Advanced    Advanced         1
            5       no  3.44    Novice      Novice         0
            19     yes  1.98  Advanced    Advanced         0
            15     yes  4.00  Advanced    Advanced         1
            40     yes  3.95    Novice    Beginner         0
            7      yes  2.33    Novice      Novice         1
            22     yes  3.46    Novice    Beginner         0
            36      no  3.00  Advanced      Novice         0
            38     yes  2.65  Advanced    Beginner         1

            # Example 1: Create an alias of teradataml DataFrame.

            >>> df2 = df.alias("adm_trn")

            # Print aliased DataFrame.
            >>> df2
               masters   gpa     stats programming  admitted
            id
            13      no  4.00  Advanced      Novice         1
            26     yes  3.57  Advanced    Advanced         1
            5       no  3.44    Novice      Novice         0
            19     yes  1.98  Advanced    Advanced         0
            15     yes  4.00  Advanced    Advanced         1
            40     yes  3.95    Novice    Beginner         0
            7      yes  2.33    Novice      Novice         1
            22     yes  3.46    Novice    Beginner         0
            36      no  3.00  Advanced      Novice         0
            38     yes  2.65  Advanced    Beginner         1
        """
        arg_info_matrix = [["alias_name", alias_name, False, (str), True]]
        _Validators._validate_function_arguments(arg_info_matrix)
        try:
            alias_df = self._from_node(self._nodeid, self._metaexpr, self._index_label,
                                       reuse_metaexpr=False, _datalake=self._datalake,
                                       _database=self._database, _table=self._table,
                                       _otf=self._otf)
            # Assigning self attributes to newly created alias dataframe.
            alias_df._table_name = self._table_name
            alias_df._index = self._index
            alias_df._index_label = self._index_label
            setattr(alias_df._metaexpr.t, "table_alias", alias_name)
            alias_df._alias = alias_name
            return alias_df
        except Exception as err:
            error_code = MessageCodes.EXECUTION_FAILED
            error_msg = Messages.get_message(
                error_code, "create alias dataFrame", '{}'.format(str(err)))
            raise TeradataMlException(error_msg, error_code)

    @classmethod
    @collect_queryband(queryband="DF_fromTable")
    def from_table(cls, table_name, index=True, index_label=None,
                   schema_name=None, datalake_name=None):
        """
        Class method for creating a DataFrame from a table or a view.

        PARAMETERS:
            table_name:
                Required Argument.
                The table name in Teradata Vantage referenced by this DataFrame.
                Types: str

            index:
                Optional Argument.
                True if using index column for sorting otherwise False.
                Default Value: True
                Types: bool

            index_label:
                Optional
                Column/s used for sorting.
                Types: str

            schema_name:
                Optional Argument.
                Specifies the schema where the table resides.
                Types: str

            datalake_name:
                Optional Argument.
                Specifies the datalake name.
                Types: str

        EXAMPLES:
            >>> from teradataml import DataFrame

            # Example 1: The following example creates a DataFrame from a table or
                         a view.
            # Load the example data.
            >>> load_example_data("dataframe","sales")

            # Create DataFrame from table
            >>> df = DataFrame.from_table('sales')

            # Create DataFrame from table and without index column sorting.
            >>> df = DataFrame.from_table("sales", False)

            # Create DataFrame from table and sorting using the 'accounts'
            # column.
            >>> df = DataFrame.from_table("sales", True, "accounts")

            # Example 2: The following example creates a DataFrame from existing Vantage
            #            table "dbcinfo" in the non-default database "dbc" using the
            #            in_schema() function.

            >>> from teradataml.dataframe.dataframe import in_schema
            >>> df = DataFrame.from_table(in_schema("dbc", "dbcinfo"))

            # Example 3: Create a DataFrame on existing DataLake
            #            table "lake_table" in the "datalake_database" database
            #            in "datalake" datalake.

            >>> datalake_df = DataFrame.from_table(table_name="lake_table",
            ...                                    schema_name="datalake_database",
            ...                                    datalake_name="datalake" )

        RETURNS:
            DataFrame

        RAISES:
            TeradataMlException - TDMLDF_CREATE_FAIL

        """
        if schema_name:
            return cls(table_name=in_schema(schema_name, table_name, datalake_name),
                       index=index, index_label=index_label)
        return cls(table_name=table_name, index=index, index_label=index_label)

    @classmethod
    @collect_queryband(queryband="DF_fromQuery")
    def from_query(cls, query, index=True, index_label=None, materialize=False, **kwargs):
        """
        Class method for creating a DataFrame from a query.

        PARAMETERS:
            query:
                Required Argument.
                Specifies the Teradata Vantage SQL query referenced by this DataFrame.
                Only "SELECT" queries are supported. Exception will be
                raised, if any other type of query is passed.
                Unsupported queries include:
                    1. DDL Queries like CREATE, DROP, ALTER etc.
                    2. DML Queries like INSERT, UPDATE, DELETE etc. except SELECT.
                    3. SELECT query with ORDER BY clause.
                Types: str

            index:
                Optional Argument.
                True if using index column for sorting otherwise False.
                Default Value: True
                Types: bool

            index_label:
                Optional Argument.
                Column/s used for sorting.
                Types: str

            materialize:
                Optional Argument.
                Whether to materialize DataFrame or not when created.

                You should use materialization, when the query  passed to from_query(),
                is expected to produce non-deterministic results, when it is executed multiple
                times. Using this option will help user to have deterministic results in the
                resulting teradataml DataFrame.
                Default Value: False (No materialization)
                Types: bool

        EXAMPLES:
            from teradataml.dataframe.dataframe import DataFrame
            load_example_data("dataframe","sales")
            df = DataFrame.from_query("select accounts, Jan, Feb from sales")
            df = DataFrame.from_query("select accounts, Jan, Feb from sales", False)
            df = DataFrame.from_query("select * from sales", True, "accounts")

        RETURNS:
            DataFrame

        RAISES:
            TeradataMlException - TDMLDF_CREATE_FAIL

        """
        return cls(index=index, index_label=index_label, query=query, materialize=materialize)

    @classmethod
    def _from_node(cls, nodeid, metaexpr, index_label=None, undropped_index=None, reuse_metaexpr=True, **kwargs):
        """
        Private class method for creating a DataFrame from a nodeid and parent metadata.

        PARAMETERS:
            nodeid:
                Required Argument.
                Node ID for the DataFrame.

            metaexpr:
                Required Argument.
                Parent metadata (_MetaExpression Object).

            index_label:
                Optional Argument.
                List specifying index column(s) for the DataFrame.

            undropped_index:
                Optional Argument.
                List specifying index column(s) to be retained as columns for printing.

            reuse_metaexpr:
                Optional Argument.
                Specifies the flag to decide whether to use same _MetaExpression object or not.
                Default Value: True
                Types: bool

        EXAMPLES:
            from teradataml.dataframe.dataframe import DataFrame
            df = DataFrame._from_node(1234, metaexpr)
            df = DataFrame._from_node(1234, metaexpr, ['col1'], ['col2'])

        RETURNS:
            DataFrame

        RAISES:
            TeradataMlException - TDMLDF_CREATE_FAIL

        """
        df = cls()
        df._nodeid = nodeid
        df._source_type = SourceType.TABLE.value


        if not reuse_metaexpr:
            # Create new _MetaExpression object using reference metaExpression
            # for newly created DataFrame.
            df._metaexpr = UtilFuncs._get_metaexpr_using_parent_metaexpr(nodeid, metaexpr)
            # When metaexpression is created using only column information from parent DataFrame,
            # underlying SQLAlchemy table is created with '' string as Table name.
            # Assign name from reference mataexpression here.
            df._metaexpr.t.name = metaexpr.t.name
            # Populate corresponding information into newly created DataFrame object
            # using newly created metaExpression.
            df._get_metadata_from_metaexpr(df._metaexpr)
        else:
            # Populate corresponding information into newly created DataFrame object
            # using reference metaExpression.
            df._get_metadata_from_metaexpr(metaexpr)

        if isinstance(index_label, str):
            index_label = [index_label]

        if index_label is not None and all(elem in [col.name for col in df._metaexpr.c] for elem in index_label):
            df._index_label = index_label
        elif index_label is not None and all(UtilFuncs._teradata_quote_arg(elem, "\"", False)
                                             in [col.name for col in df._metaexpr.c] for elem in index_label):
            df._index_label = index_label

        # Set the flag suggesting that the _index_label is set,
        # and that a database lookup won't be required even when it is None.
        df._index_query_required = False

        if isinstance(undropped_index, str):
            undropped_index = [undropped_index]

        if undropped_index is not None and all(elem in [col.name for col in df._metaexpr.c] for elem in undropped_index):
            df._undropped_index = undropped_index
        elif undropped_index is not None and all(UtilFuncs._teradata_quote_arg(elem, "\"", False)
                                                 in [col.name for col in df._metaexpr.c] for elem in undropped_index):
            df._undropped_index = undropped_index

        # Populate remaining attributes.
        for arg in kwargs:
            # Pop each argument from kwargs and assign to new DataFrame.
            arg_value = kwargs.get(arg)
            df.__setattr__(arg, arg_value)
        return df

    @classmethod
    @collect_queryband(queryband="DF_fromPandas")
    def from_pandas(cls, pandas_df, index=True, index_label=None, primary_index=None, persist=False):
        """
        DESCRIPTION:
            Creates a teradataml DataFrame from a pandas DataFrame.

        PARAMETERS:
            pandas_df:
                Required Argument.
                Specifies the pandas DataFrame to be converted to teradataml DataFrame.
                Types: pandas DataFrame

            index:
                Optional Argument.
                Specifies whether to save Pandas DataFrame index as a column or not.
                Default Value: True
                Types: bool

            index_label:
                Optional Argument.
                Specifies the column label(s) for Pandas DataFrame index column(s).
                Note:
                    * Refer to the "index_label" parameter of copy_to_sql() for more details.
                Default Value: None
                Types: str OR list of str

            primary_index:
                Optional Argument.
                Specifies which column(s) to use as primary index for the teradataml DataFrame.
                Types: str OR list of str

            persist:
                Optional Argument.
                Specifies whether to persist the DataFrame.
                Default Value: False
                Types: bool

        RETURNS:
            teradataml DataFrame

        RAISES:
            TeradataMlException

        EXAMPLES:
            >>> import pandas as pd
            >>> from teradataml import DataFrame
            >>> pdf = pd.DataFrame({"col1": [1, 2, 3], "col2": [4, 5, 6]})
            >>> pdf1 = pd.DataFrame([[1, 2], [3, 4]])

            # Example 1: Create a teradataml DataFrame from a pandas DataFrame.
            >>> df = DataFrame.from_pandas(pdf)
            >>> df
               col1  col2  index_label
            0     3     6            2
            1     2     5            1
            2     1     4            0

            # Example 2: Create a teradataml DataFrame from a pandas DataFrame
            #            and do not save the index as a column.
            >>> df = DataFrame.from_pandas(pdf, index=False)
            >>> df
               col1  col2
            0     3     6
            1     2     5
            2     1     4

            # Example 3: Create a teradataml DataFrame from a pandas DataFrame
            #            with index label as 'id' and set it as primary index.
            >>> df = DataFrame.from_pandas(pdf, index=True, index_label='id', primary_index='id')
            >>> df
                col1  col2
            id
            2      3     6
            1      2     5
            0      1     4

            # Example 4: Create a teradataml DataFrame from a pandas DataFrame where
            #            columns are not explicitly defined in the pandas DataFrame.
            >>> df = DataFrame.from_pandas(pdf1)
            >>> df
               col_0  col_1  index_label
            0      3      4            1
            1      1      2            0

            # Example 5: Persist the data from pandas DataFrame into a table which can be used across sessions.
            >>> df = DataFrame.from_pandas(pdf, persist=True)
            >>> df
                col1  col2  index_label
            0     3     6            2
            1     2     5            1
            2     1     4            0

            # Get/view the database object name.
            >>> df.db_object_name
            '"ml__from_pandas_1761717824742117"'

            # Create the teradataml DataFrame using the database object name in different session.
            >>> DataFrame('"ml__from_pandas_1761717824742117"')
                col1  col2  index_label
            0     3     6            2
            1     2     5            1
            2     1     4            0
        """
        # Validate 'pandas_df' argument, other arguments, will be validated as part of DataFrame().
        arg_type_matrix = []
        arg_type_matrix.append(["pandas_df", pandas_df, False, (pd.DataFrame,), True])
        arg_type_matrix.append(["persist", persist, True, (bool), True])
        
        _Validators._validate_function_arguments(arg_type_matrix)

        return cls(pandas_df, index, index_label, primary_index=primary_index, persist=persist)

    @classmethod
    @collect_queryband(queryband="DF_fromDict")
    def from_dict(cls, data, columns=None, persist=False):
        """
        DESCRIPTION:
            Creates a DataFrame from a dictionary containing values as lists or numpy arrays.

        PARAMETERS:
            data:
                Required Argument.
                Specifies the Python dictionary to create a teradataml DataFrame.
                Notes:
                    * Keys of the dictionary are used as column names.
                    * Values of the dictionary should be lists or numpy arrays.
                    * Nested dictionaries are not supported.
                Types: dict

            columns:
                Optional Argument.
                Specifies the column names for the DataFrame.
                Types: str OR list of str

            persist:
                Optional Argument.
                Specifies whether to persist the DataFrame.
                Default Value: False
                Types: bool

        RETURNS:
            teradataml DataFrame

        RAISES:
            TeradataMlException

        EXAMPLES:
            >>> from teradataml import DataFrame
            >>> data_dict = {"name": ["Alice", "Bob", "Charlie"], "age": [25, 30, 28]}

            # Example 1: Create a teradataml DataFrame from a dictionary where
            #            keys are column names and values are lists of column data.
            >>> df = DataFrame.from_dict(data_dict)
            >>> df
                  name  age
            0  Charlie   28
            1      Bob   30
            2    Alice   25

            # Example 2: Create a teradataml DataFrame from a dictionary where
            #            keys are column names and values are numpy arrays.
            >>> import numpy as np
            >>> data_dict = {"col1": np.array([1, 2, 3]), "col2": np.array([4, 5, 6])}
            >>> df = DataFrame.from_dict(data_dict)
            >>> df
               col1  col2
            0     3     6
            1     2     5
            2     1     4

            # Example 3: Persist the data from dictionary into a table which can be used across sessions.
            >>> df = DataFrame.from_dict(data_dict, persist=True)
            >>> df
                  name  age
            0  Charlie   28
            1      Bob   30
            2    Alice   25

            # Get/view the database object name.
            >>> df.db_object_name
            '"ml__from_pandas_1761725700090504"'

            # Create the teradataml DataFrame using the database object name in different session.
            >>> DataFrame('"ml__from_pandas_1761725700090504"')
                  name  age
            0  Charlie   28
            1      Bob   30
            2    Alice   25
        """
        arg_type_matrix = []
        arg_type_matrix.append(["data", data, False, (dict), True])
        arg_type_matrix.append(["columns", columns, True, (str, list), True])
        arg_type_matrix.append(["persist", persist, True, (bool), True])

        _Validators._validate_function_arguments(arg_type_matrix)

        return cls(data, columns=columns, index=False, persist=persist)

    @classmethod
    @collect_queryband(queryband="DF_fromRecords")
    def from_records(cls, data, columns=None, **kwargs):
        """
        DESCRIPTION:
            Create a DataFrame from a list of lists/tuples/dictionaries/numpy arrays.

        PARAMETERS:
            data:
                Required Argument.
                Specifies the iterator of data or the list of lists/tuples/dictionaries/numpy arrays to
                be converted to teradataml DataFrame.
                Note:
                    * Nested lists or tuples or dictionaries are not supported.
                Types: Iterator, list

            columns:
                Optional Argument.
                Specifies the column names for the DataFrame.
                Note:
                    * If the data is a list of lists/tuples/numpy arrays and this argument
                      is not specified, column names will be auto-generated as 'col_0', 'col_1', etc.
                Types: str OR list of str

            kwargs:
                exclude:
                    Optional Argument.
                    Specifies the columns to be excluded from the DataFrame.
                    Types: list OR tuple

                coerce_float:
                    Optional Argument.
                    Specifies whether to convert values of non-string, non-numeric objects (like decimal.Decimal)
                    to floating point, useful for SQL result sets.
                    Default Value: True
                    Types: bool

                nrows:
                    Optional Argument.
                    Specifies the number of rows to be read from the data if the data is iterator.
                    Types: int

                persist:
                    Optional Argument.
                    Specifies whether to persist the DataFrame.
                    Default Value: False
                    Types: bool

        RETURNS:
            teradataml DataFrame

        RAISES:
            TeradataMlException

        EXAMPLES:
            >>> from teradataml import DataFrame

            # Example 1: Create a teradataml DataFrame from a list of lists.
            >>> df = DataFrame.from_records([['Alice', 1], ['Bob', 2]], columns=['name', 'age'])
            >>> df
                name  age
            0    Bob    2
            1  Alice    1

            # Example 2: Create a teradataml DataFrame from a list of tuples.
            >>> df = DataFrame.from_records([('Alice', 1), ('Bob', 3)], columns=['name', 'age'])
            >>> df
                name  age
            0    Bob    3
            1  Alice    1

            # Example 3: Create a teradataml DataFrame from a list of dictionaries.
            >>> df = DataFrame.from_records([{'name': 'Alice', 'age': 4}, {'name': 'Bob', 'age': 2}])
            >>> df
                name  age
            0    Bob    2
            1  Alice    4

            # Example 4: Create a teradataml DataFrame from a list where columns
            #            are not explicitly defined.
            >>> df = DataFrame.from_records([['Alice', 1], ['Bob', 2]])
            >>> df
               col_0  col_1
            0    Bob      2
            1  Alice      1

            # Example 5: Create a teradataml DataFrame from a list by excluding 'grade' column.
            >>> df = DataFrame.from_records([['Alice', 1, 'A'], ['Bob', 2, 'B']],
            ...                               columns=['name', 'age', 'grade'],
            ...                               exclude=['grade'])
            >>> df
                name  age
            0    Bob    2
            1  Alice    1

            # Example 6: Create a teradataml DataFrame from a list of lists
            #            with "coerce_float" set to False.
            >>> df = DataFrame.from_records([[1, Decimal('2.5')], [3, Decimal('4.0')]],
            ...                              columns=['col1', 'col2'], coerce_float=False)
            >>> df
                col1  col2
            0      3   4.0
            1      1   2.5
            >>> df.tdtypes
            col1                                          BIGINT()
            col2           VARCHAR(length=1024, charset='UNICODE')

            # Example 7: Create a teradataml DataFrame from a list of lists
            #            with "coerce_float" set to True.
            >>> from decimal import Decimal
            >>> df = DataFrame.from_records([[1, Decimal('2.5')], [3, Decimal('4.0')]],
            ...                              columns=['col1', 'col2'], coerce_float=True)
            >>> df
               col1  col2
            0     3   4.0
            1     1   2.5
            >>> df.tdtypes
            col1           BIGINT()
            col2            FLOAT()

            # Example 8: Create a teradataml DataFrame from an iterator with "nrows" set to 2.
            >>> def data_gen():
            ...     yield ['Alice', 1]
            ...     yield ['Bob', 2]
            ...     yield ['Charlie', 3]
            >>> df = DataFrame.from_records(data_gen(), columns=['name', 'age'], nrows=2)
            >>> df
                name  age
            0    Bob    2
            1  Alice    1

            # Example 9: Persist the data from records into a table which can be used across sessions.
            >>> df = DataFrame.from_records([['Alice', 1], ['Bob', 2]], columns=['name', 'age'], persist=True)
            >>> df
                name  age
            0    Bob    2
            1  Alice    1

            # Get/view the database object name.
            >>> df.db_object_name
            '"ml__from_pandas_1761719973588452"'

            # Create the teradataml DataFrame using the database object name in different session.
            >>> DataFrame('"ml__from_pandas_1761719973588452"')
                name  age
            0    Bob    2
            1  Alice    1
        """

        exclude = kwargs.get("exclude", None)
        coerce_float = kwargs.get("coerce_float", True)
        nrows = kwargs.get("nrows", None)
        persist = kwargs.get("persist", False)

        arg_type_matrix = []
        dtypes = (list, tuple, dict)
        arg_type_matrix.append(["data", data, False, (Iterator, _ListOf(dtypes)), True])
        arg_type_matrix.append(["columns", columns, True, (str, _ListOf(str)), True])
        arg_type_matrix.append(["exclude", exclude, True, (_ListOf(str),), True])
        arg_type_matrix.append(["coerce_float", coerce_float, True, (bool, ), True])
        arg_type_matrix.append(["nrows", nrows, True, (int,), True])
        arg_type_matrix.append(["persist", persist, True, (bool,), True])

        _Validators._validate_function_arguments(arg_type_matrix)

        if isinstance(columns, str):
            columns = [columns]

        df = pd.DataFrame.from_records(data, columns=columns, exclude=exclude,
                                       coerce_float=coerce_float, nrows=nrows)
        return cls(df, index=False, persist=persist)

    def create_temp_view(self, name):
        """
        DESCRIPTION:
            Creates a temporary view for session on the DataFrame.

        PARAMETERS:
            name:
                Required Argument.
                Specifies the name of the temporary view.
                Type: str

        RETURNS:
            None

        RAISES:
            OperationalError (When view already exists).

        EXAMPLES:
            # Load the data to run the example.
            >>> load_example_data("dataframe", "admissions_train")
            >>> df = DataFrame("admissions_train")
            >>> df
                masters   gpa     stats programming  admitted
            id
            38     yes  2.65  Advanced    Beginner         1
            7      yes  2.33    Novice      Novice         1
            26     yes  3.57  Advanced    Advanced         1
            17      no  3.83  Advanced    Advanced         1
            34     yes  3.85  Advanced    Beginner         0
            13      no  4.00  Advanced      Novice         1
            32     yes  3.46  Advanced    Beginner         0
            11      no  3.13  Advanced    Advanced         1
            15     yes  4.00  Advanced    Advanced         1
            36      no  3.00  Advanced      Novice         0

            # Example 1: Create view 'new_admissions'.
            >>> df.create_temp_view("new_admissions")
            >>> new_df = DataFrame("new_admissions")
            >>> new_df
                masters   gpa     stats programming  admitted
            id
            38     yes  2.65  Advanced    Beginner         1
            7      yes  2.33    Novice      Novice         1
            26     yes  3.57  Advanced    Advanced         1
            17      no  3.83  Advanced    Advanced         1
            34     yes  3.85  Advanced    Beginner         0
            13      no  4.00  Advanced      Novice         1
            32     yes  3.46  Advanced    Beginner         0
            11      no  3.13  Advanced    Advanced         1
            15     yes  4.00  Advanced    Advanced         1
            36      no  3.00  Advanced      Novice         0
        """
        # Validating Arguments
        arg_type_matrix = []
        arg_type_matrix.append(["name", name, False, (str), True])
        _Validators._validate_function_arguments(arg_type_matrix)

        GarbageCollector._add_to_garbagecollector(name, TeradataConstants.TERADATA_VIEW)
        UtilFuncs._create_view(name, self.show_query())

    def materialize(self):
        """
        DESCRIPTION:
            Method to materialize teradataml DataFrame into a database object.
            Notes:
                * DataFrames are materialized in either view/table/volatile table,
                  which is decided and taken care by teradataml.
                * If user wants to materialize object into specific database object
                  such as table/volatile table, use 'to_sql()' or 'copy_to_sql()' or
                  'fastload()' functions.
                * Materialized object is garbage collected at the end of the session.

        PARAMETERS:
            None

        RETURNS:
            DataFrame

        EXAMPLES:
            >>> load_example_data("dataframe", "admissions_train")
            >>> df = DataFrame("admissions_train")
            >>> df
               masters   gpa     stats programming  admitted
            id
            13      no  4.00  Advanced      Novice         1
            26     yes  3.57  Advanced    Advanced         1
            5       no  3.44    Novice      Novice         0
            19     yes  1.98  Advanced    Advanced         0
            15     yes  4.00  Advanced    Advanced         1
            40     yes  3.95    Novice    Beginner         0
            7      yes  2.33    Novice      Novice         1
            22     yes  3.46    Novice    Beginner         0
            36      no  3.00  Advanced      Novice         0
            38     yes  2.65  Advanced    Beginner         1

            # Example 1: Perform operations on teradataml DataFrame
            #            and materializeit in a database object.
            >>> df2 = df.get([["id", "masters", "gpa"]])

            # Initially table_name will be None.
            >>> df2._table_name

            >>> df2.materialize()
               masters   gpa
            id
            15     yes  4.00
            7      yes  2.33
            22     yes  3.46
            17      no  3.83
            13      no  4.00
            38     yes  2.65
            26     yes  3.57
            5       no  3.44
            34     yes  3.85
            40     yes  3.95

            # After materialize(), view name will be assigned.
            >>> df2._table_name
            '"ALICE"."ml__select__172077355985236"'
            >>>
        """
        self.__execute_node_and_set_table_name(self._nodeid, self._metaexpr)
        return self

    @collect_queryband(queryband="DF_fillna")
    def fillna(self, value=None, columns=None, literal_value=False, partition_column=None):
        """
        DESCRIPTION:
            Method to replace the null values in a column with the value specified.

        PARAMETERS:
            value:
                Required Argument.
                Specifies the value(s) to replace the null values with. If value is a dict
                then "columns" is ignored.
                Notes:
                    * To use pre-defined strings to replace the null value set "literal_value" to True.
                    * This method does not support operations on array columns.
                Permitted Values: 
                    * Pre-defined strings:
                        * 'MEAN' - Replace null value with the average of the values in the column.
                        * 'MODE' - Replace null value with the mode of the values in the column.
                        * 'MEDIAN' - Replace null value with the median of the values in the column.
                        * 'MIN' - Replace null value with the minimum of the values in the column.
                        * 'MAX' - Replace null value with the maximum of the values in the column.
                Types: int, float, str, dict containing column names and value, list

            columns:
                Optional Argument.
                Specifies the column names to perform the null value replacement. If "columns"
                is None, then all the columns having null value and data type similar to
                the data type of the value specified are considered.
                Default Value: None
                Types: str, tuple or list of str

            literal_value:
                Optional Argument.
                Specifies whether the pre-defined strings passed to "value" should be treated
                as literal or not.
                Default Value: False
                Types: bool

            partition_column:
                Optional Argument.
                Specifies the column name to partition the data.
                Default Value: None
                Types: str

        RETURNS:
            teradataml DataFrame

        RAISES:
            TeradataMlException

        EXAMPLES:
            >>> load_example_data("dataframe", "sales")
            >>> df = DataFrame("sales")
            >>> df
                          Feb    Jan    Mar    Apr    datetime
            accounts
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017

            # Example 1: Populate null value in column 'Jan' and 'Mar'
            #            with the value specified as dictionary.
            >>> df.fillna({"Jan": 123, "Mar":234})
                     accounts    Feb  Jan  Mar    Apr  datetime
                0    Blue Inc   90.0   50   95  101.0  17/01/04
                1    Alpha Co  210.0  200  215  250.0  17/01/04
                2   Jones LLC  200.0  150  140  180.0  17/01/04
                3  Yellow Inc   90.0  123  234    NaN  17/01/04
                4  Orange Inc  210.0  123  234  250.0  17/01/04
                5     Red Inc  200.0  150  140    NaN  17/01/04

            # Example 2: Populate the null value in 'Jan' column
            #            with minimum value in that column.
            >>> df.fillna("Min", "Jan")
                     accounts    Feb  Jan    Mar    Apr  datetime
                0  Yellow Inc   90.0   50    NaN    NaN  17/01/04
                1   Jones LLC  200.0  150  140.0  180.0  17/01/04
                2     Red Inc  200.0  150  140.0    NaN  17/01/04
                3    Blue Inc   90.0   50   95.0  101.0  17/01/04
                4    Alpha Co  210.0  200  215.0  250.0  17/01/04
                5  Orange Inc  210.0   50    NaN  250.0  17/01/04

            # Example 3: Populate the null value in 'pclass' and
            #            'fare' column with mean value with partition
            #            column as 'sex'.
            # Load the example data.
            >>> load_example_data("teradataml", ["titanic"])
            >>> df = DataFrame.from_table("titanic")

            >>> df.fillna(value="mean", columns=["pclass", "fare"], partition_column="sex")
                passenger  survived  pclass                                         name     sex   age  sibsp  parch            ticket      fare cabin embarked
            0        284         1       3                   Dorking, Mr. Edward Arthur    male  19.0      0      0        A/5. 10482    8.0500  None        S
            1        589         0       3                        Gilinski, Mr. Eliezer    male  22.0      0      0             14973    8.0500  None        S
            2         17         0       3                         Rice, Master. Eugene    male   2.0      4      1            382652   29.1250  None        Q
            3        282         0       3             Olsson, Mr. Nils Johan Goransson    male  28.0      0      0            347464    7.8542  None        S
            4        608         1       1                  Daniel, Mr. Robert Williams    male  27.0      0      0            113804   30.5000  None        S
            5        404         0       3               Hakkarainen, Mr. Pekka Pietari    male  28.0      1      0  STON/O2. 3101279   15.8500  None        S
            6        427         1       2  Clarke, Mrs. Charles V (Ada Maria Winfield)  female  28.0      1      0              2003   26.0000  None        S
            7        141         0       3                Boulos, Mrs. Joseph (Sultana)  female   NaN      0      2              2678   15.2458  None        C
            8        610         1       1                    Shutes, Miss. Elizabeth W  female  40.0      0      0          PC 17582  153.4625  C125        S
            9        875         1       2        Abelson, Mrs. Samuel (Hannah Wizosky)  female  28.0      1      0         P/PP 3381   24.0000  None        C
        """
        from teradataml import SimpleImputeFit, SimpleImputeTransform

        arg_info_matrix = []
        arg_info_matrix.append(["value", value, True, (int, float, str, dict, list)])
        arg_info_matrix.append(["columns", columns, True, (list, str, tuple)])
        arg_info_matrix.append(["literal_value", literal_value, True, (bool)])
        arg_info_matrix.append(["partition_column", partition_column, True, (str)])

        # Validate argument types
        _Validators._validate_function_arguments(arg_info_matrix)

        if isinstance(columns, tuple):
            columns = list(columns)

        # If dict is passed separate the values of 'columns' and 'value'
        if isinstance(value, dict):
            columns, value = zip(*value.items())
            columns = [str(col) for col in columns]
            value = [str(val) for val in value]

        is_stats = False

        for val in UtilFuncs._as_list(value):
            if isinstance(val, str) and val.lower() in ["mean", "median", "mode", "min", "max"]:
                is_stats = True
                break

        # If "literal_value" is set to False
        if not literal_value and is_stats:
            stats = []
            stats_columns = []
            literals = []
            literals_columns = []
            # If value is a list, extract columns and values, if values match to any
            # predefined string then assign it to stats and column name to stats_column
            # else treat it as a literal value and literal column.
            if isinstance(value, list):
                for val, col in zip(value, columns):
                    if isinstance(val, str) and val.lower() in ["mean", "median", "mode", "min", "max"]:
                        stats.append(val)
                        stats_columns.append(col)
                    else:
                        literals.append(str(val))
                        literals_columns.append(col)
            else:
                # In case it is not a list then simply assign it to 'stats' and 'stats_columns'
                stats = value
                stats_columns = columns

            # In case no literal value found in the list and literal list is empty assign it as 'None'
            # instead of empty list.
            literals = None if not literals else literals
            literals_columns = None if not literals_columns else literals_columns

        else:
            # If it is a literal value then 'stats' and 'stats_column' is not required
            stats = None
            stats_columns = None

            # In case column is not specified by the user, then all the columns in that dataframe
            # should be considered else the specified columns should be considered for 'literal_columns'
            literals_columns = self.columns if (columns is None and value is not None) else columns
            literals_columns = UtilFuncs._as_list(literals_columns)
            # In case value is a list of single element, then multiply it as many times as
            # number of columns ['12'] -> ['12','12', upto number of columns]
            # else convert it to str and append it
            if isinstance(value, list):
                literals = []
                for val in value:
                    literals.append(str(val))
            else:
                literals = UtilFuncs._as_list(str(value))
            literals = literals * len(literals_columns) if len(literals) != len(literals_columns) else literals

        fit_obj = SimpleImputeFit(data=self,
                                  literals=literals,
                                  literals_columns=literals_columns,
                                  stats=stats,
                                  stats_columns=stats_columns,
                                  partition_column=partition_column)

        impute_transform = {
            'data': self,
            'data_partition_column': partition_column,
            'object_partition_column': partition_column}

        return fit_obj.transform(**impute_transform).result

    def __execute_node_and_set_table_name(self, nodeid, metaexpr=None):
        """
        Private method for executing node and setting _table_name,
        if not set already.

        PARAMETERS:
            nodeid:
                Required Argument.
                nodeid to execute.

            metaexpression:
                Optional Argument.
                Updated _metaexpr to validate

        EXAMPLES:
             __execute_node_and_set_table_name(nodeid)
             __execute_node_and_set_table_name(nodeid, metaexpr)

        """
        if self._table_name is None:
            self._table_name = df_utils._execute_node_return_db_object_name(nodeid, metaexpr)

    @property
    def is_art(self):
        """
        RETURNS:
            Getter for self._is_art.
            Returns True if the DataFrame is created on an ART table, else return False.

        EXAMPLES:
            >>> load_example_data("dataframe","sales")
            >>> df = DataFrame.from_table('sales')
            >>> df.is_art
                False
        """
        return self._set_art_table_property() if self._is_art is None else self._is_art

    def _set_art_table_property(self):
        """
        DESCRIPTION:
            Function to set the variable '_is_art'.
            Sets it to True if DataFrame is created on ART table, False otherwise.

        PARAMETERS:
            None

        EXAMPLES:
            self.__set_art_table_property()

        RETURNS:
            None

        RAISES:
            None
        """
        # Get the table name.
        # If table name is None, means DataFrame is not created on ART table,
        # it is internally created DataFrame.
        if self._table_name is None:
            self._is_art = False
            return self._is_art
        db_table_name = UtilFuncs._extract_table_name(self._table_name)

        # Get the schema name of the table.
        db_schema = UtilFuncs._extract_db_name(self._table_name)
        db_schema = db_schema if db_schema is not None else tdmlctx._get_current_databasename()

        # Check if the table is an ART table or not using TVMFlavor = 'A' in DBC.TablesV.
        query = "SELECT count(*) FROM DBC.TablesV WHERE DataBaseName = '{}' " \
                "and TableName = '{}' and TVMFlavor = 'A'".format(db_schema, db_table_name)
        res = UtilFuncs._execute_query(query)

        # If the count of result is equal to 1, set self.is_art = True
        self._is_art = res[0][0] == 1
        return self._is_art


    def _process_columns_metadata(self):
        """
        DESCRIPTION:
            Processes the metadata of columns to determine their time dimension properties
            and to check whether database object is a view, volatile table, or ART table.

        PARAMETERS:
            None

        RAISES:
            None

        RETURNS:
            Tuple containing five boolean values:
                - is_view: True if the database object is a view, False otherwise.
                - is_volatile: True if the database object is a volatile table, False otherwise.
                - is_art_table: True if the database object is an ART table, False otherwise.
                - has_valid_time: True if any column has a valid time dimension, False otherwise.
                - has_transaction_time: True if any column has a transaction time dimension, False otherwise.
        EXAMPLES:
            >>> load_example_data("teradataml", "Employee")
            >>> df = DataFrame.from_table("Employee")
            >>> is_view, is_volatile, is_art_table, valid_time, transaction_time = (
                df._process_columns_metadata()
                )
            >>> is_view, is_volatile, is_art_table, valid_time, transaction_time
            (False, False, False, True, True)

        """

        is_view = is_volatile = is_art_table = False

        for col in self._metaexpr.c:
            metadata = col.expression.info
            time_dimension = metadata.get('time_dimension')
            is_view = metadata.get('is_view', is_view)
            is_volatile = metadata.get('is_volatile', is_volatile)
            is_art_table = metadata.get('is_art_table', is_art_table)

            if time_dimension == "V":
                self._valid_time_column = col

            if time_dimension == "T":
                self._transaction_time_column = col

        has_valid_time = self._valid_time_column is not None
        has_transaction_time = self._transaction_time_column is not None

        return is_view, is_volatile, is_art_table, has_valid_time, has_transaction_time

    def _get_metadata_from_metaexpr(self, metaexpr):
        """
        Private method for setting _metaexpr and retrieving column names and types.

        PARAMETERS:
            metaexpr:
                Required Argument.
                Specifies parent meta data (_MetaExpression object).
                Types: _MetaExpression

        RETURNS:
            None

        RAISES:
            None

        """
        self._metaexpr = metaexpr
        self._column_names_and_types = []
        self._td_column_names_and_types = []
        self._td_column_names_and_sqlalchemy_types = {}
        self._column_types = {}

        for col in self._metaexpr.c:
            if isinstance(col.type, sqlalchemy.sql.sqltypes.NullType):
                tdtype = TeradataTypes.TD_NULL_TYPE.value
            else:
                tdtype = "{}".format(col.type)

            py_type = UtilFuncs._teradata_type_to_python_type(col.type)
            self._column_names_and_types.append((str(col.name), py_type))
            self._td_column_names_and_types.append((str(col.name), tdtype))
            self._td_column_names_and_sqlalchemy_types[(str(col.name)).lower()] = col.type
            self._column_types[(str(col.name)).lower()] = [py_type, col.type]

    def _get_metaexpr(self):
        """
        Private method that returns a TableExpression object for this dataframe.

        RETURNS:
            TableExpression object

        EXAMPLES:
            table_meta = self._get_metaexpr()

            # you can access the columns with the 'c' attribute
            table_meta.c
        """
        eng = self._eng if self._eng else tdmlctx.get_context()
        meta = sqlalchemy.MetaData()
        db_schema = UtilFuncs._extract_db_name(self._table_name)
        db_table_name = UtilFuncs._extract_table_name(self._table_name)

        if not self._datalake:
            t = sqlalchemy.Table(db_table_name, meta, schema=db_schema, autoload_with=eng)
            return _MetaExpression(t)

        # Get metaexpression for datalake table.
        # check existence of datalake table.
        tdmlctx.get_connection().dialect.has_table(tdmlctx.get_connection(),
                                                   self._table,
                                                   schema=self._database,
                                                   table_only=True,
                                                   datalake=self._datalake)

        # Extract column names and corresponding teradatasqlalchemy types.
        try:
            # For latest OTF help table query results.
            col_names, col_types = df_utils._get_datalake_table_columns_info(self._database,
                                                                             self._table,
                                                                             self._datalake,
                                                                             use_dialect=True)
        except NoSuchColumnError:
            # For older OTF help table query result.
            col_names, col_types = df_utils._get_datalake_table_columns_info(self._database,
                                                                             self._table,
                                                                             self._datalake)

        # Create a SQLAlchemy table object representing datalake table.
        t = sqlalchemy.Table(self._table, meta, schema=self._database,
                             *(Column(col_name, col_type) for col_name, col_type in zip(col_names, col_types)))
        return _MetaExpression(t, datalake=self._datalake)

    def __getattr__(self, name):
        """
        Returns an attribute of the DataFrame.

        PARAMETERS:
          name: the name of the attribute

        RETURNS:
          Return the value of the named attribute of object (if found).

        EXAMPLES:
          df = DataFrame('table')

          # you can access a column from the DataFrame
          df.c1

        RAISES:
          Attribute Error when the named attribute is not found
        """

        # look in the underlying _MetaExpression for columns
        for col in self._metaexpr.c:
            if col.name == name:
                col._parent_df = self
                return col

        # If attribute not found, look out for aggregate function with that
        # "name". If found, generate a new teradataml DataFrame which holds column-wise
        # aggregate function values with name being treated as aggregate function.
        # If not found, Exception is raised.
        if name in SQLFunctionConstants.AGGREGATE_FUNCTION_MAPPER.value:
            from teradataml.dataframe.sql_function_parameters import SQL_AGGREGATE_FUNCTION_ADDITIONAL_PARAMETERS
            sql_func_name = SQLFunctionConstants.AGGREGATE_FUNCTION_MAPPER.value[name]
            func_params = SQL_AGGREGATE_FUNCTION_ADDITIONAL_PARAMETERS.get(sql_func_name, [])
            return lambda *args, **kwargs: self._get_dataframe_aggregate(
                operation=name, call_from_df=True,
                **_validate_unimplemented_function(name, func_params, *args, **kwargs))

        raise AttributeError("'DataFrame' object has no attribute %s" % name)

    def __getitem__(self, key):
        """
        Return a column from the DataFrame or filter the DataFrame using an expression
        or select columns from a DataFrame using the specified list of columns..
        The following operators are supported:
          comparison: ==, !=, <, <=, >, >=
          boolean: & (and), | (or), ~ (not), ^ (xor)

        Operands can be Python literals and instances of ColumnExpressions from the DataFrame

        PARAMETERS:
            key:
                Required Argument.
                Specifies the column name(s) of filter expression (ColumnExpression).
                Specify key as:
                    * a string - to get the column of a teradataml DataFrame, i.e., a ColumnExpression.
                    * list of strings - to select columns of a teradataml DataFrame.
                    * filter expression - to filter the data from teradataml DataFrame using the specified expression.
                Types: str or list of strings or ColumnExpression

        RETURNS:
            DataFrame or ColumnExpression instance

        RAISES:
            1. KeyError   - If key is not found
            2. ValueError - When columns of different dataframes are given in ColumnExpression.
        
        EXAMPLES:
            # Load the example data.
            load_example_data("teradataml", ["titanic"])
            
            # Create teradataml DataFrame object.
            df = DataFrame("titanic")

            # Filter the DataFrame df.
            df[df.sibsp > df.parch]

            df[df.age >= 50]
            df[df.sex == "female"]

            df[1 != df.parch]
            df[~(1 < df.parch)]

            df[(df.parch > 0) & (df.sibsp > df.parch)]

            # Retrieve column cabin from df.
            df["cabin"]

            # Select columns using list of column names from a teradataml DataFrame..
            df[["embarked", "age", "cabin"]]
        """

        try:
            # get the ColumnExpression from the _MetaExpression
            if isinstance(key, str):
                return self.__getattr__(key)

            if isinstance(key, list):
                return self.select(key)

            if isinstance(key, ClauseElement):
                from teradataml.dataframe.sql import _SQLColumnExpression
                key = _SQLColumnExpression(key)

            # apply the filter expression
            if isinstance(key, ColumnExpression):

                if self._metaexpr is None:
                    msg = Messages.get_message(MessageCodes.TDMLDF_INFO_ERROR)
                    raise TeradataMlException(msg, MessageCodes.TDMLDF_INFO_ERROR)

                if key.get_flag_has_multiple_dataframes():
                    raise ValueError("Combining Columns from different dataframes is unsupported "
                                     "for filter [] operation.")

                clause_exp = key.compile()
                new_nodeid = self._aed_utils._aed_filter(self._nodeid, clause_exp)

                # Get the updated metaexpr
                new_metaexpr = UtilFuncs._get_metaexpr_using_parent_metaexpr(new_nodeid, self._metaexpr)
                return self._create_dataframe_from_node(new_nodeid, new_metaexpr, self._index_label)

        except TeradataMlException:
            raise

        except ValueError:
            raise

        except Exception as err:
            errcode = MessageCodes.TDMLDF_INFO_ERROR
            msg = Messages.get_message(errcode)
            raise TeradataMlException(msg, errcode) from err

        raise KeyError('Unable to find key: %s' % str(key))

    def keys(self):
        """
        RETURNS:
            a list containing the column names

        EXAMPLES:
            >>> load_example_data("dataframe","sales")
            >>> df = DataFrame.from_table('sales')
            >>> df.keys()
            ['accounts', 'Feb', 'Jan', 'Mar', 'Apr', 'datetime']
        """
        if self._column_names_and_types is not None:
            return [i[0] for i in self._column_names_and_types]
        else:
            return []

    @property
    def columns(self):
        """
        RETURNS:
            a list containing the column names

        EXAMPLES:
            >>> load_example_data("dataframe","sales")
            >>> df = DataFrame.from_table('sales')
            >>> df.columns
            ['accounts', 'Feb', 'Jan', 'Mar', 'Apr', 'datetime']
        """
        return self.keys()

    @property
    def _index_label(self):
        """
        RETURNS:
            The index_label for the DataFrame.

        EXAMPLES:
            >>> load_example_data("dataframe", "admissions_train")
            >>> df = DataFrame.from_table('admissions_train')
            >>> df._index_label
            ['id']
        """
        if not self._index_query_required:
            return self.__index_label
        else:
            try:
                self.__index_label = df_utils._get_primary_index_from_table(self._table_name)
            except Exception as err:
                # DataFrames generated from views (top node), _index_label is None when PI fetch fails.
                self.__index_label = None
            finally:
                self._index_query_required = False

            return self.__index_label

    @property
    def loc(self):
        """
        Access a group of rows and columns by label(s) or a boolean array.

        VALID INPUTS:

            - A single label, e.g. ``5`` or ``'a'``, (note that ``5`` is
            interpreted as a label of the index, it is not interpreted as an
            integer position along the index).

            - A list or array of column or index labels, e.g. ``['a', 'b', 'c']``.

            - A slice object with labels, e.g. ``'a':'f'``.
            Note that unlike the usual python slices where the stop index is not included, both the
                start and the stop are included

            - A conditional expression for row access.

            - A boolean array of the same length as the column axis for column access.

        RETURNS:
            teradataml DataFrame

        RAISE:
            TeradataMlException

        EXAMPLES
        --------
            >>> load_example_data("dataframe","sales")
            >>> df = DataFrame('sales')
            >>> df
                        Feb   Jan   Mar   Apr    datetime
            accounts
            Blue Inc     90.0    50    95   101  04/01/2017
            Alpha Co    210.0   200   215   250  04/01/2017
            Jones LLC   200.0   150   140   180  04/01/2017
            Yellow Inc   90.0  None  None  None  04/01/2017
            Orange Inc  210.0  None  None   250  04/01/2017
            Red Inc     200.0   150   140  None  04/01/2017

            # Retrieve row using a single label.
            >>> df.loc['Blue Inc']
                    Feb Jan Mar  Apr    datetime
            accounts
            Blue Inc  90.0  50  95  101  04/01/2017

            # List of labels. Note using ``[[]]``
            >>> df.loc[['Blue Inc', 'Jones LLC']]
                        Feb  Jan  Mar  Apr    datetime
            accounts
            Blue Inc    90.0   50   95  101  04/01/2017
            Jones LLC  200.0  150  140  180  04/01/2017

            # Single label for row and column (index)
            >>> df.loc['Yellow Inc', 'accounts']
            Empty DataFrame
            Columns: []
            Index: [Yellow Inc]

            # Single label for row and column
            >>> df.loc['Yellow Inc', 'Feb']
                Feb
            0  90.0

            # Single label for row and column access using a tuple
            >>> df.loc[('Yellow Inc', 'Feb')]
                Feb
            0  90.0

            # Slice with labels for row and single label for column. As mentioned
            # above, note that both the start and stop of the slice are included.
            >>> df.loc['Jones LLC':'Red Inc', 'accounts']
            Empty DataFrame
            Columns: []
            Index: [Orange Inc, Jones LLC, Red Inc]

            # Slice with labels for row and single label for column. As mentioned
            # above, note that both the start and stop of the slice are included.
            >>> df.loc['Jones LLC':'Red Inc', 'Jan']
                Jan
            0  None
            1   150
            2   150

            # Slice with labels for row and labels for column. As mentioned
            # above, note that both the start and stop of the slice are included.
            >>> df.loc['Jones LLC':'Red Inc', 'accounts':'Apr']
                        Mar   Jan    Feb   Apr
            accounts
            Orange Inc  None  None  210.0   250
            Red Inc      140   150  200.0  None
            Jones LLC    140   150  200.0   180

            # Empty slice for row and labels for column.
            >>> df.loc[:, :]
                        Feb   Jan   Mar    datetime   Apr
            accounts
            Jones LLC   200.0   150   140  04/01/2017   180
            Blue Inc     90.0    50    95  04/01/2017   101
            Yellow Inc   90.0  None  None  04/01/2017  None
            Orange Inc  210.0  None  None  04/01/2017   250
            Alpha Co    210.0   200   215  04/01/2017   250
            Red Inc     200.0   150   140  04/01/2017  None

            # Conditional expression
            >>> df.loc[df['Feb'] > 90]
                        Feb   Jan   Mar   Apr    datetime
            accounts
            Jones LLC   200.0   150   140   180  04/01/2017
            Red Inc     200.0   150   140  None  04/01/2017
            Alpha Co    210.0   200   215   250  04/01/2017
            Orange Inc  210.0  None  None   250  04/01/2017

            # Conditional expression with column labels specified
            >>> df.loc[df['Feb'] > 90, ['accounts', 'Jan']]
                        Jan
            accounts
            Jones LLC    150
            Red Inc      150
            Alpha Co     200
            Orange Inc  None

            # Conditional expression with multiple column labels specified
            >>> df.loc[df['accounts'] == 'Jones LLC', ['accounts', 'Jan', 'Feb']]
                    Jan    Feb
            accounts
            Jones LLC  150  200.0

            # Conditional expression and slice with column labels specified
            >>> df.loc[df['accounts'] == 'Jones LLC', 'accounts':'Mar']
                    Mar  Jan    Feb
            accounts
            Jones LLC  140  150  200.0

            # Conditional expression and boolean array for column access
            >>> df.loc[df['Feb'] > 90, [True, True, False, False, True, True]]
                      Feb   Apr    datetime
            accounts
            Alpha Co    210.0   250  04/01/2017
            Jones LLC   200.0   180  04/01/2017
            Red Inc     200.0  None  04/01/2017
            Orange Inc  210.0   250  04/01/2017
            >>>
        """
        return self._loc

    @property
    def iloc(self):
        """
        Access a group of rows and columns by integer values or a boolean array.
        VALID INPUTS:
            - A single integer values, e.g. 5.

            - A list or array of integer values, e.g. ``[1, 2, 3]``.

            - A slice object with integer values, e.g. ``1:6``.
              Note: The stop value is excluded.

            - A boolean array of the same length as the column axis for column access,

            Note: For integer indexing on row access, the integer index values are
            applied to a sorted teradataml DataFrame on the index column or the first column if
            there is no index column.

        RETURNS:
            teradataml DataFrame

        RAISE:
            TeradataMlException

        EXAMPLES
        --------
            >>> load_example_data("dataframe","sales")
            >>> df = DataFrame('sales')
            >>> df
                        Feb   Jan   Mar   Apr    datetime
            accounts
            Blue Inc     90.0    50    95   101  04/01/2017
            Alpha Co    210.0   200   215   250  04/01/2017
            Jones LLC   200.0   150   140   180  04/01/2017
            Yellow Inc   90.0  None  None  None  04/01/2017
            Orange Inc  210.0  None  None   250  04/01/2017
            Red Inc     200.0   150   140  None  04/01/2017

            # Retrieve row using a single integer.
            >>> df.iloc[1]
                    Feb Jan Mar  Apr    datetime
            accounts
            Blue Inc  90.0  50  95  101  04/01/2017

            # List of integers. Note using ``[[]]``
            >>> df.iloc[[1, 2]]
                        Feb  Jan  Mar  Apr    datetime
            accounts
            Blue Inc    90.0   50   95  101  04/01/2017
            Jones LLC  200.0  150  140  180  04/01/2017

            # Single integer for row and column
            >>> df.iloc[5, 0]
            Empty DataFrame
            Columns: []
            Index: [Yellow Inc]

            # Single integer for row and column
            >>> df.iloc[5, 1]
                Feb
            0  90.0

            # Single integer for row and column access using a tuple
            >>> df.iloc[(5, 1)]
                Feb
            0  90.0

            # Slice for row and single integer for column access. As mentioned
            # above, note the stop for the slice is excluded.
            >>> df.iloc[2:5, 0]
            Empty DataFrame
            Columns: []
            Index: [Orange Inc, Jones LLC, Red Inc]

            # Slice for row and a single integer for column access. As mentioned
            # above, note the stop for the slice is excluded.
            >>> df.iloc[2:5, 2]
                Jan
            0  None
            1   150
            2   150

            # Slice for row and column access. As mentioned
            # above, note the stop for the slice is excluded.
            >>> df.iloc[2:5, 0:5]
                        Mar   Jan    Feb   Apr
            accounts
            Orange Inc  None  None  210.0   250
            Red Inc      140   150  200.0  None
            Jones LLC    140   150  200.0   180

            # Empty slice for row and column access.
            >>> df.iloc[:, :]
                          Feb   Jan   Mar   Apr    datetime
            accounts
            Blue Inc     90.0    50    95   101  04/01/2017
            Alpha Co    210.0   200   215   250  04/01/2017
            Jones LLC   200.0   150   140   180  04/01/2017
            Yellow Inc   90.0  None  None  None  04/01/2017
            Orange Inc  210.0  None  None   250  04/01/2017
            Red Inc     200.0   150   140  None  04/01/2017

            # List of integers and boolean array for column access
            >>> df.iloc[[0, 2, 3, 4], [True, True, False, False, True, True]]
                          Feb   Apr    datetime
            accounts
            Orange Inc  210.0   250  04/01/2017
            Red Inc     200.0  None  04/01/2017
            Jones LLC   200.0   180  04/01/2017
            Alpha Co    210.0   250  04/01/2017
        """
        return self._iloc

    @columns.setter
    def columns(self, columns):
        """
        Assigns self._columns for the passed columns

        PARAMETERS:
            columns

        EXAMPLES:
            df.columns

        """
        self._columns = columns

    @_index_label.setter
    def _index_label(self, index_label):
        """
        Assigns self.__index_label for the passed column or list of columns.

        PARAMETERS:
            index_label:
                The columns or list of columns to set as the DataFrame's index_label.
                Types: str or List of str

        EXAMPLES:
            df._index_labels = index_label
        """
        self.__index_label = index_label
        self._index_query_required = False

    @property
    def dtypes(self):
        """
        Returns a MetaData containing the column names and types.

        PARAMETERS:

        RETURNS:
            MetaData containing the column names and Python types

        RAISES:

        EXAMPLES:
            >>> load_example_data("dataframe","sales")
            >>> df = DataFrame.from_table('sales')
            >>> print(df.dtypes)
            accounts              str
            Feb                 float
            Jan                   int
            Mar                   int
            Apr                   int
            datetime    datetime.date
            >>>

        """
        return MetaData(self._column_names_and_types)

    @property
    def tdtypes(self):
        """
        DESCRIPTION:
            Get the teradataml DataFrame metadata containing column names and
            corresponding teradatasqlalchemy types.

        RETURNS:
            MetaData containing the column names and Teradata types

        RAISES:
            None.

        EXAMPLES:
            >>> load_example_data("dataframe","sales")
            >>> df = DataFrame.from_table('sales')
            >>> print(df.tdtypes)
            accounts    VARCHAR(length=20, charset='LATIN')
            Feb                                     FLOAT()
            Jan                                    BIGINT()
            Mar                                    BIGINT()
            Apr                                    BIGINT()
            datetime                                 DATE()
            >>>
        """
        # String representation of teradatasqlalchemy.types.VARCHAR is only VARCHAR
        # but repr representation is VARCHAR(length=5, charset='LATIN')
        td_metadata = [(column.name, repr(column.type)) for column in self._metaexpr.c]
        return MetaData(td_metadata)

    @property
    def df_type(self):
        """
        DESCRIPTION:
            Returns the type of the DataFrame based on the underlying database object.
            Possible teradataml DataFrame types are:
                - VALID_TIME_VIEW: DataFrame is created on Valid-Time dimension view.
                - TRANSACTION_TIME_VIEW: DataFrame is created on Transaction-Time dimension view.
                - BI_TEMPORAL_VIEW: DataFrame is created on Bi-temporal view.
                - VALID_TIME: DataFrame is created on Valid-Time dimension table.
                - TRANSACTION_TIME: DataFrame is created on Transaction-Time dimension table.
                - BI_TEMPORAL: DataFrame is created on Bi-temporal dimension table.
                - VIEW: DataFrame is created on a view.
                - TABLE: DataFrame is created on a table.
                - OTF: DataFrame is created on an OTF table.
                - ART: DataFrame is created on an ART table.
                - VOLATILE_TABLE: DataFrame is created on a volatile table.
                - BI_TEMPORAL_VOLATILE_TABLE: DataFrame is created on a Bi-temporal dimension volatile table.
                - VALID_TIME_VOLATILE_TABLE: DataFrame is created on a Valid-Time dimension volatile table.
                - TRANSACTION_TIME_VOLATILE_TABLE: DataFrame is created on a Transaction-Time dimension volatile table.

        RETURNS:
            str

        RAISES:
            None

        EXAMPLES:
            # Load the data to run the example.
            >>> load_example_data("teradataml", "Employee_roles") # load valid time data.
            >>> load_example_data("teradataml", "Employee_Address") # load transaction time data.
            >>> load_example_data("teradataml", "Employee") # load bitemporal data.
            >>> load_example_data("uaf", ["ocean_buoys2"]) # load data to create art table.
            >>> load_example_data('dataframe', ['admissions_train']) # load data to create a regular table.

            # Example 1: DataFrame created on a Valid-Time dimension table.
            >>> df = DataFrame.from_table('Employee_roles')
            >>> df.df_type
            'VALID_TIME'

            # Example 2: DataFrame created on a Transaction-Time dimension table.
            >>> df = DataFrame.from_table('Employee_Address')
            >>> df.df_type
            'TRANSACTION_TIME'

            # Example 3: DataFrame created on a Bi-temporal dimension table.
            >>> df = DataFrame.from_table('Employee')
            >>> df.df_type
            'BI_TEMPORAL'

            # Example 4: DataFrame created on a ART table.
            >>> data = DataFrame.from_table('ocean_buoys2')
            >>> from teradataml import TDSeries,SInfo
            >>> data_series_df = TDSeries(data=data,
            ...                           id=["ocean_name","buoyid"],
            ...                           row_index="TD_TIMECODE",
            ...                           row_index_style="TIMECODE",
            ...                           payload_field="jsoncol.Measure.salinity",
            ...                           payload_content="REAL")
            >>> uaf_out = SInfo(data=data_series_df, output_table_name='TSINFO_RESULTS')
            >>> df = DataFrame.from_table('TSINFO_RESULTS')
            >>> df.df_type
            'ART'

            # Example 5: DataFrame created on a regular table.
            >>> df = DataFrame.from_table('admissions_train')
            >>> df.df_type
            'REGULAR_TABLE'

            # Example 6: DataFrame created on a volatile table.
            >>> df = DataFrame.from_table('admissions_train')
            >>> df.to_sql(table_name='admissions_train_volatile', temporary=True)
            >>> df = DataFrame.from_table('admissions_train_volatile')
            >>> df.df_type
            'VOLATILE_TABLE'

            # Example 7: DataFrame created on a Bi-temporal dimension view.
            >>> execute_sql('create view Employee_view AS SEQUENCED VALIDTIME AND SEQUENCED TRANSACTIONTIME select * from Employee')
            >>> df = DataFrame.from_table('Employee_view')
            >>> df.df_type
            'BI_TEMPORAL_VIEW'

        """

        if self._df_type is not None:
            return self._df_type

        is_view, is_volatile, is_art_table, valid_time, transaction_time = (
            self._process_columns_metadata()
        )

        # Check if the DataFrame is created from an OTF table
        if self._otf:
            self._df_type = DataFrameTypes.OTF_TABLE.value
            return self._df_type

        # Check if the DataFrame is created from an ART table
        if is_art_table:
            self._df_type = DataFrameTypes.ART_TABLE.value
            return self._df_type

        # Determine the type based on valid-time, transaction-time columns, and volatility
        if valid_time and transaction_time:
            if is_volatile:
                self._df_type = DataFrameTypes.BI_TEMPORAL_VOLATILE_TABLE.value
            else:
                self._df_type = (
                    DataFrameTypes.BI_TEMPORAL_VIEW.value
                    if is_view
                    else DataFrameTypes.BI_TEMPORAL.value
                )
        elif valid_time:
            if is_volatile:
                self._df_type = DataFrameTypes.VALID_TIME_VOLATILE_TABLE.value
            else:
                self._df_type = (
                    DataFrameTypes.VALID_TIME_VIEW.value
                    if is_view
                    else DataFrameTypes.VALID_TIME.value
                )
        elif transaction_time:
            if is_volatile:
                self._df_type = DataFrameTypes.TRANSACTION_TIME_VOLATILE_TABLE.value
            else:
                self._df_type = (
                    DataFrameTypes.TRANSACTION_TIME_VIEW.value
                    if is_view
                    else DataFrameTypes.TRANSACTION_TIME.value
                )
        else:
            self._df_type = (
                DataFrameTypes.VOLATILE_TABLE.value
                if is_volatile
                else (
                    DataFrameTypes.VIEW.value
                    if is_view
                    else DataFrameTypes.REGULAR_TABLE.value
                )
            )

        return self._df_type

    @collect_queryband(queryband="DF_info")
    def info(self, verbose=True, buf=None, max_cols=None, null_counts=False):
        """
        DESCRIPTION:
            Print a summary of the DataFrame.

        PARAMETERS:
            verbose:
                Optional Argument.
                Print full summary if True. Print short summary if False.
                Default Value: True
                Types: bool

            buf:
                Optional Argument.
                Speifies the writable buffer to send the output to.
                If argument is not specified, by default, the output is
                sent to sys.stdout.

            max_cols:
                Optional Argument.
                Speifies the maximum number of columns allowed for printing the
                full summary.
                Types: int

            null_counts:
                Optional Argument.
                Speifies whether to show the non-null counts.
                Display the counts if True, otherwise do not display the counts.
                Default Value: False
                Types: bool

        RETURNS:

        RAISES:
            TeradataMlException

        EXAMPLES:
            >>> load_example_data("dataframe","sales")
            >>> df = DataFrame.from_table('sales')
            >>> df.info()
            <class 'teradataml.dataframe.dataframe.DataFrame'>
            Data columns (total 6 columns):
            accounts              str
            Feb                 float
            Jan                   int
            Mar                   int
            Apr                   int
            datetime    datetime.date
            dtypes: datetime.date(1), float(1), str(1), int(3)
            >>>
            >>> df.info(null_counts=True)
            <class 'teradataml.dataframe.dataframe.DataFrame'>
            Data columns (total 6 columns):
            accounts    6 non-null str
            Feb         6 non-null float
            Jan         4 non-null int
            Mar         4 non-null int
            Apr         4 non-null int
            datetime    6 non-null datetime.date
            dtypes: datetime.date(1), float(1), str(1), int(3)
            >>>
            >>> df.info(verbose=False)
            <class 'teradataml.dataframe.dataframe.DataFrame'>
            Data columns (total 6 columns):
            dtypes: datetime.date(1), float(1), str(1), int(3)
            >>>
        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["verbose", verbose, True, (bool)])
        awu_matrix.append(["max_cols", max_cols, True, (int)])
        awu_matrix.append(["null_counts", null_counts, True, (bool)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        try:
            output_buf = sys.stdout
            if buf is not None:
                output_buf = buf

            num_columns = len(self._column_names_and_types)
            suffix = ""
            if num_columns > 1:
                suffix = "s"

            col_names = [i[0] for i in self._column_names_and_types]
            col_types = [i[1] for i in self._column_names_and_types]

            # Print the class name for self.
            print(str(type(self)), file=output_buf)
            # Print the total number of columns
            print("Data columns (total {0} column{1}):".format(num_columns, suffix), file=output_buf)

            # If max_cols and the number of columns exceeds max_cols, do not print the column names and types
            if max_cols is not None and len(col_names) > max_cols:
                verbose = False

            # If verbose, print the column names and types.
            if verbose:
                # If null_counts, print the number of non-null values for each column if this is not an empty dataframe.
                if null_counts and self._table_name is not None:
                    null_count_str = UtilFuncs._get_non_null_counts(col_names, self._table_name)
                    zipped = zip(col_names, col_types, null_count_str)
                    column_names_and_types = list(zipped)
                    null_count = True
                # Else just print the column names and types
                else:
                    column_names_and_types = self._column_names_and_types
                    null_count = False
                print("{}".format(df_utils._get_pprint_dtypes(column_names_and_types, null_count)), file=output_buf)

            # Print the dtypes and count of each dtypes
            unique_types = list(set(col_types))
            for i in range(0, len(unique_types)):
                unique_types[i] = "{0}({1})".format(unique_types[i], col_types.count(unique_types[i]))
            print("dtypes: {}".format(", ".join(unique_types)), file=output_buf)
        except TeradataMlException:
            raise
        except Exception as err:
            raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_INFO_ERROR),
                                      MessageCodes.TDMLDF_INFO_ERROR) from err

    @collect_queryband(queryband="DF_head")
    def head(self, n=display.max_rows, deterministic=False):
        """
        DESCRIPTION:
            Print the first n rows of the teradataml DataFrame.

        PARAMETERS:
            n:
                Optional Argument.
                Specifies the number of rows to select.
                Note:
                    - If 'deterministic' is False, 'n' should (0,100].
                Default Value: 10.
                Types: int

            deterministic:
                Optional Argument.
                Specifies whether to select the first n rows from the sorted DataFrame or not.
                Notes:
                    * If True, the first n rows of the sorted DataFrame are selected.
                      The DataFrame is sorted on the index column or the first column if
                      there is no index column. The column type must support sorting.
                      Unsupported types: ['BLOB', 'CLOB', 'ARRAY', 'VARRAY']
                    * If False, a random sample of n rows is selected.
                Default Value: False.
                Types: bool

        RETURNS:
            teradataml DataFrame

        RAISES:
            TeradataMlException

        EXAMPLES:
            >>> load_example_data("dataframe","admissions_train")
            >>> df = DataFrame.from_table('admissions_train')
            >>> df
               masters   gpa     stats programming admitted
            id
            15     yes  4.00  Advanced    Advanced        1
            7      yes  2.33    Novice      Novice        1
            22     yes  3.46    Novice    Beginner        0
            17      no  3.83  Advanced    Advanced        1
            13      no  4.00  Advanced      Novice        1
            38     yes  2.65  Advanced    Beginner        1
            26     yes  3.57  Advanced    Advanced        1
            5       no  3.44    Novice      Novice        0
            34     yes  3.85  Advanced    Beginner        0
            40     yes  3.95    Novice    Beginner        0
            
            >>> df.head()
               masters   gpa     stats programming admitted
            id
            3       no  3.70    Novice    Beginner        1
            5       no  3.44    Novice      Novice        0
            6      yes  3.50  Beginner    Advanced        1
            7      yes  2.33    Novice      Novice        1
            9       no  3.82  Advanced    Advanced        1
            10      no  3.71  Advanced    Advanced        1
            8       no  3.60  Beginner    Advanced        1
            4      yes  3.50  Beginner      Novice        1
            2      yes  3.76  Beginner    Beginner        0
            1      yes  3.95  Beginner    Beginner        0
            
            >>> df.head(15, deterministic=True)
               masters   gpa     stats programming admitted
            id
            3       no  3.70    Novice    Beginner        1
            5       no  3.44    Novice      Novice        0
            6      yes  3.50  Beginner    Advanced        1
            7      yes  2.33    Novice      Novice        1
            9       no  3.82  Advanced    Advanced        1
            10      no  3.71  Advanced    Advanced        1
            11      no  3.13  Advanced    Advanced        1
            12      no  3.65    Novice      Novice        1
            13      no  4.00  Advanced      Novice        1
            14     yes  3.45  Advanced    Advanced        0
            15     yes  4.00  Advanced    Advanced        1
            8       no  3.60  Beginner    Advanced        1
            4      yes  3.50  Beginner      Novice        1
            2      yes  3.76  Beginner    Beginner        0
            1      yes  3.95  Beginner    Beginner        0
            
            >>> df.head(5, deterministic=True)
               masters   gpa     stats programming admitted
            id
            3       no  3.70    Novice    Beginner        1
            5       no  3.44    Novice      Novice        0
            4      yes  3.50  Beginner      Novice        1
            2      yes  3.76  Beginner    Beginner        0
            1      yes  3.95  Beginner    Beginner        0

            >>> df.head(3, deterministic=False)
              masters   gpa     stats programming  admitted
            id                                              
            40     yes  3.95    Novice    Beginner         0
            19     yes  1.98  Advanced    Advanced         0
            38     yes  2.65  Advanced    Beginner         1
        """
        # Validate argument types
        _Validators._validate_function_arguments([["n", n, True, (int)]])
        _Validators._validate_function_arguments([["deterministic", deterministic, True, (bool)]])

        # Validate n is a positive int.
        # If deterministic is False, n should be in (0,100]
        _Validators._validate_positive_int(n, "n", 0, None if deterministic else 100)

        try:
            if self._metaexpr is None:
                raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_INFO_ERROR),
                                          MessageCodes.TDMLDF_INFO_ERROR)

            sort_col = self._get_sort_col()
            return df_utils._get_sorted_nrow(self, n, sort_col[0], asc=True, deterministic=deterministic)
        except TeradataMlException:
            raise
        except Exception as err:
            raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_INFO_ERROR),
                                      MessageCodes.TDMLDF_INFO_ERROR) from err

    @collect_queryband(queryband="DF_tail")
    def tail(self, n=display.max_rows, deterministic=False):
        """
        DESCRIPTION:
            Print the last n rows of the sorted teradataml DataFrame.

        PARAMETERS:
            n:
                Optional Argument.
                Specifies the number of rows to select.
                Default Value: 10.
                Types: int

            deterministic:
                Optional Argument.
                Specifies whether to select the last n rows from the sorted DataFrame or not.
                Notes:
                    * If True, the last n rows of the sorted DataFrame are selected.
                      The Dataframe is sorted on the index column or the first column if
                      there is no index column. The column type must support sorting.
                      Unsupported types: ['BLOB', 'CLOB', 'ARRAY', 'VARRAY']
                    * If False, a random sample of n rows is selected.
                Default Value: False.
                Types: bool

        RETURNS:
            teradataml DataFrame

        RAISES:
            TeradataMlException

        EXAMPLES:
            >>> load_example_data("dataframe","admissions_train")
            >>> df = DataFrame.from_table('admissions_train')
            >>> df
               masters   gpa     stats programming admitted
            id
            15     yes  4.00  Advanced    Advanced        1
            7      yes  2.33    Novice      Novice        1
            22     yes  3.46    Novice    Beginner        0
            17      no  3.83  Advanced    Advanced        1
            13      no  4.00  Advanced      Novice        1
            38     yes  2.65  Advanced    Beginner        1
            26     yes  3.57  Advanced    Advanced        1
            5       no  3.44    Novice      Novice        0
            34     yes  3.85  Advanced    Beginner        0
            40     yes  3.95    Novice    Beginner        0

            >>> df.tail()
               masters   gpa     stats programming admitted
            id
            38     yes  2.65  Advanced    Beginner        1
            36      no  3.00  Advanced      Novice        0
            35      no  3.68    Novice    Beginner        1
            34     yes  3.85  Advanced    Beginner        0
            32     yes  3.46  Advanced    Beginner        0
            31     yes  3.50  Advanced    Beginner        1
            33      no  3.55    Novice      Novice        1
            37      no  3.52    Novice      Novice        1
            39     yes  3.75  Advanced    Beginner        0
            40     yes  3.95    Novice    Beginner        0

            >>> df.tail(3, deterministic=True)
               masters   gpa     stats programming admitted
            id
            38     yes  2.65  Advanced    Beginner        1
            39     yes  3.75  Advanced    Beginner        0
            40     yes  3.95    Novice    Beginner        0

            >>> df.tail(15, deterministic=True)
               masters   gpa     stats programming admitted
            id
            38     yes  2.65  Advanced    Beginner        1
            36      no  3.00  Advanced      Novice        0
            35      no  3.68    Novice    Beginner        1
            34     yes  3.85  Advanced    Beginner        0
            32     yes  3.46  Advanced    Beginner        0
            31     yes  3.50  Advanced    Beginner        1
            30     yes  3.79  Advanced      Novice        0
            29     yes  4.00    Novice    Beginner        0
            28      no  3.93  Advanced    Advanced        1
            27     yes  3.96  Advanced    Advanced        0
            26     yes  3.57  Advanced    Advanced        1
            33      no  3.55    Novice      Novice        1
            37      no  3.52    Novice      Novice        1
            39     yes  3.75  Advanced    Beginner        0
            40     yes  3.95    Novice    Beginner        0

            >>> df.tail(3, deterministic=False)
               masters   gpa     stats programming  admitted
            id                                              
            33      no  3.55    Novice      Novice         1
            37      no  3.52    Novice      Novice         1
            20     yes  3.90  Advanced    Advanced         1
        """
        # Validate argument types
        _Validators._validate_function_arguments([["n", n, True, (int)]])
        _Validators._validate_function_arguments([["deterministic", deterministic, True, (bool)]])

        # Validate n is a positive int.
        _Validators._validate_positive_int(n, "n")

        try:
            if self._metaexpr is None:
                raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_INFO_ERROR),
                                          MessageCodes.TDMLDF_INFO_ERROR)

            sort_col = self._get_sort_col()
            return df_utils._get_sorted_nrow(self, n, sort_col[0], asc=False, deterministic=deterministic)
        except TeradataMlException:
            raise
        except Exception as err:
            raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_INFO_ERROR),
                                      MessageCodes.TDMLDF_INFO_ERROR) from err

    def _get_axis(self, axis):
        """
        Private method to retrieve axis value, 0 for index or 1 for columns

        PARAMETERS:
            axis - 0 or 'index' for index labels
                   1 or 'columns' for column labels

        RETURNS:
            0 or 1

        RAISE:
            TeradataMlException

        EXAMPLES:
            a = self._get_axis(0)
            a = self._get_axis(1)
            a = self._get_axis('index')
            a = self._get_axis('columns')
        """
        if isinstance(axis, str):
            if axis == "index":
                return 0
            elif axis == "columns":
                return 1
            else:
                raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_INVALID_DROP_AXIS),
                                          MessageCodes.TDMLDF_INVALID_DROP_AXIS)
        elif isinstance(axis, numbers.Integral):
            if axis in [0, 1]:
                return axis
            else:
                raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_INVALID_DROP_AXIS),
                                          MessageCodes.TDMLDF_INVALID_DROP_AXIS)
        else:
            raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_INVALID_DROP_AXIS),
                                      MessageCodes.TDMLDF_INVALID_DROP_AXIS)

    def _get_sort_col(self):
        """
        Private method to retrieve sort column.
        If _index_labels is not None, return first column and type in _index_labels.
        Otherwise return first column and type in _metadata.

        PARAMETERS:

        RETURNS:
            A tuple containing the column name and type in _index_labels or first column in _metadata.

        RAISE:

        EXAMPLES:
            sort_col = self._get_sort_col()
        """
        unsupported_types = ['BLOB', 'CLOB', 'ARRAY', 'VARRAY']

        if self._index_label is not None:
            if isinstance(self._index_label, list):
                col_name = self._index_label[0]
            else:
                col_name = self._index_label
        else:  # Use the first column from metadata
            col_name = self.columns[0]

        col_type = PythonTypes.PY_NULL_TYPE.value
        for name, py_type in self._column_names_and_types:
            if col_name == name:
                col_type = py_type

        if col_type == PythonTypes.PY_NULL_TYPE.value:
            raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_INFO_ERROR),
                                      MessageCodes.TDMLDF_INFO_ERROR)

        sort_col_sqlalchemy_type = (self._metaexpr.t.c[col_name].type)
        # convert types to string from sqlalchemy type for the columns entered for sort
        sort_col_type = repr(sort_col_sqlalchemy_type).split("(")[0]
        if sort_col_type in unsupported_types:
            raise TeradataMlException(Messages.get_message(MessageCodes.UNSUPPORTED_DATATYPE, sort_col_type,
                                                           "ANY, except following {}".format(unsupported_types)),
                                      MessageCodes.UNSUPPORTED_DATATYPE)

        return (col_name, col_type)

    @collect_queryband(queryband="DF_drop")
    def drop(self, labels=None, axis=0, columns=None):
        """
        DESCRIPTION:
            Drop specified labels from rows or columns.

            Remove rows or columns by specifying label names and corresponding
            axis, or by specifying the index or column names directly.

        PARAMETERS:
            labels:
                Optional Argument. Required when columns is not provided.
                Single label or list-like. Can be Index or column labels to drop depending on axis.
                Types: str OR list of Strings (str)

            axis:
                Optional Argument.
                0 or 'index' for index labels
                1 or 'columns' for column labels
                Default Values: 0
                Permitted Values: 0, 1, 'index', 'columns'
                Types: int OR str

            columns:
                Optional Argument. Required when labels is not provided.
                Single label or list-like. This is an alternative to specifying axis=1 with labels.
                Cannot specify both labels and columns.
                Types: str OR list of Strings (str)

        RETURNS:
            teradataml DataFrame

        RAISE:
            TeradataMlException

        EXAMPLES:
            >>> load_example_data("dataframe","admissions_train")
            >>> df = DataFrame('admissions_train')
            >>> df
               masters   gpa     stats programming admitted
            id
            5       no  3.44    Novice      Novice        0
            7      yes  2.33    Novice      Novice        1
            22     yes  3.46    Novice    Beginner        0
            17      no  3.83  Advanced    Advanced        1
            13      no  4.00  Advanced      Novice        1
            19     yes  1.98  Advanced    Advanced        0
            36      no  3.00  Advanced      Novice        0
            15     yes  4.00  Advanced    Advanced        1
            34     yes  3.85  Advanced    Beginner        0
            40     yes  3.95    Novice    Beginner        0

            # Drop columns
            >>> df.drop(['stats', 'admitted'], axis=1)
               programming masters   gpa
            id
            5       Novice      no  3.44
            34    Beginner     yes  3.85
            13      Novice      no  4.00
            40    Beginner     yes  3.95
            22    Beginner     yes  3.46
            19    Advanced     yes  1.98
            36      Novice      no  3.00
            15    Advanced     yes  4.00
            7       Novice     yes  2.33
            17    Advanced      no  3.83

            >>> df.drop(columns=['stats', 'admitted'])
               programming masters   gpa
            id
            5       Novice      no  3.44
            34    Beginner     yes  3.85
            13      Novice      no  4.00
            19    Advanced     yes  1.98
            15    Advanced     yes  4.00
            40    Beginner     yes  3.95
            7       Novice     yes  2.33
            22    Beginner     yes  3.46
            36      Novice      no  3.00
            17    Advanced      no  3.83

            # Drop a row by index
            >>> df1 = df[df.gpa == 4.00]
            >>> df1
               masters  gpa     stats programming admitted
            id
            13      no  4.0  Advanced      Novice        1
            29     yes  4.0    Novice    Beginner        0
            15     yes  4.0  Advanced    Advanced        1
            >>> df1.drop([13,15], axis=0)
               masters  gpa   stats programming admitted
            id
            29     yes  4.0  Novice    Beginner        0
            >>>
        """

        # Argument validations
        awu_matrix = []
        awu_matrix.append(["axis", axis, True, (int, str), False, [0, 1, 'index', 'columns']])
        awu_matrix.append(["columns", columns, True, (str, list), True])

        # Make sure either columns or labels is provided.
        if (labels is None and columns is None) or (labels is not None and columns is not None):
            raise TeradataMlException(Messages.get_message(MessageCodes.EITHER_THIS_OR_THAT_ARGUMENT, "labels",
                                                           "columns"),
                                      MessageCodes.EITHER_THIS_OR_THAT_ARGUMENT)

        # Validate argument types
        _Validators._validate_input_columns_not_empty(labels, "labels")
        _Validators._validate_missing_required_arguments(awu_matrix)
        _Validators._validate_function_arguments(awu_matrix)

        try:
            column_labels = None
            index_labels = None

            if labels is not None:
                if self._get_axis(axis) == 0:
                    index_labels = labels
                else:
                    column_labels = labels
            else:  # Columns is not None
                column_labels = columns

            if index_labels is not None:
                sort_col = self._get_sort_col()
                df_utils._validate_sort_col_type(sort_col[1], index_labels)

                if isinstance(index_labels, list):
                    if len(index_labels) == 0:
                        raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_DROP_ARGS),
                                                  MessageCodes.TDMLDF_DROP_ARGS)

                    if sort_col[1] == PythonTypes.PY_STRING_TYPE.value:
                        index_labels = ["'{}'".format(x) for x in index_labels]
                    index_expr = ",".join(map(str, (index_labels)))
                else:
                    if sort_col[1] == PythonTypes.PY_STRING_TYPE.value:
                        index_expr = "'{}'".format(index_labels)
                    else:
                        index_expr = index_labels

                filter_expr = "{0} not in ({1})".format(sort_col[0], index_expr)
                new_nodeid = self._aed_utils._aed_filter(self._nodeid, filter_expr)
                # Get the updated metaexpr
                new_metaexpr = UtilFuncs._get_metaexpr_using_parent_metaexpr(new_nodeid, self._metaexpr)
                return self._create_dataframe_from_node(new_nodeid, new_metaexpr, self._index_label)
            else:  # Column labels
                select_cols = []
                cols = [x.name for x in self._metaexpr.columns]
                if isinstance(column_labels, list):
                    if len(column_labels) == 0:
                        raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_DROP_ARGS),
                                                  MessageCodes.TDMLDF_DROP_ARGS)

                    if not all(isinstance(n, str) for n in column_labels):
                        raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_DROP_INVALID_COL_NAMES),
                                                  MessageCodes.TDMLDF_DROP_INVALID_COL_NAMES)
                    drop_cols = [x for x in column_labels]
                elif isinstance(column_labels, (tuple, dict)):
                    raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_DROP_ARGS),
                                              MessageCodes.TDMLDF_DROP_ARGS)
                else:
                    if not isinstance(column_labels, str):
                        raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_DROP_INVALID_COL_NAMES),
                                                  MessageCodes.TDMLDF_DROP_INVALID_COL_NAMES)
                    drop_cols = [column_labels]

                for drop_name in drop_cols:
                    if drop_name not in cols:
                        msg = Messages.get_message(MessageCodes.TDMLDF_DROP_INVALID_COL).format(drop_name, cols)
                        raise TeradataMlException(msg, MessageCodes.TDMLDF_DROP_INVALID_COL)

                for colname in cols:
                    if colname not in drop_cols:
                        select_cols.append(colname)
                if len(select_cols) > 0:
                    return self.select(select_cols)
                else:  # no columns selected
                    raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_DROP_ALL_COLS),
                                              MessageCodes.TDMLDF_DROP_ALL_COLS)

        except TeradataMlException:
            raise
        except Exception as err:
            raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_INFO_ERROR),
                                      MessageCodes.TDMLDF_INFO_ERROR) from err

    @collect_queryband(queryband="DF_dropna")
    def dropna(self, how='any', thresh=None, subset=None):
        """
        DESCRIPTION:
            Removes rows with null values.
            Note:
                * This method does not support operations on array columns.

        PARAMETERS:
            how:
                Optional Argument.
                Specifies how rows are removed.
                'any' removes rows with at least one null value.
                'all' removes rows with all null values.
                Default Value: 'any'
                Permitted Values: 'any' or 'all'
                Types: str

            thresh:
                Optional Argument.
                Specifies the minimum number of non null values in a row to include.
                Types: int

            subset:
                Optional Argument.
                Specifies list of column names to include, in array-like format.
                Types: str OR list of Strings (str)

        RETURNS:
            teradataml DataFrame

        RAISE:
            TeradataMlException

        EXAMPLES:
            >>> load_example_data("dataframe","sales")
            >>> df = DataFrame('sales')
            >>> df
                          Feb   Jan   Mar   Apr    datetime
            accounts
            Jones LLC   200.0   150   140   180  04/01/2017
            Yellow Inc   90.0  None  None  None  04/01/2017
            Orange Inc  210.0  None  None   250  04/01/2017
            Blue Inc     90.0    50    95   101  04/01/2017
            Alpha Co    210.0   200   215   250  04/01/2017
            Red Inc     200.0   150   140  None  04/01/2017

            # Drop the rows where at least one element is null.
            >>> df.dropna()
                         Feb  Jan  Mar  Apr    datetime
            accounts
            Blue Inc    90.0   50   95  101  04/01/2017
            Jones LLC  200.0  150  140  180  04/01/2017
            Alpha Co   210.0  200  215  250  04/01/2017

            # Drop the rows where all elements are nulls for columns 'Jan' and 'Mar'.
            >>> df.dropna(how='all', subset=['Jan','Mar'])
                         Feb  Jan  Mar   Apr    datetime
            accounts
            Alpha Co   210.0  200  215   250  04/01/2017
            Jones LLC  200.0  150  140   180  04/01/2017
            Red Inc    200.0  150  140  None  04/01/2017
            Blue Inc    90.0   50   95   101  04/01/2017

            # Keep only the rows with at least 4 non null values.
            >>> df.dropna(thresh=4)
                          Feb   Jan   Mar   Apr    datetime
            accounts
            Jones LLC   200.0   150   140   180  04/01/2017
            Blue Inc     90.0    50    95   101  04/01/2017
            Orange Inc  210.0  None  None   250  04/01/2017
            Alpha Co    210.0   200   215   250  04/01/2017
            Red Inc     200.0   150   140  None  04/01/2017

            # Keep only the rows with at least 5 non null values.
            >>> df.dropna(thresh=5)
                         Feb  Jan  Mar   Apr    datetime
            accounts
            Alpha Co   210.0  200  215   250  04/01/2017
            Jones LLC  200.0  150  140   180  04/01/2017
            Blue Inc    90.0   50   95   101  04/01/2017
            Red Inc    200.0  150  140  None  04/01/2017
        """

        # Argument validations
        awu_matrix = []
        awu_matrix.append(["how", how, True, (str), True, ['any', 'all']])
        awu_matrix.append(["thresh", thresh, True, (int)])
        awu_matrix.append(["subset", subset, True, (str, list), True])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        # Validate n is a positive int.
        _Validators._validate_positive_int(thresh, "thresh")

        # Checking each element in passed columns to be valid column in dataframe
        _Validators._validate_column_exists_in_dataframe(subset, self._metaexpr)

        try:
            col_names = [item.lower() for item in self.keys()]
            if subset is not None:
                col_filters = subset
            else:
                col_filters = col_names

            col_filters_decode = ["CASE WHEN \"{}\" IS NULL THEN 0 ELSE 1 END".format(col_name) for col_name in col_filters]
            fmt_filter = " + ".join(col_filters_decode)

            if thresh is not None:
                filter_expr = "{0} >= {1}".format(fmt_filter, thresh)
            elif how == 'any':
                filter_expr = "{0} = {1}".format(fmt_filter, len(col_filters))
            else:  # how == 'all'
                filter_expr = "{0} > 0".format(fmt_filter)

            new_nodeid = self._aed_utils._aed_filter(self._nodeid, filter_expr)

            # Get the updated metaexpr
            new_metaexpr = UtilFuncs._get_metaexpr_using_parent_metaexpr(new_nodeid, self._metaexpr)
            return self._create_dataframe_from_node(new_nodeid, new_metaexpr, self._index_label)
        except TeradataMlException:
            raise
        except Exception as err:
            raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_INFO_ERROR),
                                      MessageCodes.TDMLDF_INFO_ERROR) from err

    @collect_queryband(queryband="DF_sort")
    def sort(self, columns, ascending=True):
        """
        DESCRIPTION:
            Get sorted data by one or more columns in either ascending or descending order
            for a Dataframe.
            Unsupported column types for sorting: ['BLOB', 'CLOB', 'ARRAY', 'VARRAY']

        PARAMETERS:
            columns:
                Required Argument.
                Specifies the name(s) of the columns or ColumnExpression(s) to sort on.
                Types: str OR ColumnExpression OR list of Strings (str) OR list of ColumnExpressions.

            ascending:
                Optional Argument.
                Specifies whether to order in ascending or descending order for each column.
                When set to True, sort in ascending order. Otherwise, sort in descending order.
                Notes:
                     * If a list is specified, length of the 'ascending' must equal
                       length of the "columns".
                     * If a list is specified, element is ignored if the corresponding
                       element in "columns" is a ColumnExpression.
                     * The argument is ignored if "columns" is a ColumnExpression.
                Default value: True
                Types: bool or list of bool

        RETURNS:
            teradataml DataFrame

        RAISES:
            TeradataMlException

        EXAMPLES:
            >>> load_example_data("dataframe", "sales")
            >>> df = DataFrame("sales")
            >>> df
                          Feb    Jan    Mar    Apr    datetime
            accounts
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017

            # Example 1: Sort the data based on the column Feb in
            #            ascending order by passing the name of the column.
            >>> df.sort("Feb")
                          Feb    Jan    Mar    Apr    datetime
            accounts
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017
            >>>

            # Example 2: Sort the data based on the column Feb in
            #            descending order by passing the ColumnExpression.
            >>> df.sort([df.Feb.desc()])
                          Feb    Jan    Mar    Apr    datetime
            accounts
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017
            >>>

            # Example 3: Sort the data based on the columns Jan and accounts in
            #            ascending order and descending order with NULLS at first
            #            respectively.
            #            Note:
            #                * Since the second element in "columns" is a ColumnExpression,
            #                  the data is sorted in descending order even though the
            #                  second element in "ascending" is True.
            >>> df.sort(["Jan", df.accounts.desc().nulls_first()], [True, True])
                          Feb    Jan    Mar    Apr    datetime
            accounts
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017
            >>>

            # Example 4: Sort the data based on columns Jan and Apr in ascending order
            #            with NULLS at first and descending order with NULLS at first
            #            respectively.
            >>> df.sort([df.Jan.nulls_first(), df.Apr.desc().nulls_first()])
                          Feb    Jan    Mar    Apr    datetime
            accounts
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017
            >>>

            # Example 5: Sort the data based on columns Jan and Apr in ascending order
            #            with NULLS at first and descending order with NULLS at last
            #            respectively.
            >>> df.sort([df.Jan.nulls_first(), df.Apr.desc().nulls_last()])
                          Feb    Jan    Mar    Apr    datetime
            accounts
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017
            >>>
        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["columns", columns, True, (str, ColumnExpression, list), True])
        awu_matrix.append(["ascending", ascending, True, (bool, list)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        # Preserve the actual 'columns' in a seperate variable.
        columns_ = UtilFuncs._as_list(columns)

        # Regenerate 'columns' to hold only column names.
        columns = [column.name if isinstance(column, ColumnExpression) else column for column in columns_]

        # Checking each element in passed columns to be valid column in dataframe
        _Validators._validate_column_exists_in_dataframe(columns, self._metaexpr)

        try:
            orderexpr = ""
            type_expr = []
            invalid_types = []
            invalid_columns = []
            unsupported_types = ['BLOB', 'CLOB', 'ARRAY', 'VARRAY']

            if (isinstance(columns, str)):
                columns = [columns]
            if isinstance(ascending, bool):
                ascending = [ascending] * len(columns)

            # Validating lengths of passed arguments which are passed i.e. length of columns
            # must be same as ascending
            if ascending and len(columns) != len(ascending):
                raise TeradataMlException(Messages.get_message(MessageCodes.INVALID_LENGTH_ARGS,
                                                               '"columns" and "ascending"'),
                                          MessageCodes.INVALID_LENGTH_ARGS)

            # Let's get the column types for the columns passed to 'columns' argument.
            for col in columns:
                type_expr.append(self._metaexpr.t.c[col].type)

            # Convert types to string from sqlalchemy type for the columns entered for sort
            columns_types = [repr(type_expr[i]).split("(")[0] for i in range(len(type_expr))]

            # Checking each element in passed columns_types to be valid a data type for sort
            # and create a list of invalid_types
            for column_name, col_type in zip(columns, columns_types):
                if col_type in unsupported_types:
                    invalid_columns.append(column_name)
                    invalid_types.append(col_type)
            if len(invalid_types) > 0:
                raise TeradataMlException(
                    Messages.get_message(MessageCodes.INVALID_COLUMN_DATATYPE, ", ".join(invalid_columns), 'columns',
                                         "Invalid", ", ".join(unsupported_types)),
                    MessageCodes.UNSUPPORTED_DATATYPE)

            orderexpr = []
            get_column_order = lambda asc: 'ASC' if asc else 'DESC'
            for index, col in enumerate(columns_):
                if isinstance(col, str):
                    quoted_column_name = UtilFuncs._teradata_quote_arg(col, "\"", False)
                    expr = "{} {}".format(quoted_column_name, get_column_order(ascending[index]))
                else:
                    # col is a ColumnExpression.
                    # ColumnExpression.compile() gives the expression however it returns
                    # column name with out double quotes.
                    # >>> data_input = DataFrame.from_table("titanic")
                    # >>> data_input.passenger.desc().nulls_first().compile()
                    # 'passenger DESC NULLS FIRST'
                    # >>>
                    # Hence quote it and then form the expression.
                    expr_elements = col.compile().split()
                    expr_elements[0] = UtilFuncs._teradata_quote_arg(col.name, "\"", False)
                    expr = " ".join(expr_elements)
                orderexpr.append(expr)

            orderexpr = ", ".join(orderexpr)

            # We are just updating orderby clause in exisitng teradataml dataframe
            # and returning new teradataml dataframe.
            sort_df = self._create_dataframe_from_node(self._nodeid, self._metaexpr, self._index_label)
            sort_df._orderby = orderexpr

            # Assigning self attributes to newly created dataframe.
            sort_df._table_name = self._table_name
            sort_df._index = self._index
            sort_df._index_label = self._index_label
            return sort_df
        except TeradataMlException:
            raise

    @collect_queryband(queryband="DF_filter")
    def filter(self, items=None, like=None, regex=None, axis=1, **kw):
        """
        DESCRIPTION:
            Filter rows or columns of dataframe according to labels in the specified index.
            The filter is applied to the columns of the index when axis is set to 'rows'.

            Must use one of the parameters 'items', 'like', and 'regex' only.

        PARAMETERS:
            axis:
                Optional Argument.
                Specifies the axis to filter on.
                1 denotes column axis (default). Alternatively, 'columns' can be specified.
                0 denotes row axis. Alternatively, 'rows' can be specified.
                Default Values: 1
                Permitted Values: 0, 1, 'rows', 'columns'
                Types: int OR str

            items:
                Optional Argument.
                List of values that the info axis should be restricted to.
                When axis is 1, items are a list of column names.
                When axis is 0, items are a list of literal values.
                Types: list of Strings (str) or literals

            like:
                Optional Argument.
                Specifies the substring pattern.
                When axis is 1, substring pattern for matching column names.
                When axis is 0, substring pattern for checking index values
                with REGEXP_SUBSTR.
                Types: str

            regex:
                Optional Argument.
                Specifies a regular expression pattern.
                When axis is 1, regex pattern for re.search(regex, column_name).
                When axis is 0, regex pattern for checking index values
                with REGEXP_SUBSTR.
                Types: str

            **kw: optional keyword arguments

                varchar_size:
                    Specifies an integer to specify the size of varchar-casted index.
                    Used when axis=0 or axis='rows' and index must be char-like in
                    "like" and "regex" filtering.
                    Default Value: configure.default_varchar_size
                    Types: int

                match_arg: string
                    Specifies argument to pass if axis is 0/'rows' and regex is used.

                    Valid values for match_arg are:
                    - 'i': case-insensitive matching.
                    - 'c': case sensitive matching.
                    - 'n': the period character (match any character) can match
                           the newline character.
                    - 'm': index value is treated as multiple lines instead of as a single
                           line. With this option, the '^' and '$' characters apply to each
                           line in source_string instead of the entire index value.
                    - 'l': if index value exceeds the current maximum allowed size
                           (currently 16 MB), a NULL is returned instead of an error.
                           This is useful for long-running queries where you do not want
                           long strings causing an error that would make the query fail.
                    - 'x': ignore whitespace.

            The 'match_arg' argument may contain more than one character.
            If a character in 'match_arg' is not valid, then that character is ignored.

            See Teradata Database SQL Functions, Operators, Expressions, and Predicates,
            for more information on specifying arguments for REGEXP_SUBSTR.

            NOTES:
                - Using 'regex' or 'like' with axis equal to 0 will attempt to cast the
                  values in the index to a VARCHAR.
                  Note that conversion between BYTE data and other types is not supported.
                  Also, LOBs are not allowed to be compared.

                - When using 'like' or 'regex', datatypes are casted into VARCHAR.
                  This may alter the format of the value in the column(s)
                  and thus whether there is a match or not. The size of the VARCHAR may also
                  play a role since the casted value is truncated if the size is not big enough.
                  See varchar_size under **kw: optional keyword arguments.

        RETURNS:
            teradataml DataFrame

        RAISES:
            ValueError if more than one parameter: 'items', 'like', or 'regex' is used.
            TeradataMlException if invalid argument values are given.

        EXAMPLES:
            >>> load_example_data("dataframe","admissions_train")
            >>> df = DataFrame('admissions_train')
            >>> df
               masters   gpa     stats programming admitted
            id
            22     yes  3.46    Novice    Beginner        0
            37      no  3.52    Novice      Novice        1
            35      no  3.68    Novice    Beginner        1
            12      no  3.65    Novice      Novice        1
            4      yes  3.50  Beginner      Novice        1
            38     yes  2.65  Advanced    Beginner        1
            27     yes  3.96  Advanced    Advanced        0
            39     yes  3.75  Advanced    Beginner        0
            7      yes  2.33    Novice      Novice        1
            40     yes  3.95    Novice    Beginner        0
            >>>
            >>> # retrieve columns masters, gpa, and stats in df
            ... df.filter(items = ['masters', 'gpa', 'stats'])
              masters   gpa     stats
            0     yes  4.00  Advanced
            1     yes  3.45  Advanced
            2     yes  3.50  Advanced
            3     yes  4.00    Novice
            4     yes  3.59  Advanced
            5      no  3.87    Novice
            6     yes  3.50  Beginner
            7     yes  3.79  Advanced
            8      no  3.00  Advanced
            9     yes  1.98  Advanced
            >>>
            >>> # retrieve rows where index matches 2, 4
            ... df.filter(items = ['2', '4'], axis = 0)
               masters   gpa     stats programming admitted
            id
            2      yes  3.76  Beginner    Beginner        0
            4      yes  3.50  Beginner      Novice        1
            >>>
            >>> df = DataFrame('admissions_train', index_label="programming")
            >>> df
                         id masters   gpa     stats admitted
            programming
            Beginner     22     yes  3.46    Novice        0
            Novice       37      no  3.52    Novice        1
            Beginner     35      no  3.68    Novice        1
            Novice       12      no  3.65    Novice        1
            Novice        4     yes  3.50  Beginner        1
            Beginner     38     yes  2.65  Advanced        1
            Advanced     27     yes  3.96  Advanced        0
            Beginner     39     yes  3.75  Advanced        0
            Novice        7     yes  2.33    Novice        1
            Beginner     40     yes  3.95    Novice        0
            >>>
            >>> # retrieve columns with a matching substring
            ... df.filter(like = 'masters')
              masters
            0     yes
            1     yes
            2     yes
            3     yes
            4     yes
            5      no
            6     yes
            7     yes
            8      no
            9     yes
            >>>
            >>> # retrieve rows where index values have vice as a subtring
            ... df.filter(like = 'vice', axis = 'rows')
                         id masters   gpa     stats admitted
            programming
            Novice       12      no  3.65    Novice        1
            Novice        5      no  3.44    Novice        0
            Novice       24      no  1.87  Advanced        1
            Novice       36      no  3.00  Advanced        0
            Novice       23     yes  3.59  Advanced        1
            Novice       13      no  4.00  Advanced        1
            Novice       33      no  3.55    Novice        1
            Novice       30     yes  3.79  Advanced        0
            Novice        4     yes  3.50  Beginner        1
            Novice       37      no  3.52    Novice        1
            >>>
            >>> # give a regular expression to match column names
            ... df.filter(regex = '^a.+')
              admitted
            0        0
            1        1
            2        1
            3        1
            4        1
            5        1
            6        0
            7        0
            8        1
            9        0
            >>>
            >>> # give a regular expression to match values in index
            ... df.filter(regex = '^B.+', axis = 0)
                         id masters   gpa     stats admitted
            programming
            Beginner     39     yes  3.75  Advanced        0
            Beginner     38     yes  2.65  Advanced        1
            Beginner      3      no  3.70    Novice        1
            Beginner     31     yes  3.50  Advanced        1
            Beginner     21      no  3.87    Novice        1
            Beginner     34     yes  3.85  Advanced        0
            Beginner     32     yes  3.46  Advanced        0
            Beginner     29     yes  4.00    Novice        0
            Beginner     35      no  3.68    Novice        1
            Beginner     22     yes  3.46    Novice        0
            >>>
            >>> # case-insensitive, ignore white space when matching index values
            ... df.filter(regex = '^A.+', axis = 0, match_args = 'ix')
                         id masters   gpa     stats admitted
            programming
            Advanced     20     yes  3.90  Advanced        1
            Advanced      8      no  3.60  Beginner        1
            Advanced     25      no  3.96  Advanced        1
            Advanced     19     yes  1.98  Advanced        0
            Advanced     14     yes  3.45  Advanced        0
            Advanced      6     yes  3.50  Beginner        1
            Advanced     17      no  3.83  Advanced        1
            Advanced     11      no  3.13  Advanced        1
            Advanced     15     yes  4.00  Advanced        1
            Advanced     18     yes  3.81  Advanced        1
            >>>
            >>> # case-insensitive/ ignore white space/ match up to 32 characters
            ... df.filter(regex = '^A.+', axis = 0, match_args = 'ix', varchar_size = 32)
                         id masters   gpa     stats admitted
            programming
            Advanced     20     yes  3.90  Advanced        1
            Advanced      8      no  3.60  Beginner        1
            Advanced     25      no  3.96  Advanced        1
            Advanced     19     yes  1.98  Advanced        0
            Advanced     14     yes  3.45  Advanced        0
            Advanced      6     yes  3.50  Beginner        1
            Advanced     17      no  3.83  Advanced        1
            Advanced     11      no  3.13  Advanced        1
            Advanced     15     yes  4.00  Advanced        1
            Advanced     18     yes  3.81  Advanced        1
            >>>
        """

        # Argument validations
        awu_matrix = []
        awu_matrix.append(["axis", axis, True, (int, str), False, [0, 1, 'columns', 'rows']])
        awu_matrix.append(["like", like, True, (str)])
        awu_matrix.append(["regex", regex, True, (str)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        if self._index_label is None and axis in (0, 'rows'):
            raise AttributeError('DataFrame must have index_label set to a valid column')

        axis = 1 if axis == 'columns' or axis == 1 else 0
        errcode = MessageCodes.UNSUPPORTED_DATATYPE

        # validate items, like, regex type and value
        op = ''

        if items is not None:
            op += 'items'
            valid_value = (type(items) is list) and len(set(map(lambda x: type(x), items))) == 1

        if like is not None:
            op += 'like'
            valid_value = type(like) is str

        if regex is not None:
            op += 'regex'
            valid_value = type(regex) is str

        if op not in ('items', 'like', 'regex'):
            raise ValueError('Must use exactly one of the parameters items, like, and regex.')

        if not valid_value:
            msg = 'The "items" parameter must be list of strings or tuples of column labels/index values. ' + \
                  'The "regex" parameter and "like" parameter must be strings.'
            raise TeradataMlException(msg, errcode)

        # validate multi index labels for items
        if op == 'items' and axis == 0:

            num_col_indexes = len(self._index_label)
            if num_col_indexes > 1 and not all(map(lambda entry: len(entry) == num_col_indexes, items)):
                raise ValueError('tuple length in items must match length of multi index: %d' % num_col_indexes)

        # validate the optional keyword args
        if kw is not None and 'match_arg' in kw:
            if not isinstance(kw['match_arg'], str):
                msg = Messages.get_message(errcode, type(kw['match_arg']), 'match_arg', 'string')
                raise TeradataMlException(msg, errcode)

        if kw is not None and 'varchar_size' in kw:
            if not isinstance(kw['varchar_size'], int):
                msg = Messages.get_message(errcode, type(kw['varchar_size']), 'varchar_size', 'int')
                raise TeradataMlException(msg, errcode)

        # generate the sql expression
        expression = self._metaexpr._filter(axis, op, self._index_label,
                                            items=items,
                                            like=like,
                                            regex=regex,
                                            **kw)

        if axis == 1 and isinstance(expression, list):
            return self.select(expression)

        elif axis == 0 and isinstance(expression, ColumnExpression):
            return self.__getitem__(expression)

        else:
            errcode = MessageCodes.TDMLDF_INFO_ERROR
            msg = Messages.get_message(errcode)
            raise TeradataMlException(msg, errcode)

    @collect_queryband(queryband="DF_describe")
    def describe(self, percentiles=[.25, .5, .75], verbose=False, distinct=False, statistics=None,
                 columns=None, pivot=False):
        """
        DESCRIPTION:
            Generates statistics for numeric columns. This function can be used in two modes:
                1. Regular Aggregate Mode.
                    It computes the count, mean, std, min, percentiles, and max for numeric columns.
                    Default statistics include:
                        "count", "mean", "std", "min", "percentile", "max"
                2. Time Series Aggregate Mode.
                    It computes max, mean, min, std, median, mode, and percentiles for numeric columns.
                    Default statistics include:
                        'max', 'mean', 'min', 'std'

            Notes:
                * Regular Aggregate Mode: If describe() is used on the output of any DataFrame API or groupby(),
                                        then describe() is used as regular aggregation.
                * Time Series Aggregate Mode: If describe() is used on the output of groupby_time(), then describe()
                                            is a time series aggregate, where time series aggregates are used
                                            to calculate the statistics.
                * This method does not support operations on array columns.

        PARAMETERS:
            percentiles:
                Optional Argument.
                A list of values between 0 and 1. Applicable for both modes.
                By default, percentiles are calculated for statistics for 'Regular Aggregate Mode', whereas
                for 'Time Series Aggregate Mode', percentiles are calculated when verbose is set to True.
                Default Values: [.25, .5, .75], which returns the 25th, 50th, and 75th percentiles.
                Types: float or List of floats

            verbose:
                Optional Argument.
                Specifies a boolean value to be used for time series aggregation, stating whether to get
                verbose output or not.
                When this argument is set to 'True', function calculates median, mode, and percentile values
                on top of its default statistics.
                Note:
                    verbose as 'True' is not applicable for 'Regular Aggregate Mode'.
                Default Values: False
                Types: bool

            distinct:
                Optional Argument.
                Specifies a boolean value to decide whether to consider duplicate rows in statistic
                calculation or not. By default, duplicate values are considered for statistic calculation.
                When this is set to True, only distinct rows are considered for statistic calculation.
                Default Values: False
                Types: bool

            statistics:
                Optional Argument.
                Specifies the aggregate operation to be performed.
                Computes count, mean, std, min, percentiles, and max for numeric columns.
                Computes count and unique for non-numeric columns.
                Notes:
                    1. statistics is not applicable for 'Time Series Aggregate Mode'.
                Permitted Values: count, mean, min, max, unique, std, describe, percentile
                Default Values: None
                Types: str or List of str

            columns:
                Optional Argument.
                Specifies the name(s) of the columns we are collecting statistics for.
                Default Values: None
                Types: str or List of str
            
            pivot:
                Optional Argument.
                Specifies a boolean value to pivot the output.
                Note:
                    * "pivot" is not supported for PTI tables.
                Default Values: 'False'
                Types: bool

        RETURNS:
            teradataml DataFrame

        RAISE:
            TeradataMlException

        EXAMPLES:
            >>> load_example_data("dataframe","sales")
            >>> df = DataFrame('sales')
            >>> print(df)
                          Feb   Jan   Mar   Apr    datetime
            accounts
            Blue Inc     90.0    50    95   101  04/01/2017
            Alpha Co    210.0   200   215   250  04/01/2017
            Jones LLC   200.0   150   140   180  04/01/2017
            Yellow Inc   90.0  None  None  None  04/01/2017
            Red Inc     200.0   150   140  None  04/01/2017
            Orange Inc  210.0  None  None   250  04/01/2017

            # Computes count, mean, std, min, percentiles, and max for numeric columns.
            >>> df.describe(pivot=True)
                      Apr      Feb     Mar     Jan
            func
            count       4        6       4       4
            mean   195.25  166.667   147.5   137.5
            std    70.971   59.554  49.749  62.915
            min       101       90      95      50
            25%    160.25    117.5  128.75     125
            50%       215      200     140     150
            75%       250    207.5  158.75   162.5
            max       250      210     215     200

            # Computes count, mean, std, min, percentiles, and max for numeric columns with 
            # default arugments.
            >>> df.describe()
            ATTRIBUTE   StatName            StatValue
            Jan	        MAXIMUM	            200.0
            Jan	        STANDARD DEVIATION	62.91528696058958
            Jan	        PERCENTILES(25)	    125.0
            Jan	        PERCENTILES(50)	    150.0
            Mar	        COUNT	            4.0
            Mar	        MINIMUM	            95.0
            Mar	        MAXIMUM	            215.0
            Mar	        MEAN	            147.5
            Mar	        STANDARD DEVIATION	49.749371855331
            Mar	        PERCENTILES(25)	    128.75
            Mar	        PERCENTILES(50)	    140.0
            Apr	        COUNT	            4.0
            Apr	        MINIMUM	            101.0
            Apr	        MAXIMUM	            250.0
            Apr	        MEAN	            195.25
            Apr	        STANDARD DEVIATION	70.97123830585646
            Apr	        PERCENTILES(25)	    160.25
            Apr	        PERCENTILES(50)	    215.0
            Apr	        PERCENTILES(75)	    250.0
            Feb	        COUNT	            6.0
            Feb	        MINIMUM	            90.0
            Feb	        MAXIMUM	            210.0
            Feb	        MEAN	            166.66666666666666
            Feb	        STANDARD DEVIATION	59.553897157672786
            Feb	        PERCENTILES(25)	    117.5
            Feb	        PERCENTILES(50)	    200.0
            Feb	        PERCENTILES(75)	    207.5
            Mar	        PERCENTILES(75)	    158.75
            Jan	        PERCENTILES(75)	    162.5
            Jan	        MEAN	            137.5
            Jan	        MINIMUM	            50.0
            Jan	        COUNT	            4.0

            # Computes count, mean, std, min, percentiles, and max for numeric columns with 30th and 60th percentiles.
            >>> df.describe(percentiles=[.3, .6], pivot=True)
                      Apr      Feb     Mar     Jan
            func
            count       4        6       4       4
            mean   195.25  166.667   147.5   137.5
            std    70.971   59.554  49.749  62.915
            min       101       90      95      50
            30%     172.1      145   135.5     140
            60%       236      200     140     150
            max       250      210     215     200

            # Computes count, mean, std, min, percentiles, and max for numeric columns group by "datetime" and "Feb".
            >>> df1 = df.groupby(["datetime", "Feb"])
            >>> df1.describe(pivot=True)
                                     Jan   Mar   Apr
            datetime   Feb   func
            04/01/2017 90.0  25%      50    95   101
                             50%      50    95   101
                             75%      50    95   101
                             count     1     1     1
                             max      50    95   101
                             mean     50    95   101
                             min      50    95   101
                             std    None  None  None
                       200.0 25%     150   140   180
                             50%     150   140   180
                             75%     150   140   180
                             count     2     2     1
                             max     150   140   180
                             mean    150   140   180
                             min     150   140   180
                             std       0     0  None
                       210.0 25%     200   215   250
                             50%     200   215   250
                             75%     200   215   250
                             count     1     1     2
                             max     200   215   250
                             mean    200   215   250
                             min     200   215   250
                             std    None  None     0

            # Examples for describe() function as Time Series Aggregate.
            #
            >>> # Load the example datasets.
            ... load_example_data("dataframe", ["ocean_buoys"])
            >>>

            >>> # Create the required DataFrames.
            ... # DataFrame on non-sequenced PTI table
            ... ocean_buoys = DataFrame("ocean_buoys")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys.columns
            ['buoyid', 'TD_TIMECODE', 'temperature', 'salinity']
            >>> ocean_buoys.head()
                                   TD_TIMECODE  temperature  salinity
            buoyid
            0       2014-01-06 08:10:00.000000        100.0        55
            0       2014-01-06 08:08:59.999999          NaN        55
            1       2014-01-06 09:01:25.122200         77.0        55
            1       2014-01-06 09:03:25.122200         79.0        55
            1       2014-01-06 09:01:25.122200         70.0        55
            1       2014-01-06 09:02:25.122200         71.0        55
            1       2014-01-06 09:03:25.122200         72.0        55
            0       2014-01-06 08:09:59.999999         99.0        55
            0       2014-01-06 08:00:00.000000         10.0        55
            0       2014-01-06 08:10:00.000000         10.0        55

            >>> ocean_buoys_grpby = ocean_buoys.groupby_time(timebucket_duration="2cy", value_expression="buoyid", fill="NULLS")
            >>>

            #
            # Example 1: Get the basic statistics for time series aggregation for all the numeric columns.
            #            This returns max, mean, min and std values.
            #
            >>> ocean_buoys_grpby.describe()
                                                                                                       temperature salinity
            TIMECODE_RANGE                                     GROUP BY TIME(CAL_YEARS(2)) buoyid func
            ('2014-01-01 00:00:00.000000-00:00', '2016-01-0... 2                           0      max          100       55
                                                                                                  mean       54.75       55
                                                                                                  min           10       55
                                                                                                  std       51.674        0
                                                                                           1      max           79       55
                                                                                                  mean        74.5       55
                                                                                                  min           70       55
                                                                                                  std        3.937        0
                                                                                           2      max           82       55
                                                                                                  mean          81       55
                                                                                                  min           80       55
                                                                                                  std            1        0
                                                                                           44     max           56       55
                                                                                                  mean      48.077       55
                                                                                                  min           43       55
                                                                                                  std        5.766        0
            >>>

            #
            # Example 2: Get the verbose statistics for time series aggregation for all the numeric columns.
            #            This returns max, mean, min, std, median, mode, 25th, 50th and 75th percentile.
            #
            >>> ocean_buoys_grpby.describe(verbose=True)
                                                                                                         temperature salinity
            TIMECODE_RANGE                                     GROUP BY TIME(CAL_YEARS(2)) buoyid func
            ('2014-01-01 00:00:00.000000-00:00', '2016-01-0... 2                           0      25%             10       55
                                                                                                  50%           54.5       55
                                                                                                  75%          99.25       55
                                                                                                  max            100       55
                                                                                                  mean         54.75       55
                                                                                                  median        54.5       55
                                                                                                  min             10       55
                                                                                                  mode            10       55
                                                                                                  std         51.674        0
                                                                                           1      25%          71.25       55
                                                                                                  50%           74.5       55
                                                                                                  75%          77.75       55
                                                                                                  max             79       55
                                                                                                  mean          74.5       55
                                                                                                  median        74.5       55
                                                                                                  min             70       55
                                                                                                  mode            71       55
                                                                                                  mode            72       55
                                                                                                  mode            77       55
                                                                                                  mode            78       55
                                                                                                  mode            79       55
                                                                                                  mode            70       55
                                                                                                  std          3.937        0
                                                                                           2      25%           80.5       55
                                                                                                  50%             81       55
                                                                                                  75%           81.5       55
                                                                                                  max             82       55
                                                                                                  mean            81       55
                                                                                                  median          81       55
                                                                                                  min             80       55
                                                                                                  mode            80       55
                                                                                                  mode            81       55
                                                                                                  mode            82       55
                                                                                                  std              1        0
                                                                                           44     25%             43       55
                                                                                                  50%             43       55
                                                                                                  75%             53       55
                                                                                                  max             56       55
                                                                                                  mean        48.077       55
                                                                                                  median          43       55
                                                                                                  min             43       55
                                                                                                  mode            43       55
                                                                                                  std          5.766        0
            >>>

            #
            # Example 3: Get the basic statistics for time series aggregation for all the numeric columns,
            #            consider only unique values.
            #            This returns max, mean, min and std values.
            #
            >>> ocean_buoys_grpby.describe(distinct=True)
                                                                                                       temperature salinity
            TIMECODE_RANGE                                     GROUP BY TIME(CAL_YEARS(2)) buoyid func
            ('2014-01-01 00:00:00.000000-00:00', '2016-01-0... 2                           0      max          100       55
                                                                                                  mean      69.667       55
                                                                                                  min           10       55
                                                                                                  std       51.675     None
                                                                                           1      max           79       55
                                                                                                  mean        74.5       55
                                                                                                  min           70       55
                                                                                                  std        3.937     None
                                                                                           2      max           82       55
                                                                                                  mean          81       55
                                                                                                  min           80       55
                                                                                                  std            1     None
                                                                                           44     max           56       55
                                                                                                  mean        52.2       55
                                                                                                  min           43       55
                                                                                                  std        5.263     None
            >>>

            #
            # Example 4: Get the verbose statistics for time series aggregation for all the numeric columns.
            #            This select non-default percentiles 33rd and 66th.
            #            This returns max, mean, min, std, median, mode, 33rd, and 66th percentile.
            #
            >>> ocean_buoys_grpby.describe(verbose=True, percentiles=[0.33, 0.66])
                                                                                                         temperature salinity
            TIMECODE_RANGE                                     GROUP BY TIME(CAL_YEARS(2)) buoyid func
            ('2014-01-01 00:00:00.000000-00:00', '2016-01-0... 2                           0      33%             10       55
                                                                                                  66%          97.22       55
                                                                                                  max            100       55
                                                                                                  mean         54.75       55
                                                                                                  median        54.5       55
                                                                                                  min             10       55
                                                                                                  mode            10       55
                                                                                                  std         51.674        0
                                                                                           1      33%          71.65       55
                                                                                                  66%           77.3       55
                                                                                                  max             79       55
                                                                                                  mean          74.5       55
                                                                                                  median        74.5       55
                                                                                                  min             70       55
                                                                                                  mode            70       55
                                                                                                  mode            71       55
                                                                                                  mode            77       55
                                                                                                  mode            78       55
                                                                                                  mode            79       55
                                                                                                  mode            72       55
                                                                                                  std          3.937        0
                                                                                           2      33%          80.66       55
                                                                                                  66%          81.32       55
                                                                                                  max             82       55
                                                                                                  mean            81       55
                                                                                                  median          81       55
                                                                                                  min             80       55
                                                                                                  mode            80       55
                                                                                                  mode            81       55
                                                                                                  mode            82       55
                                                                                                  std              1        0
                                                                                           44     33%             43       55
                                                                                                  66%             53       55
                                                                                                  max             56       55
                                                                                                  mean        48.077       55
                                                                                                  median          43       55
                                                                                                  min             43       55
                                                                                                  mode            43       55
                                                                                                  std          5.766        0
            >>>
        """

        # -------------Argument validations---------------#
        awu_matrix = []
        awu_matrix.append(["columns", columns, True, (str, list), True])
        awu_matrix.append(["percentiles", percentiles, True, (float, list)])
        awu_matrix.append(["verbose", verbose, True, (bool)])
        awu_matrix.append(["distinct", distinct, True, (bool)])
        awu_matrix.append(["statistics", statistics, True, (str, list), True,
                           ["count", "mean", "min", "max", "unique", "std", "describe", "percentile"]])
        awu_matrix.append(["pivot", pivot, True, (bool)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        # Checking each element in passed columns to be valid column in dataframe
        _Validators._validate_column_exists_in_dataframe(columns, self._metaexpr)

        # Validate argument values.
        if isinstance(percentiles, float):
            percentiles = [percentiles]

        if columns:
            columns = UtilFuncs._as_list(columns)

        # Converting the statistics list to lower case
        if statistics:
            statistics = [stats.lower() for stats in UtilFuncs._as_list(statistics)]

        # Percentiles must be a list of values between 0 and 1.
        if not isinstance(percentiles, list) or not all(p > 0 and p < 1 for p in percentiles):
            raise ValueError(Messages.get_message(MessageCodes.INVALID_ARG_VALUE, percentiles, "percentiles",
                                                  "percentiles must be a list of values between 0 and 1"))

        # Argument 'statistics' is not allowed for DataFrameGroupByTime
        if statistics is not None and isinstance(self, DataFrameGroupByTime):
            raise ValueError(Messages.get_message(MessageCodes.ARG_VALUE_CLASS_DEPENDENCY).format(
                'statistics', 'Aggregation', statistics, 'describe()', 'DataFrame or DataFrameGroupBy'))

        # Argument 'verbose' with value True is not allowed for non DataFrameGroupByTime objects.
        if verbose and not isinstance(self, DataFrameGroupByTime):
            raise ValueError(Messages.get_message(MessageCodes.ARG_VALUE_CLASS_DEPENDENCY).format(
                'verbose', 'Aggregation', 'True', 'describe()', 'DataFrameGroupByTime'))
        # -------------End of argument validations---------------#

        function_label = "func"
        sort_cols = []
        try:
            self.__execute_node_and_set_table_name(self._nodeid)

            groupby_column_list = None
            if isinstance(self, DataFrameGroupByTime) or isinstance(self, DataFrameGroupBy):
                groupby_column_list = self.groupby_column_list
                if columns:
                    df_utils._validate_describe_columns(columns=columns, metaexpr=self._metaexpr,
                                                        groupby_column_list=groupby_column_list)
                sort_cols = list(groupby_column_list)

            # 'func' column will be always there in result.
            sort_cols.append(function_label)

            # Handle DataFrameGroupByTime using union all approach and
            # other DataFrames using TD_UnivariateStatistics approach.
            if isinstance(self, DataFrameGroupByTime):
                # Construct the aggregate query.
                agg_query = df_utils._construct_describe_query(df=self, columns=columns, metaexpr=self._metaexpr,
                                                               percentiles=percentiles, function_label=function_label,
                                                               groupby_column_list=groupby_column_list, include=None,
                                                               is_time_series_aggregate=True, verbose=verbose,
                                                               distinct=distinct,
                                                               timebucket_duration=self._timebucket_duration,
                                                               value_expression=self._value_expression,
                                                               timecode_column=self._timecode_column,
                                                               sequence_column=self._sequence_column,
                                                               fill=self._fill)

                if groupby_column_list is not None:
                    df = DataFrame.from_query(agg_query, index_label=sort_cols)
                    df2 = df.sort(sort_cols)
                    df2._metaexpr._n_rows = 100
                    describe_df = df2
                else:
                    describe_df = DataFrame.from_query(agg_query, index_label=function_label)

                # Check if numeric overflow can occur for result DataFrame.
                if self._check_numeric_overflow(describe_df):
                    result_df = self._promote_dataframe_types()
                    describe_df = result_df.describe(pivot=True)
                return describe_df

            else:
                # If pivot is True, then construct the aggregate query and return the result DataFrame.
                # Otherwise, return the result DataFrame in the regular aggregate mode using UnivariateStatistics.

                if pivot:
                    # Construct the aggregate query.
                    agg_query = df_utils._construct_describe_query(df=self, columns=columns, metaexpr=self._metaexpr,
                                                                percentiles=percentiles, function_label=function_label,
                                                                groupby_column_list=groupby_column_list, include=None,
                                                                is_time_series_aggregate=False, verbose=verbose,
                                                                distinct=distinct, statistics=statistics)

                    if groupby_column_list is not None:
                        sort_cols = [i for i in groupby_column_list]
                        sort_cols.append(function_label)
                        df = DataFrame.from_query(agg_query, index_label=sort_cols)
                        df2 = df.sort(sort_cols)
                        df2._metaexpr._n_rows = 100
                        describe_df = df2
                    else:
                        describe_df = DataFrame.from_query(agg_query, index_label=function_label)

                    # Check if numeric overflow can occur for result DataFrame.
                    if self._check_numeric_overflow(describe_df):
                        result_df = self._promote_dataframe_types()
                        describe_df = result_df.describe(pivot=True)
                    
                    return describe_df

                # If columns is None, then all dataframe columns are considered.
                if columns is None:
                    columns = self.columns
                    # Exclude groupby columns
                    if groupby_column_list is not None:
                        columns = [col for col in columns if col not in groupby_column_list]

                numeric_cols = []

                # Extract numeric columns and their types of all columns
                for col in self._metaexpr.c:
                    if type(col.type) in UtilFuncs()._get_numeric_datatypes() and \
                    col.name in columns:
                        numeric_cols.append(col.name)

                if numeric_cols:
                    # Default statistics for 'Regular Aggregate Mode'
                    sql_stat = ["COUNT", "MAXIMUM", "MEAN", "MINIMUM", "PERCENTILES", "STANDARD DEVIATION"]
                    
                    if statistics is not None:
                        py_to_sql_func_map = {"count": "COUNT",
                                              "max": "MAXIMUM",
                                              "mean": "MEAN",
                                              "unique": 'UNIQUE ENTITY COUNT',
                                              "min": "MINIMUM",
                                              "percentile": "PERCENTILES",
                                              "std": "STANDARD DEVIATION"}
                        # Convert statistics into corresponding SQL function names
                        sql_stat = [py_to_sql_func_map[stat] for stat in UtilFuncs()._as_list(statistics)]

                    # Convert percentiles to centiles for univariate statistics
                    centiles = list(map(lambda n: int(n * 100), percentiles))

                    # UnivariateStatistics parameters
                    univar_param = {
                        "newdata": self.select(self.columns),
                        "target_columns": numeric_cols,
                        "partition_columns": groupby_column_list,
                        "centiles": centiles,
                        "stats": sql_stat
                    }

                    from teradataml import UnivariateStatistics
                    # Run UnivariateStatistics
                    aggr_df = UnivariateStatistics(**univar_param).result

                    # Return the result in teradataml format
                    return aggr_df

        except TeradataMlException:
            raise
        except Exception as err:
            raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_INFO_ERROR),
                                      MessageCodes.TDMLDF_INFO_ERROR) from err

    @collect_queryband(queryband="DF_kurtosis")
    def kurtosis(self, distinct=False):
        """
        DESCRIPTION:
            Returns column-wise kurtosis value of the dataframe.
            Kurtosis is the fourth moment of the distribution of the standardized (z) values.
            It is a measure of the outlier (rare, extreme observation) character of the distribution as
            compared with the normal (or Gaussian) distribution.
                * The normal distribution has a kurtosis of 0.
                * Positive kurtosis indicates that the distribution is more outlier-prone than the
                  normal distribution.
                * Negative kurtosis indicates that the distribution is less outlier-prone than the
                  normal distribution.

            Notes:
                1. This function is valid only on columns with numeric types.
                2. Null values are not included in the result computation.
                3. Following conditions will produce null result:
                    a. Fewer than three non-null data points in the data used for the computation.
                    b. Standard deviation for a column is equal to 0.
                4. This method does not support DataFrame containing array columns.

        PARAMETERS:
            distinct:
                Optional Argument.
                Specifies whether to exclude duplicate values while calculating the kurtosis value.
                Default Values: False
                Types: bool

        RETURNS:
            teradataml DataFrame object with kurtosis()
            operation performed.

        RAISES:
            TeradataMLException
            1. EXECUTION_FAILED - If kurtosis() operation fails to
               generate the column-wise kurtosis value of the dataframe.

               Possible error message:
               Failed to perform 'kurtosis'. (Followed by error message)

            2. TDMLDF_AGGREGATE_COMBINED_ERR - If the kurtosis() operation
               doesn't support all the columns in the dataframe.

               Possible error message:
               No results. Below is/are the error message(s):
               All selected columns [(col2 -  PERIOD_TIME), (col3 -
               BLOB)] is/are unsupported for 'kurtosis' operation.

        EXAMPLES :
            # Load the data to run the example.
            >>> from teradataml.data.load_example_data import load_example_data
            >>> load_example_data("dataframe", ["admissions_train"])

            # Create teradataml dataframe.
            >>> df1 = DataFrame("admissions_train")
            >>> print(df1.sort("id"))
               masters   gpa     stats programming  admitted
            id
            1      yes  3.95  Beginner    Beginner         0
            2      yes  3.76  Beginner    Beginner         0
            3       no  3.70    Novice    Beginner         1
            4      yes  3.50  Beginner      Novice         1
            5       no  3.44    Novice      Novice         0
            6      yes  3.50  Beginner    Advanced         1
            7      yes  2.33    Novice      Novice         1
            8       no  3.60  Beginner    Advanced         1
            9       no  3.82  Advanced    Advanced         1
            10      no  3.71  Advanced    Advanced         1
            >>>

            # Prints kurtosis value of each column
            >>> df1.kurtosis()
               kurtosis_id  kurtosis_gpa  kurtosis_admitted
            0         -1.2      4.052659            -1.6582
            >>>

            #
            # Using kurtosis() as Time Series Aggregate.
            #
            >>> # Load the example datasets.
            ... load_example_data("dataframe", ["ocean_buoys"])
            >>>
            >>> # Create the required DataFrames.
            ... # DataFrame on non-sequenced PTI table
            ... ocean_buoys = DataFrame("ocean_buoys")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys.columns
            ['buoyid', 'TD_TIMECODE', 'temperature', 'salinity']
            >>> ocean_buoys.head()
                                   TD_TIMECODE  temperature  salinity
            buoyid
            0       2014-01-06 08:10:00.000000        100.0        55
            0       2014-01-06 08:08:59.999999          NaN        55
            1       2014-01-06 09:01:25.122200         77.0        55
            1       2014-01-06 09:03:25.122200         79.0        55
            1       2014-01-06 09:01:25.122200         70.0        55
            1       2014-01-06 09:02:25.122200         71.0        55
            1       2014-01-06 09:03:25.122200         72.0        55
            0       2014-01-06 08:09:59.999999         99.0        55
            0       2014-01-06 08:00:00.000000         10.0        55
            0       2014-01-06 08:10:00.000000         10.0        55

            #
            # Time Series Aggregate Example 1: Executing kurtosis() function on DataFrame created on
            #                                  non-sequenced PTI table. We will consider all rows for the
            #                                  columns while calculating the kurtosis values.
            #
            # To use kurtosis() as Time Series Aggregate we must run groupby_time() first, followed by kurtosis().
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.kurtosis().sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid kurtosis_salinity  kurtosis_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0              None             -5.998128
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1              None             -2.758377
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2              None                   NaN
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44              None             -2.195395
            >>>

            #
            # Time Series Aggregate Example 2: Executing kurtosis() function on DataFrame created on
            #                                  non-sequenced PTI table. We will consider DISTINCT values for the
            #                                  columns while calculating the kurtosis value.
            #
            # To use kurtosis() as Time Series Aggregate we must run groupby_time() first, followed by kurtosis().
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.kurtosis(distinct = True).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid kurtosis_salinity  kurtosis_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0              None                   NaN
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1              None             -2.758377
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2              None                   NaN
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44              None              4.128426
            >>>
        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["distinct", distinct, True, (bool)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        return self._get_dataframe_aggregate(operation='kurtosis', distinct=distinct)

    @collect_queryband(queryband="DF_min")
    def min(self, distinct=False):
        """
        DESCRIPTION:
            Returns column-wise minimum value of the dataframe.
            Notes:
                * Null values are not included in the result computation.
                * This method does not support DataFrame containing array columns.

        PARAMETERS:
            distinct:
                Optional Argument.
                Specifies whether to exclude duplicate values while calculating the minimum value.
                Default Values: False
                Types: bool

        RETURNS:
            teradataml DataFrame object with min()
            operation performed.

        RAISES:
            TeradataMLException
            1. EXECUTION_FAILED - If min() operation fails to
                generate the column-wise minimum value of the dataframe.

                Possible error message:
                Failed to perform 'min'. (Followed by error message)

            2. TDMLDF_AGGREGATE_COMBINED_ERR - If the min() operation
                doesn't support all the columns in the dataframe.

                Possible error message:
                No results. Below is/are the error message(s):
                All selected columns [(col2 -  PERIOD_TIME), (col3 -
                BLOB)] is/are unsupported for 'min' operation.

        EXAMPLES :
            # Load the data to run the example.
            >>> from teradataml.data.load_example_data import load_example_data
            >>> load_example_data("dataframe", ["employee_info"])

            # Create teradataml dataframe.
            >>> df1 = DataFrame("employee_info")
            >>> print(df1)
                        first_name marks   dob joined_date
            employee_no
            101              abcde  None  None    02/12/05
            100               abcd  None  None        None
            112               None  None  None    18/12/05
            >>>

            # Prints minimum value of each column(with supported data types).
            >>> df1.min()
              min_employee_no min_first_name min_marks min_dob min_joined_date
            0             100           abcd      None    None        02/12/05
            >>>

            # Select only subset of columns from the DataFrame.
            # Prints minimum value of each column(with supported data types).
            >>> df3 = df1.select(['employee_no', 'first_name', 'joined_date'])
            >>> df3.min()
              min_employee_no min_first_name min_joined_date
            0             100           abcd        02/12/05
            >>>

            #
            # Using min() as Time Series Aggregate.
            #
            >>> # Load the example datasets.
            ... load_example_data("dataframe", ["ocean_buoys"])
            >>>
            >>> # Create the required DataFrames.
            ... # DataFrame on non-sequenced PTI table
            ... ocean_buoys = DataFrame("ocean_buoys")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys.columns
            ['buoyid', 'TD_TIMECODE', 'temperature', 'salinity']
            >>> ocean_buoys.head()
                                   TD_TIMECODE  temperature  salinity
            buoyid
            0       2014-01-06 08:10:00.000000        100.0        55
            0       2014-01-06 08:08:59.999999          NaN        55
            1       2014-01-06 09:01:25.122200         77.0        55
            1       2014-01-06 09:03:25.122200         79.0        55
            1       2014-01-06 09:01:25.122200         70.0        55
            1       2014-01-06 09:02:25.122200         71.0        55
            1       2014-01-06 09:03:25.122200         72.0        55
            0       2014-01-06 08:09:59.999999         99.0        55
            0       2014-01-06 08:00:00.000000         10.0        55
            0       2014-01-06 08:10:00.000000         10.0        55

            #
            # Time Series Aggregate Example 1: Executing min() function on DataFrame created on
            #                                  non-sequenced PTI table. We will consider all rows for the
            #                                  columns while calculating the minimum values.
            #
            # To use min() as Time Series Aggregate we must run groupby_time() first, followed by min().
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.min().sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid             min_TD_TIMECODE  min_salinity  min_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0  2014-01-06 08:00:00.000000            55               10
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1  2014-01-06 09:01:25.122200            55               70
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2  2014-01-06 21:01:25.122200            55               80
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44  2014-01-06 10:00:24.000000            55               43
            >>>

            #
            # Time Series Aggregate Example 2: Executing min() function on DataFrame created on
            #                                  non-sequenced PTI table. We will consider DISTINCT values for the
            #                                  columns while calculating the minimum value.
            #
            # To use min() as Time Series Aggregate we must run groupby_time() first, followed by min().
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.min(distinct = True).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid             min_TD_TIMECODE  min_salinity  min_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0  2014-01-06 08:00:00.000000            55               10
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1  2014-01-06 09:01:25.122200            55               70
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2  2014-01-06 21:01:25.122200            55               80
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44  2014-01-06 10:00:24.000000            55               43
            >>>
        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["distinct", distinct, True, (bool)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        return self._get_dataframe_aggregate(operation='min', distinct=distinct)

    @collect_queryband(queryband="DF_max")
    def max(self, distinct=False):
        """
        DESCRIPTION:
            Returns column-wise maximum value of the dataframe.
            Notes:
                * Null values are not included in the result computation.
                * This method does not support DataFrame containing array columns.

        PARAMETERS:
            distinct:
                Optional Argument.
                Specifies whether to exclude duplicate values while calculating the maximum value.
                Default Values: False
                Types: bool

        RETURNS:
            teradataml DataFrame object with max()
            operation performed.

        RAISES:
            TeradataMLException
            1. EXECUTION_FAILED - If max() operation fails to
                generate the column-wise maximum value of the dataframe.

                Possible error message:
                Failed to perform 'max'. (Followed by error message)

            2. TDMLDF_AGGREGATE_COMBINED_ERR - If the max() operation
                doesn't support all the columns in the dataframe.

                Possible error message:
                No results. Below is/are the error message(s):
                All selected columns [(col2 -  PERIOD_TIME), (col3 -
                BLOB)] is/are unsupported for 'max' operation.

        EXAMPLES :
            # Load the data to run the example.
            >>> from teradataml.data.load_example_data import load_example_data
            >>> load_example_data("dataframe", ["employee_info"])

            # Create teradataml dataframe.
            >>> df1 = DataFrame("employee_info")
            >>> print(df1)
                        first_name marks   dob joined_date
            employee_no
            101              abcde  None  None    02/12/05
            100               abcd  None  None        None
            112               None  None  None    18/12/05
            >>>

            # Prints maximum value of each column(with supported data types).
            >>> df1.max()
              max_employee_no max_first_name max_marks max_dob max_joined_date
            0             112          abcde      None    None        18/12/05
            >>>

            # Select only subset of columns from the DataFrame.
            >>> df3 = df1.select(['employee_no', 'first_name', 'joined_date'])

            # Prints maximum value of each column(with supported data types).
            >>> df3.max()
              max_employee_no max_first_name max_joined_date
            0             112          abcde        18/12/05
            >>>

            #
            # Using max() as Time Series Aggregate.
            #
            >>> # Load the example datasets.
            ... load_example_data("dataframe", ["ocean_buoys"])
            >>>
            >>> # Create the required DataFrames.
            ... # DataFrame on non-sequenced PTI table
            ... ocean_buoys = DataFrame("ocean_buoys")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys.columns
            ['buoyid', 'TD_TIMECODE', 'temperature', 'salinity']
            >>> ocean_buoys.head()
                                   TD_TIMECODE  temperature  salinity
            buoyid
            0       2014-01-06 08:10:00.000000        100.0        55
            0       2014-01-06 08:08:59.999999          NaN        55
            1       2014-01-06 09:01:25.122200         77.0        55
            1       2014-01-06 09:03:25.122200         79.0        55
            1       2014-01-06 09:01:25.122200         70.0        55
            1       2014-01-06 09:02:25.122200         71.0        55
            1       2014-01-06 09:03:25.122200         72.0        55
            0       2014-01-06 08:09:59.999999         99.0        55
            0       2014-01-06 08:00:00.000000         10.0        55
            0       2014-01-06 08:10:00.000000         10.0        55

            #
            # Time Series Aggregate Example 1: Executing max() function on DataFrame created on
            #                                  non-sequenced PTI table. We will consider all rows for the
            #                                  columns while calculating the maximum values.
            #
            # To use max() as Time Series Aggregate we must run groupby_time() first, followed by max().
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.max().sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid             max_TD_TIMECODE  max_salinity  max_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0  2014-01-06 08:10:00.000000            55              100
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1  2014-01-06 09:03:25.122200            55               79
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2  2014-01-06 21:03:25.122200            55               82
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44  2014-01-06 10:52:00.000009            55               56
            >>>

            #
            # Time Series Aggregate Example 2: Executing max() function on DataFrame created on
            #                                  non-sequenced PTI table. We will consider DISTINCT values for the
            #                                  columns while calculating the maximum value.
            #
            # To use max() as Time Series Aggregate we must run groupby_time() first, followed by max().
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.max(distinct = True).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid             max_TD_TIMECODE  max_salinity  max_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0  2014-01-06 08:10:00.000000            55              100
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1  2014-01-06 09:03:25.122200            55               79
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2  2014-01-06 21:03:25.122200            55               82
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44  2014-01-06 10:52:00.000009            55               56
            >>>
        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["distinct", distinct, True, (bool)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        return self._get_dataframe_aggregate(operation='max', distinct=distinct)

    @collect_queryband(queryband="DF_mean")
    def mean(self, distinct=False):
        """
        DESCRIPTION:
            Returns column-wise mean value of the dataframe.
            Notes:
                * This function is valid only on columns with numeric types.
                * Null values are not included in the result computation.
                * This method does not support operations on array columns.

        PARAMETERS:
            distinct:
                Optional Argument.
                Specifies whether to exclude duplicate values while calculating the mean.
                Default Values: False

        RETURNS:
            teradataml DataFrame object with mean()
            operation performed.

        RAISES:
            TeradataMLException
            1. EXECUTION_FAILED - If mean() operation fails to
                generate the column-wise mean value of the dataframe.

                Possible error message:
                Failed to perform 'mean'. (Followed by error message)

            2. TDMLDF_AGGREGATE_COMBINED_ERR - If the mean() operation
                doesn't support all the columns in the dataframe.

                Possible error message:
                No results. Below is/are the error message(s):
                All selected columns [(col2 -  PERIOD_TIME), (col3 -
                BLOB)] is/are unsupported for 'mean' operation.

        EXAMPLES :
            # Load the data to run the example.
            >>> from teradataml.data.load_example_data import load_example_data
            >>> load_example_data("dataframe", ["employee_info"])

            # Create teradataml dataframe.
            >>> df1 = DataFrame("employee_info")
            >>> print(df1)
                        first_name marks   dob joined_date
            employee_no
            101              abcde  None  None    02/12/05
            100               abcd  None  None        None
            112               None  None  None    18/12/05
            >>>

            # Select only subset of columns from the DataFrame.
            >>> df2 = df1.select(['employee_no', 'marks', 'first_name'])

            # Prints mean value of each column(with supported data types).
            >>> df2.mean()
               mean_employee_no mean_marks
            0        104.333333       None
            >>>

            #
            # Using mean() as Time Series Aggregate.
            #
            >>> # Load the example datasets.
            ... load_example_data("dataframe", ["ocean_buoys"])
            >>>
            >>> # Create the required DataFrames.
            ... # DataFrame on non-sequenced PTI table
            ... ocean_buoys = DataFrame("ocean_buoys")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys.columns
            ['buoyid', 'TD_TIMECODE', 'temperature', 'salinity']
            >>> ocean_buoys.head()
                                   TD_TIMECODE  temperature  salinity
            buoyid
            0       2014-01-06 08:10:00.000000        100.0        55
            0       2014-01-06 08:08:59.999999          NaN        55
            1       2014-01-06 09:01:25.122200         77.0        55
            1       2014-01-06 09:03:25.122200         79.0        55
            1       2014-01-06 09:01:25.122200         70.0        55
            1       2014-01-06 09:02:25.122200         71.0        55
            1       2014-01-06 09:03:25.122200         72.0        55
            0       2014-01-06 08:09:59.999999         99.0        55
            0       2014-01-06 08:00:00.000000         10.0        55
            0       2014-01-06 08:10:00.000000         10.0        55

            #
            # Time Series Aggregate Example 1: Executing mean() function on DataFrame created on
            #                                  non-sequenced PTI table. We will consider all rows for the
            #                                  columns while calculating the mean values.
            #
            # To use mean() as Time Series Aggregate we must run groupby_time() first, followed by mean().
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.mean().sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid  mean_salinity  mean_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0           55.0         54.750000
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1           55.0         74.500000
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2           55.0         81.000000
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44           55.0         48.076923
            >>>

            #
            # Time Series Aggregate Example 2: Executing mean() function on DataFrame created on
            #                                  non-sequenced PTI table. We will consider DISTINCT values for the
            #                                  columns while calculating the mean value.
            #
            # To use mean() as Time Series Aggregate we must run groupby_time() first, followed by mean().
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.mean(distinct = True).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid  mean_salinity  mean_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0           55.0         69.666667
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1           55.0         74.500000
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2           55.0         81.000000
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44           55.0         52.200000
            >>>
        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["distinct", distinct, True, (bool)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        return self._get_dataframe_aggregate(operation='mean', distinct=distinct)

    @collect_queryband(queryband="DF_skew")
    def skew(self, distinct=False):
        """
        DESCRIPTION:
            Returns column-wise skewness of the distribution of the dataframe.
            Skewness is the third moment of a distribution. It is a measure of the asymmetry of the
            distribution about its mean compared with the normal (or Gaussian) distribution.
                * The normal distribution has a skewness of 0.
                * Positive skewness indicates a distribution having an asymmetric tail
                  extending toward more positive values.
                * Negative skewness indicates an asymmetric tail extending toward more negative values.

            Notes:
                * This function is valid only on columns with numeric types.
                * Nulls are not included in the result computation.
                * Following conditions will produce null result:
                    * Fewer than three non-null data points in the data used for the computation.
                    * Standard deviation for a column is equal to 0.
                * This method does not support DataFrame containing array columns.

        PARAMETERS:
            distinct:
                Optional Argument.
                Specifies whether to exclude duplicate values while calculating the skewness of the distribution.
                Default Values: False
                Types: bool

        RETURNS:
            teradataml DataFrame object with skew()
            operation performed.

        RAISES:
            TeradataMLException
            1. EXECUTION_FAILED - If the skew() operation fails to
                generate the column-wise skew value of the dataframe.

                Possible error message:
                Failed to perform 'skew'. (Followed by error message)

            2. TDMLDF_AGGREGATE_COMBINED_ERR - If the skew() operation
                doesn't support all the columns in the dataframe.

                Possible error message:
                No results. Below is/are the error message(s):
                All selected columns [(col2 -  PERIOD_TIME), (col3 -
                BLOB)] is/are unsupported for 'skew' operation.

        EXAMPLES:
            # Load the data to run the example.
            >>> load_example_data("dataframe", ["admissions_train"])

            # Create teradataml dataframe.
            >>> df1 = DataFrame("admissions_train")
            >>> print(df1.sort('id'))
               masters   gpa     stats programming  admitted
            id
            1      yes  3.95  Beginner    Beginner         0
            2      yes  3.76  Beginner    Beginner         0
            3       no  3.70    Novice    Beginner         1
            4      yes  3.50  Beginner      Novice         1
            5       no  3.44    Novice      Novice         0
            6      yes  3.50  Beginner    Advanced         1
            7      yes  2.33    Novice      Novice         1
            8       no  3.60  Beginner    Advanced         1
            9       no  3.82  Advanced    Advanced         1
            10      no  3.71  Advanced    Advanced         1
            >>>

            # Prints skew value of each column(with supported data types).
            >>> df1.skew()
               skew_id  skew_gpa  skew_admitted
            0      0.0 -2.058969      -0.653746
            >>>

            #
            # Using skew() as Time Series Aggregate.
            #
            >>> # Load the example datasets.
            ... load_example_data("dataframe", ["ocean_buoys"])
            >>>
            >>> # Create the required DataFrames.
            ... # DataFrame on non-sequenced PTI table
            ... ocean_buoys = DataFrame("ocean_buoys")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys.columns
            ['buoyid', 'TD_TIMECODE', 'temperature', 'salinity']
            >>> ocean_buoys.head()
                                   TD_TIMECODE  temperature  salinity
            buoyid
            0       2014-01-06 08:10:00.000000        100.0        55
            0       2014-01-06 08:08:59.999999          NaN        55
            1       2014-01-06 09:01:25.122200         77.0        55
            1       2014-01-06 09:03:25.122200         79.0        55
            1       2014-01-06 09:01:25.122200         70.0        55
            1       2014-01-06 09:02:25.122200         71.0        55
            1       2014-01-06 09:03:25.122200         72.0        55
            0       2014-01-06 08:09:59.999999         99.0        55
            0       2014-01-06 08:00:00.000000         10.0        55
            0       2014-01-06 08:10:00.000000         10.0        55

            #
            # Time Series Aggregate Example 1: Executing skew() function on DataFrame created on
            #                                  non-sequenced PTI table. We will consider all rows for the
            #                                  columns while calculating the skew values.
            #
            # To use skew() as Time Series Aggregate we must run groupby_time() first, followed by skew().
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.skew().sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid skew_salinity  skew_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0          None          0.000324
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1          None          0.000000
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2          None          0.000000
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44          None          0.246084
            >>>

            #
            # Time Series Aggregate Example 2: Executing skew() function on DataFrame created on
            #                                  non-sequenced PTI table. We will consider DISTINCT values for the
            #                                  columns while calculating the skew value.
            #
            # To use skew() as Time Series Aggregate we must run groupby_time() first, followed by skew().
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.skew(distinct = True).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid skew_salinity  skew_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0          None         -1.731321
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1          None          0.000000
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2          None          0.000000
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44          None         -1.987828
            >>>
        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["distinct", distinct, True, (bool)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        return self._get_dataframe_aggregate(operation='skew', distinct=distinct)

    @collect_queryband(queryband="DF_sum")
    def sum(self, distinct=False):
        """
        DESCRIPTION:
            Returns column-wise sum value of the dataframe.
            Notes:
                * teradataml doesn't support sum operation on columns of str, datetime types.
                * Null values are not included in the result computation.
                * This method does not support DataFrame containing array columns.

        PARAMETERS:
            distinct:
                Optional Argument.
                Specifies whether to exclude duplicate values while calculating the sum.
                Default Value: False 
                Types: bool

        RETURNS:
            teradataml DataFrame object with sum()
            operation performed.

        RAISES:
            TeradataMLException
            1. EXECUTION_FAILED - If sum() operation fails to
                generate the column-wise summation value of the dataframe.

                Possible error message:
                Failed to perform 'sum'. (Followed by error message).

            2. TDMLDF_AGGREGATE_COMBINED_ERR - If the sum() operation
                doesn't support all the columns in the dataframe.

                Possible error message:
                No results. Below is/are the error message(s):
                All selected columns [(col2 -  PERIOD_TIME), (col3 -
                BLOB)] is/are unsupported for 'sum' operation.

        EXAMPLES :
            # Load the data to run the example.
            >>> from teradataml.data.load_example_data import load_example_data
            >>> load_example_data("dataframe", ["employee_info"])

            # Create teradataml dataframe.
            >>> df1 = DataFrame("employee_info")
            >>> print(df1)
                        first_name marks   dob joined_date
            employee_no
            101              abcde  None  None    02/12/05
            100               abcd  None  None        None
            112               None  None  None    18/12/05
            >>>

            # Prints sum of the values of each column(with supported data types).
            >>> df1.sum()
              sum_employee_no sum_marks
            0             313      None
            >>>

            #
            # Using sum() as Time Series Aggregate.
            #
            >>> # Load the example datasets.
            ... load_example_data("dataframe", ["ocean_buoys"])
            >>>

            #
            # Time Series Aggregate Example 1: Executing sum() function on DataFrame created on
            #                                  non-sequenced PTI table. We will consider all rows for the
            #                                  columns while calculating the sum value.
            #
            >>> # Create the required DataFrames.
            ... # DataFrame on non-sequenced PTI table
            ... ocean_buoys = DataFrame("ocean_buoys")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys.columns
            ['buoyid', 'TD_TIMECODE', 'temperature', 'salinity']
            >>> ocean_buoys.head()
                                   TD_TIMECODE  temperature  salinity
            buoyid
            0       2014-01-06 08:10:00.000000        100.0        55
            0       2014-01-06 08:08:59.999999          NaN        55
            1       2014-01-06 09:01:25.122200         77.0        55
            1       2014-01-06 09:03:25.122200         79.0        55
            1       2014-01-06 09:01:25.122200         70.0        55
            1       2014-01-06 09:02:25.122200         71.0        55
            1       2014-01-06 09:03:25.122200         72.0        55
            0       2014-01-06 08:09:59.999999         99.0        55
            0       2014-01-06 08:00:00.000000         10.0        55
            0       2014-01-06 08:10:00.000000         10.0        55

            # To use sum() as Time Series Aggregate we must run groupby_time() first, followed by sum().
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.sum().sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid  sum_salinity  sum_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0           275              219
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1           330              447
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2           165              243
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44           715              625
            >>>

            #
            # Time Series Aggregate Example 2: Executing sum() function on DataFrame created on
            #                                  non-sequenced PTI table. We will consider DISTINCT values for the
            #                                  columns while calculating the sum value.
            #
            # To use sum() as Time Series Aggregate we must run groupby_time() first, followed by sum().
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.sum(distinct = True).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid  sum_salinity  sum_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0            55              209
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1            55              447
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2            55              243
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44            55              261
            >>>

        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["distinct", distinct, True, (bool)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        return self._get_dataframe_aggregate(operation='sum', distinct=distinct)

    @collect_queryband(queryband="DF_count")
    def count(self, distinct=False):
        """
        DESCRIPTION:
            Returns column-wise count of the dataframe.

        PARAMETERS:
            distinct:
                Optional Argument.
                Specifies whether to exclude duplicate values while calculating the count.
                Default Value: False
                Types: bool

        RETURNS:
            teradataml DataFrame object with count() operation
            performed.

        RAISES:
            TeradataMLException
            1. EXECUTION_FAILED - If count() operation fails to
                generate the column-wise count of the dataframe.

                Possible error message:
                Failed to perform 'count'. (Followed by error message).

            2. TDMLDF_AGGREGATE_COMBINED_ERR - If the count() operation
                doesn't support all the columns in the dataframe.

                Possible error message:
                No results. Below is/are the error message(s):
                All selected columns [(col2 -  PERIOD_TIME), (col3 -
                BLOB)] is/are unsupported for 'count' operation.

        EXAMPLES :
            # Load the data to run the example.
            >>> from teradataml.data.load_example_data import load_example_data
            >>> load_example_data("dataframe", ["employee_info"])

            # Create teradataml dataframe.
            >>> df1 = DataFrame("employee_info")
            >>> print(df1)
                        first_name marks   dob joined_date
            employee_no
            101              abcde  None  None    02/12/05
            100               abcd  None  None        None
            112               None  None  None    18/12/05
            >>>

            # Select only subset of columns from the DataFrame.
            >>> df2 = df1.select(['employee_no', 'first_name', 'marks'])

            # Prints count of the values in all the selected columns
            # (excluding None types).
            >>> df2.count()
              count_employee_no count_first_name count_marks
            0                 3                2           0
            >>>

            #
            # Using count() as Time Series Aggregate.
            #
            >>> # Load the example datasets.
            ... load_example_data("dataframe", ["ocean_buoys_seq"])
            >>>

            #
            # Time Series Aggregate Example 1: Executing count() function on DataFrame created on
            #                                  sequenced PTI table. We will consider all rows for the
            #                                  columns while calculating the count.
            #
            >>> ocean_buoys_seq = DataFrame("ocean_buoys_seq")
            >>> ocean_buoys_seq.columns
            ['TD_TIMECODE', 'TD_SEQNO', 'buoyid', 'salinity', 'temperature', 'dates']
            >>> ocean_buoys_seq
                                   TD_TIMECODE  TD_SEQNO  salinity  temperature       dates
            buoyid
            44      2014-01-06 10:00:25.122200         6        55           43  2014-06-06
            44      2014-01-06 10:01:25.122200         8        55           53  2014-08-08
            44      2014-01-06 10:01:25.122200        20        55           54  2015-08-20
            1       2014-01-06 09:01:25.122200        11        55           70  2014-11-11
            1       2014-01-06 09:02:25.122200        12        55           71  2014-12-12
            1       2014-01-06 09:02:25.122200        24        55           78  2015-12-24
            1       2014-01-06 09:03:25.122200        13        55           72  2015-01-13
            1       2014-01-06 09:03:25.122200        25        55           79  2016-01-25
            1       2014-01-06 09:01:25.122200        23        55           77  2015-11-23
            44      2014-01-06 10:00:26.122200         7        55           43  2014-07-07
            >>>

            # To use count() as Time Series Aggregate we must run groupby_time() first, followed by count().
            >>> ocean_buoys_grpby1 = ocean_buoys_seq.groupby_time(timebucket_duration="2cy", value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.count().sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid  count_TD_TIMECODE  count_TD_SEQNO  count_salinity  count_temperature  count_dates
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0                  5               5               5                  4            5
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1                  6               6               6                  6            6
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2                  3               3               3                  3            3
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      22                  1               1               1                  1            1
            4  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44                 13              13              13                 13           13
            >>>

            #
            # Time Series Aggregate Example 2: Executing count() function on DataFrame created on
            #                                  sequenced PTI table. We will consider DISTINCT rows for the
            #                                  columns while calculating the count.
            #
            # To use count() as Time Series Aggregate we must run groupby_time() first, followed by count().
            >>> ocean_buoys_grpby1 = ocean_buoys_seq.groupby_time(timebucket_duration="2cy",value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.count(distinct = True).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid  count_TD_TIMECODE  count_TD_SEQNO  count_salinity  count_temperature  count_dates
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0                  4               5               1                  3            5
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1                  3               6               1                  6            6
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2                  3               3               1                  3            3
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      22                  1               1               1                  1            1
            4  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44                 10              13               1                  5           13
            >>>
        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["distinct", distinct, True, (bool)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)
        return self._get_dataframe_aggregate(operation='count', distinct=distinct)

    @collect_queryband(queryband="DF_csum")
    def csum(self, sort_columns, drop_columns=False):
        """
        DESCRIPTION:
            Returns column-wise cumulative sum value for rows in the partition
            of the dataframe.
            Note:
                csum does not support below type of columns.
                    * BLOB
                    * BYTE
                    * CHAR
                    * CLOB
                    * DATE
                    * PERIOD_DATE
                    * PERIOD_TIME
                    * PERIOD_TIMESTAMP
                    * TIME
                    * TIMESTAMP
                    * VARBYTE
                    * VARCHAR  
                    * ARRAY          

        PARAMETERS:
            sort_columns:
                Required Argument.
                Specifies the columns to use for sorting.
                Note:
                    "sort_columns" does not support CLOB and BLOB type of
                    columns.
                Types: str (or) ColumnExpression (or) List of strings(str)
                       or ColumnExpressions

            drop_columns:
                Optional Argument.
                Specifies whether to retain all the input DataFrame columns
                in the output or not. When set to False, columns from input
                DataFrame are retained, dropped otherwise.
                Default Value: False
                Types: bool

        RAISES:
            TeradataMlException, TypeError

        RETURNS:
            teradataml DataFrame.

        EXAMPLES:
            # Load the data to run the example.
            >>> from teradataml import load_example_data
            >>> load_example_data("dataframe","sales")

            # Create teradataml dataframe.
            >>> df = DataFrame.from_table('sales')
            >>> print(df)
                          Feb    Jan    Mar    Apr    datetime
            accounts
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017
            >>>

            # Sorts the Data on column accounts in ascending order and
            # calculates cumulative sum.
            >>> df.csum(df.accounts)
                          Feb    Jan    Mar    Apr    datetime  csum_Feb  csum_Jan  csum_Mar  csum_Apr
            accounts
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017     500.0       400       450       531
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017     910.0       550       590       781
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017    1000.0       550       590       781
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017     710.0       400       450       781
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017     300.0       250       310       351
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017     210.0       200       215       250
            >>>

            # Sorts the Data on column accounts in ascending order and column
            # Feb in descending order, then calculates cumulative sum by dropping
            # the input DataFrame columns.
            >>> df.csum(sort_columns=[df.accounts, df.Feb.desc()], drop_columns=True)
               csum_Feb  csum_Jan  csum_Mar  csum_Apr
            0     500.0       400       450       531
            1     910.0       550       590       781
            2    1000.0       550       590       781
            3     710.0       400       450       781
            4     300.0       250       310       351
            5     210.0       200       215       250
            >>>
        """

        from teradataml.dataframe.sql import _SQLColumnExpression

        # Argument validations
        awu_matrix = []
        awu_matrix.append(["sort_columns", sort_columns, False, (_SQLColumnExpression, list, str), True])
        awu_matrix.append(["drop_columns", drop_columns, False, bool, True])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        # Validate argument values.
        self._validate_window_aggregates_arguments(sort_columns)
        return self._get_dataframe_aggregate(operation='csum', sort_columns=sort_columns, drop_columns=drop_columns)

    @collect_queryband(queryband="DF_msum")
    def msum(self, width, sort_columns, drop_columns=False):
        """
        DESCRIPTION:
            Computes the moving sum for the current row and the preceding
            "width"-1 rows in a partition, by sorting the rows according to
            "sort_columns".
            Note:
                msum does not support below type of columns.
                    * BLOB
                    * BYTE
                    * CHAR
                    * CLOB
                    * DATE
                    * PERIOD_DATE
                    * PERIOD_TIME
                    * PERIOD_TIMESTAMP
                    * TIME
                    * TIMESTAMP
                    * VARBYTE
                    * VARCHAR    
                    * ARRAY        

        PARAMETERS:
            width:
                Required Argument.
                Specifies the width of the partition. "width" must be
                greater than 0 and less than or equal to 4096.
                Types: int

            sort_columns:
                Required Argument.
                Specifies the columns to use for sorting.
                Note:
                    "sort_columns" does not support CLOB and BLOB type of
                    columns.
                Types: str (or) ColumnExpression (or) List of strings(str)
                       or ColumnExpressions

            drop_columns:
                Optional Argument.
                Specifies whether to retain all the input DataFrame columns
                in the output or not. When set to False, columns from input
                DataFrame are retained, dropped otherwise.
                Default Value: False
                Types: bool

        RAISES:
            TeradataMlException, TypeError

        RETURNS:
            teradataml DataFrame.

        EXAMPLES:
            # Load the data to run the example.
            >>> from teradataml import load_example_data
            >>> load_example_data("dataframe","sales")

            # Create teradataml dataframe.
            >>> df = DataFrame.from_table('sales')
            >>> print(df)
                          Feb    Jan    Mar    Apr    datetime
            accounts
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017
            >>>

            # Sorts the Data on column accounts in ascending order and
            # calculates moving sum on the window of size 2.
            >>> df.msum(width=2, sort_columns=df.accounts)
                          Feb    Jan    Mar    Apr    datetime  msum_Feb  msum_Jan  msum_Mar  msum_Apr
            accounts
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017     290.0       200       235       281
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017     410.0       150       140       250
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017     290.0       150       140         0
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017     410.0       150       140       430
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017     300.0       250       310       351
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017     210.0       200       215       250
            >>>

            # Sorts the Data on column accounts in ascending order and column
            # Feb in descending order, then calculates moving sum by dropping the
            # input DataFrame columns on the window of size 2.
            >>> df.msum(width=2, sort_columns=[df.accounts, df.Feb.desc()], drop_columns=True)
               msum_Feb  msum_Jan  msum_Mar  msum_Apr
            0     290.0       200       235       281
            1     410.0       150       140       250
            2     290.0       150       140         0
            3     410.0       150       140       430
            4     300.0       250       310       351
            5     210.0       200       215       250
            >>>
        """

        from teradataml.dataframe.sql import _SQLColumnExpression

        # Argument validations.
        awu_matrix = []
        awu_matrix.append(["width", width, False, int])
        awu_matrix.append(["sort_columns", sort_columns, False, (_SQLColumnExpression, list, str), True])
        awu_matrix.append(["drop_columns", drop_columns, False, bool, True])

        # Validate argument types.
        _Validators._validate_function_arguments(awu_matrix)

        # Validate argument values.
        self._validate_window_aggregates_arguments(sort_columns, "sort_columns", width, "width", 0, 4096)
        return self._get_dataframe_aggregate(
            operation='msum', width=width, sort_columns=sort_columns, drop_columns=drop_columns)

    @collect_queryband(queryband="DF_mavg")
    def mavg(self, width, sort_columns, drop_columns=False):
        """
        DESCRIPTION:
            Computes the moving average for the current row and the preceding
            "width"-1 rows in a partition, by sorting the rows according to
            "sort_columns".
            Note:
                mavg does not support below type of columns.
                    * BLOB
                    * BYTE
                    * CHAR
                    * CLOB
                    * DATE
                    * PERIOD_DATE
                    * PERIOD_TIME
                    * PERIOD_TIMESTAMP
                    * TIME
                    * TIMESTAMP
                    * VARBYTE
                    * VARCHAR
                    * ARRAY

        PARAMETERS:
            width:
                Required Argument.
                Specifies the width of the partition. "width" must be
                greater than 0 and less than or equal to 4096.
                Types: int

            sort_columns:
                Required Argument.
                Specifies the columns to use for sorting.
                Note:
                    "sort_columns" does not support CLOB and BLOB type of
                    columns.
                Types: str (or) ColumnExpression (or) List of strings(str)
                       or ColumnExpressions

            drop_columns:
                Optional Argument.
                Specifies whether to retain all the input DataFrame columns
                in the output or not. When set to False, columns from input
                DataFrame are retained, dropped otherwise.
                Default Value: False
                Types: bool

        RAISES:
            TeradataMlException, TypeError

        RETURNS:
            teradataml DataFrame.

        EXAMPLES:
            # Load the data to run the example.
            >>> from teradataml import load_example_data
            >>> load_example_data("dataframe","sales")

            # Create teradataml dataframe.
            >>> df = DataFrame.from_table('sales')
            >>> print(df)
                          Feb    Jan    Mar    Apr    datetime
            accounts
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017
            >>>

            # Sorts the Data on column accounts in ascending order and
            # calculates moving avergae on the window of size 2.
            >>> df.mavg(width=2, sort_columns=df.accounts)
                          Feb    Jan    Mar    Apr    datetime  mavg_Feb  mavg_Jan  mavg_Mar  mavg_Apr mavg_datetime
            accounts
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017     145.0     100.0     117.5     140.5    04/01/2017
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017     205.0     150.0     140.0     250.0    04/01/2017
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017     145.0     150.0     140.0       NaN    04/01/2017
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017     205.0     150.0     140.0     215.0    04/01/2017
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017     150.0     125.0     155.0     175.5    04/01/2017
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017     210.0     200.0     215.0     250.0    04/01/2017
            >>>

            # Sorts the Data on column accounts in ascending order and column
            # Feb in descending order, then calculates moving average by dropping
            # the input DataFrame columns on the window of size 2.
            >>> df.mavg(width=2, sort_columns=[df.accounts, df.Feb.desc()], drop_columns=True)
               mavg_Feb  mavg_Jan  mavg_Mar  mavg_Apr mavg_datetime
            0     145.0     100.0     117.5     140.5    04/01/2017
            1     205.0     150.0     140.0     250.0    04/01/2017
            2     145.0     150.0     140.0       NaN    04/01/2017
            3     205.0     150.0     140.0     215.0    04/01/2017
            4     150.0     125.0     155.0     175.5    04/01/2017
            5     210.0     200.0     215.0     250.0    04/01/2017
            >>>
        """

        from teradataml.dataframe.sql import _SQLColumnExpression

        # Argument validations.
        awu_matrix = []
        awu_matrix.append(["width", width, False, int])
        awu_matrix.append(["sort_columns", sort_columns, False, (_SQLColumnExpression, list, str), True])
        awu_matrix.append(["drop_columns", drop_columns, False, bool, True])

        # Validate argument types.
        _Validators._validate_function_arguments(awu_matrix)

        # Validate argument values.
        self._validate_window_aggregates_arguments(sort_columns, "sort_columns", width, "width", 0, 4096)

        return self._get_dataframe_aggregate(
            operation='mavg', width=width, sort_columns=sort_columns, drop_columns=drop_columns)

    @collect_queryband(queryband="DF_mdiff")
    def mdiff(self, width, sort_columns, drop_columns=False):
        """
        DESCRIPTION:
            Computes the moving difference for the current row and the preceding
            "width" rows in a partition, by sorting the rows according to
            "sort_columns".
            Note:
                mdiff does not support below type of columns.
                    * BLOB
                    * BYTE
                    * CHAR
                    * CLOB
                    * DATE
                    * PERIOD_DATE
                    * PERIOD_TIME
                    * PERIOD_TIMESTAMP
                    * TIME
                    * TIMESTAMP
                    * VARBYTE
                    * VARCHAR
                    * ARRAY

        PARAMETERS:
            width:
                Required Argument.
                Specifies the width of the partition. "width" must be
                greater than 0 and less than or equal to 4096.
                Types: int

            sort_columns:
                Required Argument.
                Specifies the columns to use for sorting.
                Note:
                    "sort_columns" does not support CLOB and BLOB type of
                    columns.
                Types: str (or) ColumnExpression (or) List of strings(str)
                       or ColumnExpressions

            drop_columns:
                Optional Argument.
                Specifies whether to retain all the input DataFrame columns
                in the output or not. When set to False, columns from input
                DataFrame are retained, dropped otherwise.
                Default Value: False
                Types: bool

        RAISES:
            TeradataMlException, TypeError

        RETURNS:
            teradataml DataFrame.

        EXAMPLES:
            # Load the data to run the example.
            >>> from teradataml import load_example_data
            >>> load_example_data("dataframe","sales")

            # Create teradataml dataframe.
            >>> df = DataFrame.from_table('sales')
            >>> print(df)
                          Feb    Jan    Mar    Apr    datetime
            accounts
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017
            >>>

            # Sorts the Data on column accounts in ascending order and
            # calculates moving difference on the window of size 2.
            >>> df.mdiff(width=2, sort_columns=df.accounts)
                          Feb    Jan    Mar    Apr    datetime  mdiff_Feb  mdiff_Jan  mdiff_Mar  mdiff_Apr  mdiff_datetime
            accounts
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017      -10.0      -50.0      -75.0      -70.0             0.0
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017        0.0        0.0        0.0     -180.0             0.0
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017     -120.0        NaN        NaN     -250.0             0.0
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017      120.0      -50.0      -95.0      149.0             0.0
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017        NaN        NaN        NaN        NaN             NaN
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017        NaN        NaN        NaN        NaN             NaN
            >>>

            # Sorts the Data on column accounts in ascending order and column
            # Feb in descending order, then calculates moving difference by
            # dropping the input DataFrame columns on the window of size 2.
            >>> df.mdiff(width=2, sort_columns=[df.accounts, df.Feb.desc()], drop_columns=True)
               mdiff_Feb  mdiff_Jan  mdiff_Mar  mdiff_Apr
            0      -10.0      -50.0      -75.0      -70.0
            1        0.0        0.0        0.0     -180.0
            2     -120.0        NaN        NaN     -250.0
            3      120.0      -50.0      -95.0      149.0
            4        NaN        NaN        NaN        NaN
            5        NaN        NaN        NaN        NaN
            >>>
        """

        from teradataml.dataframe.sql import _SQLColumnExpression

        # Argument validations.
        awu_matrix = []
        awu_matrix.append(["width", width, False, int])
        awu_matrix.append(["sort_columns", sort_columns, False, (_SQLColumnExpression, list, str), True])
        awu_matrix.append(["drop_columns", drop_columns, False, bool, True])

        # Validate argument types.
        _Validators._validate_function_arguments(awu_matrix)

        # Validate argument values.
        self._validate_window_aggregates_arguments(sort_columns, "sort_columns", width, "width", 0, 4096)

        return self._get_dataframe_aggregate(
            operation='mdiff', width=width, sort_columns=sort_columns, drop_columns=drop_columns)

    @collect_queryband(queryband="DF_mlinreg")
    def mlinreg(self, width, sort_column, drop_columns=False):
        """
        DESCRIPTION:
            Computes the moving linear regression for the current row and the
            preceding "width"-1 rows in a partition, by sorting the rows
            according to "sort_columns".
            Note:
                mlinreg does not support below type of columns.
                    * BLOB
                    * BYTE
                    * CHAR
                    * CLOB
                    * DATE
                    * PERIOD_DATE
                    * PERIOD_TIME
                    * PERIOD_TIMESTAMP
                    * TIME
                    * TIMESTAMP
                    * VARBYTE
                    * VARCHAR
                    * ARRAY

        PARAMETERS:
            width:
                Required Argument.
                Specifies the width of the partition. "width" must be
                greater than 2 and less than or equal to 4096.
                Types: int

            sort_column:
                Required Argument.
                Specifies the column to use for sorting.
                Note:
                    "sort_column" does not support CLOB and BLOB type of
                    columns.
                Types: str (or) ColumnExpression

            drop_columns:
                Optional Argument.
                Specifies whether to retain all the input DataFrame columns
                in the output or not. When set to False, columns from input
                DataFrame are retained, dropped otherwise.
                Default Value: False
                Types: bool

        RAISES:
            TeradataMlException, TypeError

        RETURNS:
            teradataml DataFrame.

        EXAMPLES:
            # Load the data to run the example.
            >>> from teradataml import load_example_data
            >>> load_example_data("dataframe","admissions_train")
            >>> df = DataFrame('admissions_train')
            >>> print(df)
               masters   gpa     stats programming  admitted
            id
            15     yes  4.00  Advanced    Advanced         1
            7      yes  2.33    Novice      Novice         1
            22     yes  3.46    Novice    Beginner         0
            17      no  3.83  Advanced    Advanced         1
            13      no  4.00  Advanced      Novice         1
            38     yes  2.65  Advanced    Beginner         1
            26     yes  3.57  Advanced    Advanced         1
            5       no  3.44    Novice      Novice         0
            34     yes  3.85  Advanced    Beginner         0
            40     yes  3.95    Novice    Beginner         0
            >>>

            # Sorts the Data on column id in descending order and
            # calculates moving linear regression on the window of size 3.
            >>> df.mlinreg(width=3, sort_column=df.id.desc())
               masters   gpa     stats programming  admitted  mlinreg_id  mlinreg_gpa  mlinreg_admitted
            id
            6      yes  3.50  Beginner    Advanced         1         6.0         1.06               1.0
            17      no  3.83  Advanced    Advanced         1        17.0         5.64               2.0
            16      no  3.70  Advanced    Advanced         1        16.0         3.85               1.0
            28      no  3.93  Advanced    Advanced         1        28.0         4.21               0.0
            26     yes  3.57  Advanced    Advanced         1        26.0         3.99              -1.0
            40     yes  3.95    Novice    Beginner         0         NaN          NaN               NaN
            39     yes  3.75  Advanced    Beginner         0         NaN          NaN               NaN
            38     yes  2.65  Advanced    Beginner         1        38.0         3.55               0.0
            27     yes  3.96  Advanced    Advanced         0        27.0         3.86               2.0
            18     yes  3.81  Advanced    Advanced         1        18.0         0.06              -1.0
            >>>

            # Sorts the Data on column id in ascending order and
            # calculates moving linear regression by dropping the
            # input DataFrame columns on the window of size 3.
            >>> df.mlinreg(width=3, sort_column=df.id.asc(), drop_columns=True)
               mlinreg_id  mlinreg_gpa  mlinreg_admitted
            0        35.0         4.15              -1.0
            1        24.0         3.72               2.0
            2        25.0         0.15               1.0
            3        13.0         4.17               1.0
            4        15.0         2.90              -1.0
            5         NaN          NaN               NaN
            6         NaN          NaN               NaN
            7         3.0         3.57               0.0
            8        14.0         4.35               1.0
            9        23.0         3.05              -1.0
            >>>
        """

        from teradataml.dataframe.sql import _SQLColumnExpression

        # Argument validations.
        awu_matrix = []
        awu_matrix.append(["width", width, False, int])
        awu_matrix.append(["sort_columns", sort_column, False, (_SQLColumnExpression, str), True])
        awu_matrix.append(["drop_columns", drop_columns, False, bool, True])

        # Validate argument types.
        _Validators._validate_function_arguments(awu_matrix)

        # Argument validations values.
        self._validate_window_aggregates_arguments(sort_column, "sort_column", width, "width", 2, 4096)

        return self._get_dataframe_aggregate(
            operation='mlinreg', width=width, sort_column=sort_column, drop_columns=drop_columns)

    def _validate_window_aggregates_arguments(self,
                                              sort_columns,
                                              sort_columns_arg_name="sort_columns",
                                              width=None,
                                              width_arg_name="width",
                                              lower_bound=None,
                                              upper_bound=None):
        """
        DESCRIPTION:
            Function to validate the arguments passed to DataFrame Window
            aggregates. This function does the following:
                * Checks width is in between "lower_bound" value and
                  "upper_bound", if width is not None.
                * Checks the columns mentioned in "sort_columns" are sortable
                  or Not.

        PARAMETERS:
            sort_columns:
                Required Argument.
                Specifies the columns to be used to sort the values.
                Types: _SQLColumnExpression OR list of _SQLColumnExpression
                       OR str OR list of str

            sort_columns_arg_name:
                Optional Argument.
                Specifies the name of argument "sort_columns".
                Default Value: sort_columns
                Types: str

            width:
                Optional Argument.
                Specifies the width of window for a window aggregate function.
                Types: int

            width_arg_name:
                Optional Argument.
                Specifies the name of argument "width".
                Default Value: width
                Types: str

            lower_bound:
                Optional Argument.
                Specifies the expected lower bound value for "width".
                Types: int

            upper_bound:
                Optional Argument.
                Specifies the expected upper bound value for "width".
                Types: int

        RAISES:
            AttributeError, ValueError

        RETURNS:
            None.

        EXAMPLES:
            self._validate_window_aggregates_arguments("csum", "col1", "sort_columns")
            self._validate_window_aggregates_arguments("msum", "col1", "sort_columns", 5, 0, 4096)
        """

        # Validate whether width is in the expected range or not.
        # Note that lower bound is not inclusive.
        _Validators._validate_positive_int(width, width_arg_name, lower_bound, upper_bound)

        _Validators._validate_unexpected_column_type(
            self, sort_columns, sort_columns_arg_name, _Dtypes._get_sort_unsupported_data_types())

    @collect_queryband(queryband="DF_std")
    def std(self, distinct=False, population=False):
        """
        DESCRIPTION:
            Returns column-wise sample or population standard deviation value of the
            dataframe. The standard deviation is the second moment of a distribution.
                * For a sample, it is a measure of dispersion from the mean of that sample.
                * For a population, it is a measure of dispersion from the mean of that population.
            The computation is more conservative for the population standard deviation
            to minimize the effect of outliers on the computed value.

            Notes:
                * When there are fewer than two non-null data points in the sample used
                   for the computation, then std returns None.
                * Null values are not included in the result computation.
                * If data represents only a sample of the entire population for the
                   columns, Teradata recommends to calculate sample standard deviation,
                   otherwise calculate population standard deviation.
                * This method does not support DataFrame containing array columns.

        PARAMETERS:
            distinct:
                Optional Argument.
                Specifies whether to exclude duplicate values while calculating
                the standard deviation.
                Default Value: False
                Types: bool

            population:
                Optional Argument.
                Specifies whether to calculate standard deviation on entire population or not.
                Set this argument to True only when the data points represent the complete
                population. If your data represents only a sample of the entire population for the
                columns, then set this variable to False, which will compute the sample standard
                deviation. As the sample size increases, even though the values for sample
                standard deviation and population standard deviation approach the same number,
                you should always use the more conservative sample standard deviation calculation,
                unless you are absolutely certain that your data constitutes the entire population
                for the columns.
                Default Value: False
                Types: bool

        RETURNS:
            teradataml DataFrame object with std() operation performed.

        RAISES:
            1. EXECUTION_FAILED - If std() operation fails to
                generate the column-wise standard deviation of the
                dataframe.

                Possible error message:
                Failed to perform 'std'. (Followed by error message)

            2. TDMLDF_AGGREGATE_COMBINED_ERR - If the std() operation
                doesn't support all the columns in the dataframe.

                Possible error message:
                No results. Below is/are the error message(s):
                All selected columns [(col2 -  PERIOD_TIME), (col3 -
                BLOB)] is/are unsupported for 'std' operation.

        EXAMPLES :
            # Load the data to run the example.
            >>> from teradataml.data.load_example_data import load_example_data
            >>> load_example_data("dataframe", ["employee_info"])

            # Create teradataml dataframe.
            >>> df1 = DataFrame("employee_info")
            >>> print(df1)
                        first_name marks   dob joined_date
            employee_no
            101              abcde  None  None    02/12/05
            100               abcd  None  None        None
            112               None  None  None    18/12/05
            >>>

            # Select only subset of columns from the DataFrame.
            >>> df2 = df1.select(['employee_no', 'first_name', 'marks', 'joined_date'])

            # Prints sample standard deviation of each column(with supported data types).
            >>> df2.std()
               std_employee_no std_marks std_joined_date
            0         6.658328      None        82/03/09
            >>>

            # Prints population standard deviation of each column(with supported data types).
            >>> df2.std(population=True)
               std_employee_no std_marks std_joined_date
            0         5.436502      None        58/02/28
            >>>

            #
            # Using std() as Time Series Aggregate.
            #
            >>> # Load the example datasets.
            ... load_example_data("dataframe", ["ocean_buoys"])
            >>>

            #
            # Time Series Aggregate Example 1: Executing std() function on DataFrame created on
            #                                  non-sequenced PTI table. We will consider all rows for the
            #                                  columns while calculating the standard deviation.
            #
            >>> # Create the required DataFrames.
            ... # DataFrame on non-sequenced PTI table
            ... ocean_buoys = DataFrame("ocean_buoys")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys.columns
            ['buoyid', 'TD_TIMECODE', 'temperature', 'salinity']
            >>> ocean_buoys.head()
                                   TD_TIMECODE  temperature  salinity
            buoyid
            0       2014-01-06 08:10:00.000000        100.0        55
            0       2014-01-06 08:08:59.999999          NaN        55
            1       2014-01-06 09:01:25.122200         77.0        55
            1       2014-01-06 09:03:25.122200         79.0        55
            1       2014-01-06 09:01:25.122200         70.0        55
            1       2014-01-06 09:02:25.122200         71.0        55
            1       2014-01-06 09:03:25.122200         72.0        55
            0       2014-01-06 08:09:59.999999         99.0        55
            0       2014-01-06 08:00:00.000000         10.0        55
            0       2014-01-06 08:10:00.000000         10.0        55

            # To use std() as Time Series Aggregate we must run groupby_time() first, followed by std().
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.std().sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid  std_salinity  std_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0           0.0        51.674462
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1           0.0         3.937004
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2           0.0         1.000000
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44           0.0         5.765725
            >>>

            #
            # Time Series Aggregate Example 2: Executing std() function on DataFrame created on
            #                                  non-sequenced PTI table. We will consider DISTINCT rows for the
            #                                  columns while calculating the standard deviation.
            #
            # To use std() as Time Series Aggregate we must run groupby_time() first, followed by std().
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.std(distinct = True).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid std_salinity  std_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0         None        51.675268
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1         None         3.937004
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2         None         1.000000
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44         None         5.263079
            >>>

            #
            # Time Series Aggregate Example 3: Executing std() function on DataFrame created on
            #                                  non-sequenced PTI table. We shall calculate the
            #                                  standard deviation on entire population, with
            #                                  all non-null data points considered for calculations.
            #
            # To use std() as Time Series Aggregate we must run groupby_time() first, followed by std().
            # To calculate population standard deviation we must set population=True.
            #
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.std(population=True).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid  std_salinity  std_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0           0.0        44.751397
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1           0.0         3.593976
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2           0.0         0.816497
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44           0.0         5.539530
            >>>
        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["distinct", distinct, True, (bool)])
        awu_matrix.append(["population", population, True, (bool)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)
        return self._get_dataframe_aggregate(operation='std', distinct=distinct, population=population)

    @collect_queryband(queryband="DF_median")
    def median(self, distinct=False):
        """
        DESCRIPTION:
            Returns column-wise median value of the dataframe.
            Notes:
                * This function is valid only on columns with numeric types.
                * Null values are not included in the result computation.
                * This method does not support DataFrame containing array columns.

        PARAMETERS:
            distinct:
                Optional Argument.
                Specifies whether to exclude duplicate values while calculating the median.
                Note:
                    This is allowed only when median() is used as Time Series Aggregate function, i.e.,
                    this can be set to True, only when median() is operated on DataFrameGroupByTime object.
                    Otherwise, an exception will be raised.
                Default Values: False

        RETURNS:
            teradataml DataFrame object with median() operation
            performed.

        RAISES:
            1. TDMLDF_AGGREGATE_FAILED - If median() operation fails to
                generate the column-wise median value of the dataframe.

                Possible error message:
                Unable to perform 'median()' on the dataframe.

            2. TDMLDF_AGGREGATE_COMBINED_ERR - If the median() operation
                doesn't support all the columns in the dataframe.

                Possible error message:
                No results. Below is/are the error message(s):
                All selected columns [(col2 -  PERIOD_TIME), (col3 -
                BLOB)] is/are unsupported for 'median' operation.

        EXAMPLES :
            # Load the data to run the example.
            >>> from teradataml.data.load_example_data import load_example_data
            >>> load_example_data("dataframe", ["employee_info"])

            # Create teradataml dataframe.
            >>> df1 = DataFrame("employee_info")
            >>> print(df1)
                        first_name marks   dob joined_date
            employee_no
            101              abcde  None  None    02/12/05
            100               abcd  None  None        None
            112               None  None  None    18/12/05
            >>>

            # Prints median value of each column(with supported data types).
            >>> df1.median()
              median_employee_no median_marks
            0                101         None
            >>>

            #
            # Using median() as Time Series Aggregate.
            #
            >>> # Load the example datasets.
            ... load_example_data("dataframe", ["ocean_buoys"])
            >>>

            #
            # Time Series Aggregate Example 1: Executing median() function on DataFrame created on
            #                                  non-sequenced PTI table. We will consider all rows for the
            #                                  columns while calculating the median value.
            #
            >>> # Create the required DataFrames.
            ... # DataFrame on non-sequenced PTI table
            ... ocean_buoys = DataFrame("ocean_buoys")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys.columns
            ['buoyid', 'TD_TIMECODE', 'temperature', 'salinity']
            >>> ocean_buoys.head()
                                   TD_TIMECODE  temperature  salinity
            buoyid
            0       2014-01-06 08:10:00.000000        100.0        55
            0       2014-01-06 08:08:59.999999          NaN        55
            1       2014-01-06 09:01:25.122200         77.0        55
            1       2014-01-06 09:03:25.122200         79.0        55
            1       2014-01-06 09:01:25.122200         70.0        55
            1       2014-01-06 09:02:25.122200         71.0        55
            1       2014-01-06 09:03:25.122200         72.0        55
            0       2014-01-06 08:09:59.999999         99.0        55
            0       2014-01-06 08:00:00.000000         10.0        55
            0       2014-01-06 08:10:00.000000         10.0        55

            # To use median() as Time Series Aggregate we must run groupby_time() first, followed by median().
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.median().sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid  median_temperature  median_salinity
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0                54.5             55.0
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1                74.5             55.0
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2                81.0             55.0
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44                43.0             55.0
            >>>

            #
            # Time Series Aggregate Example 2: Executing median() function on DataFrame created on
            #                                  non-sequenced PTI table. We will consider DISTINCT rows for the
            #                                  columns while calculating the median value.
            #
            # To use median() as Time Series Aggregate we must run groupby_time() first, followed by median().
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.median(distinct = True).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid  median_temperature  median_salinity
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0                99.0             55.0
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1                74.5             55.0
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2                81.0             55.0
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44                54.0             55.0
            >>>
        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["distinct", distinct, True, (bool)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        if distinct and not isinstance(self, DataFrameGroupByTime):
            raise ValueError(
                Messages.get_message(MessageCodes.ARG_VALUE_CLASS_DEPENDENCY).format('distinct', 'Aggregation',
                                                                                     'True', 'median()',
                                                                                     'DataFrameGroupByTime'))

        return self._get_dataframe_aggregate(operation='median', distinct=distinct)

    @collect_queryband(queryband="DF_var")
    def var(self, distinct=False, population=False):
        """
        DESCRIPTION:
            Returns column-wise sample or population variance of the columns in a
            dataframe.
                * The variance of a population is a measure of dispersion from the
                  mean of that population.
                * The variance of a sample is a measure of dispersion from the mean
                  of that sample. It is the square of the sample standard deviation.
            Note:
                1. When there are fewer than two non-null data points in the sample used
                   for the computation, then var returns None.
                2. Null values are not included in the result computation.
                3. If data represents only a sample of the entire population for the
                   columns, Teradata recommends to calculate sample variance,
                   otherwise calculate population variance.
                4. This method does not support DataFrame containing array columns.

        PARAMETERS:
            distinct:
                Optional Argument.
                Specifies whether to exclude duplicate column values while calculating the
                variance value.
                Default Values: False
                Types: bool

            population:
                Optional Argument.
                Specifies whether to calculate variance on entire population or not.
                Set this argument to True only when the data points represent the complete
                population. If your data represents only a sample of the entire population
                for the columns, then set this variable to False, which will compute the
                sample variance. As the sample size increases, even though the values for
                sample variance and population variance approach the same number, but you
                should always use the more conservative sample standard deviation calculation,
                unless you are absolutely certain that your data constitutes the entire
                population for the columns.
                Default Value: False
                Types: bool
            
        RETURNS:
            teradataml DataFrame object with var() operation performed.

        RAISES:
            1. TDMLDF_AGGREGATE_FAILED - If var() operation fails to
                generate the column-wise variance of the dataframe.

                Possible error message:
                Unable to perform 'var()' on the dataframe.

            2. TDMLDF_AGGREGATE_COMBINED_ERR - If the var() operation
                doesn't support all the columns in the dataframe.

                Possible error message:
                No results. Below is/are the error message(s):
                All selected columns [(col2 -  PERIOD_TIME), (col3 -
                BLOB)] is/are unsupported for 'var' operation.

        EXAMPLES :
            # Load the data to run the example.
            >>> from teradataml.data.load_example_data import load_example_data
            >>> load_example_data("dataframe", ["employee_info", "sales"])

            # Example 1 - Applying var on table 'employee_info' that has all
            #             NULL values in marks and dob columns which are
            #             captured as None in variance dataframe.

            # Create teradataml dataframe.
            >>> df1 = DataFrame("employee_info")
            >>> print(df1)
                        first_name marks   dob joined_date
            employee_no
            101              abcde  None  None    02/12/05
            100               abcd  None  None        None
            112               None  None  None    18/12/05
            >>>

            # Select only subset of columns from the DataFrame.
            >>> df3 = df1.select(["employee_no", "first_name", "dob", "marks"])

            # Prints unbiased variance of each column(with supported data types).
            >>> df3.var()
                   var_employee_no var_dob var_marks
                0        44.333333    None      None

            # Example 2 - Applying var on table 'sales' that has different
            #             types of data like floats, integers, strings
            #             some of which having NULL values which are ignored.

            # Create teradataml dataframe.
            >>> df1 = DataFrame("sales")
            >>> print(df1)
                              Feb   Jan   Mar   Apr    datetime
            accounts
            Blue Inc     90.0    50    95   101  04/01/2017
            Orange Inc  210.0  None  None   250  04/01/2017
            Red Inc     200.0   150   140  None  04/01/2017
            Yellow Inc   90.0  None  None  None  04/01/2017
            Jones LLC   200.0   150   140   180  04/01/2017
            Alpha Co    210.0   200   215   250  04/01/2017

            # Prints unbiased sample variance of each column(with supported data types).
            >>> df3 = df1.select(["accounts","Feb","Jan","Mar","Apr"])
            >>> df3.var()
                   var_Feb      var_Jan  var_Mar      var_Apr
            0  3546.666667  3958.333333   2475.0  5036.916667
            >>>

            # Prints population variance of each column(with supported data types).
            >>> df3.var(population=True)
                   var_Feb  var_Jan  var_Mar    var_Apr
            0  2955.555556  2968.75  1856.25  3777.6875
            >>>

            #
            # Using var() as Time Series Aggregate.
            #
            >>> # Load the example datasets.
            ... load_example_data("dataframe", ["ocean_buoys"])
            >>>

            #
            # Time Series Aggregate Example 1: Executing var() function on DataFrame created on
            #                                  non-sequenced PTI table. We will consider all rows for the
            #                                  columns while calculating the variance value.
            #
            >>> # Create the required DataFrames.
            ... # DataFrame on non-sequenced PTI table
            ... ocean_buoys = DataFrame("ocean_buoys")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys.columns
            ['buoyid', 'TD_TIMECODE', 'temperature', 'salinity']
            >>> ocean_buoys.head()
                                   TD_TIMECODE  temperature  salinity
            buoyid
            0       2014-01-06 08:10:00.000000        100.0        55
            0       2014-01-06 08:08:59.999999          NaN        55
            1       2014-01-06 09:01:25.122200         77.0        55
            1       2014-01-06 09:03:25.122200         79.0        55
            1       2014-01-06 09:01:25.122200         70.0        55
            1       2014-01-06 09:02:25.122200         71.0        55
            1       2014-01-06 09:03:25.122200         72.0        55
            0       2014-01-06 08:09:59.999999         99.0        55
            0       2014-01-06 08:00:00.000000         10.0        55
            0       2014-01-06 08:10:00.000000         10.0        55

            # To use var() as Time Series Aggregate we must run groupby_time() first, followed by var().
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.var().sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid  var_salinity  var_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0           0.0       2670.25000
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1           0.0         15.50000
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2           0.0          1.00000
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44           0.0         33.24359
            >>>

            #
            # Time Series Aggregate Example 2: Executing var() function on DataFrame created on
            #                                  non-sequenced PTI table. We will consider DISTINCT rows for the
            #                                  columns while calculating the variance value.
            #
            # To use var() as Time Series Aggregate we must run groupby_time() first, followed by var().
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.var(distinct = True).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid var_salinity  var_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0         None      2670.333333
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1         None        15.500000
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2         None         1.000000
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44         None        27.700000
            >>>

            #
            # Time Series Aggregate Example 3: Executing var() function on DataFrame created on
            #                                  non-sequenced PTI table. We shall calculate the
            #                                  variance on entire population, with all non-null
            #                                  data points considered for calculations.
            #
            # To use var() as Time Series Aggregate we must run groupby_time() first, followed by var().
            # To calculate population variance we must set population=True.
            #
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.var(population=True).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid  var_salinity  var_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0           0.0      2002.687500
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1           0.0        12.916667
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2           0.0         0.666667
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44           0.0        30.686391
            >>>
        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["distinct", distinct, True, (bool)])
        awu_matrix.append(["population", population, True, (bool)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        return self._get_dataframe_aggregate(operation='var', distinct=distinct, population=population)

    @collect_queryband(queryband="DF_agg")
    def agg(self, func=None):
        """
        DESCRIPTION:
            Perform aggregates using one or more operations.

        PARAMETERS:
            func:
                Required Argument.
                Specifies the function(s) to apply on DataFrame columns.

                Valid values for func are:
                    * 'count', 'sum', 'min', 'max', 'mean', 'std', 'percentile', 'percentile_<floatvalue>', 'unique',
                      'median', 'var'
                    * Note: In 'percentile_<floatvalue>', <floatvalue> specifies the desired percentile value to
                            calculate aggregate. It should be in the range of 0.0 to 1.0 (both inclusive).

                Acceptable formats for function(s) are
                    string, dictionary, list of strings/functions/ColumnExpression or ColumnExpression.

                Accepted combinations are:
                    1. String function name
                    2. List of string functions
                    3. Dictionary containing column name as key and
                       aggregate function name (string or list of
                       strings) as value
                    4. ColumnExpression built using the aggregate functions. 
                    5. List of ColumnExpression built using the aggregate functions.

                Notes:
                * The name of the output columns are generated based on aggregate functions and column names.
                    For Example, 
                    1. "func" passed as a string. 
                        >>> df.agg('mean')
                        Assume that the column names of the dataframe are employee_no, first_name, marks, dob, joined_date.
                        After the above operation, the output column names are: 
                          mean_employee_no, mean_marks, mean_dob, mean_joined_date 

                    2. "func" passed as a list of string functions.
                        >>> df.agg(['min', 'sum'])
                        Assume that the column names of the dataframe are employee_no, first_name, marks, dob, joined_date.
                        After the above operation, the output column names are:
                          min_employee_no, sum_employee_no, min_first_name, min_marks, sum_marks, min_dob, min_joined_date

                    3. "func" passed as a dictionary containing column name as key and aggregate function name as value. 
                        >>> df.agg({'employee_no' : ['min', 'sum', 'var'], 'first_name' : ['min']})
                        Output column names after the above operation are:
                          min_employee_no, sum_employee_no, var_employee_no, min_first_name

                    4. "percentile_<floatvalue>" passed to agg.
                        >>> df.agg({'employee_no' : ['percentile_0.25', 'percentile_0.75', 'min']})
                        >>> df.agg(['percentile_0.25', 'percentile_0.75', 'sum'])
                        >>> df.agg('percentile_0.25')

                    5. "func" passed as a ColumnExpression built using the aggregate functions.
                        >>> df.agg(df.first_name.count())
                        Output column name after the above operation is:
                          count(first_name)

                    6. "func" passed as a list of ColumnExpression built using the aggregate functions.
                        >>> df.agg([df.employee_no.min(), df.first_name.count()])
                        Output column names after the above operation are:
                          min(employee_no), count(first_name)

                * On ColumnExpression or list of ColumnExpression alias() can be used to 
                  return the output columns with aliased name.
                    For Example,
                    >>> df.agg(df.first_name.count().alias("total_names"))
                    Output column name after the above operation is:
                      total_names

                    >>> df.agg([df.joined_date.min().alias("min_date"), df.first_name.count().alias("total_names")])
                    Output column names after the above operation are:
                      min_date, total_names
                * This method does not support DataFrame containing array columns.


        RETURNS:
            teradataml DataFrame object with operations
            mentioned in parameter 'func' performed on specified
            columns.


        RAISES:
            TeradataMLException
            1. TDMLDF_AGGREGATE_FAILED - If operations on given columns
                fail to generate aggregate dataframe.

                Possible error message:
                Unable to perform 'agg()' on the dataframe.

            2. TDMLDF_AGGREGATE_COMBINED_ERR - If the provided
                aggregate operations do not support specified columns.

                Possible error message:
                No results. Below is/are the error message(s):
                All selected columns [(col1 - VARCHAR)] is/are
                unsupported for 'sum' operation.

            3. TDMLDF_INVALID_AGGREGATE_OPERATION - If the aggregate
                operation(s) received in parameter 'func' is/are
                invalid.

                Possible error message:
                Invalid aggregate operation(s): minimum, counter.
                Valid aggregate operation(s): count, max, mean, min,
                std, sum.

            4. TDMLDF_AGGREGATE_INVALID_COLUMN - If any of the columns
                specified in 'func' is not present in the dataframe.

                Possible error message:
                Invalid column(s) given in parameter func: col1.
                Valid column(s) : A, B, C, D.

            5. MISSING_ARGS - If the argument 'func' is missing.

                Possible error message:
                Following required arguments are missing: func.

            6. UNSUPPORTED_DATATYPE - If the argument 'func' is not of
                valid datatype.

                Possible error message:
                Invalid type(s) passed to argument 'func', should be:
                ['str, dict, ColumnExpression or list of values of type(s): str, ColumnExpression'].

        EXAMPLES :
            # Load the data to run the example.
            >>> from teradataml.data.load_example_data import load_example_data
            >>> load_example_data("dataframe", ["employee_info", "sales"])

            # Create teradataml dataframe.
            >>> df = DataFrame("employee_info")
            >>> print(df)
                        first_name marks   dob joined_date
            employee_no
            101              abcde  None  None    02/12/05
            100               abcd  None  None        None
            112               None  None  None    18/12/05
            >>>

            # Get the minimum, sum and variance of employee number and minimum and mean of name,
            # by passing dictionary of column names to string function/list of string functions as parameter.
            >>> df.agg({'employee_no' : ['min', 'sum', 'var'], 'first_name' : ['min', 'mean']})
              min_employee_no sum_employee_no  var_employee_no min_first_name
            0             100             313        44.333333           abcd

            # Get the minimum, 25 percentile value and variance of employee number, by passing dictionary of
            # column names to string function/list of string functions as parameter.
            >>> df.agg({'employee_no' : ['min', 'percentile_0.25', 'var']})
              min_employee_no  percentile_0.25_employee_no  var_employee_no
            0              100                          100        44.333333

            # Get the minimum and sum of all the columns in the dataframe,
            # by passing list of string functions as parameter.
            >>> df.agg(['min', 'sum'])
              min_employee_no sum_employee_no min_first_name min_marks sum_marks min_dob min_joined_date
            0             100             313           abcd      None      None    None      1902-05-12

            # Get the mean of all the columns in the dataframe, by passing string function as parameter.
            >>> df.agg('mean')
               mean_employee_no mean_marks mean_dob mean_joined_date
            0        104.333333       None     None         60/12/04

            # Get the total names in the dataframe, by running count() on the "first_name"
            # and passing ColumnExpression as parameter.
            >>> df.agg(df.first_name.count())
               count(first_name)
            0                  2

            # Get the minimum of joining date and total of names in the dataframe, 
            # by running min() on joined_date and count() on the "first_name"
            # and passing list of ColumnExpression as parameter.
            >>> df.agg([df.employee_no.min(), df.first_name.count()])
               min(employee_no)  count(first_name)
            0               100                  2

            # Get the total names in the dataframe, by running count() on the "first_name" and
            # use alias() to have the output column named as "total_names".
            >>> df.agg(df.first_name.count().alias("total_names"))
               total_names
            0            2

            # Get the minimum of joining date and total names in the dataframe, 
            # by running min() on joined_date and count() on the "first_name" and 
            # use alias() to have the output column named as "min_date" and "total_names".
            >>> df.agg([df.joined_date.min().alias("min_date"), df.first_name.count().alias("total_names")])
               min_date  total_names
            0  02/12/05            2

            # Select only subset of columns from the DataFrame.
            >>> df1 = df.select(['employee_no', 'first_name', 'joined_date'])

            # List of string functions as parameter.
            >>> df1.agg(['mean', 'unique'])
               mean_employee_no unique_employee_no unique_first_name mean_joined_date unique_joined_date
            0        104.333333                  3                 2         60/12/04                  2

            # Get the percentile of each column in the dataframe with default value 0.5.
            >>> df.agg('percentile')
                percentile_employee_no percentile_marks
            0                    101             None

            # Get 80 percentile of each column in the datafame.
            >>> df.agg('percentile_0.8')
               percentile_0.8_employee_no percentile_0.8_marks
            0                         107                 None

            # Using another table 'sales' (having repeated values) to demonstrate operations
            # 'unique' and 'percentile'.

            # Create teradataml dataframe.
            >>> df = DataFrame('sales')
            >>> df
                              Feb   Jan   Mar   Apr    datetime
                accounts
                Yellow Inc   90.0  None  None  None  2017-04-01
                Alpha Co    210.0   200   215   250  2017-04-01
                Jones LLC   200.0   150   140   180  2017-04-01
                Orange Inc  210.0  None  None   250  2017-04-01
                Blue Inc     90.0    50    95   101  2017-04-01
                Red Inc     200.0   150   140  None  2017-04-01

            # Get 80 and 40 percentile values of each column in the dataframe.
            >>> df1 = df.select(['Feb', 'Jan', 'Mar', 'Apr'])
            >>> df1.agg(['percentile_0.8', 'percentile_0.4'])
                percentile_0.8_Feb  percentile_0.4_Feb  percentile_0.8_Jan  percentile_0.4_Jan  percentile_0.8_Mar  percentile_0.4_Mar  percentile_0.8_Apr  percentile_0.4_Apr
            0               210.0               200.0                 170                 150                 170                 140                 250                 194

            >>> df.agg('unique')
                  unique_accounts unique_Feb unique_Jan unique_Mar unique_Apr unique_datetime
                0               6          3          3          3          3               1
        """

        if func is None:
            raise TeradataMlException(Messages.get_message(MessageCodes.MISSING_ARGS, "func"),
                                      MessageCodes.MISSING_ARGS)

        if not isinstance(func, (str, list, dict, ColumnExpression)):
            raise TeradataMlException(Messages.get_message(MessageCodes.UNSUPPORTED_DATATYPE,
                                      'func', ['str, dict, ColumnExpression or list of values of type(s): str, ColumnExpression']),
                                      MessageCodes.UNSUPPORTED_DATATYPE)

        return self._get_dataframe_aggregate(func)

    @collect_queryband(arg_name="operation", prefix="DF")
    def _get_dataframe_aggregate(self, operation, **kwargs):
        """
        Returns the DataFrame given the aggregate operation or list of
        operations or dictionary of column names -> operations.

        PARAMETERS:
            operation - Required Argument. Specifies the function(s) to be
                    applied on teradataml DataFrame columns.
                    Acceptable formats for function(s) are string,
                    dictionary or list of strings/functions.
                    Accepted combinations are:
                    1. String function name
                    2. List of string functions
                    3. Dictionary containing column name as key and
                       aggregate function name (string or list of
                       strings) as value
                    4. ColumnExpression built using the aggregate functions. 
                    5. List of ColumnExpression built using the aggregate functions.

            **kwargs: Keyword arguments. Mainly used for Time Series Aggragates.

        RETURNS:
            teradataml DataFrame object with required
            operations mentioned in 'operation' parameter performed.

        RAISES:
            TeradataMLException
            1. TDMLDF_AGGREGATE_FAILED - If operations on given columns
                fail to generate output dataframe.

                Possible error message:
                Unable to perform 'agg()' on the dataframe.

            2. TDMLDF_AGGREGATE_COMBINED_ERR - If the provided
                aggregate operations do not support specified columns.

                Possible error message:
                No results. Below is/are the error message(s):
                All selected columns [(col1 - VARCHAR)] is/are
                unsupported for 'sum' operation.

            3. TDMLDF_INVALID_AGGREGATE_OPERATION - If the aggregate
                operation(s) received in parameter 'operation' is/are
                invalid.

                Possible error message:
                Invalid aggregate operation(s): minimum, counter.
                Valid aggregate operation(s): count, max, mean, min,
                std, sum.

            4. TDMLDF_AGGREGATE_INVALID_COLUMN - If any of the columns
                specified in the parameter 'operation' is not present
                in the dataframe.

                Possible error message:
                Invalid column(s) given in parameter func: col1.
                Valid column(s) : A, B, C, D.

        EXAMPLES :
            df = _get_dataframe_aggregate(operation = 'mean')
            or
            df = _get_dataframe_aggregate(operation = ['mean', 'min'])
            or
            df = _get_dataframe_aggregate(operation = {'col1' :
                                    ['mean', 'min'], 'col2' : 'count'})
        """
        if isinstance(self, DataFrameGroupByTime) and self._timebucket_duration == "*" and operation != 'delta_t':
            raise ValueError(Messages.get_message(
                MessageCodes.INVALID_ARG_VALUE).format("*", 'timebucket_duration while grouping time series data',
                                                       'a valid timebucket duration, as mentioned in user guide. '
                                                       'DELTA_T is the only aggregate function that can be used with '
                                                       'timebucket_duration as \'*\''))

        try:
            # Check if aggregation is to be performed on specific set of columns or not.
            columns = kwargs['columns'] if "columns" in kwargs.keys() else None
            if columns is not None:
                col_names = []
                col_types = []
                if isinstance(columns, str):
                    columns = [columns]
                for col in columns:
                    col_names.append(col)
                    col_types.append(self[col].type)
            else:
                # Retrieve the column names and types from the metaexpr.
                col_names, col_types = df_utils._get_column_names_and_types_from_metaexpr(self._metaexpr)

            # Remove columns from metaexpr before passing to stated aggr func if self
            # is of DataFrameGroupBy or DataFrameGroupByTime type so that no duplicate
            # columns shown in result
            groupby_col_names = []
            groupby_col_types = []
            pti_default_cols_proj = []
            pti_default_cols_types = []
            is_time_series_aggregate = True if isinstance(self, DataFrameGroupByTime) else False
            if isinstance(self, DataFrameGroupBy) or isinstance(self, DataFrameGroupByTime):
                for col in self.groupby_column_list:
                    if "GROUP BY TIME" not in col and col != "TIMECODE_RANGE":
                        # If group by columns are not time series specific columns, then process
                        # these group by columns, so that they are removed from the actual projection
                        # list and aggregate operation will not performed on those.
                        groupby_col_names.append(col)
                        groupby_col_types.append(self[col].type)

                        include_grouping_columns = True if isinstance(self, DataFrameGroupBy) and \
                                                           self._include_grouping_columns else False
                        if not include_grouping_columns and col in col_names:
                            # If 'include_grouping_columns' argument is set to True and,
                            # group by column is not specified in the columns argument,
                            # then, we should ignore this processing, otherwise we
                            # should process it in the same way to remove the reference
                            # for grouping column from aggregation list.
                            colindex = col_names.index(col)

                            # Remove the grouping column and it's type from the lists.
                            del col_names[colindex]
                            del col_types[colindex]
                    else:
                        # If grouping columns are timeseries columns, then process those
                        # and generate a separate list of columns and their types.
                        pti_default_cols_proj.append(col)
                        if "GROUP BY TIME" not in col:
                            ctypes = PERIOD_TIMESTAMP
                        else:
                            ctypes = BIGINT
                        pti_default_cols_types.append(ctypes)

            # Return Empty DataFrame if all the columns are selected in groupby as parent has
            if len(col_names) == 0:
                aggregate_expression, new_column_names, new_column_types = \
                    df_utils._construct_sql_expression_for_aggregations(self,
                                                                        groupby_col_names, groupby_col_types, operation,
                                                                        as_time_series_aggregate=is_time_series_aggregate,
                                                                        **kwargs)
                self._index_label = new_column_names
            else:
                aggregate_expression, new_column_names, new_column_types = \
                    df_utils._construct_sql_expression_for_aggregations(self,
                                                                        col_names, col_types, operation,
                                                                        as_time_series_aggregate=is_time_series_aggregate,
                                                                        **kwargs)
                new_column_names = pti_default_cols_proj + groupby_col_names + new_column_names
                new_column_types = pti_default_cols_types + groupby_col_types + new_column_types

            if isinstance(operation, dict) or isinstance(operation, list):
                operation = 'agg'

            aggregate_node_id = self._aed_utils._aed_aggregate(self._nodeid, aggregate_expression,
                                                               operation)

            new_metaexpr = UtilFuncs._get_metaexpr_using_columns(aggregate_node_id,
                                                                 zip(new_column_names,
                                                                     new_column_types),
                                                                 datalake=self._metaexpr.datalake)
            agg_df = self._create_dataframe_from_node \
                (aggregate_node_id, new_metaexpr, self._index_label)

            if (operation in ["sum", "csum", "mean"] and self._check_numeric_overflow(agg_df)):
                # As typecasting required for lower types, get a new DataFrame with columns of higher
                # datatype and perform same the operation on it.
                promoted_df = self._promote_dataframe_types()
                if operation == "csum":
                    agg_df = promoted_df.csum(sort_columns=kwargs.get("sort_columns"),
                                              drop_columns=kwargs.get("drop_columns"))
                else:
                    agg_df = getattr(promoted_df, operation)(distinct=kwargs.get("distinct"))

            return agg_df

        except TeradataMlException:
            raise
        except ValueError:
            raise
        except Exception as err:
            raise TeradataMlException(Messages.get_message(
                MessageCodes.EXECUTION_FAILED, "perform {} on DataFrame".format(operation), str(err)),
                MessageCodes.EXECUTION_FAILED) from err

    def _check_numeric_overflow(self, result_df):
        """
        Internal function to check if numeric overflow can occur for
        DataFrame operations.

        PARAMETERS:
            result_df:
                Required Argument.
                Specifies the result DataFrame of aggregate operation.
                Types: teradataml DataFrame

        RETURNS:
            True if numeric overflow occurs, otherwise False.

        RAISES:
            TeradataMlException
            
        EXAMPLES :
            result = self._check_numeric_overflow(agg_df)
        """
        try:
            # Printing the DF will actually run underlying select query and
            # will brought up numeric overflow if any. Only materializing won't work.
            repr(result_df)
            return False
        except TeradataMlException as tme:
            if "Numeric overflow occurred during computation" in str(tme):
                return True
            else:
                raise tme

    def _promote_dataframe_types(self):
        """
        Function promotes numeric column type to higher type to avoid numeric overflow
        for aggregate operations(describe, mean, sum, csum).

        PARAMETERS:
            None

        RETURNS:
            teradataml DataFrame object with required columns casted
            to higher type.

        RAISES:
            None
            
        EXAMPLES :
            new_df = self._promote_dataframe_types(agg_df)
        """
        new_cols = {}
        # Dictionary to map highest type.
        next_type = {BYTEINT: SMALLINT,
                     SMALLINT: INTEGER,
                     INTEGER: BIGINT,
                     DECIMAL: FLOAT
                     }
        keys = tuple(next_type.keys())

        for col in self.columns:
            if isinstance(self[col].type, keys):
                new_cols[col] = self[col].cast(type_=next_type[self[col].type.__class__])
            else:
                new_cols[col] = self[col]

        return self.assign(True, **new_cols)

    def __repr__(self):
        """
        Returns the string representation for a teradataml DataFrame instance.
        The string contains:
            1. Column names of the dataframe.
            2. At most the first no_of_rows rows of the dataframe.
            3. A default index for row numbers.

        NOTES:
          - This makes an explicit call to get rows from the database.
          - To change number of rows to be printed set the max_rows option in
            "options.display.display".
          - Default value of max_rows is 10.

        EXAMPLES:
            df = DataFrame.from_table("table1")
            print(df)

            df = DataFrame.from_query("select col1, col2, col3 from table1")
            print(df)
        """
        try:

            data, columns = self.__get_data_columns()
            pandas_df = pd.DataFrame.from_records(data, columns=columns, coerce_float=True)

            if self._index_label:
                pandas_df.set_index(self._index_label, inplace=True)

            if self._undropped_index is not None:
                for col in self._undropped_index:
                    pandas_df.insert(0, col, pandas_df.index.get_level_values(col).tolist(), allow_duplicates=True)

            return pandas_df.to_string()

        except TeradataMlException:
            raise

        except Exception as err:
            raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_INFO_ERROR) + str(err),
                                      MessageCodes.TDMLDF_INFO_ERROR) from err

    def _repr_html_(self):
        """ Print method for teradataml for iPython rich display. """
        self._generate_output_html()
        if display.enable_ui:
            # EDA Ui widget representation using teradatamlwidgets
            if self._eda_ui is None:
                try:
                    from teradatamlwidgets.eda.Ui import Ui
                    self._eda_ui = Ui(df=self, html=self.html)
                except ImportError:
                    # If teradatamlwidgets is not installed, disable EDA UI display
                    # and return default HTML representation
                    display.enable_ui = False
                    return self.html
            else:
                self._eda_ui.display_ui()
        return self.html

    def get_eda_ui(self):
        """
        Returns the EDA representation UI.

        PARAMETERS:
            None.

        EXCEPTIONS:
            None.

        RETURNS:
            teradatamlwidgets.eda.Ui

        EXAMPLE:
            df = ui.get_eda_ui()
        """
        return self._eda_ui

    def _generate_output_html(self, disable_types=True):
        # Check if class attributes __data and __data_columns are not None.
        # If not None, reuse the data and columns.
        # If None, generate latest results.
        if self.__data is None and self.__data_columns is None:
            self.__get_data_columns()

        # Generate the HTML content from the class attributes __data and __data_columns
        # which are updated by _repr_() function call which always executes before _repr_html_().
        indent = "\t"
        dindent = indent + indent

        header_html = ['<style type="text/css">',
                       'table { border:ridge 5px}',
                       'table td {border:inset 1px;}',
                       'table tr#HeaderRow {background-color:grey; color:white;}',
                       '</style>\n'
                       ]
        html = "\n{0}".format(indent).join(header_html)
        html += '<html><table style="min-width:1000px;">\n{0}<tr id="HeaderRow">\n'.format(indent)

        columns_html = "</th><th>".join(self.__data_columns)
        html += "<th>{0}</th>\n".format(columns_html)
        html += "</tr>\n"

        if not disable_types:
            html += '<tr>\n'.format(indent)
            col_types = [repr(self._td_column_names_and_sqlalchemy_types[column]) for column in
                         self.__data_columns]
            columns_types_html = "</td>\n{0}<td>".format(dindent).join(col_types)
            html += "{0}<td>{1}</td>\n".format(dindent, columns_types_html)
            html += "{0}</tr>\n".format(indent)

        for row in self.__data:
            row_html = ["{0}<td>{1}</td>\n".format(dindent,
                                                   cell) for cell in row]
            html += "{1}<tr>\n{0}{1}</tr>\n".format("".join(row_html), indent)

        html += "</table></html>"
        self.html = html

    def get_output(self, output_index=0):
        """
        DESCRIPTION:
            Returns the result of analytic function when analytic function is
            run from 'Analyze' tab in EDA UI.
            Note:
                * The function does not return anything if analytic function is
                  not run from EDA UI.

        PARAMETERS:
            output_index:
                Optional Argument.
                Specifies the index of the output dataframe to be returned.
                Default Value: 0
                Types: int

        RAISES:
            IndexError

        RETURNS:
            teradataml DataFrame object.
        """
        return self._eda_ui.get_output_dataframe(output_index=output_index)

    def __get_data_columns(self):
        """
        DESCRIPTION:
            Internal function to execute the node and get the result.

        RETURNS:
            tuple, first element represents data for the underlying query
                   and second element represents the column names.

        RAISES:
            None.

        EXAMPLES:
            self.__get_data_columns()
        """
        if not self._table_name:
            if not self._aed_query:
                self.__generate_aed_query()
            # TODO: Check the length of query and if it fails, create a view in catch block.
            # Address in this JIRA: https://teradata-pe.atlassian.net/browse/ELE-6922
            query = repr(self._metaexpr) + ' FROM ( ' + self._aed_query + ' ) as temp_table'
        else:
            query = repr(self._metaexpr) + ' FROM ' + self._table_name

        if self._orderby is not None:
            query += ' ORDER BY ' + self._orderby

        query += ';'
        # Execute the query and get the results in a list.
        self.__data, self.__data_columns = UtilFuncs._execute_query(query=query, fetchWarnings=True)

        return self.__data, self.__data_columns

    def __generate_aed_query(self, full_query=False):
        """
        DESCRIPTION:
            Internal function to return underlying SQL for the teradataml
            DataFrame. It is the same SQL that is used to view the data for
            a teradataml DataFrame.

        PARAMETERS:
            full_query:
                Optional Argument.
                Specifies if the complete query for the dataframe should be returned.
                When this parameter is set to True, query for the dataframe is returned
                with respect to the base dataframe's table (from_table() or from_query())
                or from the output tables of analytical functions (if there are any in the
                workflow). This query may or may not be directly used to retrieve data
                for the dataframe upon which the function is called.
                When this parameter is not used, string returned is the query already used
                or will be used to retrieve data for the teradataml DataFrame.
                Default Value: False
                Types: bool

        RETURNS:
            String representing the underlying SQL query for the teradataml DataFrame.

        RAISES:
            None.

        EXAMPLES:
            self.__generate_aed_query()
        """
        # Run aed call only when _aed_query is None or
        # the type of current stored query (full/short) is not matching
        # with asked query type.
        if (not self._aed_query) or (not self._is_full_query == full_query):
            node_id = self._nodeid

            if isinstance(self, (DataFrameGroupBy, DataFrameGroupByTime)):
                # If dataframe is either of type groupby or groupbytime
                # then get its parent dataframe nodeid and return queries
                # for the same
                node_id = self._aed_utils._aed_get_parent_nodeids(self._nodeid)[0]

            queries = self._aed_utils._aed_show_query(node_id, query_with_reference_to_top=full_query)
            # Store query and type of query in class attributes to avoid future runs.
            self._aed_query = queries[0][0]
            self._is_full_query = full_query

        return self._aed_query

    @collect_queryband(queryband="DF_select")
    def select(self, select_expression):
        """
        DESCRIPTION:
            Select required columns from DataFrame using an expression.
            Returns a new teradataml DataFrame with selected columns only.

        PARAMETERS:

            select_expression:
                Required Argument.
                String or List representing columns to select.
                Types: str OR List of Strings (str)

                The following formats (only) are supported for select_expression:

                A] Single Column String: df.select("col1")
                B] Single Column List: df.select(["col1"])
                C] Multi-Column List: df.select(['col1', 'col2', 'col3'])
                D] Multi-Column List of List: df.select([["col1", "col2", "col3"]])

                Column Names ("col1", "col2"..) are Strings representing Teradata Vantage table Columns.
                All Standard Teradata data types for columns supported: INTEGER, VARCHAR(5), FLOAT.

                Note: Multi-Column selection of the same column such as df.select(['col1', 'col1']) is not supported.

        RETURNS:
            teradataml DataFrame

        RAISES:
            TeradataMlException (TDMLDF_SELECT_INVALID_COLUMN, TDMLDF_SELECT_INVALID_FORMAT,
                                 TDMLDF_SELECT_DF_FAIL, TDMLDF_SELECT_EXPR_UNSPECIFIED,
                                 TDMLDF_SELECT_NONE_OR_EMPTY)

        EXAMPLES:
            >>> load_example_data("dataframe","admissions_train")
            >>> df = DataFrame('admissions_train')
            >>> df
               masters   gpa     stats programming admitted
            id
            5       no  3.44    Novice      Novice        0
            7      yes  2.33    Novice      Novice        1
            22     yes  3.46    Novice    Beginner        0
            17      no  3.83  Advanced    Advanced        1
            13      no  4.00  Advanced      Novice        1
            19     yes  1.98  Advanced    Advanced        0
            36      no  3.00  Advanced      Novice        0
            15     yes  4.00  Advanced    Advanced        1
            34     yes  3.85  Advanced    Beginner        0
            40     yes  3.95    Novice    Beginner        0

            A] Single String Column
            >>> df.select("id")
            Empty DataFrame
            Columns: []
            Index: [22, 34, 13, 19, 15, 38, 26, 5, 36, 17]

            B] Single Column List
            >>> df.select(["id"])
            Empty DataFrame
            Columns: []
            Index: [15, 26, 5, 40, 22, 17, 34, 13, 7, 38]

            C] Multi-Column List
            >>> df.select(["id", "masters", "gpa"])
               masters   gpa
            id
            5       no  3.44
            36      no  3.00
            15     yes  4.00
            17      no  3.83
            13      no  4.00
            40     yes  3.95
            7      yes  2.33
            22     yes  3.46
            34     yes  3.85
            19     yes  1.98

            D] Multi-Column List of List
            >>> df.select([['id', 'masters', 'gpa']])
               masters   gpa
            id
            5       no  3.44
            34     yes  3.85
            13      no  4.00
            40     yes  3.95
            22     yes  3.46
            19     yes  1.98
            36      no  3.00
            15     yes  4.00
            7      yes  2.33
            17      no  3.83
        """
        try:
            if self._metaexpr is None:
                raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_INFO_ERROR),
                                          MessageCodes.TDMLDF_INFO_ERROR)

            # If invalid, appropriate exception raised; Processing ahead only for valid expressions
            select_exp_col_list = self.__validate_select_expression(select_expression)

            # Constructing New Column names & Types for selected columns ONLY using Parent _metaexpr
            col_names_types = df_utils._get_required_columns_types_from_metaexpr(self._metaexpr, select_exp_col_list)

            # Create a node in AED using _aed_select
            column_expression = ','.join(select_exp_col_list)
            sel_nodeid = self._aed_utils._aed_select(self._nodeid, column_expression)

            # Constructing new Metadata (_metaexpr) without DB; using dummy select_nodeid and underlying table name.
            new_metaexpr = UtilFuncs._get_metaexpr_using_columns(sel_nodeid, col_names_types.items(),
                                                                 datalake=self._metaexpr.datalake)
            return self._create_dataframe_from_node(sel_nodeid, new_metaexpr, self._index_label)

        except TeradataMlException:
            raise

        except Exception as err:
            raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_SELECT_DF_FAIL, str(err.exception)),
                                      MessageCodes.TDMLDF_SELECT_DF_FAIL) from err

    def __validate_select_expression(self, select_expression):
        """
        This is an internal function used to validate the select expression for the Select API.
        When the select expression is valid, a list of valid columns to be selected is returned.
        Appropriate TeradataMlException is raised when validation fails.

        PARAMETERS:
            select_expression - The expression to be validated.
            Types: Single String or List of Strings or List of List (single-level)
            Required: Yes

        RETURNS:
            List of column name strings, when valid select_expression is passed.

        RAISES:
            TeradataMlException, when parameter validation fails.

        EXAMPLES:
            self.__validate_select_expression(select_expression = 'col1')
            self.__validate_select_expression(select_expression = ["col1"])
            self.__validate_select_expression(select_expression = [['col1', 'col2', 'col3']])
        """
        tdp = preparer(td_dialect)

        if select_expression is None:
            raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_SELECT_EXPR_UNSPECIFIED),
                                      MessageCodes.TDMLDF_SELECT_EXPR_UNSPECIFIED)

        else:
            # _extract_select_string returns column list only if valid; else raises appropriate exception
            select_exp_col_list = df_utils._extract_select_string(select_expression)
            df_column_list = [tdp.quote("{0}".format(column.name)) for column in self._metaexpr.c]

            # TODO: Remove this check when same column multiple selection enabled
            if len(select_exp_col_list) > len(df_column_list):
                raise TeradataMlException(
                    Messages.get_message(MessageCodes.TDMLDF_SELECT_INVALID_COLUMN, ', '.join(df_column_list)),
                    MessageCodes.TDMLDF_SELECT_INVALID_COLUMN)

            all_cols_exist = all(col in df_column_list for col in select_exp_col_list)

            if not all_cols_exist:
                raise TeradataMlException(
                    Messages.get_message(MessageCodes.TDMLDF_SELECT_INVALID_COLUMN, ', '.join(df_column_list)),
                    MessageCodes.TDMLDF_SELECT_INVALID_COLUMN)

            return select_exp_col_list

    def _get_array_col_type_map(self, result):
        """
        DESCRIPTION:
            Internal function to get the column name and type mapping for ARRAY_* columns.

        PARAMETERS:
            result:
                Required Argument.
                Specifies the pandas DataFrame produced by data extraction (fastexport or regular).
                Types: pandas.DataFrame or tuple (pandas.DataFrame, errors, warnings)

        RETURNS:
            Dictionary with column name as key and ARRAY_* type string (class name) as value
            for array typed columns only. Returns empty dict if none or on failure.
        """
        result = result[0] if isinstance(result, tuple) else result
        name_type_map = {c.name: c.type for c in self._metaexpr.c}
        col_type_map = {}
        for col in result.columns:
            t = name_type_map.get(col)
            if t.__class__.__name__.startswith('ARRAY_'):
                col_type_map[col] = t.__class__.__name__
        return col_type_map

    def _post_process_pandas_result(self, result):
        """
        DESCRIPTION:
            Internal function to post-process the pandas DataFrame produced by
            data extraction (fastexport or regular) to reconstruct ARRAY_* columns.

        PARAMETERS:
            result:
                Required Argument.
                Specifies the pandas DataFrame produced by data extraction (fastexport or regular).
                Types: pandas.DataFrame OR tuple (pandas.DataFrame, errors, warnings)

        RETURNS:
            pandas DataFrame with ARRAY_* columns reconstructed.
            If input is a tuple, returns a tuple with the first element as the
            reconstructed DataFrame and the rest of the elements unchanged.
        """
        col_type_map = self._get_array_col_type_map(result)
        if not col_type_map:
            return result
        if isinstance(result, tuple):
            df_obj = DataFrameUtils._reconstruct_array_columns(result[0], col_type_map)
            return (df_obj,) + result[1:]
        else:
            return DataFrameUtils._reconstruct_array_columns(result, col_type_map)
        

    @collect_queryband(queryband="DF_toPandas")
    def to_pandas(self, index_column=None, num_rows=99999, all_rows=False,
                  fastexport=False, catch_errors_warnings=False, **kwargs):
        """
        DESCRIPTION:
            Returns a Pandas DataFrame for the corresponding teradataml
            DataFrame Object.

        PARAMETERS:
            index_column:
                Optional Argument.
                Specifies column(s) to be used as Pandas index.
                When the argument is provided, the specified column is used as
                the Pandas index. Otherwise, the teradataml DataFrame's index
                (if exists) is used as the Pandas index or the primary index of
                the table on Vantage is used as the Pandas index. The default
                integer index is used if none of the above indexes exists.
                Default Value: Integer index
                Types: str OR list of Strings (str)

            num_rows:
                Optional Argument.
                Specifies the number of rows to retrieve randomly from the DataFrame
                while creating Pandas Dataframe.
                Default Value: 99999
                Types: int
                Note:
                    This argument is ignored if "all_rows" is set to True.

            all_rows:
                Optional Argument.
                Specifies whether all rows from teradataml DataFrame should be
                retrieved while creating Pandas DataFrame.
                Default Value: False
                Types: bool

            fastexport:
                Optional Argument.
                Specifies whether fastexport protocol should be used while
                converting teradataml DataFrame to a Pandas DataFrame. If the
                argument is set to True, fastexport wire protocol is used
                internally for data transfer. By default, fastexport protocol will not be
                used while converting teradataml DataFrame to a Pandas DataFrame.
                When set to None, the approach is decided based on the number of rows
                requested by the user for extraction.
                If requested number of rows are greater than or equal to 100000,
                then fastexport is used, otherwise regular mode is used for data
                extraction.
                Note:
                    1. Teradata recommends to use FastExport when number of rows
                       in teradataml DataFrame are atleast 100,000. To extract
                       lesser rows ignore this option and go with regular
                       approach. FastExport opens multiple data transfer connections
                       to the database.
                    2. FastExport does not support all Teradata Database data types.
                       For example, tables with BLOB and CLOB type columns cannot
                       be extracted.
                    3. FastExport cannot be used to extract data from a
                       volatile or temporary table.
                    4. For best efficiency, do not use DataFrame.groupby() and
                       DataFrame.sort() with FastExport.

                For additional information about FastExport protocol through
                teradatasql driver, please refer to FASTEXPORT section of
                https://pypi.org/project/teradatasql/#FastExport driver documentation.
                Default Value: False
                Types: bool

            catch_errors_warnings:
                Optional Argument.
                Specifies whether to catch errors/warnings(if any) raised by
                fastexport protocol while converting teradataml DataFrame to
                Pandas DataFrame. When this is set to True and fastexport is used,
                to_pandas() returns a tuple containing:
                    a. Pandas DataFrame.
                    b. Errors(if any) in a list thrown by fastexport.
                    c. Warnings(if any) in a list thrown by fastexport.
                When set to False and fastexport is used, prints the fastexport
                errors/warnings to the standard output, if there are any.
                Note:
                    This argument is ignored if "fastexport" is set to False.
                Default Value: False
                Types: bool

            kwargs:
                Optional Argument.
                Specifies keyword arguments. Arguments "coerce_float"
                "parse_dates" and "open_sessions" can be passed as keyword arguments.
                    * "coerce_float" specifies a boolean to for attempting to
                      convert non-string, non-numeric objects to floating point.
                    * "parse_dates" specifies columns to parse as dates.
                    * "open_sessions" specifies the number of Teradata data transfer
                      sessions to be opened for fastexport. This argument is only applicable
                      in fastexport mode.
                    * Function returns the pandas dataframe with Decimal columns types as float instead of object.
                      If user want datatype to be object, set argument "coerce_float" to False.

                Notes:
                    1. For additional information about "coerce_float" and
                       "parse_date" arguments please refer to:
                       https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html
                    2. If "open_sessions" argument is not provided, the default value
                       is the smaller of 8 or the number of AMPs avaialble.
                       For additional information about number of Teradata data-transfer
                       sessions opened during fastexport, please refer to:
                       https://pypi.org/project/teradatasql/#FastExport

        RETURNS:
            When "catch_errors_warnings" is set to True and if protocol used for
            data transfer is fastexport, then the function returns a tuple
            containing:
                a. Pandas DataFrame.
                b. Errors, if any, thrown by fastexport in a list of strings.
                c. Warnings, if any, thrown by fastexport in a list of strings.
            Only Pandas DataFrame otherwise.

            Notes:
                * Column types of the resulting Pandas DataFrame depends on
                  pandas.from_records().
                * If a Pandas DataFrame containing array columns is used to create 
                  a teradataml DataFrame and then converted back using 'to_pandas()', 
                  the resulting Pandas DataFrame may differ from the original.
                  
        RAISES:
            TeradataMlException

        EXAMPLES:

            Teradata supports the following formats:

            A] No parameter(s): df.to_pandas()
            B] Single index_column parameter: df.to_pandas(index_column = "col1")
            C] Multiple index_column (list) parameters:
            df.to_pandas(index_column = ['col1', 'col2'])
            D] Only num_rows parameter specified:  df.to_pandas(num_rows = 100)
            E] Both index_column & num_rows specified:
            df.to_pandas(index_column = 'col1', num_rows = 100)
            F] Only all_rows parameter specified:  df.to_pandas(all_rows = True)

            Column names ("col1", "col2"..) are strings representing Teradata
            Vantage table Columns. It supports all standard Teradata data types
            for columns: INTEGER, VARCHAR(5), FLOAT etc.
            df is a Teradata DataFrame object: df = DataFrame.from_table('admissions_train')

            >>> load_example_data("dataframe","admissions_train")
            >>> df = DataFrame("admissions_train")
            >>> df
               masters   gpa     stats programming admitted
            id
            22     yes  3.46    Novice    Beginner        0
            37      no  3.52    Novice      Novice        1
            35      no  3.68    Novice    Beginner        1
            12      no  3.65    Novice      Novice        1
            4      yes  3.50  Beginner      Novice        1
            38     yes  2.65  Advanced    Beginner        1
            27     yes  3.96  Advanced    Advanced        0
            39     yes  3.75  Advanced    Beginner        0
            7      yes  2.33    Novice      Novice        1
            40     yes  3.95    Novice    Beginner        0
            >>> pandas_df = df.to_pandas()
            >>> pandas_df
               masters   gpa     stats programming  admitted
            id
            15     yes  4.00  Advanced    Advanced         1
            14     yes  3.45  Advanced    Advanced         0
            31     yes  3.50  Advanced    Beginner         1
            29     yes  4.00    Novice    Beginner         0
            23     yes  3.59  Advanced      Novice         1
            21      no  3.87    Novice    Beginner         1
            17      no  3.83  Advanced    Advanced         1
            34     yes  3.85  Advanced    Beginner         0
            13      no  4.00  Advanced      Novice         1
            32     yes  3.46  Advanced    Beginner         0
            11      no  3.13  Advanced    Advanced         1
            ...

            >>> pandas_df = df.to_pandas(index_column = 'id')
            >>> pandas_df
               masters   gpa     stats programming  admitted
            id
            15     yes  4.00  Advanced    Advanced         1
            14     yes  3.45  Advanced    Advanced         0
            31     yes  3.50  Advanced    Beginner         1
            29     yes  4.00    Novice    Beginner         0
            23     yes  3.59  Advanced      Novice         1
            21      no  3.87    Novice    Beginner         1
            17      no  3.83  Advanced    Advanced         1
            34     yes  3.85  Advanced    Beginner         0
            13      no  4.00  Advanced      Novice         1
            32     yes  3.46  Advanced    Beginner         0
            11      no  3.13  Advanced    Advanced         1
            28      no  3.93  Advanced    Advanced         1
            ...

            >>> pandas_df = df.to_pandas(index_column = 'gpa')
            >>> pandas_df
                  id masters     stats programming  admitted
            gpa
            4.00  15     yes  Advanced    Advanced         1
            3.45  14     yes  Advanced    Advanced         0
            3.50  31     yes  Advanced    Beginner         1
            4.00  29     yes    Novice    Beginner         0
            3.59  23     yes  Advanced      Novice         1
            3.87  21      no    Novice    Beginner         1
            3.83  17      no  Advanced    Advanced         1
            3.85  34     yes  Advanced    Beginner         0
            4.00  13      no  Advanced      Novice         1
            3.46  32     yes  Advanced    Beginner         0
            3.13  11      no  Advanced    Advanced         1
            3.93  28      no  Advanced    Advanced         1
            ...

            >>> pandas_df = df.to_pandas(index_column = ['masters', 'gpa'])
            >>> pandas_df
                          id     stats programming  admitted
            masters gpa
            yes     4.00  15  Advanced    Advanced         1
                    3.45  14  Advanced    Advanced         0
                    3.50  31  Advanced    Beginner         1
                    4.00  29    Novice    Beginner         0
                    3.59  23  Advanced      Novice         1
            no      3.87  21    Novice    Beginner         1
                    3.83  17  Advanced    Advanced         1
            yes     3.85  34  Advanced    Beginner         0
            no      4.00  13  Advanced      Novice         1
            yes     3.46  32  Advanced    Beginner         0
            no      3.13  11  Advanced    Advanced         1
                    3.93  28  Advanced    Advanced         1
            ...

            >>> pandas_df = df.to_pandas(index_column = 'gpa', num_rows = 3)
            >>> pandas_df
                  id masters   stats programming  admitted
            gpa
            3.46  22     yes  Novice    Beginner         0
            2.33   7     yes  Novice      Novice         1
            3.95  40     yes  Novice    Beginner         0

            >>> pandas_df = df.to_pandas(all_rows = True)
            >>> pandas_df
               masters   gpa     stats programming  admitted
            id
            15     yes  4.00  Advanced    Advanced         1
            14     yes  3.45  Advanced    Advanced         0
            31     yes  3.50  Advanced    Beginner         1
            29     yes  4.00    Novice    Beginner         0
            23     yes  3.59  Advanced      Novice         1
            21      no  3.87    Novice    Beginner         1
            17      no  3.83  Advanced    Advanced         1
            34     yes  3.85  Advanced    Beginner         0
            13      no  4.00  Advanced      Novice         1
            32     yes  3.46  Advanced    Beginner         0
            11      no  3.13  Advanced    Advanced         1
            ...

            # Convert teradataml DataFrame to pandas DataFrame using fastexport.
            # Prints errors/warnings if any on to the screen as catch_errors_warnings
            # argument is not set.
            >>> pandas_df = df.to_pandas(fastexport = True)
            Errors: []
            Warnings: []
            >>> pandas_df
               masters   gpa     stats programming  admitted
            id
            38     yes  2.65  Advanced    Beginner         1
            26     yes  3.57  Advanced    Advanced         1
            5       no  3.44    Novice      Novice         0
            24      no  1.87  Advanced      Novice         1
            3       no  3.70    Novice    Beginner         1
            1      yes  3.95  Beginner    Beginner         0
            20     yes  3.90  Advanced    Advanced         1
            18     yes  3.81  Advanced    Advanced         1
            8       no  3.60  Beginner    Advanced         1
            25      no  3.96  Advanced    Advanced         1
            2      yes  3.76  Beginner    Beginner         0
            ...

            # Convert teradataml DataFrame to pandas DataFrame using fastexport
            # also catch warnings/errors if any raised by fastexport. Returns
            # a tuple.
            >>> pandas_df, err, warn = df.to_pandas(fastexport = True,
                                                    catch_errors_warnings = True)
            # Print pandas df.
            >>> pandas_df
               masters   gpa     stats programming  admitted
            id
            38     yes  2.65  Advanced    Beginner         1
            26     yes  3.57  Advanced    Advanced         1
            5       no  3.44    Novice      Novice         0
            24      no  1.87  Advanced      Novice         1
            3       no  3.70    Novice    Beginner         1
            1      yes  3.95  Beginner    Beginner         0
            20     yes  3.90  Advanced    Advanced         1
            18     yes  3.81  Advanced    Advanced         1
            8       no  3.60  Beginner    Advanced         1
            25      no  3.96  Advanced    Advanced         1
            2      yes  3.76  Beginner    Beginner         0
            17      no  3.83  Advanced    Advanced         1
            ...
            # Print errors list.
            >>> err
            []
            # Print warnings list.
            >>> warn
            []

            # Convert teradataml DataFrame to pandas DataFrame without
            # fastexport.
            >>> pandas_df = df.to_pandas(fastexport = False)
            >>> pandas_df
               masters   gpa     stats programming  admitted
            id
            38     yes  2.65  Advanced    Beginner         1
            26     yes  3.57  Advanced    Advanced         1
            5       no  3.44    Novice      Novice         0
            24      no  1.87  Advanced      Novice         1
            3       no  3.70    Novice    Beginner         1
            1      yes  3.95  Beginner    Beginner         0
            20     yes  3.90  Advanced    Advanced         1
            18     yes  3.81  Advanced    Advanced         1
            8       no  3.60  Beginner    Advanced         1
            25      no  3.96  Advanced    Advanced         1
            2      yes  3.76  Beginner    Beginner         0
            ...

            # Convert teradataml DataFrame to pandas DataFrame using fastexport
            # by opening 2 Teradata data transfer sessiosns.
            >>> pandas_df = df.to_pandas(fastexport = True, open_sessions = 2)
            Errors: []
            Warnings: []
            >>> pandas_df
               masters   gpa     stats programming  admitted
            id
            38     yes  2.65  Advanced    Beginner         1
            26     yes  3.57  Advanced    Advanced         1
            5       no  3.44    Novice      Novice         0
            24      no  1.87  Advanced      Novice         1
            3       no  3.70    Novice    Beginner         1
            1      yes  3.95  Beginner    Beginner         0
            20     yes  3.90  Advanced    Advanced         1
            18     yes  3.81  Advanced    Advanced         1
            8       no  3.60  Beginner    Advanced         1
            25      no  3.96  Advanced    Advanced         1
            2      yes  3.76  Beginner    Beginner         0
            ...

        """
        try:
            awu_matrix = []
            awu_matrix.append(["fastexport", fastexport, True, (bool)])
            # Validate 'fastexport' argument as it is unique to to_pandas()
            # function.
            # Remaining args are validated as part of _DataTransferUtils
            # class constructor.
            _Validators._validate_function_arguments(awu_matrix)

            # Get "open_sessions" argument.
            open_sessions = kwargs.get("open_sessions", None)

            dt_obj = _DataTransferUtils(df=self, index_column=index_column,
                                        num_rows=num_rows, all_rows=all_rows,
                                        catch_errors_warnings=
                                        catch_errors_warnings,
                                        open_sessions=open_sessions)
            # If fastexport is set to True, call fastexport function.
            if fastexport:
                result = dt_obj._fastexport_get_pandas_df(require=True, **kwargs)
                return self._post_process_pandas_result(result)
            # If fastexport is None & num_rows greater than or equal to 100,000
            # call fastexport function.
            elif fastexport is None and (not all_rows and num_rows >= 100000):
                result = dt_obj._fastexport_get_pandas_df(require=False, **kwargs)
                return self._post_process_pandas_result(result)
            # If fastexport is not specified/required fallback to regular
            # approach.
            else:
                # Validate df index column.
                dt_obj._validate_df_index_column()
                # Execute node if un-executed.
                self.__execute_node_and_set_table_name(self._nodeid,
                                                       self._metaexpr)
                # Call regular approach function.
                result = dt_obj._get_pandas_dataframe(**kwargs)
                return self._post_process_pandas_result(result)
        except TeradataMlException:
            raise
        except TypeError:
            raise
        except ValueError:
            raise
        except Exception as err:
            raise TeradataMlException(
                Messages.get_message(MessageCodes.DATA_EXPORT_FAILED, "to_pandas",
                                     "Pandas DataFrame", str(err)),
                MessageCodes.DATA_EXPORT_FAILED)

    @collect_queryband(queryband="DF_join")
    def join(self, other, on=None, how="left", lsuffix=None, rsuffix=None,
             lprefix=None, rprefix=None):
        """
        DESCRIPTION:
            Joins two different teradataml DataFrames together based on column comparisons
            specified in argument 'on' and type of join is specified in the argument 'how'.
            Supported join operations are:
             Inner join: Returns only matching rows, non matching rows are eliminated.
             Left outer join: Returns all matching rows plus non matching rows from the left table.
             Right outer join: Returns all matching rows plus non matching rows from the right table.
             Full outer join: Returns all rows from both tables, including non matching rows.
             Cross join: Returns all rows from both tables where each row from the first table
                          is joined with each row from the second table. The result of the join
                          is a cartesian cross product.
                          Note: For a cross join, the 'on' argument is ignored.
            Supported join operators are =, ==, <, <=, >, >=, <> and != (= and <> operators are
            not supported when using DataFrame columns as operands).

            Notes:
                1.  When multiple join conditions are given as a list string/ColumnExpression,
                    they are joined using AND operator.
                2.  Two or more on conditions can be combined using & and | operators
                    and can be passed as single ColumnExpression.
                    You can use (df1.a == df1.b) & (df1.c == df1.d) in place of
                    [df1.a == df1.b, df1.c == df1.d].
                3.  Two or more on conditions can not be combined using pythonic 'and'
                    and 'or'.
                    You can use (df1.a == df1.b) & (df1.c == df1.d) in place of
                    [df1.a == df1.b and df1.c == df1.d].
                4.  Performing self join using same DataFrame object in 'other'
                    argument is not supported. In order to perform self join,
                    first create aliased DataFrame using alias() API and pass it
                    for 'other' argument. Refer to Example 10 in EXAMPLES section.
                5.  This method does not support DataFrame that contain array columns.

        PARAMETERS:

            other:
                Required Argument.
                Specifies the right teradataml DataFrame on which join is to be performed.
                Types: teradataml DataFrame

            on:
                Optional argument when "how" is "cross", otherwise required.
                If specified when "how" is "cross", it is ignored.
                Specifies list of conditions that indicate the columns to be join keys.

                It can take the following forms:
                 String comparisons, in the form of "col1 <= col2", where col1 is
                  the column of left dataframe df1 and col2 is the column of right
                  dataframe df2.
                  Examples:
                    1. ["a","b"] indicates df1.a = df2.a and df1.b = df2.b.
                    2. ["a = b", "c == d"] indicates df1.a = df2.b and df1.c = df2.d.
                    3. ["a <= b", "c > d"] indicates df1.a <= df2.b and df1.c > df2.d.
                    4. ["a < b", "c >= d"] indicates df1.a < df2.b and df1.c >= df2.d.
                    5. ["a <> b"] indicates df1.a != df2.b. Same is the case for ["a != b"].
                 Column comparisons, in the form of df1.col1 <= df2.col2, where col1
                  is the column of left dataframe df1 and col2 is the column of right
                  dataframe df2.
                  Examples:
                    1. [df1.a == df2.a, df1.b == df2.b] indicates df1.a = df2.a AND df1.b = df2.b.
                    2. [df1.a == df2.b, df1.c == df2.d] indicates df1.a = df2.b AND df1.c = df2.d.
                    3. [df1.a <= df2.b & df1.c > df2.d] indicates df1.a <= df2.b AND df1.c > df2.d.
                    4. [df1.a < df2.b | df1.c >= df2.d] indicates df1.a < df2.b OR df1.c >= df2.d.
                    5. df1.a != df2.b indicates df1.a != df2.b.
                 The combination of both string comparisons and comparisons as column expressions.
                  Examples:
                    1. ["a", df1.b == df2.b] indicates df1.a = df2.a AND df1.b = df2.b.
                    2. [df1.a <= df2.b, "c > d"] indicates df1.a <= df2.b AND df1.c > df2.d.
                 ColumnExpressions containing FunctionExpressions which represent SQL functions
                  invoked on DataFrame Columns.
                  Examples:
                    1. (df1.a.round(1) - df2.a.round(1)).mod(2.5) > 2
                    2. df1.a.floor() - df2.b.floor() > 2

                Types: str (or) ColumnExpression (or) List of strings(str) or ColumnExpressions

            how:
                Optional Argument.
                Specifies the type of join to perform.
                Default value is "left".
                Permitted Values : "inner", "left", "right", "full" and "cross"
                Types: str

            lsuffix:
                Optional Argument.
                Specifies the suffix to be added to the left table columns.
                Default Value: None.
                Types: str

            rsuffix:
                Optional Argument.
                Specifies the suffix to be added to the right table columns.
                Default Value: None.
                Types: str

            lprefix:
                Optional Argument.
                Specifies the prefix to be added to the left table columns.
                Default Value: None.
                Types: str

            rprefix:
                Optional Argument.
                Specifies the prefix to be added to the right table columns.
                Default Value: None.
                Types: str

        RETURNS:
            teradataml DataFrame

        RAISES:
            TeradataMlException

        EXAMPLES:
            # Load the data to run the example.
            >>> from teradataml import load_example_data
            >>> load_example_data("dataframe", ["join_table1", "join_table2"])
            >>> load_example_data("glm", "admissions_train") # used in cross join

            >>> df1 = DataFrame("join_table1")
            >>> df2 = DataFrame("join_table2")

            # Print dataframe.
            >>> df1
                       col2  col3 col5
            col1
            2     analytics   2.3    b
            1      teradata   1.3    a
            3      platform   3.3    c

            # Print dataframe.
            >>> df2
                       col4  col3 col7
            col1
            2     analytics   2.3    b
            1      teradata   1.3    a
            3       are you   4.3    d

            # Example 1: Both "on" argument conditions as strings.
            >>> df1.join(other = df2, on = ["col2=col4", "col1"], how = "inner", lprefix = "t1", rprefix = "t2")
              t1_col1 t2_col1       col2  t1_col3  t2_col3 col5       col4 col7
            0       2       2  analytics      2.3      2.3    b  analytics    b
            1       1       1   teradata      1.3      1.3    a   teradata    a

            # Example 2: One "on" argument condition is ColumnExpression and other is string having two
            #            columns with left outer join.
            >>> df1.join(df2, on = [df1.col2 == df2.col4,"col5 = col7"], how = "left", lprefix = "t1", rprefix = "t2")
              t1_col1 t2_col1       col2  t1_col3  t2_col3 col5       col4  col7
            0       3    None   platform      3.3      None    c       None  None
            1       2       2  analytics      2.3      2.3    b  analytics     b
            2       1       1   teradata      1.3      1.3    a   teradata     a

            # Example 3: One "on" argument condition is ColumnExpression and other is string having only one column.
            >>> df1.join(other = df2, on = [df1.col2 == df2.col4,"col3"], how = "inner", lprefix = "t1", rprefix = "t2")
              t1_col1 t2_col1       col2  t1_col3  t2_col3 col5       col4 col7
            0       2       2  analytics      2.3      2.3    b  analytics    b
            1       1       1   teradata      1.3      1.3    a   teradata    a

            # Example 4: One "on" argument condition is ColumnExpression and other is string having two
            #            columns with full join.
            >>> df1.join(other = df2, on = ["col2=col4",df1.col5 == df2.col7], how = "full", lprefix = "t1", rprefix = "t2")
              t1_col1 t2_col1       col2  t1_col3  t2_col3  col5       col4  col7
            0       3    None   platform      3.3      None     c       None  None
            1    None       3       None      None      4.3  None    are you     d
            2       1       1   teradata      1.3      1.3     a   teradata     a
            3       2       2  analytics      2.3      2.3     b  analytics     b

            # Example 5: Using not equal operation in ColumnExpression condition.
            >>> df1.join(other = df2, on = ["col5==col7",df1.col2 != df2.col4], how = "full", lprefix = "t1", rprefix = "t2")
              t1_col1 t2_col1       col2  t1_col3  t2_col3   col5       col4  col7
            0       1    None   teradata      1.3      None     a       None  None
            1       2    None  analytics      2.3      None     b       None  None
            2    None       2       None      None      2.3  None  analytics     b
            3    None       1       None      None      1.3  None   teradata     a
            4       3    None   platform      3.3      None     c       None  None
            5    None       3       None      None      4.3  None    are you     d

            # Example 6: Using only one string expression with <> operation.
            >>> df1.join(other = df2, on = "col2<>col4", how = "left", lprefix = "t1", rprefix = "t2")
              t1_col1 t2_col1       col2  t1_col3  t2_col3 col5       col4 col7
            0       2       3  analytics      2.3      4.3    b    are you    d
            1       2       1  analytics      2.3      1.3    b   teradata    a
            2       3       2   platform      3.3      2.3    c  analytics    b
            3       1       2   teradata      1.3      2.3    a  analytics    b
            4       3       1   platform      3.3      1.3    c   teradata    a
            5       1       3   teradata      1.3      4.3    a    are you    d
            6       3       3   platform      3.3      4.3    c    are you    d

            # Example 7: Using only one ColumnExpression in "on" argument conditions.
            >>> df1.join(other = df2, on = df1.col5 != df2.col7, how = "full", lprefix = "t1", rprefix = "t2")
              t1_col1 t2_col1       col2  t1_col3  t2_col3 col5       col4 col7
            0       1       3   teradata      1.3      4.3    a    are you    d
            1       3       1   platform      3.3      1.3    c   teradata    a
            2       1       2   teradata      1.3      2.3    a  analytics    b
            3       3       2   platform      3.3      2.3    c  analytics    b
            4       2       1  analytics      2.3      1.3    b   teradata    a
            5       3       3   platform      3.3      4.3    c    are you    d
            6       2       3  analytics      2.3      4.3    b    are you    d

            # Example 8: Both "on" argument conditions as ColumnExpressions.
            >>> df1.join(df2, on = [df1.col2 == df2.col4, df1.col5 > df2.col7], how = "right", lprefix = "t1", rprefix ="t2")
              t1_col1 t2_col1  col2 t1_col3  t2_col3  col5       col4 col7
            0    None       2  None    None      2.3  None  analytics    b
            1    None       1  None    None      1.3  None   teradata    a
            2    None       3  None    None      4.3  None    are you    d

            # Example 9: cross join "admissions_train" with "admissions_train".
            >>> df1 = DataFrame("admissions_train").head(3).sort("id")
            >>> print(df1)
               masters   gpa     stats programming admitted
            id
            1      yes  3.95  Beginner    Beginner        0
            2      yes  3.76  Beginner    Beginner        0
            3       no  3.70    Novice    Beginner        1

            >>> df2 = DataFrame("admissions_train").head(3).sort("id")
            >>> print(df2)
               masters   gpa     stats programming admitted
            id
            1      yes  3.95  Beginner    Beginner        0
            2      yes  3.76  Beginner    Beginner        0
            3       no  3.70    Novice    Beginner        1

            >>> df3 = df1.join(other=df2, how="cross", lprefix="l", rprefix="r")
            >>> df3.set_index("l_id").sort("l_id")
                 l_programming   r_stats r_id  r_gpa r_programming  l_gpa   l_stats r_admitted l_admitted l_masters r_masters
            l_id
            1         Beginner    Novice    3   3.70      Beginner   3.95  Beginner          1          0       yes        no
            1         Beginner  Beginner    1   3.95      Beginner   3.95  Beginner          0          0       yes       yes
            1         Beginner  Beginner    2   3.76      Beginner   3.95  Beginner          0          0       yes       yes
            2         Beginner  Beginner    1   3.95      Beginner   3.76  Beginner          0          0       yes       yes
            2         Beginner  Beginner    2   3.76      Beginner   3.76  Beginner          0          0       yes       yes
            2         Beginner    Novice    3   3.70      Beginner   3.76  Beginner          1          0       yes        no
            3         Beginner  Beginner    1   3.95      Beginner   3.70    Novice          0          1        no       yes
            3         Beginner  Beginner    2   3.76      Beginner   3.70    Novice          0          1        no       yes
            3         Beginner    Novice    3   3.70      Beginner   3.70    Novice          1          1        no        no

            # Example 10: Perform self join using aliased DataFrame.
            # Create an aliased DataFrame.
            >>> lhs  = DataFrame("admissions_train").head(3).sort("id")
            >>> rhs = lhs.alias("rhs")
            # Use aliased DataFrame for self join.
            >>> joined_df = lhs.join(other=rhs, how="cross", lprefix="l", rprefix="r")
            >>> joined_df
               l_id  r_id l_masters r_masters  l_gpa  r_gpa   l_stats   r_stats l_programming r_programming  l_admitted  r_admitted
            0     1     3       yes        no   3.95   3.70  Beginner    Novice      Beginner      Beginner           0           1
            1     2     2       yes       yes   3.76   3.76  Beginner  Beginner      Beginner      Beginner           0           0
            2     2     3       yes        no   3.76   3.70  Beginner    Novice      Beginner      Beginner           0           1
            3     3     1        no       yes   3.70   3.95    Novice  Beginner      Beginner      Beginner           1           0
            4     3     3        no        no   3.70   3.70    Novice    Novice      Beginner      Beginner           1           1
            5     3     2        no       yes   3.70   3.76    Novice  Beginner      Beginner      Beginner           1           0
            6     2     1       yes       yes   3.76   3.95  Beginner  Beginner      Beginner      Beginner           0           0
            7     1     2       yes       yes   3.95   3.76  Beginner  Beginner      Beginner      Beginner           0           0
            8     1     1       yes       yes   3.95   3.95  Beginner  Beginner      Beginner      Beginner           0           0

            # Example 11: Perform join with compound 'on' condition having
            #             more than one binary operator.
            >>> rhs_2 = lhs.assign(double_gpa=lhs.gpa * 2)
            >>> joined_df_2 = lhs.join(rhs_2, on=rhs_2.double_gpa == lhs.gpa * 2, how="left", lprefix="l", rprefix="r")
            >>> joined_df_2
               l_id  r_id l_masters r_masters  l_gpa  r_gpa   l_stats   r_stats l_programming r_programming  l_admitted  r_admitted  double_gpa
            0     3     3        no        no   3.70   3.70    Novice    Novice      Beginner      Beginner           1           1        7.40
            1     2     2       yes       yes   3.76   3.76  Beginner  Beginner      Beginner      Beginner           0           0        7.52
            2     1     1       yes       yes   3.95   3.95  Beginner  Beginner      Beginner      Beginner           0           0        7.90

            # Example 12: Perform join on DataFrames with 'on' condition
            #             having FunctionExpression.
            >>> df = DataFrame("admissions_train")
            >>> df2 = df.alias("rhs_df")
            >>> joined_df_3 = df.join(df2, on=(df.gpa.round(1) - df2.gpa.round(1)).mod(2.5) > 2,
            >>>                       how="inner", lprefix="l")
            >>> joined_df_3.sort(["id", "l_id"])
                l_id	id	l_masters	masters	 l_gpa	 gpa	 l_stats	   stats  l_programming	programming	l_admitted	admitted
            0      1	24	      yes	     no	  3.95	1.87	Beginner	Advanced	   Beginner	     Novice	         0	       1
            1     13	24	       no	     no	   4.0	1.87	Advanced	Advanced	     Novice	     Novice	         1	       1
            2     15	24	      yes	     no	   4.0	1.87	Advanced	Advanced	   Advanced	     Novice	         1	       1
            3     25	24	       no	     no	  3.96	1.87	Advanced	Advanced	   Advanced	     Novice	         1	       1
            4     27	24	      yes	     no	  3.96	1.87	Advanced	Advanced	   Advanced	     Novice	         0	       1
            5     29	24	      yes	     no	   4.0	1.87	  Novice	Advanced	   Beginner	     Novice          0	       1
            6     40	24	     yes	     no	  3.95	1.87      Novice	Advanced	   Beginner	     Novice	         0	       1

        """

        # Argument validations
        awu_matrix = []
        awu_matrix.append(["other", other, False, (DataFrame)])
        awu_matrix.append(["on", on, True, (str, ColumnExpression, list)])
        awu_matrix.append(["how", how, True, (str), False, TeradataConstants.TERADATA_JOINS.value])
        awu_matrix.append(["lsuffix", lsuffix, True, (str), True])
        awu_matrix.append(["rsuffix", rsuffix, True, (str), True])
        awu_matrix.append(["lprefix", lprefix, True, (str), True])
        awu_matrix.append(["rprefix", rprefix, True, (str), True])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        # If self and other DataFrames are pointing to same Table object,
        # raise error.
        if self._metaexpr.t is other._metaexpr.t:
            raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_ALIAS_REQUIRED, "join"),
                                      MessageCodes.TDMLDF_ALIAS_REQUIRED)

        how_lc = how.lower()

        if on is None and how_lc != "cross":
            raise TeradataMlException(
                Messages.get_message(MessageCodes.MISSING_ARGS, "on"),
                MessageCodes.MISSING_ARGS)

        # Get the lowercase column names for the DataFrames involved in the operation.
        self_columns_lower_actual_map = {}
        other_columns_lower_actual_map = {}
        for col in self.columns:
            self_columns_lower_actual_map[col.lower()] = col

        for col in other.columns:
            other_columns_lower_actual_map[col.lower()] = col

        # Set the affix variables (laffix and raffix) with provided value(s)
        # of lsuffix, rsuffix, lprefix and rprefix.
        # Also set affix_type appropriately.
        laffix = None
        raffix = None
        affix_type = None
        if lsuffix is not None or rsuffix is not None:
            laffix = lsuffix
            raffix = rsuffix
            affix_type = "suffix"
        elif lprefix is not None or rprefix is not None:
            laffix = lprefix
            raffix = rprefix
            affix_type = "prefix"

        # Same column names can be present in two dataframes involved
        # in join operation in below two cases:
        # Case 1: Self join.
        # Case 2: Two tables having common column names.
        # In any case, at least one kind of affix is required to generate
        # distinct column names in resultant table. Throw error if no affix
        # is available.
        if not set(self_columns_lower_actual_map.keys()).isdisjoint(other_columns_lower_actual_map.keys()):
            if affix_type is None:
                raise TeradataMlException(
                    Messages.get_message(MessageCodes.TDMLDF_REQUIRED_TABLE_ALIAS),
                    MessageCodes.TDMLDF_REQUIRED_TABLE_ALIAS)

        # Both affixes should not be equal to perform join.
        if laffix == raffix and laffix is not None:
            raise TeradataMlException(
                Messages.get_message(MessageCodes.TDMLDF_INVALID_TABLE_ALIAS,
                                     "'l{affix_type}' and 'r{affix_type}'".format(affix_type=affix_type)),
                MessageCodes.TDMLDF_INVALID_TABLE_ALIAS)

        try:
            # Set an attribute named '_join_alias' to underlying SQLAlchemy table objects
            # and use it as default alias for compiling.
            setattr(self._metaexpr.t, "_join_alias", "lhs")
            setattr(other._metaexpr.t, "_join_alias", "rhs")
            lhs_alias = "lhs"
            rhs_alias = "rhs"

            # Step 1: Generate the on clause string.
            if how_lc != "cross":
                on = UtilFuncs._as_list(on)

                all_join_conditions = []
                invalid_join_conditions = []
                # Forming join condition
                for condition in on:
                    # Process only when the on condition is either a string or a ColumnExpression.
                    if not isinstance(condition, (ColumnExpression, str)):
                        invalid_join_conditions.append(condition)
                        continue

                    # Generate final on clause string from string representation of condition.
                    if isinstance(condition, str):
                        # Process the string manually.
                        # 1. Parse the string to get operator.
                        for op in TeradataConstants.TERADATA_JOIN_OPERATORS.value:
                            if op in condition:
                                conditional_separator = op
                                break
                        else:
                            # If no join condition is mentioned, then string represents the column.
                            # In this case, default operator is taken as equal.
                            # If on is ['a'], then it is equal to 'lhs.a = rhs.a'
                            columns = [condition, condition]
                            condition = "{0} = {0}".format(condition)
                            conditional_separator = "="
                        # 2. Split the string using operator and extract LHS and RHS
                        # columns from a binary expression.
                        columns = [column.strip() for column in condition.split(sep=conditional_separator)
                                   if len(column) > 0]

                        if len(columns) != 2:
                            invalid_join_conditions.append(condition)
                            # TODO: Raise exception here only.
                        else:
                            # 3. Generate fully qualified names using affix and table alias
                            # and create final on clause condition string.
                            left_col = self.__add_alias_to_column(columns[0], self, lhs_alias)
                            right_col = self.__add_alias_to_column(columns[1], other, rhs_alias)
                            if conditional_separator == "!=":
                                # "!=" is python way of expressing 'not equal to'. "<>" is Teradata way of
                                # expressing 'not equal to'. Adding support for "!=".
                                conditional_separator = "<>"
                            all_join_conditions.append(
                                '{0} {1} {2}'.format(left_col, conditional_separator, right_col))

                    # Generate on clause string from column expression.
                    if isinstance(condition, ColumnExpression):
                        compiled_condition = condition.compile(compile_kwargs={'include_table': True,
                                                                               'literal_binds': True,
                                                                               'table_name_kind': '_join_alias',
                                                                               'compile_with_caller_table': True,
                                                                               'table_only': True})

                        all_join_conditions.append(compiled_condition)

                # Raise error if invalid on conditions are passed.
                if len(invalid_join_conditions) > 0:
                    raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_INVALID_JOIN_CONDITION,
                                                                   ", ".join(invalid_join_conditions)),
                                              MessageCodes.TDMLDF_INVALID_JOIN_CONDITION)

                # Generate final on condition.
                join_condition = " and ".join(all_join_conditions)
            else:
                # In case of cross join no need of condition.
                join_condition = ""

            # Step 2: Generate the select clause string.
            # Generate new column names for overlapping column names using lsuffix, rsuffix, lprefix, rprefix.
            # Also, use table alias while addressing overlapping column names.
            lhs_columns_types = df_utils._get_required_columns_types_from_metaexpr(self._metaexpr)
            rhs_columns_types = df_utils._get_required_columns_types_from_metaexpr(other._metaexpr)

            select_columns = []
            new_metaexpr_columns_types = OrderedDict()

            # Processing columns in LHS DF/ self DF.
            for column in self.columns:
                if df_utils._check_column_exists(column.lower(), other_columns_lower_actual_map.keys()):
                    # Check if column found in other DataFrame has same case or different.
                    # Return the column name from the other DataFrame.
                    other_column = other_columns_lower_actual_map[column.lower()]

                    # Check if column name in LHS dataframe is same as that of in RHS dataframe.
                    # If so, generate new name for LHS DF column using provided affix.
                    df1_column_with_affix = self.__check_and_return_new_column_name(laffix, other_column,
                                                                                    other_columns_lower_actual_map.keys(),
                                                                                    "right", affix_type)

                    # Generate select clause string for current column and append to list.
                    select_columns.append("{0} as {1}".format(
                        self.__get_fully_qualified_col_name(other_column, lhs_alias),
                        df1_column_with_affix))

                    # Check if column name in RHS dataframe is same as that of in LHS dataframe.
                    # If so, generate new name for RHS DF column using provided affix.
                    df2_column_with_affix = self.__check_and_return_new_column_name(raffix, column,
                                                                                    self_columns_lower_actual_map.keys(),
                                                                                    "left", affix_type)
                    # Generate select clause string for current column and append to list.
                    select_columns.append("{0} as {1}".format(
                        self.__get_fully_qualified_col_name(column, rhs_alias),
                        df2_column_with_affix))

                    # As we are creating new column name, adding it to new metadata dict for new dataframe from join.
                    self.__add_column_type_item_to_dict(new_metaexpr_columns_types,
                                                        UtilFuncs._teradata_unquote_arg(df1_column_with_affix, "\""),
                                                        column, lhs_columns_types)

                    self.__add_column_type_item_to_dict(new_metaexpr_columns_types,
                                                        UtilFuncs._teradata_unquote_arg(df2_column_with_affix, "\""),
                                                        other_column, rhs_columns_types)

                else:
                    # As column with same name is not present in RHS DataFrame now,
                    # directly adding column to new metadata dict.
                    self.__add_column_type_item_to_dict(new_metaexpr_columns_types, column, column, lhs_columns_types)
                    select_columns.append(UtilFuncs._teradata_quote_arg(column, "\"", False))

            # Processing columns in RHS DF/ other DF.
            # Here we will only be processing columns which are not overlapping.
            for column in other.columns:
                if not df_utils._check_column_exists(column.lower(), self_columns_lower_actual_map.keys()):
                    # As column not present in left DataFrame, directly adding column to new metadata dict.
                    self.__add_column_type_item_to_dict(new_metaexpr_columns_types, column, column, rhs_columns_types)
                    select_columns.append(UtilFuncs._teradata_quote_arg(column, "\"", False))

            # Step 3: Create a node in AED using _aed_join using appropriate alias for involved tables.
            join_node_id = self._aed_utils._aed_join(self._nodeid, other._nodeid, ", ".join(select_columns),
                                                     how_lc, join_condition, lhs_alias, rhs_alias)

            # Step 4: Constructing new Metadata (_metaexpr) without DB; using dummy select_nodeid
            # and underlying table name.
            new_metaexpr = UtilFuncs._get_metaexpr_using_columns(join_node_id, new_metaexpr_columns_types.items(),
                                                                 datalake=self._metaexpr.datalake)

            # Return a new joined dataframe.
            return self._create_dataframe_from_node(join_node_id, new_metaexpr, self._index_label)
        finally:
            # Delete the '_join_alias' attribute attached to underlying
            # SQLALchemy table objects.
            delattr(self._metaexpr.t, "_join_alias")
            delattr(other._metaexpr.t, "_join_alias")

    def __add_alias_to_column(self, column, df, alias):
        """
        This function check column exists in list of columns, if exists add alias of table
        to the column name.

        PARAMETERS:
            column  - Column name.
            df - DataFrame to look into for columns.
            alias - table alias to be added to column.

        EXAMPLES:
            df1 = DataFrame("table1")
            df2 = DataFrame("table2")
            __add_alias_to_column("a", df1, "t1")

        RAISES:
            ValueError - If column not found in DataFrame
        """
        # Checking each element in passed columns to be valid column in dataframe
        column_name = column
        if isinstance(column, ColumnExpression):
            column_name = column.name
        # Checking each element in passed columns to be valid column in dataframe
        _Validators._validate_column_exists_in_dataframe(column_name, df._metaexpr, True)

        return self.__get_fully_qualified_col_name(column, alias)

    def __get_fully_qualified_col_name(self, column, alias):
        """
        Returns fully qualified column name.
        For example:
            Column "a" is in table with alias as "t1",
            value returned is string '"t1"."a"'.

        PARAMETERS:
            column  - Column name.
            alias - table alias to be associated with the column.

        EXAMPLES:
            __get_fully_qualified_col_name("a", "t1")

        RAISES:
            None
        """
        # Process only when the column is ColumneExpression
        if isinstance(column, ColumnExpression):
            column_name = column.name
            column = column.compile()
            if column_name != column:
                # Process it only when the ColumnExpression has a label.
                # E.g. if df.id.label(..) then column is "label(id as ..)" and column_name is "id".
                return column.replace("(" + column_name, "({0}.{1}".format(
                    UtilFuncs._teradata_quote_arg(alias, "\"", False),
                    UtilFuncs._teradata_quote_arg(column_name, "\"", False)))

        return "{0}.{1}".format(UtilFuncs._teradata_quote_arg(alias, "\"", False),
                                UtilFuncs._teradata_quote_arg(column, "\"", False))

    def __check_and_return_new_column_name(self, affix, column, col_list, other_df_side, affix_type):
        """
         Check new column name alias with column exists in col_list or not, if exists throws exception else
         returns new column name.

         PARAMETERS:
             affix  - affix to be added to column.
             column - column name.
             col_list - list of columns to check in which new column is exists or not.
             other_df_side - Side on which the other dataframe in current join operation resides.
             affix_type - Type of affix. Either "prefix" or "suffix".

         EXAMPLES:
             df = DataFrame("t1")
             __check_and_return_new_column_name("df1", "column_name", df.columns, "right", "prefix")

         RAISES:
             None
        """
        if affix is None:
            return UtilFuncs._teradata_quote_arg(column, "\"", False)

        # If Prefix, affix is added before column name else it is appended.
        column_with_affix = "{0}_{1}" if affix_type == "prefix" else "{1}_{0}"
        column_with_affix = column_with_affix.format(affix,
                                                     UtilFuncs._teradata_unquote_arg(column, "\""))
        if df_utils._check_column_exists(column_with_affix.lower(), col_list):
            if other_df_side == "right":
                affix_type = "l{}".format(affix_type)
            else:
                affix_type = "r{}".format(affix_type)
            raise TeradataMlException(
                Messages.get_message(MessageCodes.TDMLDF_COLUMN_ALREADY_EXISTS, column_with_affix, other_df_side,
                                     affix_type),
                MessageCodes.TDMLDF_COLUMN_ALREADY_EXISTS)
        return UtilFuncs._teradata_quote_arg(column_with_affix, "\"", False)

    def __add_column_type_item_to_dict(self, new_metadata_dict, new_column, column, column_types):
        """
        Add a column as key and datatype as a value to dictionary

        PARAMETERS:
            new_metadata_dict  - Dictionary to which new item to be added.
            new_column - key fo new item.
            column - column to which datatype to be get.
            column_types - datatypes of the columns.
        EXAMPLES:
            __add_to_column_types_dict( metadata_dict, "col1","integer")

        RAISES:
            None
        """
        try:
            new_metadata_dict[new_column] = column_types[column]
        except KeyError:
            try:
                new_metadata_dict[new_column] = column_types[UtilFuncs._teradata_quote_arg(column, "\"", False)]
            except KeyError:
                new_metadata_dict[new_column] = column_types[UtilFuncs._teradata_unquote_arg(column, "\"")]

    def __get_sorted_list(self, colnames_list, ascending, kind):
        """
        Private method to return sorted list with different algoritms in either ascending or decending order.
        
        PARAMETERS:
            colnames_list - List of values to be sorted
            ascending - Specifies a flag to sort columns in either ascending (True) or descending (False).
            kind - Type of sorting algorithm to be applied upon.            
        
        EXAMPLES:
            __get_sorted_list(colnames_list, False, 'mergesort')
            
        RAISES:
            None
            
        RETURNS:
            Sorted list of column names
        """
        if kind == 'quicksort':
            less = []
            equal = []
            greater = []
            if len(colnames_list) > 1:
                pivot = colnames_list[0]
                for col in colnames_list:
                    if col < pivot:
                        less.append(col)
                    elif col == pivot:
                        equal.append(col)
                    else:
                        greater.append(col)
                greater = self.__get_sorted_list(greater, ascending=ascending, kind=kind)
                less = self.__get_sorted_list(less, ascending=ascending, kind=kind)
                if ascending:
                    final = less + equal + greater
                else:
                    final = greater + equal + less
                return final
            else:
                return colnames_list

        elif kind == 'mergesort':
            if ascending == True:
                return sorted(colnames_list)
            else:
                return sorted(colnames_list, reverse=True)

        elif kind == 'heapsort':
            end = len(colnames_list)
            start = end // 2 - 1
            for i in range(start, -1, -1):
                self.__get_heap(colnames_list, end, i)
            for i in range(end - 1, 0, -1):
                # swap(i, 0)
                colnames_list[i], colnames_list[0] = colnames_list[0], colnames_list[i]
                colnames_list = self.__get_heap(colnames_list, i, 0)
            if ascending == True:
                return colnames_list
            else:
                return colnames_list[::-1]

    def __get_heap(self, colnames_list, n, i):
        """
        Private method to make a subtree rooted at index i.
        
        PARAMETERS:
            colnames_list - List of values for which heap is to be created.
            n - Size of the heap.
            i - Index to be taken as a root.
            
        EXAMPLES:
            __get_heap(colnames_list, 5, 3)
            
        RAISES:
            None
            
        RETURNS:
            Sorted list of column names indexed at i
        """
        l = 2 * i + 1
        r = 2 * (i + 1)
        max = i
        if l < n and colnames_list[i] < colnames_list[l]:
            max = l
        if r < n and colnames_list[max] < colnames_list[r]:
            max = r
        if max != i:
            colnames_list[i], colnames_list[max] = colnames_list[max], colnames_list[i]
            self.__get_heap(colnames_list, n, max)
        return colnames_list

    @collect_queryband(queryband="DF_toSql")
    def to_sql(self, table_name, if_exists='fail', primary_index=None, temporary=False, schema_name=None, types=None,
               primary_time_index_name=None, timecode_column=None, timebucket_duration=None,
               timezero_date=None, columns_list=None, sequence_column=None, seq_max=None, set_table=False):
        """
        DESCRIPTION:
            Writes records stored in a teradataml DataFrame to Teradata Vantage.

        PARAMETERS:

            table_name:
                Required Argument.
                Specifies the name of the table to be created in Teradata Vantage.
                Types: str

            schema_name:
                Optional Argument.
                Specifies the name of the SQL schema in Teradata Vantage to write to.
                Default Value: None (Use default Teradata Vantage schema).
                Types: str

                Note: schema_name will be ignored when temporary=True.

            if_exists:
                Optional Argument.
                Specifies the action to take when table already exists in Teradata Vantage.
                Default Value: 'fail'
                Permitted Values: 'fail', 'replace', 'append'
                    - fail: If table exists, do nothing.
                    - replace: If table exists, drop it, recreate it, and insert data.
                    - append: If table exists, insert data. Create table, if does not exist.
                Types: str

                Note: Replacing a table with the contents of a teradataml DataFrame based on
                      the same underlying table is not supported.

            primary_index:
                Optional Argument.
                Creates Teradata table(s) with primary index column(s) when specified.
                When None, No primary index Teradata tables are created.
                Default Value: None
                Types: str or List of Strings (str)
                    Example:
                        primary_index = 'my_primary_index'
                        primary_index = ['my_primary_index1', 'my_primary_index2', 'my_primary_index3']

            temporary:
                Optional Argument.
                Creates Teradata SQL tables as permanent or volatile.
                When True,
                    1. volatile tables are created, and
                    2. schema_name is ignored.
                When False, permanent tables are created.
                Default Value: False
                Types: boolean
                
            types:
                Optional Argument.
                Specifies required data types for requested columns to be saved in Vantage.
                Types: Python dictionary ({column_name1: type_value1, ... column_nameN: type_valueN})
                Default: None

                Note:
                    1. This argument accepts a dictionary of columns names and their required teradatasqlalchemy types
                       as key-value pairs, allowing to specify a subset of the columns of a specific type.
                       When only a subset of all columns are provided, the column types for the rest are retained.
                       When types argument is not provided, the column types are retained.
                    2. This argument does not have any effect when the table specified using table_name and schema_name
                       exists and if_exists = 'append'.

            primary_time_index_name:
                Optional Argument.
                Specifies a name for the Primary Time Index (PTI) when the table
                to be created must be a PTI table.
                Types: String

                Note: This argument is not required or used when the table to be created
                      is not a PTI table. It will be ignored if specified without the timecode_column.

            timecode_column:
                Optional Argument.
                Required when the DataFrame must be saved as a PTI table.
                Specifies the column in the DataFrame that reflects the form
                of the timestamp data in the time series.
                This column will be the TD_TIMECODE column in the table created.
                It should be of SQL type TIMESTAMP(n), TIMESTAMP(n) WITH TIMEZONE, or DATE,
                corresponding to Python types datetime.datetime or datetime.date.
                Types: String

                Note: When you specify this parameter, an attempt to create a PTI table
                      will be made. This argument is not required when the table to be created
                      is not a PTI table. If this argument is specified, primary_index will be ignored.

            timezero_date:
                Optional Argument.
                Used when the DataFrame must be saved as a PTI table.
                Specifies the earliest time series data that the PTI table will accept;
                a date that precedes the earliest date in the time series data.
                Value specified must be of the following format: DATE 'YYYY-MM-DD'
                Default Value: DATE '1970-01-01'.
                Types: String

                Note: This argument is not required or used when the table to be created
                      is not a PTI table. It will be ignored if specified without the timecode_column.

            timebucket_duration:
                Optional Argument.
                Required if columns_list is not specified or is None.
                Used when the DataFrame must be saved as a PTI table.
                Specifies a duration that serves to break up the time continuum in
                the time series data into discrete groups or buckets.
                Specified using the formal form time_unit(n), where n is a positive
                integer, and time_unit can be any of the following:
                CAL_YEARS, CAL_MONTHS, CAL_DAYS, WEEKS, DAYS, HOURS, MINUTES,
                SECONDS, MILLISECONDS, or MICROSECONDS.
                Types:  String

                Note: This argument is not required or used when the table to be created
                      is not a PTI table. It will be ignored if specified without the timecode_column.

            columns_list:
                Optional Argument.
                Required if timebucket_duration is not specified.
                Used when the DataFrame must be saved as a PTI table.
                Specifies a list of one or more PTI table column names.
                Types: String or list of Strings

                Note: This argument is not required or used when the table to be created
                      is not a PTI table. It will be ignored if specified without the timecode_column.

            sequence_column:
                Optional Argument.
                Used when the DataFrame must be saved as a PTI table.
                Specifies the column of type Integer containing the unique identifier for
                time series data readings when they are not unique in time.
                * When specified, implies SEQUENCED, meaning more than one reading from the same
                  sensor may have the same timestamp.
                  This column will be the TD_SEQNO column in the table created.
                * When not specified, implies NONSEQUENCED, meaning there is only one sensor reading
                  per timestamp.
                  This is the default.
                Types: str

                Note: This argument is not required or used when the table to be created
                      is not a PTI table. It will be ignored if specified without the timecode_column.

            seq_max:
                Optional Argument.
                Used when the DataFrame must be saved as a PTI table.
                Specifies the maximum number of sensor data rows that can have the
                same timestamp. Can be used when 'sequenced' is True.
                Accepted range:  1 - 2147483647.
                Default Value: 20000.
                Types: int

                Note: This argument is not required or used when the table to be created
                      is not a PTI table. It will be ignored if specified without the timecode_column.

            set_table:
                Optional Argument.
                Specifies a flag to determine whether to create a SET or a MULTISET table.
                When True, a SET table is created.
                When False, a MULTISET table is created.
                Default value: False
                Types: boolean

                Note: 1. Specifying set_table=True also requires specifying primary_index or timecode_column.
                      2. Creating SET table (set_table=True) may result in loss of duplicate rows.
                      3. This argument has no effect if the table already exists and if_exists='append'.

        RETURNS:
            None

        RAISES:
            TeradataMlException

        EXAMPLES:
            >>> load_example_data("dataframe","admissions_train")
            >>> df = DataFrame("admissions_train")
            >>> df2 = df[(df.gpa == 4.00)]
            >>> df2.to_sql('to_sql_example', primary_index='id')
            >>> df3 = DataFrame('to_sql_example')
            >>> df3
               masters  gpa     stats programming admitted
            id
            13      no  4.0  Advanced      Novice        1
            29     yes  4.0    Novice    Beginner        0
            15     yes  4.0  Advanced    Advanced        1
            >>>
            >>> # Save as PTI table making sure it is a SET table
            >>> load_example_data("sessionize", "sessionize_table")
            >>> df4 = DataFrame('sessionize_table')
            >>> df4.to_sql("test_copyto_pti",
                           timecode_column='clicktime',
                           columns_list='event',
                           set_table=True
                          )
            >>> df5 = DataFrame('test_copyto_pti')
            >>> df5
                                    TD_TIMECODE partition_id adid productid
            event
            click    2009-07-04 09:18:17.000000         1231    1      1001
            click    2009-07-24 04:18:10.000000         1167    2      1001
            click    2009-08-08 02:18:12.000000         9231    3      1001
            click    2009-08-11 00:01:24.000000         9231    3      1001
            page_02  2009-08-22 04:20:05.000000         1039    5      1001
            page_02  2009-08-27 23:03:05.000000         1039    5      1001
            view     2009-02-09 15:17:59.000000         1263    4      1001
            view     2009-03-09 21:17:59.000000         1199    2      1001
            view     2009-03-13 17:17:59.000000         1071    4      1001
            view     2009-03-19 01:17:59.000000         1199    1      1001
            
            >>> # Save DataFrame with Array column to Teradata Vantage.
            >>> from teradataml.dataframe.array import Array
            >>> arr = Array((10,20,30), atype=ARRAY_NUMBER('[4]', default_null=True))
            >>> df_with_array = df.assign(arr_col=arr)
            >>> df2 = df_with_array.select(['id', 'masters', 'arr_col'])
            >>> df2.to_sql(table_name = 'my_tdml_arr', if_exists='replace')

        """

        return copy_to_sql(df=self, table_name=table_name, schema_name=schema_name,
                           index=False, index_label=None, temporary=temporary,
                           primary_index=primary_index, if_exists=if_exists, types=types,
                           primary_time_index_name=primary_time_index_name, timecode_column=timecode_column,
                           timebucket_duration=timebucket_duration, timezero_date=timezero_date,
                           columns_list=columns_list,
                           sequence_column=sequence_column, seq_max=seq_max, set_table=set_table)

    def _get_assign_allowed_types(self):
        """
        DESCRIPTION:
            Get allowed types for DataFrame.assign() function.

        PARAMETERS:
            None.

        RETURNS:
            A tuple containing supported types for DataFrame.assign() operation.

        RAISES:
            None.

        EXAMPLES:
            allowed_types = self._get_assign_allowed_types()
        """
        return (type(None), int, float, str, decimal.Decimal, ColumnExpression, ClauseElement, Array)

    def _generate_assign_metaexpr_aed_nodeid(self, drop_columns, node_id, **kwargs):
        """
        DESCRIPTION:
            Function generates the MetaExpression and AED nodeid for DataFrame.assign()
            function based on the inputs to the function.

        PARAMETERS:
            drop_columns:
                Required Argument.
                If True, drop columns that are not specified in assign.
                Default Value: False
                Types: bool

            node_id:
                Optional Argument.
                Specifies the input nodeid for the assign operation.
                Types: str

            kwargs:
                keyword, value pairs
                - keywords are the column names.
                - values can be:
                    * Column arithmetic expressions.
                    * int/float/string literals.
                    * DataFrameColumn a.k.a. ColumnExpression Functions.
                      (Visit DataFrameColumn Functions in Function reference guide for more
                      details)
                    * SQLAlchemy ClauseElements.
                      (Visit teradataml extension with SQLAlchemy in teradataml User Guide
                       and Function reference guide for more details)

        RETURNS:
            A tuple containing new MetaExpression and AED nodeid for the operation.

        RAISES:
            None.

        EXAMPLES:
            (new_meta, new_nodeid) = self._generate_assign_metaexpr_aed_nodeid(drop_columns, **kwargs)
        """
        # Apply the assign expression.
        (new_meta, result) = self._metaexpr._assign(drop_columns, **kwargs)

        # Join the expressions in result.
        assign_expression = ', '.join(list(map(lambda x: x[1], result)))
        new_nodeid = self._aed_utils._aed_assign(node_id,
                                                 assign_expression,
                                                 AEDConstants.AED_ASSIGN_DROP_EXISITING_COLUMNS.value)

        # Regenerate the metaexpression with the new table name.
        new_meta = UtilFuncs._get_metaexpr_using_parent_metaexpr(new_nodeid, new_meta)
        return (new_meta, new_nodeid)

    def _create_dataframe_from_node(self, nodeid, metaexpr, index_label, undropped_columns=None):
        """
        DESCRIPTION:
            Function to create a teradataml DataFrame from node.
            Acts as wrapper over 'DataFrame._from_node()', so that it can
            be overridden by the child classes if required.

            For example,
                This will always return a teradataml DataFrame, but for
                GeoDataFrame, we will return teradataml DataFrame or teradataml
                GeoDataFrame, based on whether the resultant DataFrame contains
                geometry column or not.

        PARAMETERS:
            nodeid:
                Required Argument.
                Specifies the nodeid for the DataFrame or GeoDataFrame.
                Types: str

            metaexpr:
                Required Argument.
                Specifies the metadata for the resultant object.
                Types: _MetaExpression

            index_label:
                Required Argument.
                Specifies list specifying index column(s) for the DataFrame.
                Types: str OR list of Strings (str)

            undropped_columns:
                Optional Argument.
                Specifies list of index column(s) to be retained as columns for printing.
                Types: list

        RETURNS:
            teradataml DataFrame

        RAISES:
            None

        EXAMPLES:
            self._create_dataframe_from_node(new_nodeid, new_meta,
                                             self._index_label, undropped_columns)
        """
        return DataFrame._from_node(nodeid, metaexpr, index_label, undropped_columns)

    def __assign_conditional_node_execution(self, val):
        """
        DESCRIPTION:
            Internal function for DataFrame.assign() to execute the node, if
            following conditions are satisfied:
                1. If 'kwargs' accept value(s) of SQLAlchemy ClauseElement type AND
                2. ClauseElement represents a function whoes output type depends on
                   the input argument of the function.

        PARAMETER:
            val:
                Required Argument.
                Specifies a ColumnExpression.
                Types: SQLAlchemy ClauseElement

        RETURNS:
            True, if node is executed.

        RAISES:
            None.

        EXAMPLES:
            self.__assign_conditional_node_execution(val)
        """
        # If user has requested to execute a function using SQLAlchemy, we must
        # execute the node, if it is not already.
        # We should execute it only when a specific functions are being called.
        if isinstance(val, ClauseElement):
            import sqlalchemy as sqlalc
            function_name = None
            try:
                if isinstance(val, sqlalc.sql.elements.Over):
                    # If expression is of type Over, i.e., window aggregate then we should
                    # check it's element to get the function name.
                    function_name = val.element.name.upper()
                else:
                    # Get the function name.
                    function_name = val.name.upper()
            except:
                pass

            from teradataml.dataframe.vantage_function_types import VANTAGE_FUNCTION_ARGTYPE_DEPENDENT_MAPPER
            if function_name is None or function_name in VANTAGE_FUNCTION_ARGTYPE_DEPENDENT_MAPPER:
                self.__execute_node_and_set_table_name(self._nodeid)
                return True
            
    def _assign_udf(self, udf_expr):
        """
        DESCRIPTION:
            Internal function for DataFrame.assign() to execute the udf using 
            Script Table Operator and create new column for teradataml DataFrame.

        PARAMETER:
            udf_expr:
                Required Argument.
                Specifies a dictionary of column name to UDF expressions.
                Types: dict

        RETURNS:
            teradataml DataFrame

        RAISES:
            None.

        EXAMPLES:
            self._assign_udf(udf_expr)
        """

        df = self
        env_name = None
        # Create a dictionary of env_name to list of output columns to be run on that env.
        env_mapper = OrderedDict()
        # Convert array columns to VARCHAR columns before passing to UDF.
        # Store the original array columns and its type to convert them back after UDF execution.
        column_types = OrderedDict([(column.name, str(column.type)) for column in df._metaexpr.c])
        df, array_columns = DataFrameUtils._convert_df_col_array_to_varchar(df)

        exec_mode = 'REMOTE' if UtilFuncs._is_lake() else 'IN-DB'
        if exec_mode == 'REMOTE':
            _Validators._check_auth_token("udf")
            for colname, col in udf_expr.items():
                env_name = UtilFuncs._get_env_name(col)
                # Store the env_name and its corresponding output column
                if env_name in env_mapper:
                    env_mapper[env_name].append(colname)
                else:
                    env_mapper[env_name] = [colname]
        else:
            env_mapper[env_name] = udf_expr.keys()
        debug = False
        for env_name, cols in env_mapper.items():
            # Create a dictionary of output columns to column type.
            returns = OrderedDict([(column.name, column.type) for column in df._metaexpr.c])
            # Store the udf functions
            user_function = []
            # Create a dictionary of output column name to udf name
            columns_definitions = {}
            # Create a dictionary of output column name to udf arguments
            function_args = {}
            for colname, col in udf_expr.items():
                debug |= col._debug
                delimiter = col._delimiter
                quotechar = col._quotechar
                if colname in cols:
                    user_function.append(col._udf)
                    function_args[colname] = col._udf_args if col._udf_args else ()
                    # If column is of type array, we need to change the return type to VARCHAR
                    # of appropriate length to accommodate all array elements.
                    if isinstance(col.type, TeradataTypes.TD_ARRAY_TYPES.value):
                        varchar_length = DataFrameUtils._get_varchar_length_for_array_type(col.type)
                        arr_type = VARCHAR(varchar_length)
                        array_columns[colname] = col.type
                    returns[colname] = col.type if not isinstance(col.type, TeradataTypes.TD_ARRAY_TYPES.value) else arr_type
                    column_types[colname] = str(col.type)
                    columns_definitions[colname] = col._udf.__name__

            tbl_operators = _TableOperatorUtils([],
                                                df,
                                                "udf",
                                                user_function,
                                                exec_mode,
                                                chunk_size=None,
                                                returns=returns,
                                                delimiter=delimiter,
                                                quotechar=quotechar,
                                                num_rows=1,
                                                auth=None,
                                                data_partition_column=None,
                                                data_hash_column=None,
                                                data_order_column=None,
                                                is_local_order=None,
                                                nulls_first=None,
                                                sort_ascending=None,
                                                charset=None,
                                                env_name = env_name,
                                                style = "csv",
                                                function_args=function_args,
                                                columns_definitions=columns_definitions,
                                                output_type_converters={
                                                    col_name: _Dtypes._teradata_type_to_python_type(col_type)
                                                    for col_name, col_type in returns.items()},
                                                debug=debug,
                                                column_types=column_types
                                                )

            df = tbl_operators.execute()
            
        # Convert VARCHAR columns back to their original array types.
        if array_columns:
            df = DataFrameUtils._convert_df_col_varchar_to_array(df, array_columns)
        return df

    def _assign_call_udf(self, call_udf_expr):
        """
        DESCRIPTION:
            Internal function for DataFrame.assign() to execute the call_udf using
            Script/Apply Table Operator and create new column for teradataml DataFrame.

        PARAMETER:
            call_udf_expr:
                Required Argument.
                Specifies a dictionary of column name to call_udf expressions.
                Types: dict

        RETURNS:
            teradataml DataFrame

        RAISES:
            None.

        EXAMPLES:
            # call_udf_expr is a dictionary of column names to call_udf expressions.
            call_udf_expr = {'upper_col': <teradataml.dataframe.sql._SQLColumnExpression object at 0x0000028E59C44310>,
                            'sum_col': <teradataml.dataframe.sql._SQLColumnExpression object at 0x0000028E59C41690>}
            self._assign_register(call_udf_expr)
        """
        df = self
        # Create a dictionary of output columns to column type (teradata type).
        returns = OrderedDict([(column.name, column.type) for column in df._metaexpr.c])
        # Create a dictionary of output columns to column type (python types).
        output_type_converters = {col_name: _Dtypes._teradata_type_to_python_type(col_type) \
                                  for col_name, col_type in returns.items()}

        for colname, col in call_udf_expr.items():
            returns[colname] = col.type
            output_type_converters[colname] = _Dtypes._teradata_type_to_python_type(col.type)
            script_name  = col._udf_script
            delimiter = col._delimiter
            quotechar = col._quotechar

            # Create a dictionary of arguments to be passed to the script.
            script_data = {}
            script_data['input_cols'] = df.columns
            script_data['output_cols'] = list(returns.keys())
            script_data['output_type_converters'] = output_type_converters
            script_data['function_args'] = {colname: col._udf_args}
            script_data['delimiter'] = delimiter
            script_data['qoutechar'] = quotechar

            # Convert the dictionary to a string.
            # The string is URL encoded to pass it as a parameter to the script.
            script_data = urllib.parse.quote_plus(json.dumps(script_data))

            if UtilFuncs._is_lake():
                from teradataml.table_operators.Apply import Apply
                apply_op_obj = Apply(data=df,
                                    script_name=script_name,
                                    env_name=col._env_name,
                                    returns = returns,
                                    delimiter = delimiter,
                                    quotechar=quotechar,
                                    files_local_path=GarbageCollector._get_temp_dir_name(),
                                    apply_command="python3 {} {}".format(script_name, script_data)
                                    )
                try:
                    df = apply_op_obj.execute_script(
                        output_style=OutputStyle.OUTPUT_TABLE.value)
                except Exception:
                    raise
            else:
                import teradataml.context.context as context
                database = context._get_current_databasename()

                check_reserved_keyword = False if sorted(list(returns.keys())) == sorted(df.columns) else True

                from teradataml.table_operators.Script import Script
                table_op_obj = Script(data=df,
                                    script_name=script_name,
                                    files_local_path=GarbageCollector._get_temp_dir_name(),
                                    script_command="{}/bin/python3  ./{}/{} {}".format(
                                        configure.indb_install_location, database, script_name, script_data),
                                    returns=returns,
                                    quotechar=quotechar,
                                    delimiter = delimiter
                                    )
                table_op_obj.check_reserved_keyword = check_reserved_keyword
                try:
                    df = table_op_obj.execute_script(
                        output_style=OutputStyle.OUTPUT_TABLE.value)
                except Exception:
                    raise
        return df
    
    def _assign_array(self, val):
        """
        DESCRIPTION:
            Internal function for DataFrame.assign() to execute the array objects
            using UDT and create new column for teradataml DataFrame.

        PARAMETERS:
            val:
                Required Argument.
                Specifies an Array object.
                Types: Array

        RETURNS:
            SQLColumnExpression

        RAISES:
            None.

        """
        from teradataml.dataframe.sql import _SQLColumnExpression
        udt_name = _get_or_create_internal_array_udt(val)
        # Build the SQL for the array constructor
        # Elements can be ColumnExpression or literals.
        elements_sql = []
        for e in val.elements:

            if isinstance(e, _SQLColumnExpression):
                # When array functions are called with literal columns,
                # we need to ensure that the generated SQL inlines those literal values directly in the statement.
                # Using SQLAlchemy's compile with 'literal_binds=True' ensures that all literal values are rendered
                # as part of the SQL string, rather than as parameter placeholders. This is required for Teradata SQL
                # to correctly interpret array constructors with a mix of columns and literal arguments.

                # EXAMPLE:
                # >>> df2 = df.assign(arr_col = Array((1, 2)))

                # Without literal_binds=True SQLAlchemy may treat the Python literals inside the
                # internal _SQLColumnExpression objects as bind parameters and could render:
                #   NEW tdml_array_integer_100_dnn(:param_1, :param_1)
           
                # With literal_binds=True each literal is inlined into the SQL text:
                #   NEW tdml_array_integer_100_dnn(1, 2)
                from teradatasqlalchemy.dialect import dialect as td_dialect
                # Include table qualification to avoid ambiguous column errors when
                # the expression refers to columns from the temporary/resultant
                # table. Also inline literal binds so array constructor is valid SQL.
                compiled_sql = str(e.expression.compile(dialect=td_dialect(),
                                                        compile_kwargs={'literal_binds': True,
                                                                        'include_table': False}))
                elements_sql.append(compiled_sql)
                
            elif isinstance(e, (date, datetime, pd.Timestamp)):
                elements_sql.append(f"'{e.strftime('%Y-%m-%d %H:%M:%S')}'")
            elif isinstance(e, str):
                # For VARBYTE and BYTE, do not add quotes if already in correct format
                atype_str = str(val.atype).upper()
                if "VARBYTE" in atype_str or "BYTE" in atype_str:
                    elements_sql.append(e)
                else:
                    elements_sql.append(f"'{e}'")

            elif isinstance(e, bool):
                elements_sql.append('1' if e else '0') 
            else:
                elements_sql.append(e)
        
        # Construct the SQL expression for the array column.
        array_sql = f"NEW {udt_name}({', '.join(map(str, elements_sql))})"

        # Create a SQLAlchemy expression.
        return  _SQLColumnExpression(literal_column(array_sql), type=val.atype)

    @collect_queryband(queryband="DF_assign")
    def assign(self, drop_columns=False, **kwargs):
        """
        DESCRIPTION:
            Assign new columns to a teradataml DataFrame.

        PARAMETERS:
            drop_columns:
                Optional Argument.
                If True, drop columns that are not specified in assign.
                Notes:
                    1. When DataFrame.assign() is run on DataFrame.groupby(), this argument
                       is ignored. In such cases, all columns are dropped and only new columns
                       and grouping columns are returned.
                    2. Argument is ignored for UDF functions.

                Default Value: False
                Types: bool

            kwargs:
                Specifies keyword-value pairs.
                - keywords are the column names.
                - values can be:
                    * Column arithmetic expressions.
                    * int/float/string literals.
                    * DataFrameColumn a.k.a. ColumnExpression Functions.
                      (Seee DataFrameColumn Functions in Function reference guide for more
                       details)
                    * SQLAlchemy ClauseElements.
                      (See teradataml extension with SQLAlchemy in teradataml User Guide
                       and Function reference guide for more details)
                    * Function - udf, call_udf.
                    * Array object.


        RETURNS:
            teradataml DataFrame
            A new DataFrame is returned with:
                1. New columns in addition to all the existing columns if "drop_columns" is False.
                2. Only new columns if "drop_columns" is True.
                3. New columns in addition to group by columns, i.e., columns used for grouping,
                   if assign() is run on DataFrame.groupby().

        NOTES:
             1. The values in kwargs cannot be callable (functions).
             2. The original DataFrame is not modified.
             3. Since ``kwargs`` is a dictionary, the order of your
               arguments may not be preserved. To make things predictable,
               the columns are inserted in alphabetical order, after the existing columns
               in the DataFrame. Assigning multiple columns within the same ``assign`` is
               possible, but you cannot reference other columns created within the same
               ``assign`` call.
             4. The maximum number of columns that a DataFrame can have is 2048.
             5. With DataFrame.groupby(), only aggregate functions and literal values
               are advised to use. Other functions, such as string functions, can also be
               used, but the column used in such function must be a part of group by columns.
               See examples for teradataml extension with SQLAlchemy on using various
               functions with DataFrame.assign().
             6. UDF expressions can run on both Vantage Cloud Lake leveraging Apply Table Operator
               of Open Analytics Framework and Enterprise leveraging Vantage's Script Table Operator.
             7. One can pass both regular expressions and udf expressions to this API.
               However, regular expressions are computed first followed by udf expressions.
               Hence the order of columns also maintained in same order.
               Look at Example 18 to understand more.
             8. While passing multiple udf expressions, one can not pass one column output
               as another column input in the same ``assign`` call.
             9. If user pass multiple udf expressions, delimiter and quotechar specified in
               last udf expression are considered for processing.
            10. When Array object is passed as a value, all columns or literal values within the
               array should be of similar type.

        RAISES:
             1. ValueError - When a callable is passed as a value, or columns from different
                             DataFrames are passed as values in kwargs.
             2. TeradataMlException - When the return DataFrame initialization fails, or
                                      invalid argument types are passed.

        EXAMPLES:
            >>> load_example_data("dataframe", "admissions_train")
            >>> df = DataFrame("admissions_train")
            >>> c1 = df.gpa
            >>> c2 = df.id
            >>>

            #
            # Executing assign() with Arithmetic operations on columns.
            # All below examples use columns "gpa" and "id" for
            # arithmetic operations to create a new DataFrame including the new columns.
            #
            # Let's take a look at various operations that can be performed
            # using assign() and arithmetic operations on columns.

            # Example 1: Addition of two columns "gpa" and "id".
            >>> df.assign(new_column = c1 + c2).sort("id")
               masters   gpa     stats programming admitted  new_column
            id
            1      yes  3.95  Beginner    Beginner        0        4.95
            2      yes  3.76  Beginner    Beginner        0        5.76
            3       no  3.70    Novice    Beginner        1        6.70
            4      yes  3.50  Beginner      Novice        1        7.50
            5       no  3.44    Novice      Novice        0        8.44
            6      yes  3.50  Beginner    Advanced        1        9.50
            7      yes  2.33    Novice      Novice        1        9.33
            8       no  3.60  Beginner    Advanced        1       11.60
            9       no  3.82  Advanced    Advanced        1       12.82
            10      no  3.71  Advanced    Advanced        1       13.71
            >>>

            # Example 2: Multiplication of columns "gpa" and "id".
            >>> df.assign(new_column = c1 * c2).sort("id")
               masters   gpa     stats programming admitted  new_column
            id
            1      yes  3.95  Beginner    Beginner        0        3.95
            2      yes  3.76  Beginner    Beginner        0        7.52
            3       no  3.70    Novice    Beginner        1       11.10
            4      yes  3.50  Beginner      Novice        1       14.00
            5       no  3.44    Novice      Novice        0       17.20
            6      yes  3.50  Beginner    Advanced        1       21.00
            7      yes  2.33    Novice      Novice        1       16.31
            8       no  3.60  Beginner    Advanced        1       28.80
            9       no  3.82  Advanced    Advanced        1       34.38
            10      no  3.71  Advanced    Advanced        1       37.10
            >>>

            # Example 3: Division of columns. Divide "id" by "gpa".
            >>> df.assign(new_column = c2 / c1).sort("id")
               masters   gpa     stats programming admitted  new_column
            id
            1      yes  3.95  Beginner    Beginner        0    0.253165
            2      yes  3.76  Beginner    Beginner        0    0.531915
            3       no  3.70    Novice    Beginner        1    0.810811
            4      yes  3.50  Beginner      Novice        1    1.142857
            5       no  3.44    Novice      Novice        0    1.453488
            6      yes  3.50  Beginner    Advanced        1    1.714286
            7      yes  2.33    Novice      Novice        1    3.004292
            8       no  3.60  Beginner    Advanced        1    2.222222
            9       no  3.82  Advanced    Advanced        1    2.356021
            10      no  3.71  Advanced    Advanced        1    2.695418
            >>>

            # Example 4: Subtract values in column "id" from "gpa".
            >>> df.assign(new_column = c1 - c2).sort("id")
               masters   gpa     stats programming admitted  new_column
            id
            1      yes  3.95  Beginner    Beginner        0        2.95
            2      yes  3.76  Beginner    Beginner        0        1.76
            3       no  3.70    Novice    Beginner        1        0.70
            4      yes  3.50  Beginner      Novice        1       -0.50
            5       no  3.44    Novice      Novice        0       -1.56
            6      yes  3.50  Beginner    Advanced        1       -2.50
            7      yes  2.33    Novice      Novice        1       -4.67
            8       no  3.60  Beginner    Advanced        1       -4.40
            9       no  3.82  Advanced    Advanced        1       -5.18
            10      no  3.71  Advanced    Advanced        1       -6.29

            # Example 5: Modulo division of values in column "id" and "gpa".
            >>> df.assign(new_column = c2 % c1).sort("id")
               masters   gpa     stats programming admitted  new_column
            id
            1      yes  3.95  Beginner    Beginner        0        1.00
            2      yes  3.76  Beginner    Beginner        0        2.00
            3       no  3.70    Novice    Beginner        1        3.00
            4      yes  3.50  Beginner      Novice        1        0.50
            5       no  3.44    Novice      Novice        0        1.56
            6      yes  3.50  Beginner    Advanced        1        2.50
            7      yes  2.33    Novice      Novice        1        0.01
            8       no  3.60  Beginner    Advanced        1        0.80
            9       no  3.82  Advanced    Advanced        1        1.36
            10      no  3.71  Advanced    Advanced        1        2.58
            >>>

            #
            # Executing assign() with literal values.
            #
            # Example 6: Adding an integer literal value to the values of columns "gpa" and "id".
            >>> df.assign(c3 = c1 + 1, c4 = c2 + 1).sort("id")
               masters   gpa     stats programming admitted    c3  c4
            id
            1      yes  3.95  Beginner    Beginner        0  4.95   2
            2      yes  3.76  Beginner    Beginner        0  4.76   3
            3       no  3.70    Novice    Beginner        1  4.70   4
            4      yes  3.50  Beginner      Novice        1  4.50   5
            5       no  3.44    Novice      Novice        0  4.44   6
            6      yes  3.50  Beginner    Advanced        1  4.50   7
            7      yes  2.33    Novice      Novice        1  3.33   8
            8       no  3.60  Beginner    Advanced        1  4.60   9
            9       no  3.82  Advanced    Advanced        1  4.82  10
            10      no  3.71  Advanced    Advanced        1  4.71  11
            >>>

            # Example 7: Create a new column with an integer literal value.
            >>> df.assign(c1 = 1).sort("id")
               masters   gpa     stats programming admitted c1
            id
            1      yes  3.95  Beginner    Beginner        0  1
            2      yes  3.76  Beginner    Beginner        0  1
            3       no  3.70    Novice    Beginner        1  1
            4      yes  3.50  Beginner      Novice        1  1
            5       no  3.44    Novice      Novice        0  1
            6      yes  3.50  Beginner    Advanced        1  1
            7      yes  2.33    Novice      Novice        1  1
            8       no  3.60  Beginner    Advanced        1  1
            9       no  3.82  Advanced    Advanced        1  1
            10      no  3.71  Advanced    Advanced        1  1
            >>>

            # Example 8: Create a new column with a string literal value.
            >>> df.assign(c3 = 'string').sort("id")
               masters   gpa     stats programming admitted      c3
            id
            1      yes  3.95  Beginner    Beginner        0  string
            2      yes  3.76  Beginner    Beginner        0  string
            3       no  3.70    Novice    Beginner        1  string
            4      yes  3.50  Beginner      Novice        1  string
            5       no  3.44    Novice      Novice        0  string
            6      yes  3.50  Beginner    Advanced        1  string
            7      yes  2.33    Novice      Novice        1  string
            8       no  3.60  Beginner    Advanced        1  string
            9       no  3.82  Advanced    Advanced        1  string
            10      no  3.71  Advanced    Advanced        1  string
            >>>

            # Example 9: Concatenation of strings, a string literal and value from
            #            "masters" column.
            #            '+' operator is overridden for string columns.
            >>> df.assign(concatenated = "Completed? " + df.masters).sort("id")
               masters   gpa     stats programming admitted    concatenated
            id
            1      yes  3.95  Beginner    Beginner        0  Completed? yes
            2      yes  3.76  Beginner    Beginner        0  Completed? yes
            3       no  3.70    Novice    Beginner        1   Completed? no
            4      yes  3.50  Beginner      Novice        1  Completed? yes
            5       no  3.44    Novice      Novice        0   Completed? no
            6      yes  3.50  Beginner    Advanced        1  Completed? yes
            7      yes  2.33    Novice      Novice        1  Completed? yes
            8       no  3.60  Beginner    Advanced        1   Completed? no
            9       no  3.82  Advanced    Advanced        1   Completed? no
            10      no  3.71  Advanced    Advanced        1   Completed? no
            >>>

            #
            # Significance of "drop_columns" in assign().
            # Setting drop_columns to True will only return assigned expressions.
            #
            # Example 10: Drop all column and return new assigned expressions.
            >>> df.assign(drop_columns = True,
            ...           addc = c1 + c2,
            ...           subc = c1 - c2,
            ...           mulc = c1 * c2,
            ...           divc = c1/c2).sort("addc")
                addc      divc   mulc  subc
            0   4.95  3.950000   3.95  2.95
            1   5.76  1.880000   7.52  1.76
            2   6.70  1.233333  11.10  0.70
            3   7.50  0.875000  14.00 -0.50
            4   8.44  0.688000  17.20 -1.56
            5   9.33  0.332857  16.31 -4.67
            6   9.50  0.583333  21.00 -2.50
            7  11.60  0.450000  28.80 -4.40
            8  12.82  0.424444  34.38 -5.18
            9  13.71  0.371000  37.10 -6.29
            >>>

            # Example 11: Duplicate a column with a new name.
            #             In the example here, we are duplicating:
            #               1. Column "id" to new column "c1".
            #               2. Column "gpa" to new column "c2".
            >>> df.assign(c1 = c2, c2 = c1).sort("id")
               masters   gpa     stats programming admitted  c1    c2
            id
            1      yes  3.95  Beginner    Beginner        0   1  3.95
            2      yes  3.76  Beginner    Beginner        0   2  3.76
            3       no  3.70    Novice    Beginner        1   3  3.70
            4      yes  3.50  Beginner      Novice        1   4  3.50
            5       no  3.44    Novice      Novice        0   5  3.44
            6      yes  3.50  Beginner    Advanced        1   6  3.50
            7      yes  2.33    Novice      Novice        1   7  2.33
            8       no  3.60  Beginner    Advanced        1   8  3.60
            9       no  3.82  Advanced    Advanced        1   9  3.82
            10      no  3.71  Advanced    Advanced        1  10  3.71
            >>>

            # Example 12: Renaming columns.
            #             Example 6 command can be modified a bit to rename columns, rather than
            #             duplicating it.
            #             Use "drop_column=True" in example 6 command to select all the desired columns.
            >>> df.assign(drop_columns=True, c1 = c2, c2 = c1,
            ...           masters=df.masters,
            ...           stats=df.stats,
            ...           programming=df.programming,
            ...           admitted=df.admitted).sort("c1")
              masters     stats programming  admitted  c1    c2
            0     yes  Beginner    Beginner         0   1  3.95
            1     yes  Beginner    Beginner         0   2  3.76
            2      no    Novice    Beginner         1   3  3.70
            3     yes  Beginner      Novice         1   4  3.50
            4      no    Novice      Novice         0   5  3.44
            5     yes  Beginner    Advanced         1   6  3.50
            6     yes    Novice      Novice         1   7  2.33
            7      no  Beginner    Advanced         1   8  3.60
            8      no  Advanced    Advanced         1   9  3.82
            9      no  Advanced    Advanced         1  10  3.71
            >>>

            #
            # Executing Aggregate Functions with assign() and DataFrame.groupby().
            #
            # Here, we will be using 'func' from sqlalchemy to run some aggregate functions.
            >>> from sqlalchemy import func
            >>>

            # Example 13: Calculate average "gpa" for values in the "stats" column.
            >>> df.groupby("stats").assign(res=func.ave(df.gpa.expression))
                  stats       res
            0  Beginner  3.662000
            1  Advanced  3.508750
            2    Novice  3.559091
            >>>

            # Example 14: Calculate standard deviation, kurtosis value and sum of values in
            #             the "gpa" column with values grouped by values in the "stats" column.
            #             Alternate approach for DataFrame.agg(). This allows user to name the
            #             result columns.
            >>> df.groupby("stats").assign(gpa_std_=func.ave(df.gpa.expression),
            ...                            gpa_kurtosis_=func.kurtosis(df.gpa.expression),
            ...                            gpa_sum_=func.sum(df.gpa.expression))
                  stats  gpa_kurtosis_  gpa_std_  gpa_sum_
            0  Beginner      -0.452859  3.662000     18.31
            1  Advanced       2.886226  3.508750     84.21
            2    Novice       6.377775  3.559091     39.15
            >>>

            #
            # Executing user defined function (UDF) with assign()
            #
            # Example 15: Create two user defined functions to 'to_upper' and 'sum',
            #            'to_upper' to get the values in 'accounts' to upper case and 
            #            'sum' to add length of string values in column 'accounts' 
            #             with column 'Feb' and store the result in Integer type column.
            >>> @udf
            ... def to_upper(s):
            ...     if s is not None:
            ...         return s.upper()
            >>>
            >>> from teradatasqlalchemy.types import INTEGER
            >>> @udf(returns=INTEGER()) 
            ... def sum(x, y):
            ...     return len(x)+y
            >>>
            # Assign both Column Expressions returned by user defined functions
            # to the DataFrame.
            >>> res = df.assign(upper_stats = to_upper('accounts'), len_sum = sum('accounts', 'Feb'))
            >>> res
                        Feb    Jan    Mar    Apr  datetime upper_stats  len_sum
            accounts                                                             
            Blue Inc     90.0   50.0   95.0  101.0  17/01/04    BLUE INC       98
            Red Inc     200.0  150.0  140.0    NaN  17/01/04     RED INC      207
            Yellow Inc   90.0    NaN    NaN    NaN  17/01/04  YELLOW INC      100
            Jones LLC   200.0  150.0  140.0  180.0  17/01/04   JONES LLC      209
            Orange Inc  210.0    NaN    NaN  250.0  17/01/04  ORANGE INC      220
            Alpha Co    210.0  200.0  215.0  250.0  17/01/04    ALPHA CO      218
            >>>

            # Example 16: Create a user defined function to add 4 to the 'datetime' column
            #             and store the result in DATE type column.
            >>> from teradatasqlalchemy.types import DATE
            >>> import datetime
            >>> @udf(returns=DATE())
            ... def add_date(x, y):
            ...     return (datetime.datetime.strptime(x, "%y/%m/%d")+datetime.timedelta(y)).strftime("%y/%m/%d")
            >>>
            # Assign the Column Expression returned by user defined function
            # to the DataFrame.
            >>> res = df.assign(new_date = add_date('datetime', 4))
            >>> res
                            Feb    Jan    Mar    Apr  datetime  new_date
            accounts                                                  
            Alpha Co    210.0  200.0  215.0  250.0  17/01/04  17/01/08
            Blue Inc     90.0   50.0   95.0  101.0  17/01/04  17/01/08
            Jones LLC   200.0  150.0  140.0  180.0  17/01/04  17/01/08
            Orange Inc  210.0    NaN    NaN  250.0  17/01/04  17/01/08
            Yellow Inc   90.0    NaN    NaN    NaN  17/01/04  17/01/08
            Red Inc     200.0  150.0  140.0    NaN  17/01/04  17/01/08
            >>>

            # Example 17: Create a user defined functions to 'to_upper' to get
            #             the values in 'accounts' to upper case and create a
            #             new column with a string literal value.
            >>> @udf
            ... def to_upper(s):
            ...     if s is not None:
            ...         return s.upper()
            >>>
            # Assign both expressions to the DataFrame.
            >>> res = df.assign(upper_stats = to_upper('accounts'), new_col = 'string')
            >>> res
                          Feb    Jan    Mar    Apr  datetime new_col upper_stats
            accounts                                                            
            Alpha Co    210.0  200.0  215.0  250.0  17/01/04  string    ALPHA CO
            Blue Inc     90.0   50.0   95.0  101.0  17/01/04  string    BLUE INC
            Yellow Inc   90.0    NaN    NaN    NaN  17/01/04  string  YELLOW INC
            Jones LLC   200.0  150.0  140.0  180.0  17/01/04  string   JONES LLC
            Red Inc     200.0  150.0  140.0    NaN  17/01/04  string     RED INC
            Orange Inc  210.0    NaN    NaN  250.0  17/01/04  string  ORANGE INC
            >>>

            # Example 18: Create two user defined functions to 'to_upper' and 'sum'
            #             and create new columns with string literal value and
            #             arithmetic operation on column 'Feb'.
            >>> @udf
            ... def to_upper(s):
            ...     if s is not None:
            ...         return s.upper()
            >>>
            >>> from teradatasqlalchemy.types import INTEGER
            >>> @udf(returns=INTEGER()) 
            ... def sum(x, y):
            ...     return len(x)+y
            >>>
            # Assign all expressions to the DataFrame.
            >>> res = df.assign(upper_stats = to_upper('accounts'),new_col = 'abc', 
            ...                 len_sum = sum('accounts', 'Feb'), col_sum = df.Feb+1)
            >>> res
                          Feb    Jan    Mar    Apr  datetime  col_sum new_col upper_stats  len_sum
            accounts                                                                              
            Blue Inc     90.0   50.0   95.0  101.0  17/01/04     91.0     abc    BLUE INC       98
            Alpha Co    210.0  200.0  215.0  250.0  17/01/04    211.0     abc    ALPHA CO      218
            Jones LLC   200.0  150.0  140.0  180.0  17/01/04    201.0     abc   JONES LLC      209
            Yellow Inc   90.0    NaN    NaN    NaN  17/01/04     91.0     abc  YELLOW INC      100
            Orange Inc  210.0    NaN    NaN  250.0  17/01/04    211.0     abc  ORANGE INC      220
            Red Inc     200.0  150.0  140.0    NaN  17/01/04    201.0     abc     RED INC      207
            >>>

            # Example 19: Convert the values is 'accounts' column to upper case using a user
            #             defined function on Vantage Cloud Lake.
            # Create a Python 3.10.5 environment with given name and description in Vantage.
            >>> env = create_env('test_udf', 'python_3.10.5', 'Test environment for UDF')
            User environment 'test_udf' created.
            >>>
            # Create a user defined functions to 'to_upper' to get the values in upper case
            # and pass the user env to run it on.
            >>> from teradataml.dataframe.functions import udf
            >>> @udf(env_name = env)
            ... def to_upper(s):
            ...     if s is not None:
            ...         return s.upper()
            >>>
            # Assign the Column Expression returned by user defined function
            # to the DataFrame.
            >>> df.assign(upper_stats = to_upper('accounts'))
                          Feb    Jan    Mar    Apr  datetime upper_stats
            accounts
            Alpha Co    210.0  200.0  215.0  250.0  17/01/04    ALPHA CO
            Blue Inc     90.0   50.0   95.0  101.0  17/01/04    BLUE INC
            Yellow Inc   90.0    NaN    NaN    NaN  17/01/04  YELLOW INC
            Jones LLC   200.0  150.0  140.0  180.0  17/01/04   JONES LLC
            Orange Inc  210.0    NaN    NaN  250.0  17/01/04  ORANGE INC
            Red Inc     200.0  150.0  140.0    NaN  17/01/04     RED INC
            >>>

            # Example 20: Register and Call the user defined function to get the values upper case.
            >>> from teradataml.dataframe.functions import udf, register, call_udf
            >>> @udf
            ... def to_upper(s):
            ...     if s is not None:
            ...         return s.upper()
            >>>
            # Register the created user defined function with name "upper".
            >>> register("upper", to_upper)
            >>>
            # Call the user defined function registered with name "upper" and assign the
            # ColumnExpression returned to the DataFrame.
            >>> res = df.assign(upper_col = call_udf("upper", ('accounts',)))
            >>> res
                          Feb    Jan    Mar    Apr  datetime   upper_col
            accounts
            Alpha Co    210.0  200.0  215.0  250.0  17/01/04    ALPHA CO
            Blue Inc     90.0   50.0   95.0  101.0  17/01/04    BLUE INC
            Yellow Inc   90.0    NaN    NaN    NaN  17/01/04  YELLOW INC
            Jones LLC   200.0  150.0  140.0  180.0  17/01/04   JONES LLC
            Orange Inc  210.0    NaN    NaN  250.0  17/01/04  ORANGE INC
            Red Inc     200.0  150.0  140.0    NaN  17/01/04     RED INC
            >>>

            # Example 21: Assign a new array column which stores literal values 1, 2, 3, 4.
            >>> from teradataml import Array
            >>> array_ = Array((1, 2, 3, 4))
            >>> res = df.assign(array_col=array_)
            >>> res
                          Feb    Jan    Mar    Apr    datetime  array_col
            accounts                                                     
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017  (1,2,3,4)
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017  (1,2,3,4)
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017  (1,2,3,4)
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017  (1,2,3,4)
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017  (1,2,3,4)
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017  (1,2,3,4)

            >>> res.tdtypes
            accounts     VARCHAR(length=20, charset='LATIN')
            Feb                                      FLOAT()
            Jan                                     BIGINT()
            Mar                                     BIGINT()
            Apr                                     BIGINT()
            datetime                                  DATE()
            array_col                 ARRAY_INTEGER('[100]')

            # Example 22: Assign a new array column to store the `datetime` column. 
            >>> res = df.assign(arr_col=Array(([df.datetime],), atype=ARRAY_DATE('[1:2]'), 
                                                default_null=True))
            >>> res
                          Feb    Jan    Mar    Apr    datetime            arr_col
            accounts                                                     
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017  (2017-01-04,NULL)
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017  (2017-01-04,NULL)
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017  (2017-01-04,NULL)
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017  (2017-01-04,NULL)
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017  (2017-01-04,NULL)
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017  (2017-01-04,NULL)

            >>> res.tdtypes
            accounts       VARCHAR(length=20, charset='LATIN')
            Feb                                        FLOAT()
            Jan                                       BIGINT()
            Mar                                       BIGINT()
            Apr                                       BIGINT()
            datetime                                    DATE()
            arr_col     ARRAY_DATE('[1:2]', default_null=True)


            # Example 23: Assign a new array column to store columns Jan, Mar and a literal values.
            >>> res = df.assign(new_col=Array((df.Jan, "10", df.Feb, 200)))
            >>> res
                          Feb    Jan    Mar    Apr    datetime            new_col
            accounts                                                             
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017   (200,10,210,200)
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017     (50,10,90,200)
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017   (150,10,200,200)
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017  (NULL,10,210,200)
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017   (NULL,10,90,200)
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017   (150,10,200,200)

            >>> res.tdtypes
            accounts    VARCHAR(length=20, charset='LATIN')
            Feb                                     FLOAT()
            Jan                                    BIGINT()
            Mar                                    BIGINT()
            Apr                                    BIGINT()
            datetime                                 DATE()
            new_col                   ARRAY_BIGINT('[100]')

            # Example 24: Assign a new array column to store timestamp values.
            >>> arr_timestamp = Array((datetime.datetime(2024, 6, 20, 12, 0, 52), 
                                       datetime.datetime(2025, 6, 2, 15, 32)), 
                                       atype=ARRAY_TIMESTAMP('[1:2]', timezone=True))

            >>> res = df.assign(arr_col=arr_timestamp)
            >>> res
                          Feb    Jan    Mar    Apr    datetime                                                              new_col
            accounts                                                                                                               
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017  (2024-06-20 12:00:52.000000-00:00,2025-06-02 15:32:00.000000-00:00)
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017  (2024-06-20 12:00:52.000000-00:00,2025-06-02 15:32:00.000000-00:00)
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017  (2024-06-20 12:00:52.000000-00:00,2025-06-02 15:32:00.000000-00:00)
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017  (2024-06-20 12:00:52.000000-00:00,2025-06-02 15:32:00.000000-00:00)
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017  (2024-06-20 12:00:52.000000-00:00,2025-06-02 15:32:00.000000-00:00)
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017  (2024-06-20 12:00:52.000000-00:00,2025-06-02 15:32:00.000000-00:00)

            >>> res.tdtypes
            accounts        VARCHAR(length=20, charset='LATIN')
            Feb                                         FLOAT()
            Jan                                        BIGINT()
            Mar                                        BIGINT()
            Apr                                        BIGINT()
            datetime                                     DATE()
            arr_col     ARRAY_TIMESTAMP('[1:2]', timezone=True)

            # Example 25: Assign a new array column to store columns Jan, Mar and a literal values.
            >>> res = df.assign(new_col=Array((df.Jan, "10", df.Feb, 200),  atype=ARRAY_INTEGER('[-2:1]')))
            >>> res
                          Feb    Jan    Mar    Apr    datetime            new_col
            accounts                                                             
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017     (50,10,90,200)
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017   (150,10,200,200)
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017   (NULL,10,90,200)
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017   (150,10,200,200)
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017  (NULL,10,210,200)
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017   (200,10,210,200)

            >>> res.tdtypes
            accounts    VARCHAR(length=20, charset='LATIN')
            Feb                                     FLOAT()
            Jan                                    BIGINT()
            Mar                                    BIGINT()
            Apr                                    BIGINT()
            datetime                                 DATE()
            new_col                 ARRAY_INTEGER('[-2:1]')

            # Example 26: Assign a new array column to store accounts column values.
            >>> res = df.assign(quarter_sales=Array((df.accounts,), atype=ARRAY_VARCHAR(length=4, scope='[1:2]')))
            >>> res
                          Feb    Jan    Mar    Apr    datetime quarter_sales
            accounts                                                        
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017      ('Alph')
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017      ('Blue')
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017      ('Jone')
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017      ('Oran')
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017      ('Yell')
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017      ('Red ')

            >>> res.tdtypes
            accounts         VARCHAR(length=20, charset='LATIN')
            Feb                                          FLOAT()
            Jan                                         BIGINT()
            Mar                                         BIGINT()
            Apr                                         BIGINT()
            datetime                                      DATE()
            quarter_sales       ARRAY_VARCHAR('[1:2]', length=4)

            # Example 27: Assign a new array column to store the NUMBER type.
            >>> res = df.assign(quarter_sales=Array((1.123, 2.454, 3.967), atype=ARRAY_NUMBER(scope='[1:3]', 
                                precision=2, scale=1)))
            >>> res

                          Feb    Jan    Mar    Apr    datetime quarter_sales
            accounts                                                        
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017   (1.1,2.5,4)
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017   (1.1,2.5,4)
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017   (1.1,2.5,4)
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017   (1.1,2.5,4)
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017   (1.1,2.5,4)
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017   (1.1,2.5,4)

            >>> res.tdtypes
            accounts               VARCHAR(length=20, charset='LATIN')
            Feb                                                FLOAT()
            Jan                                               BIGINT()
            Mar                                               BIGINT()
            Apr                                               BIGINT()
            datetime                                            DATE()
            quarter_sales  ARRAY_NUMBER('[1:3]', precision=2, scale=1)

            # Example 28: Assign a new array column to store the interval year values.
            >>> year_values = ("2345", "2021", "2020", "2025")
            >>> year_array = Array(year_values, atype=ARRAY_INTERVAL_YEAR('[4]', precision=4))
            >>> df.assign(quarter_sales=year_array)
            >>> res
                          Feb    Jan    Mar    Apr    datetime          quarter_sales
            accounts                                                                 
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017  (2345,2021,2020,2025)
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017  (2345,2021,2020,2025)
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017  (2345,2021,2020,2025)
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017  (2345,2021,2020,2025)
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017  (2345,2021,2020,2025)
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017  (2345,2021,2020,2025)

            >>> res.tdtypes
            accounts             VARCHAR(length=20, charset='LATIN')
            Feb                                              FLOAT()
            Jan                                             BIGINT()
            Mar                                             BIGINT()
            Apr                                             BIGINT()
            datetime                                          DATE()
            quarter_sales    ARRAY_INTERVAL_YEAR('[4]', precision=4)

        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["drop_columns", drop_columns, True, (bool)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        if len(kwargs) == 0:
            return self

        elif len(kwargs) >= TeradataConstants['TABLE_COLUMN_LIMIT'].value:
            errcode = MessageCodes.TD_MAX_COL_MESSAGE
            raise TeradataMlException(Messages.get_message(errcode), errcode)

        allowed_types = self._get_assign_allowed_types()

        node_executed = False
        for key, val in kwargs.items():
            if isinstance(val, ColumnExpression):
                # Values can be ColumnExpressions representing column operations.
                # Array functions like array_agg and array_repeat may defer type resolution when column
                # types are unknown (NullType), storing parts for later resolution using DataFrame metadata.
                df_utils._resolve_array_parts(val, self._metaexpr)

            if isinstance(val, Array):
                kwargs[key] = self._assign_array(val)

            if isinstance(val, ColumnExpression) and val.get_flag_has_multiple_dataframes():
                raise ValueError("Combining Columns from different dataframes is unsupported for "
                                 "assign operation.")

            is_allowed = lambda x: isinstance(*x) and type(x[0]) != bool
            value_type_allowed = map(is_allowed, ((val, t) for t in allowed_types))

            # if callable(val):
            #    err = 'Unsupported callable value for key: {}'.format(key)
            #    raise ValueError(err)

            if not any(list(value_type_allowed)):
                err = 'Unsupported values of type {t} for key {k}'.format(k=key, t=type(val))
                raise ValueError(err)

            if isinstance(val, ClauseElement) and not node_executed:
                # We should execute the node, if input kwargs contains a value, i.e.,
                # val of type ClauseElements and if such function needs a node in
                # executed state, i.e., underlying table/view must exist on the system.
                node_executed = self.__assign_conditional_node_execution(val)

        if self._metaexpr is None:
            msg = Messages.get_message(MessageCodes.TDMLDF_INFO_ERROR)
            raise TeradataMlException(msg, MessageCodes.TDMLDF_INFO_ERROR)

        # Create a dictionary of column name to udf expressions and 
        # column name to normal/regular expressions.
        udf_expr = {}
        regular_expr = {}
        call_udf_expr = {}
        for colname, col in kwargs.items():
            # If value passed in kwargs is a ColumnExpression and is a udf, store it.
            if isinstance(col, ColumnExpression) and col._udf:
                udf_expr[colname] = col
            # If value passed in kwargs is a ColumnExpression and is a registerd udf script, store it.
            elif isinstance(col, ColumnExpression) and col._udf_script:
                call_udf_expr[colname] = col
            else:
                regular_expr[colname] = col
        df = self

        # If kwargs contains both regular and udf expressions, first create new columns 
        # from normal/regular expressions then on the output dataframe create new columns
        # from udf expression.
        if bool(regular_expr):
            try:
                root_node_id = None
                root_df_col = df.columns

                # Get the previous node type, if it is assign and drop_columns is False,
                # then check if the previous assign arguments exists and are not present
                # in either the root dataframe columns or the current assign arguments.
                # if these conditions are met, obtain the root node id (i.e., the first 
                # node of the assign operation) and merge the previous assign arguments with the current ones.

                prev_node_type = df._aed_utils._aed_get_node_query_type(df._nodeid)
                if not drop_columns and prev_node_type == "assign" and df._previous_assign_args is not None:
                    if not df._root_columns & df._previous_assign_args.keys() and \
                       not df._previous_assign_args.keys() & regular_expr.keys(): 
                        # Get the root node id and root dataframe columns.
                        root_df_col = df._root_columns
                        root_node_id = df._aed_utils._aed_get_parent_nodeids(df._nodeid)[0]
                        regular_expr = {**df._previous_assign_args, **regular_expr}

                # If root_node_id is None, assign the current node id as root node of assign operation 
                node_id = root_node_id if root_node_id is not None else df._nodeid  

                # Generate new meta expression and node id for the new dataframe.
                (new_meta, new_nodeid) = df._generate_assign_metaexpr_aed_nodeid(
                                drop_columns, node_id = node_id, **regular_expr)
                df = df._create_dataframe_from_node(new_nodeid, new_meta, df._index_label)
                df._previous_assign_args = regular_expr
                df._root_columns = root_df_col
                
            except Exception as err:
                errcode = MessageCodes.TDMLDF_INFO_ERROR
                msg = Messages.get_message(MessageCodes.TDMLDF_INFO_ERROR)
                raise TeradataMlException(msg, errcode) from err
            
        if bool(udf_expr):
            df = df._assign_udf(udf_expr)

        if bool(call_udf_expr):
            df = df._assign_call_udf(call_udf_expr)

        return df

    @collect_queryband(queryband="DF_get")
    def get(self, key):
        """
        DESCRIPTION:
            Retrieve required columns from DataFrame using column name(s) as key.
            Returns a new teradataml DataFrame with requested columns only.

        PARAMETERS:

            key:
                Required Argument.
                Specifies column(s) to retrieve from the teradataml DataFrame.
                Types: str OR List of Strings (str)

            teradataml supports the following formats (only) for the "get" method:

            1] Single Column String: df.get("col1")
            2] Single Column List: df.get(["col1"])
            3] Multi-Column List: df.get(['col1', 'col2', 'col3'])
            4] Multi-Column List of List: df.get([["col1", "col2", "col3"]])

            Note: Multi-Column retrieval of the same column such as df.get(['col1', 'col1']) is not supported.

        RETURNS:
            teradataml DataFrame

        RAISES:
            TeradataMlException

        EXAMPLES:
            >>> load_example_data("dataframe","admissions_train")
            >>> df = DataFrame("admissions_train")
            >>> df.sort('id')
               masters   gpa     stats programming admitted
            id
            1      yes  3.95  Beginner    Beginner        0
            2      yes  3.76  Beginner    Beginner        0
            3       no  3.70    Novice    Beginner        1
            4      yes  3.50  Beginner      Novice        1
            ...

            1] Single String Column
            >>> df.get("gpa")
                gpa
            0  3.46
            1  3.52
            2  3.68
            3  3.65
            ...

            2] Single Column List
            >>> df.get(["gpa"])
                gpa
            0  3.46
            1  3.52
            2  3.68
            3  3.65
            ...

            3] Multi-Column List
            >>> df.get(["programming", "masters", "gpa"])
              programming masters   gpa
            0    Beginner     yes  3.46
            1      Novice      no  3.52
            2    Beginner      no  3.68
            3      Novice      no  3.65
            ...

            4] Multi-Column List of List
            >>> df.get([['programming', 'masters', 'gpa']])
              programming masters   gpa
            0    Advanced     yes  4.00
            1    Advanced     yes  3.45
            2    Beginner     yes  3.50
            3    Beginner     yes  4.00
            ...

        """
        return self.select(key)

    @collect_queryband(queryband="DF_setIndex")
    def set_index(self, keys, drop=True, append=False):
        """
        DESCRIPTION:
            Assigns one or more existing columns as the new index to a teradataml DataFrame.
            Note:
                * This method does not support DataFrame containing array columns.

        PARAMETERS:

            keys:
                Required Argument.
                Specifies the column name or a list of column names to use as the DataFrame index.
                Types: str OR list of Strings (str)

            drop:
                Optional Argument.
                Specifies whether or not to display the column(s) being set as index as
                teradataml DataFrame columns anymore.
                When drop is True, columns are set as index and not displayed as columns.
                When drop is False, columns are set as index; but also displayed as columns.
                Note: When the drop argument is set to True, the column being set as index does not cease to
                      be a part of the underlying table upon which the teradataml DataFrame is based off.
                      A column that is dropped while being set as an index is merely not used for display
                      purposes anymore as a column of the teradataml DataFrame.
                Default Value: True
                Types: bool

            append:
                Optional Argument.
                Specifies whether or not to append requested columns to the existing index.
                When append is False, replaces existing index.
                When append is True, retains both existing & currently appended index.
                Default Value: False
                Types: bool

        RETURNS:
            teradataml DataFrame

        RAISES:
            TeradataMlException

        EXAMPLES:
            >>> load_example_data("dataframe","admissions_train")
            >>> df = DataFrame("admissions_train")
            >>> df.sort('id')
               masters   gpa     stats programming admitted
            id
            1      yes  3.95  Beginner    Beginner        0
            2      yes  3.76  Beginner    Beginner        0
            3       no  3.70    Novice    Beginner        1
            4      yes  3.50  Beginner      Novice        1
            5       no  3.44    Novice      Novice        0
            6      yes  3.50  Beginner    Advanced        1
            7      yes  2.33    Novice      Novice        1
            8       no  3.60  Beginner    Advanced        1
            9       no  3.82  Advanced    Advanced        1
            10      no  3.71  Advanced    Advanced        1

            >>> # Set new index.
            >>> df.set_index('masters').sort('id')
                     id   gpa     stats programming admitted
            masters
            yes       1  3.95  Beginner    Beginner        0
            yes       2  3.76  Beginner    Beginner        0
            no        3  3.70    Novice    Beginner        1
            yes       4  3.50  Beginner      Novice        1
            no        5  3.44    Novice      Novice        0
            yes       6  3.50  Beginner    Advanced        1
            yes       7  2.33    Novice      Novice        1
            no        8  3.60  Beginner    Advanced        1
            no        9  3.82  Advanced    Advanced        1
            no       10  3.71  Advanced    Advanced        1

            >>> # Set multiple indexes using list of columns
            >>> df.set_index(['masters', 'id']).sort('id')
                         gpa     stats programming admitted
            id masters
            1  yes      3.95  Beginner    Beginner        0
            2  yes      3.76  Beginner    Beginner        0
            3  no       3.70    Novice    Beginner        1
            4  yes      3.50  Beginner      Novice        1
            5  no       3.44    Novice      Novice        0
            6  yes      3.50  Beginner    Advanced        1
            7  yes      2.33    Novice      Novice        1
            8  no       3.60  Beginner    Advanced        1
            9  no       3.82  Advanced    Advanced        1
            10 no       3.71  Advanced    Advanced        1

            >>> # Append to new index to the existing set of index.
            >>> df.set_index(['masters', 'id']).set_index('gpa', drop = False, append = True).sort('id')
                                stats programming admitted
            gpa  masters id
            3.95 yes     1   Beginner    Beginner        0
            3.76 yes     2   Beginner    Beginner        0
            3.70 no      3     Novice    Beginner        1
            3.50 yes     4   Beginner      Novice        1
            3.44 no      5     Novice      Novice        0
            3.50 yes     6   Beginner    Advanced        1
            2.33 yes     7     Novice      Novice        1
            3.60 no      8   Beginner    Advanced        1
            3.82 no      9   Advanced    Advanced        1
            3.71 no      10  Advanced    Advanced        1
            >>>
        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["keys", keys, False, (str, list), True])
        awu_matrix.append(["drop", drop, True, (bool)])
        awu_matrix.append(["append", append, True, (bool)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        # Checking each element in passed columns to be valid column in dataframe
        _Validators._validate_column_exists_in_dataframe(keys, self._metaexpr)

        try:
            
            # Slicing creates a new list instance with the same contents.
            new_index_list = self._index_label[:] if self._index_label is not None else []

            # Creating a list with requested index labels bases on append
            if append:
                if isinstance(keys, str):
                    new_index_list.append(keys)
                elif isinstance(keys, list):
                    new_index_list.extend(keys)
            else:
                if isinstance(keys, str):
                    new_index_list = [keys]
                elif isinstance(keys, list):
                    new_index_list = keys

            # Takes care of appending already existing index
            new_index_list = list(dict.fromkeys(new_index_list))

            # In case requested index is same as existing index, return same DF
            if new_index_list == self._index_label:
                return self

            # Creating list of undropped columns for printing
            undropped_columns = []
            if not drop:
                if isinstance(keys, str):
                    undropped_columns = [keys]
                elif isinstance(keys, list):
                    undropped_columns = keys

            if len(undropped_columns) == 0:
                undropped_columns = None

            # Assigning self attributes to newly created dataframe.
            new_df = self._create_dataframe_from_node(self._nodeid, self._metaexpr, new_index_list, undropped_columns)
            new_df._table_name = self._table_name
            new_df._index = self._index
            return new_df

        except TeradataMlException:
            raise
        except Exception as err:
            raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_INFO_ERROR),
                                      MessageCodes.TDMLDF_INFO_ERROR) from err

    @property
    def index(self):
        """
        DESCRIPTION:
            Returns the index_label of the teradataml DataFrame.
            Note:
                * This method does not support DataFrame containing array columns.

        RETURNS:
            str or List of Strings (str) representing the index_label of the DataFrame.

        RAISES:
            None

        EXAMPLES:
            >>> load_example_data("dataframe","admissions_train")
            >>> df = DataFrame("admissions_train")
            >>> df
               masters   gpa     stats programming  admitted
            id
            5       no  3.44    Novice      Novice         0
            3       no  3.70    Novice    Beginner         1
            1      yes  3.95  Beginner    Beginner         0
            20     yes  3.90  Advanced    Advanced         1
            8       no  3.60  Beginner    Advanced         1
            25      no  3.96  Advanced    Advanced         1
            18     yes  3.81  Advanced    Advanced         1
            24      no  1.87  Advanced      Novice         1
            26     yes  3.57  Advanced    Advanced         1
            38     yes  2.65  Advanced    Beginner         1

            >>> # Get the index_label
            >>> df.index
            ['id']

            >>> # Set new index_label
            >>> df = df.set_index(['id', 'masters'])
            >>> df
                         gpa     stats programming  admitted
            id masters
            5  no       3.44    Novice      Novice         0
            3  no       3.70    Novice    Beginner         1
            1  yes      3.95  Beginner    Beginner         0
            17 no       3.83  Advanced    Advanced         1
            13 no       4.00  Advanced      Novice         1
            32 yes      3.46  Advanced    Beginner         0
            11 no       3.13  Advanced    Advanced         1
            9  no       3.82  Advanced    Advanced         1
            34 yes      3.85  Advanced    Beginner         0
            24 no       1.87  Advanced      Novice         1

            >>> # Get the index_label
            >>> df.index
            ['id', 'masters']

        """
        return self._index_label

    @collect_queryband(queryband="DF_groupby")
    def groupby(self, columns_expr, **kwargs):
        """
        DESCRIPTION:
            Applies GroupBy to one or more columns of a teradataml Dataframe.
            The result will always behaves like calling groupby with as_index=False
            in pandas.

        PARAMETERS:
            columns_expr:
                Required Argument.
                Specifies the column name(s) to group by.
                Types: str OR list of Strings (str)

            kwargs:
                Optional Argument.
                Specifies keyword arguments.

                option:
                    Optional Argument.
                    Specifies the groupby option.
                    Permitted Values: "CUBE", "ROLLUP", None
                    Types: str or NoneType

                include_grouping_columns:
                    Optional Argument.
                    Specifies whether to include aggregations on the grouping column(s) or not.
                    When set to True, the resultant DataFrame will have the aggregations on the
                    columns mentioned in "columns_expr". Otherwise, resultant DataFrame will not have
                    aggregations on the columns mentioned in "columns_expr".
                    Default Value: False
                    Types: bool

        NOTES:
            * Users can still apply teradataml DataFrame methods (filters/sort/etc) on top of the result.
            * Consecutive operations of grouping, i.e., groupby_time(), resample() and groupby() are not permitted.
               An exception will be raised. Following are some cases where exception will be raised as
               "Invalid operation applied, check documentation for correct usage."
                    a. df.resample().groupby()
                    b. df.resample().resample()
                    c. df.resample().groupby_time()
            * This method does not support operations on array columns.

        RETURNS:
            teradataml DataFrameGroupBy Object

        RAISES:
            TeradataMlException

        EXAMPLES:
            # Load the data to run the example.
            >>> load_example_data("dataframe","admissions_train")

            # Create a DataFrame on 'admissions_train' table.
            >>> df = DataFrame("admissions_train")
            >>> df
               masters   gpa     stats programming  admitted
            id
            15     yes  4.00  Advanced    Advanced         1
            34     yes  3.85  Advanced    Beginner         0
            13      no  4.00  Advanced      Novice         1
            38     yes  2.65  Advanced    Beginner         1
            5       no  3.44    Novice      Novice         0
            40     yes  3.95    Novice    Beginner         0
            7      yes  2.33    Novice      Novice         1
            22     yes  3.46    Novice    Beginner         0
            26     yes  3.57  Advanced    Advanced         1
            17      no  3.83  Advanced    Advanced         1

            # Example 1: Find the minimum value of all valid columns by
            #            grouping the DataFrame with column 'masters'.
            >>> df1 = df.groupby(["masters"])
            >>> df1.min()
              masters min_id  min_gpa min_stats min_programming min_admitted
            0      no      3     1.87  Advanced        Advanced            0
            1     yes      1     1.98  Advanced        Advanced            0

            # Example 2: Find the sum of all valid columns by grouping the DataFrame
            #            with columns 'masters' and 'admitted'. Include grouping columns
            #            in aggregate function 'sum'.
            >>> df1 = df.groupby(["masters", "admitted"], include_grouping_columns=True)
            >>> df1.sum()
              masters  admitted  sum_id  sum_gpa  sum_admitted
            0     yes         1     188    34.35            10
            1     yes         0     289    43.36             0
            2      no         0      41     6.44             0
            3      no         1     302    57.52            16

            # Example 3: Find the sum of all valid columns by grouping the DataFrame with
            #            columns 'masters' and 'admitted'. Do not include grouping columns
            #            in aggregate function 'sum'.
            >>> df1 = df.groupby(["masters", "admitted"], include_grouping_columns=False)
            >>> df1.sum()
              masters  admitted  sum_id  sum_gpa
            0     yes         0     289    43.36
            1      no         0      41     6.44
            2      no         1     302    57.52
            3     yes         1     188    34.35
        """
        # Argument validations
        arg_info_matrix = []
        arg_info_matrix.append(["columns_expr", columns_expr, False, (str, list), True])
        option = kwargs.get("option", None)
        arg_info_matrix.append(["option", option, True, (str, type(None)), True,
                                ["CUBE", "ROLLUP", None]])
        include_grouping_columns = kwargs.get("include_grouping_columns", False)
        arg_info_matrix.append(["include_grouping_columns", include_grouping_columns, True, (bool)])

        # Validate argument types
        _Validators._validate_function_arguments(arg_info_matrix)

        # Checking each element in passed columns to be valid column in dataframe
        _Validators._validate_column_exists_in_dataframe(columns_expr, self._metaexpr)

        try:
            column_list = []
            unsupported_types = ['BLOB', 'CLOB', 'PERIOD_DATE', 'PERIOD_TIME', 'PERIOD_TIMESTAMP', 'ARRAY', 'VARRAY',
                                 'XML', 'JSON']
            type_expr = []
            invalid_types = []
            # check for consecutive groupby operations
            if isinstance(self, DataFrameGroupBy) or isinstance(self, DataFrameGroupByTime):
                raise TeradataMlException(Messages.get_message(MessageCodes.UNSUPPORTED_OPERATION),
                                          MessageCodes.UNSUPPORTED_OPERATION)

            if (isinstance(columns_expr, list)):
                column_list = columns_expr

            elif (isinstance(columns_expr, str)):
                column_list.append(columns_expr)

            # Checking each element in columns_expr to be valid column in dataframe
            for col in column_list:
                type_expr.append(self._metaexpr.t.c[col].type)

            # Convert types to string from sqlalchemy type
            columns_types = [repr(type_expr[i]).split("(")[0] for i in range(len(type_expr))]

            # Checking each element in passed columns_types to be valid a data type for groupby
            # and create a list of invalid_types
            for col_type in columns_types:
                if col_type in unsupported_types:
                    invalid_types.append(col_type)

            if len(invalid_types) > 0:
                raise TeradataMlException(Messages.get_message(MessageCodes.UNSUPPORTED_DATATYPE, invalid_types,
                                                               "ANY, except following {}".format(unsupported_types)),
                                          MessageCodes.UNSUPPORTED_DATATYPE)

            groupbyexpr = ', '.join(UtilFuncs._teradata_quote_arg(col, "\"", False) for col in column_list)
            groupbyObj = DataFrameGroupBy(self._nodeid, self._metaexpr, self._column_names_and_types, self.columns,
                                          groupbyexpr, column_list, option, include_grouping_columns)

            return groupbyObj
        except TeradataMlException:
            raise

    def __group_time_series_data(self, timebucket_duration, timebucket_duration_arg_name="timebucket_duration",
                                 value_expression=None, timecode_column=None,
                                 timecode_column_arg_name="timecode_column", sequence_column=None,
                                 fill=None, fill_arg_name="fill"):
        """
        DESCRIPTION:
            Internal function to resample/group time series data using Group By Time and a column.

        PARAMETERS:
            timebucket_duration:
                Required Argument.
                Specifies the duration of each timebucket for aggregation and is used to
                assign each potential timebucket a unique number.
                Permitted Values:
                     ===================================================================================================
                    | Time Units        | Formal Form       | Shorthand Equivalents for time_units                      |
                     ===================================================================================================
                    | Calendar Years    | CAL_YEARS(N)      | Ncy OR Ncyear OR Ncyears                                  |
                     ---------------------------------------------------------------------------------------------------
                    | Calendar Months   | CAL_MONTHS(N)     | Ncm OR Ncmonth OR Ncmonths                                |
                     ---------------------------------------------------------------------------------------------------
                    | Calendar Days     | CAL_DAYS(N)       | Ncd OR Ncday OR Ncdays                                    |
                     ---------------------------------------------------------------------------------------------------
                    | Weeks             | WEEKS(N)          | Nw  OR Nweek OR Nweeks                                    |
                     ---------------------------------------------------------------------------------------------------
                    | Days              | DAYS(N)           | Nd  OR Nday OR Ndays                                      |
                     ---------------------------------------------------------------------------------------------------
                    | Hours             | HOURS(N)          | Nh  OR Nhr OR Nhrs OR Nhour OR Nhours                     |
                     ---------------------------------------------------------------------------------------------------
                    | Minutes           | MINUTES(N)        | Nm  OR Nmins OR Nminute OR Nminutes                       |
                     ---------------------------------------------------------------------------------------------------
                    | Seconds           | SECONDS(N)        | Ns  OR Nsec OR Nsecs OR Nsecond OR Nseconds               |
                     ---------------------------------------------------------------------------------------------------
                    | Milliseconds      | MILLISECONDS(N)   | Nms OR Nmsec OR Nmsecs OR Nmillisecond OR Nmilliseconds   |
                     ---------------------------------------------------------------------------------------------------
                    | Microseconds      | MICROSECONDS(N)   | Nus OR Nusec OR Nusecs OR Nmicrosecond OR Nmicroseconds   |
                     ===================================================================================================
                    Where, N is a 16-bit positive integer with a maximum value of 32767.
                Types: str
                Example: MINUTES(23) which is equal to 23 Minutes
                         CAL_MONTHS(5) which is equal to 5 calendar months

            timebucket_duration_arg_name:
                Optional Argument.
                Specifies the name of the timebucket_duration argument used in exposed API.
                Default Values: "timebucket_duration"
                Types: str

            value_expression:
                Optional Argument.
                Specifies a column or any expression involving columns (except for scalar subqueries).
                These expressions are used for grouping purposes not related to time.
                Types: str or List of Strings
                Example: col1 or ["col1", "col2"]

            timecode_column:
                Optional Argument.
                Specifies a column expression that serves as the timecode for a non-PTI table.
                TD_TIMECODE is used implicitly for PTI tables, but can also be specified
                explicitly by the user with this parameter.
                Types: str

            timecode_column_arg_name:
                Optional Argument.
                Specifies the name of the timecode_column argument used in exposed API.
                Default Values: "timecode_column"
                Types: str

            sequence_column:
                Optional Argument.
                Specifies a column expression (with an optional table name) that is the sequence number.
                For a PTI table, it can be TD_SEQNO or any other column that acts as a sequence number.
                For a non-PTI table, sequence_column is a column that plays the role of TD_SEQNO
                (because non-PTI tables do not have TD_SEQNO).
                Types: str

            fill:
                Optional Argument.
                Specifies values for missing timebucket values.
                Below is the description for the accepted values:
                    NULLS:
                        The missing timebuckets are returned to the user with a null value for all
                        aggregate results.

                    numeric_constant:
                        Any Teradata Database supported Numeric literal. The missing timebuckets
                        are returned to the user with the specified constant value for all
                        aggregate results. If the data type specified in the fill argument is
                        incompatible with the input data type for an aggregate function,
                        an error is reported.

                    PREVIOUS/PREV:
                        The missing timebuckets are returned to the user with the aggregate results
                        populated by the value of the closest previous timebucket with a non-missing
                        value. If the immediate predecessor of a missing timebucket is also missing,
                        both buckets, and any other immediate predecessors with missing values,
                        are loaded with the first preceding non-missing value. If a missing
                        timebucket has no predecessor with a result (for example, if the
                        timebucket is the first in the series or all the preceding timebuckets in
                        the entire series are missing), the missing timebuckets are returned to the
                        user with a null value for all aggregate results. The abbreviation PREV may
                        be used instead of PREVIOUS.

                    NEXT:
                        The missing timebuckets are returned to the user with the aggregate results populated
                        by the value of the closest succeeding timebucket with a non-missing value. If the
                        immediate successor of a missing timebucket is also missing, both buckets, and any
                        other immediate successors with missing values, are loaded with the first succeeding
                        non-missing value. If a missing timebucket has no successor with a result
                        (for example, if the timebucket is the last in the series or all the succeeding
                        timebuckets in the entire series are missing), the missing timebuckets are returned
                        to the user with a null value for all aggregate results.

                Permitted values: NULLS, PREV / PREVIOUS, NEXT, and any numeric_constant
                Types: str or int or float

            fill_arg_name:
                Optional Argument.
                Specifies the name of the fill argument used in exposed API.
                Default Values: "fill"
                Types: str

        NOTES:
            Users can still apply teradataml DataFrame methods (filters/sort/etc) on top of the result.

        RETURNS:
            teradataml DataFrameGroupBy Object

        RAISES:
            TypeError, ValueError, TeradataMLException

        EXAMPLES:
            self.__group_time_series_data(timebucket_duration=timebucket_duration, value_expression=value_expression,
                                             timecode_column = timecode_column, sequence_column = sequence_column,
                                             fill = fill)
        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append([timebucket_duration_arg_name, timebucket_duration, False, str, True])
        awu_matrix.append(["value_expression", value_expression, True, (str, list), True])
        awu_matrix.append([timecode_column_arg_name, timecode_column, True, str, True])
        awu_matrix.append(["sequence_column", sequence_column, True, str, True])
        awu_matrix.append([fill_arg_name, fill, True, (float, int, str), True])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        # Checking each element in passed columns to be valid column in dataframe
        _Validators._validate_column_exists_in_dataframe(value_expression, self._metaexpr)
        _Validators._validate_column_exists_in_dataframe(timecode_column, self._metaexpr)
        _Validators._validate_column_exists_in_dataframe(sequence_column, self._metaexpr)

        # Validate Permitted values for fills.
        if isinstance(fill, str):
            _Validators._validate_permitted_values(fill, ["NULLS", "PREV", "PREVIOUS", "NEXT", "any numeric_constant"],
                                                   fill_arg_name)

        # Validate the types passed to timecode column and sequence column.
        if timecode_column is not None:
            _Validators._validate_column_type(self, timecode_column, timecode_column_arg_name,
                                              PTITableConstants.VALID_TIMECODE_DATATYPES.value)
        if sequence_column is not None:
            _Validators._validate_column_type(self, sequence_column, 'sequence_column',
                                              PTITableConstants.VALID_SEQUENCE_COL_DATATYPES.value)
            _Validators._validate_dependent_argument("sequence_column", sequence_column,
                                                     timecode_column_arg_name, timecode_column)

        # Validate for consecutive groupby operations, if consecutive, raise error.
        if isinstance(self, DataFrameGroupByTime) or isinstance(self, DataFrameGroupBy):
            raise TeradataMlException(Messages.get_message(MessageCodes.UNSUPPORTED_OPERATION),
                                      MessageCodes.UNSUPPORTED_OPERATION)

        # Validate the values passed to timebucket duration
        if timebucket_duration.strip() == "*":
            # 'timebucket_duration' set to '*' is also allowed for 'delta_t' time series aggregate function.
            pass
        else:
            # If 'timebucket_duration' is set to anything other than '*', we should validate it's values.
            _Validators._validate_timebucket_duration(timebucket_duration, timebucket_duration_arg_name)

        # Check the timebucket_duration, is it in shorthand notation or formal notation
        if timebucket_duration[0].isdigit():
            # timebucket_duration value is provided in shorthand format.
            # Let's convert it to formal notation, so that column name can
            # be easily retrieved.
            # Even though we use shorthand notation in here, generated column name contains
            # formal notation.
            short_to_formal_mapper = PTITableConstants.TIMEBUCKET_DURATION_FORMAT_MAPPER.value
            timebucket_duration_formal = short_to_formal_mapper[re.sub(r'\d', '', timebucket_duration)]
            n = re.match(r'\d+', timebucket_duration).group(0)
            timebucket_duration = timebucket_duration_formal.format(str(n))

        try:
            unsupported_types = ['BLOB', 'CLOB', 'PERIOD_DATE', 'PERIOD_TIME', 'PERIOD_TIMESTAMP', 'ARRAY', 'VARRAY',
                                 'XML', 'JSON']
            type_expr = []
            invalid_types = []

            group_by_column_list = ["TIMECODE_RANGE"]
            if timebucket_duration.strip() != "*":
                group_by_column_list.append("GROUP BY TIME({})".format(timebucket_duration))
            else:
                group_by_column_list.append("GROUP BY TIME( * )")

            # If fill is a numeric constant, then convert it to a string as AED required it in string format.
            if isinstance(fill, (int, float)):
                fill = str(fill)

            if value_expression is not None:
                if isinstance(value_expression, str):
                    value_expression = [value_expression]

                # Column used in sequence_column should not appear in value_expression,
                # which is part of GROUP BY TIME column list
                if sequence_column is not None and sequence_column in value_expression:
                    # ARG_VALUE_INTERSECTION_NOT_ALLOWED
                    raise TeradataMlException(Messages.get_message(MessageCodes.ARG_VALUE_INTERSECTION_NOT_ALLOWED,
                                                                   sequence_column, "sequence_column",
                                                                   "value_expression"),
                                              MessageCodes.ARG_VALUE_INTERSECTION_NOT_ALLOWED)

                # Column used in timecode_column should not appear in value_expression,
                # which is part of GROUP BY TIME column list
                if timecode_column is not None and timecode_column in value_expression:
                    # ARG_VALUE_INTERSECTION_NOT_ALLOWED
                    raise TeradataMlException(Messages.get_message(MessageCodes.ARG_VALUE_INTERSECTION_NOT_ALLOWED,
                                                                   timecode_column, timecode_column_arg_name,
                                                                   "value_expression"),
                                              MessageCodes.ARG_VALUE_INTERSECTION_NOT_ALLOWED)

                # Checking each element in columns_expr to be valid column in dataframe
                for col in value_expression:
                    type_expr.append(self._metaexpr.t.c[col].type)
                    # Add column from value_expression to group_by_column_list
                    group_by_column_list.append(col)

                # Convert types to string from sqlalchemy type
                columns_types = [repr(type_expr[i]).split("(")[0] for i in range(len(type_expr))]

                # Checking each element in passed columns_types to be valid a data type for group by time
                # and create a list of invalid_types
                for col_type in columns_types:
                    if col_type in unsupported_types:
                        invalid_types.append(col_type)

                if len(invalid_types) > 0:
                    raise TeradataMlException(Messages.get_message(MessageCodes.UNSUPPORTED_DATATYPE, invalid_types,
                                                                   "ANY, except following {}".format(
                                                                       unsupported_types)),
                                              MessageCodes.UNSUPPORTED_DATATYPE)

                groupby_column_expr = ', '.join(UtilFuncs._teradata_quote_arg(col, "\"", False)
                                                for col in value_expression)
            else:
                groupby_column_expr = None

            groupbyObj = DataFrameGroupByTime(nodeid=self._nodeid, metaexpr=self._metaexpr,
                                              column_names_and_types=self._column_names_and_types, columns=self.columns,
                                              groupby_value_expr=groupby_column_expr,
                                              column_list=group_by_column_list, timebucket_duration=timebucket_duration,
                                              value_expression=value_expression, timecode_column=timecode_column,
                                              sequence_column=sequence_column, fill=fill)
            return groupbyObj
        except TeradataMlException:
            raise

    @collect_queryband(queryband="DF_groupbyTime")
    def groupby_time(self, timebucket_duration, value_expression=None, timecode_column=None, sequence_column=None,
                     fill=None):
        """
        DESCRIPTION:
            Apply Group By Time to one or more columns of a teradataml DataFrame.
            The result always behaves like calling group by time. Outcome of this function
            can be used to run Time Series Aggregate functions.

        PARAMETERS:
            timebucket_duration:
                Required Argument.
                Specifies the time unit duration of each timebucket for aggregation and is used to
                assign each potential timebucket a unique number.
                Permitted Values:
                     ===================================================================================================
                    | Time Units        | Formal Form       | Shorthand Equivalents for time_units                      |
                     ===================================================================================================
                    | Calendar Years    | CAL_YEARS(N)      | Ncy OR Ncyear OR Ncyears                                  |
                     ---------------------------------------------------------------------------------------------------
                    | Calendar Months   | CAL_MONTHS(N)     | Ncm OR Ncmonth OR Ncmonths                                |
                     ---------------------------------------------------------------------------------------------------
                    | Calendar Days     | CAL_DAYS(N)       | Ncd OR Ncday OR Ncdays                                    |
                     ---------------------------------------------------------------------------------------------------
                    | Weeks             | WEEKS(N)          | Nw  OR Nweek OR Nweeks                                    |
                     ---------------------------------------------------------------------------------------------------
                    | Days              | DAYS(N)           | Nd  OR Nday OR Ndays                                      |
                     ---------------------------------------------------------------------------------------------------
                    | Hours             | HOURS(N)          | Nh  OR Nhr OR Nhrs OR Nhour OR Nhours                     |
                     ---------------------------------------------------------------------------------------------------
                    | Minutes           | MINUTES(N)        | Nm  OR Nmins OR Nminute OR Nminutes                       |
                     ---------------------------------------------------------------------------------------------------
                    | Seconds           | SECONDS(N)        | Ns  OR Nsec OR Nsecs OR Nsecond OR Nseconds               |
                     ---------------------------------------------------------------------------------------------------
                    | Milliseconds      | MILLISECONDS(N)   | Nms OR Nmsec OR Nmsecs OR Nmillisecond OR Nmilliseconds   |
                     ---------------------------------------------------------------------------------------------------
                    | Microseconds      | MICROSECONDS(N)   | Nus OR Nusec OR Nusecs OR Nmicrosecond OR Nmicroseconds   |
                     ===================================================================================================
                    Where, N is a 16-bit positive integer with a maximum value of 32767.

                    Notes:
                        1. When timebucket_duration is Calendar Days, it will group the columns in
                           24 hour periods starting at 00:00:00.000000 and ending at 23:59:59.999999 on
                           the day identified by time zero.
                        2. A DAYS time unit is a 24 hour span relative to any moment in time.
                           For example,
                                If time zero (in teradataml DataFraame created on PTI tables) equal to
                                2016-10-01 12:00:00, the day buckets are:
                                    2016-10-01 12:00:00.000000 - 2016-10-02 11:59:59.999999.
                                This spans multiple calendar days, but encompasses one 24 hour period
                                representative of a day.
                        3. The time units do not store values such as the year or the month.
                           For example,
                                CAL_YEARS(2017) does not set the year to 2017. It sets the timebucket_duration
                                to intervals of 2017 years. Similarly, CAL_MONTHS(7) does not set the month to
                                July. It sets the timebucket_duration to intervals of 7 months.
                Types: str
                Example: MINUTES(23) which is equal to 23 Minutes
                         CAL_MONTHS(5) which is equal to 5 calendar months

            value_expression:
                Optional Argument.
                Specifies a column used for grouping purposes not related to time.
                Types: str or List of Strings
                Example: col1 or ["col1", "col2"]

            timecode_column:
                Optional Argument.
                Specifies a column that serves as the timecode for a non-PTI table. This is the column
                used for resampling time series data.
                For teradataml DataFrame created on PTI table:
                    TD_TIMECODE is used implicitly for PTI tables, but can also be specified explicitly
                    by the user with this parameter.
                For teradataml DataFrame created on non-PTI table:
                    You must pass column name to this argument for teradataml DataFrame created on non-PTI table,
                    otherwise an exception is raised.

            sequence_column:
                Optional Argument.
                Specifies a column that is the sequence number.
                For teradataml DataFrame created on PTI table:
                    It can be TD_SEQNO or any other column that acts as a sequence number.
                For teradataml DataFrame created on non-PTI table:
                    sequence_column is a column that plays the role of TD_SEQNO, because non-PTI
                    tables do not have TD_SEQNO.
                Types: str

            fill:
                Optional Argument.
                Specifies values for missing timebucket values.
                Permitted values: NULLS, PREV / PREVIOUS, NEXT, and any numeric_constant
                    NULLS:
                        The missing timebuckets are returned to the user with a null value for all
                        aggregate results.

                    numeric_constant:
                        Any Teradata Database supported Numeric literal. The missing timebuckets
                        are returned to the user with the specified constant value for all
                        aggregate results. If the data type specified in the fill argument is
                        incompatible with the input data type for an aggregate function,
                        an error is reported.

                    PREVIOUS/PREV:
                        The missing timebuckets are returned to the user with the aggregate results
                        populated by the value of the closest previous timebucket with a non-missing
                        value. If the immediate predecessor of a missing timebucket is also missing,
                        both buckets, and any other immediate predecessors with missing values,
                        are loaded with the first preceding non-missing value. If a missing
                        timebucket has no predecessor with a result (for example, if the
                        timebucket is the first in the series or all the preceding timebuckets in
                        the entire series are missing), the missing timebuckets are returned to the
                        user with a null value for all aggregate results. The abbreviation PREV may
                        be used instead of PREVIOUS.

                    NEXT:
                        The missing timebuckets are returned to the user with the aggregate results populated
                        by the value of the closest succeeding timebucket with a non-missing value. If the
                        immediate successor of a missing timebucket is also missing, both buckets, and any
                        other immediate successors with missing values, are loaded with the first succeeding
                        non-missing value. If a missing timebucket has no successor with a result
                        (for example, if the timebucket is the last in the series or all the succeeding
                        timebuckets in the entire series are missing), the missing timebuckets are returned
                        to the user with a null value for all aggregate results.

                Types: str or int or float

        NOTES:
            * This API is similar to resample().
            * Users can still apply teradataml DataFrame methods (filters/sort/etc) on top of the result.
            * Consecutive operations of grouping, i.e., groupby_time(), resample() and groupby() are not permitted.
               An exception will be raised. Following are some cases where exception will be raised as
               "Invalid operation applied, check documentation for correct usage."
                    a. df.groupby_time().groupby()
                    b. df.groupby_time().resample()
                    c. df.groupby_time().groupby_time()
            * This method does not support operations on array columns.

        RETURNS:
            teradataml DataFrameGroupBy Object

        RAISES:
            TypeError, ValueError, TeradataMLException

        EXAMPLES:
            >>> # Load the example datasets.
            ... load_example_data("dataframe", ["ocean_buoys", "ocean_buoys_nonpti"])
            >>>

            >>> # Create the required DataFrames.
            ... # DataFrame on non-sequenced PTI table
            ... ocean_buoys = DataFrame("ocean_buoys")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys.columns
            ['buoyid', 'TD_TIMECODE', 'temperature', 'salinity']
            >>> ocean_buoys.head()
                                   TD_TIMECODE  temperature  salinity
            buoyid
            0       2014-01-06 08:10:00.000000        100.0        55
            0       2014-01-06 08:08:59.999999          NaN        55
            1       2014-01-06 09:01:25.122200         77.0        55
            1       2014-01-06 09:03:25.122200         79.0        55
            1       2014-01-06 09:01:25.122200         70.0        55
            1       2014-01-06 09:02:25.122200         71.0        55
            1       2014-01-06 09:03:25.122200         72.0        55
            0       2014-01-06 08:09:59.999999         99.0        55
            0       2014-01-06 08:00:00.000000         10.0        55
            0       2014-01-06 08:10:00.000000         10.0        55

            >>> # DataFrame on NON-PTI table
            ... ocean_buoys_nonpti = DataFrame("ocean_buoys_nonpti")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys_nonpti.columns
            ['buoyid', 'timecode', 'temperature', 'salinity']
            >>> ocean_buoys_nonpti.head()
                                        buoyid  temperature  salinity
            timecode
            2014-01-06 08:09:59.999999       0         99.0        55
            2014-01-06 08:10:00.000000       0         10.0        55
            2014-01-06 09:01:25.122200       1         70.0        55
            2014-01-06 09:01:25.122200       1         77.0        55
            2014-01-06 09:02:25.122200       1         71.0        55
            2014-01-06 09:03:25.122200       1         72.0        55
            2014-01-06 09:02:25.122200       1         78.0        55
            2014-01-06 08:10:00.000000       0        100.0        55
            2014-01-06 08:08:59.999999       0          NaN        55
            2014-01-06 08:00:00.000000       0         10.0        55

            #
            # Example 1: Group by timebucket of 2 calendar years, using formal notation and buoyid column on
            #            DataFrame created on non-sequenced PTI table.
            #            Fill missing values with Nulls.
            #
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="CAL_YEARS(2)",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> number_of_values_to_column = {2: "temperature"}
            >>> ocean_buoys_grpby1.bottom(number_of_values_to_column).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid  bottom2temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0                  10
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0                  10
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1                  71
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1                  70
            4  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2                  80
            5  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2                  81
            6  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44                  43
            7  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44                  43
            >>>

            #
            # Example 2: Group by timebucket of 2 minutes, using shorthand notation to specify timebucket,
            #            on DataFrame created on non-PTI table. Fill missing values with Nulls.
            #            Time column must be specified for non-PTI table.
            #
            >>> ocean_buoys_nonpti_grpby2 = ocean_buoys_nonpti.groupby_time(timebucket_duration="2m",
            ...                                                             value_expression="buoyid",
            ...                                                             timecode_column="timecode", fill="NULLS")
            >>> number_of_values_to_column = {2: "temperature"}
            >>> ocean_buoys_nonpti_grpby2.bottom(number_of_values_to_column, with_ties=True).sort(["TIMECODE_RANGE",
            ...                                                                                    "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(MINUTES(2))  buoyid  bottom_with_ties2temperature
            0  ('2014-01-06 08:00:00.000000+00:00', '2014-01-...                   11574961       0                          10.0
            1  ('2014-01-06 08:02:00.000000+00:00', '2014-01-...                   11574962       0                           NaN
            2  ('2014-01-06 08:04:00.000000+00:00', '2014-01-...                   11574963       0                           NaN
            3  ('2014-01-06 08:06:00.000000+00:00', '2014-01-...                   11574964       0                           NaN
            4  ('2014-01-06 08:08:00.000000+00:00', '2014-01-...                   11574965       0                          99.0
            5  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                   11574966       0                         100.0
            6  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                   11574966       0                          10.0
            7  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                   11574991       1                          70.0
            8  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                   11574991       1                          77.0
            9  ('2014-01-06 09:02:00.000000+00:00', '2014-01-...                   11574992       1                          71.0
            >>>

            #
            # Example 3: Group by timebucket of 2 minutes, using shorthand notation to specify timebucket,
            #            on DataFrame created on non-PTI table. Fill missing values with previous values.
            #            Time column must be specified for non-PTI table.
            #
            >>> ocean_buoys_nonpti_grpby2 = ocean_buoys_nonpti.groupby_time(timebucket_duration="2mins",
            ...                                                             value_expression="buoyid",
            ...                                                             timecode_column="timecode", fill="prev")
            >>> number_of_values_to_column = {2: "temperature"}
            >>> ocean_buoys_nonpti_grpby2.bottom(number_of_values_to_column, with_ties=True).sort(["TIMECODE_RANGE",
            ...                                                                                    "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(MINUTES(2))  buoyid  bottom_with_ties2temperature
            0  ('2014-01-06 08:00:00.000000+00:00', '2014-01-...                   11574961       0                            10
            1  ('2014-01-06 08:02:00.000000+00:00', '2014-01-...                   11574962       0                            10
            2  ('2014-01-06 08:04:00.000000+00:00', '2014-01-...                   11574963       0                            10
            3  ('2014-01-06 08:06:00.000000+00:00', '2014-01-...                   11574964       0                            10
            4  ('2014-01-06 08:08:00.000000+00:00', '2014-01-...                   11574965       0                            99
            5  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                   11574966       0                            10
            6  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                   11574966       0                           100
            7  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                   11574991       1                            77
            8  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                   11574991       1                            70
            9  ('2014-01-06 09:02:00.000000+00:00', '2014-01-...                   11574992       1                            71

            #
            # Example 4: Group by timebucket of 2 minutes, using shorthand notation to specify timebucket,
            #            on DataFrame created on non-PTI table. Fill missing values with numeric constant 12345.
            #            Time column must be specified for non-PTI table.
            #
            >>> ocean_buoys_nonpti_grpby2 = ocean_buoys_nonpti.groupby_time(timebucket_duration="2minute",
            ...                                                             value_expression="buoyid",
            ...                                                             timecode_column="timecode", fill=12345)
            >>> number_of_values_to_column = {2: "temperature"}
            >>> ocean_buoys_nonpti_grpby2.bottom(number_of_values_to_column, with_ties=True).sort(["TIMECODE_RANGE",
            ...                                                                                    "buoyid"])

                                                  TIMECODE_RANGE  GROUP BY TIME(MINUTES(2))  buoyid  bottom_with_ties2temperature
            0  ('2014-01-06 08:00:00.000000+00:00', '2014-01-...                   11574961       0                            10
            1  ('2014-01-06 08:02:00.000000+00:00', '2014-01-...                   11574962       0                         12345
            2  ('2014-01-06 08:04:00.000000+00:00', '2014-01-...                   11574963       0                         12345
            3  ('2014-01-06 08:06:00.000000+00:00', '2014-01-...                   11574964       0                         12345
            4  ('2014-01-06 08:08:00.000000+00:00', '2014-01-...                   11574965       0                            99
            5  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                   11574966       0                            10
            6  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                   11574966       0                           100
            7  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                   11574991       1                            77
            8  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                   11574991       1                            70
            9  ('2014-01-06 09:02:00.000000+00:00', '2014-01-...                   11574992       1                            71
            >>>

        """
        return self.__group_time_series_data(timebucket_duration=timebucket_duration, value_expression=value_expression,
                                             timecode_column=timecode_column, sequence_column=sequence_column,
                                             fill=fill)

    @collect_queryband(queryband="DF_resample")
    def resample(self, rule, value_expression=None, on=None, sequence_column=None,
                 fill_method=None):
        """
        DESCRIPTION:
            Resample time series data. This function allows grouping done by time on
            a datetime column of a teradataml DataFrame. Outcome of this function
            can be used to run Time Series Aggregate functions.

        PARAMETERS:
            rule:
                Required Argument.
                Specifies the time unit duration/interval of each timebucket for resampling and is used to
                assign each potential timebucket a unique number.
                Permitted Values:
                     ===================================================================================================
                    | Time Units        | Formal Form       | Shorthand Equivalents for time_units                      |
                     ===================================================================================================
                    | Calendar Years    | CAL_YEARS(N)      | Ncy OR Ncyear OR Ncyears                                  |
                     ---------------------------------------------------------------------------------------------------
                    | Calendar Months   | CAL_MONTHS(N)     | Ncm OR Ncmonth OR Ncmonths                                |
                     ---------------------------------------------------------------------------------------------------
                    | Calendar Days     | CAL_DAYS(N)       | Ncd OR Ncday OR Ncdays                                    |
                     ---------------------------------------------------------------------------------------------------
                    | Weeks             | WEEKS(N)          | Nw  OR Nweek OR Nweeks                                    |
                     ---------------------------------------------------------------------------------------------------
                    | Days              | DAYS(N)           | Nd  OR Nday OR Ndays                                      |
                     ---------------------------------------------------------------------------------------------------
                    | Hours             | HOURS(N)          | Nh  OR Nhr OR Nhrs OR Nhour OR Nhours                     |
                     ---------------------------------------------------------------------------------------------------
                    | Minutes           | MINUTES(N)        | Nm  OR Nmins OR Nminute OR Nminutes                       |
                     ---------------------------------------------------------------------------------------------------
                    | Seconds           | SECONDS(N)        | Ns  OR Nsec OR Nsecs OR Nsecond OR Nseconds               |
                     ---------------------------------------------------------------------------------------------------
                    | Milliseconds      | MILLISECONDS(N)   | Nms OR Nmsec OR Nmsecs OR Nmillisecond OR Nmilliseconds   |
                     ---------------------------------------------------------------------------------------------------
                    | Microseconds      | MICROSECONDS(N)   | Nus OR Nusec OR Nusecs OR Nmicrosecond OR Nmicroseconds   |
                     ===================================================================================================
                    Where, N is a 16-bit positive integer with a maximum value of 32767.

                    Notes:
                        1. When timebucket_duration is Calendar Days, it will group the columns in
                           24 hour periods starting at 00:00:00.000000 and ending at 23:59:59.999999 on
                           the day identified by time zero.
                        2. A DAYS time unit is a 24 hour span relative to any moment in time.
                           For example,
                                If time zero (in teradataml DataFraame created on PTI tables) equal to
                                2016-10-01 12:00:00, the day buckets are:
                                    2016-10-01 12:00:00.000000 - 2016-10-02 11:59:59.999999.
                                This spans multiple calendar days, but encompasses one 24 hour period
                                representative of a day.
                        3. The time units do not store values such as the year or the month.
                           For example,
                                CAL_YEARS(2017) does not set the year to 2017. It sets the timebucket_duration
                                to intervals of 2017 years. Similarly, CAL_MONTHS(7) does not set the month to
                                July. It sets the timebucket_duration to intervals of 7 months.

                Types: str
                Example: MINUTES(23) which is equal to 23 Minutes
                         CAL_MONTHS(5) which is equal to 5 calendar months

            value_expression:
                Optional Argument.
                Specifies a column used for grouping purposes not related to time.
                Types: str or List of Strings
                Example: col1 or ["col1", "col2"]

            on:
                Optional Argument.
                Specifies a column that serves as the timecode for a non-PTI table. This is the column
                used for resampling time series data.
                For teradataml DataFrame created on PTI table:
                    TD_TIMECODE is used implicitly for PTI tables, but can also be specified explicitly
                    by the user with this parameter.
                For teradataml DataFrame created on non-PTI table:
                    Column must be specified to this argument if DataFrame is created on non-PTI table,
                    otherwise, an exception is raised.
                Types: str

            sequence_column:
                Optional Argument.
                Specifies a column that is the sequence number.
                For teradataml DataFrame created on PTI table:
                    It can be TD_SEQNO or any other column that acts as a sequence number.
                For teradataml DataFrame created on non-PTI table:
                    sequence_column is a column that plays the role of TD_SEQNO, because non-PTI
                    tables do not have TD_SEQNO.
                Types: str

            fill_method:
                Optional Argument.
                Specifies values for missing timebucket values.
                Permitted values: NULLS, PREV / PREVIOUS, NEXT, and any numeric_constant
                    NULLS:
                        The missing timebuckets are returned to the user with a null value for all
                        aggregate results.

                    numeric_constant:
                        Any Teradata Database supported Numeric literal. The missing timebuckets
                        are returned to the user with the specified constant value for all
                        aggregate results. If the data type specified in the fill_method argument is
                        incompatible with the input data type for an aggregate function,
                        an error is reported.

                    PREVIOUS/PREV:
                        The missing timebuckets are returned to the user with the aggregate results
                        populated by the value of the closest previous timebucket with a non-missing
                        value. If the immediate predecessor of a missing timebucket is also missing,
                        both buckets, and any other immediate predecessors with missing values,
                        are loaded with the first preceding non-missing value. If a missing
                        timebucket has no predecessor with a result (for example, if the
                        timebucket is the first in the series or all the preceding timebuckets in
                        the entire series are missing), the missing timebuckets are returned to the
                        user with a null value for all aggregate results. The abbreviation PREV may
                        be used instead of PREVIOUS.

                    NEXT:
                        The missing timebuckets are returned to the user with the aggregate results populated
                        by the value of the closest succeeding timebucket with a non-missing value. If the
                        immediate successor of a missing timebucket is also missing, both buckets, and any
                        other immediate successors with missing values, are loaded with the first succeeding
                        non-missing value. If a missing timebucket has no successor with a result
                        (for example, if the timebucket is the last in the series or all the succeeding
                        timebuckets in the entire series are missing), the missing timebuckets are returned
                        to the user with a null value for all aggregate results.

                Types: str or int or float

        NOTES:
            * This API is similar to groupby_time().
            * Users can still apply teradataml DataFrame methods (filters/sort/etc) on top of the result.
            * Consecutive operations of grouping, i.e., groupby_time(), resample() and groupby() are not permitted.
               An exception will be raised. Following are some cases where exception will be raised as
               "Invalid operation applied, check documentation for correct usage."
                    a. df.resample().groupby()
                    b. df.resample().resample()
                    c. df.resample().groupby_time()
            * This method does not support operations on array columns.

        RETURNS:
            teradataml DataFrameGroupBy Object

        RAISES:
            TypeError, ValueError, TeradataMLException

        EXAMPLES:
            # Load the example datasets.
            >>> load_example_data("dataframe", ["ocean_buoys", "ocean_buoys_nonpti"])
            >>>

            # Create the required DataFrames.
            # DataFrame on non-sequenced PTI table
            >>> ocean_buoys = DataFrame("ocean_buoys")
            # Check DataFrame columns and let's peek at the data
            >>> ocean_buoys.columns
            ['buoyid', 'TD_TIMECODE', 'temperature', 'salinity']
            >>> ocean_buoys.head()
                                   TD_TIMECODE  temperature  salinity
            buoyid
            0       2014-01-06 08:10:00.000000        100.0        55
            0       2014-01-06 08:08:59.999999          NaN        55
            1       2014-01-06 09:01:25.122200         77.0        55
            1       2014-01-06 09:03:25.122200         79.0        55
            1       2014-01-06 09:01:25.122200         70.0        55
            1       2014-01-06 09:02:25.122200         71.0        55
            1       2014-01-06 09:03:25.122200         72.0        55
            0       2014-01-06 08:09:59.999999         99.0        55
            0       2014-01-06 08:00:00.000000         10.0        55
            0       2014-01-06 08:10:00.000000         10.0        55

            # DataFrame on NON-PTI table
            >>> ocean_buoys_nonpti = DataFrame("ocean_buoys_nonpti")
            # Check DataFrame columns and let's peek at the data
            >>> ocean_buoys_nonpti.columns
            ['buoyid', 'timecode', 'temperature', 'salinity']
            >>> ocean_buoys_nonpti.head()
                                        buoyid  temperature  salinity
            timecode
            2014-01-06 08:09:59.999999       0         99.0        55
            2014-01-06 08:10:00.000000       0         10.0        55
            2014-01-06 09:01:25.122200       1         70.0        55
            2014-01-06 09:01:25.122200       1         77.0        55
            2014-01-06 09:02:25.122200       1         71.0        55
            2014-01-06 09:03:25.122200       1         72.0        55
            2014-01-06 09:02:25.122200       1         78.0        55
            2014-01-06 08:10:00.000000       0        100.0        55
            2014-01-06 08:08:59.999999       0          NaN        55
            2014-01-06 08:00:00.000000       0         10.0        55

            #
            # Example 1: Resample by timebucket of 2 calendar years, using formal notation and buoyid column on
            #            DataFrame created on non-sequenced PTI table.
            #            Fill missing values with Nulls.
            #
            >>> ocean_buoys_grpby1 = ocean_buoys.resample(rule="CAL_YEARS(2)", value_expression="buoyid", fill_method="NULLS")
            >>> number_of_values_to_column = {2: "temperature"}
            >>> ocean_buoys_grpby1.bottom(number_of_values_to_column).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid  bottom2temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0                  10
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0                  10
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1                  71
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1                  70
            4  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2                  80
            5  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2                  81
            6  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44                  43
            7  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44                  43
            >>>

            #
            # Example 2: Resample data by timebucket of 2 minutes, using shorthand notation to specify timebucket,
            #            on DataFrame created on non-PTI table. Fill missing values with Nulls.
            #            Time column must be specified for non-PTI table using 'on' argument.
            #
            >>> ocean_buoys_nonpti_grpby2 = ocean_buoys_nonpti.resample(rule="2m", value_expression="buoyid",
            ...                                                         on="timecode", fill_method="NULLS")
            >>> number_of_values_to_column = {2: "temperature"}
            >>> ocean_buoys_nonpti_grpby2.bottom(number_of_values_to_column, with_ties=True).sort(["TIMECODE_RANGE",
            ...                                                                                    "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(MINUTES(2))  buoyid  bottom_with_ties2temperature
            0  ('2014-01-06 08:00:00.000000+00:00', '2014-01-...                   11574961       0                          10.0
            1  ('2014-01-06 08:02:00.000000+00:00', '2014-01-...                   11574962       0                           NaN
            2  ('2014-01-06 08:04:00.000000+00:00', '2014-01-...                   11574963       0                           NaN
            3  ('2014-01-06 08:06:00.000000+00:00', '2014-01-...                   11574964       0                           NaN
            4  ('2014-01-06 08:08:00.000000+00:00', '2014-01-...                   11574965       0                          99.0
            5  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                   11574966       0                         100.0
            6  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                   11574966       0                          10.0
            7  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                   11574991       1                          70.0
            8  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                   11574991       1                          77.0
            9  ('2014-01-06 09:02:00.000000+00:00', '2014-01-...                   11574992       1                          71.0
            >>>

            #
            # Example 3: Resample time series data by timebucket of 2 minutes, using shorthand notation to specify
            #            timebucket, on teradataml DataFrame created on non-PTI table. Fill missing values with
            #            previous values. Time column must be specified for non-PTI table using 'on' argument.
            #
            >>> ocean_buoys_nonpti_grpby2 = ocean_buoys_nonpti.resample(rule="2mins", value_expression="buoyid",
            ...                                                         on="timecode", fill_method="prev")
            >>> number_of_values_to_column = {2: "temperature"}
            >>> ocean_buoys_nonpti_grpby2.bottom(number_of_values_to_column, with_ties=True).sort(["TIMECODE_RANGE",
            ...                                                                                    "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(MINUTES(2))  buoyid  bottom_with_ties2temperature
            0  ('2014-01-06 08:00:00.000000+00:00', '2014-01-...                   11574961       0                            10
            1  ('2014-01-06 08:02:00.000000+00:00', '2014-01-...                   11574962       0                            10
            2  ('2014-01-06 08:04:00.000000+00:00', '2014-01-...                   11574963       0                            10
            3  ('2014-01-06 08:06:00.000000+00:00', '2014-01-...                   11574964       0                            10
            4  ('2014-01-06 08:08:00.000000+00:00', '2014-01-...                   11574965       0                            99
            5  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                   11574966       0                            10
            6  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                   11574966       0                           100
            7  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                   11574991       1                            77
            8  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                   11574991       1                            70
            9  ('2014-01-06 09:02:00.000000+00:00', '2014-01-...                   11574992       1                            71

            #
            # Example 4: Resample time series data by timebucket of 2 minutes, using shorthand notation to specify
            #            timebucket, on teradataml DataFrame created on non-PTI table. Fill missing values with
            #            numeric 12345. Time column must be specified for non-PTI table using 'on' argument.
            #
            >>> ocean_buoys_nonpti_grpby2 = ocean_buoys_nonpti.resample(rule="2minute", value_expression="buoyid",
            ...                                                         on="timecode", fill_method=12345)
            >>> number_of_values_to_column = {2: "temperature"}
            >>> ocean_buoys_nonpti_grpby2.bottom(number_of_values_to_column, with_ties=True).sort(["TIMECODE_RANGE",
            ...                                                                                    "buoyid"])

                                                  TIMECODE_RANGE  GROUP BY TIME(MINUTES(2))  buoyid  bottom_with_ties2temperature
            0  ('2014-01-06 08:00:00.000000+00:00', '2014-01-...                   11574961       0                            10
            1  ('2014-01-06 08:02:00.000000+00:00', '2014-01-...                   11574962       0                         12345
            2  ('2014-01-06 08:04:00.000000+00:00', '2014-01-...                   11574963       0                         12345
            3  ('2014-01-06 08:06:00.000000+00:00', '2014-01-...                   11574964       0                         12345
            4  ('2014-01-06 08:08:00.000000+00:00', '2014-01-...                   11574965       0                            99
            5  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                   11574966       0                            10
            6  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                   11574966       0                           100
            7  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                   11574991       1                            77
            8  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                   11574991       1                            70
            9  ('2014-01-06 09:02:00.000000+00:00', '2014-01-...                   11574992       1                            71
            >>>

        """
        return self.__group_time_series_data(timebucket_duration=rule, timebucket_duration_arg_name="rule",
                                             value_expression=value_expression, timecode_column_arg_name="on",
                                             timecode_column=on, sequence_column=sequence_column,
                                             fill=fill_method, fill_arg_name="fill_method")

    @collect_queryband(queryband="DF_getValues")
    def get_values(self, num_rows=99999):
        """
        DESCRIPTION:
            Retrieves all values (only) present in a teradataml DataFrame.
            Values are retrieved as per a numpy.ndarray representation of a teradataml DataFrame.
            This format is equivalent to the get_values() representation of a Pandas DataFrame.

        PARAMETERS:
            num_rows:
                Optional Argument.
                Specifies the number of rows to retrieve values for from a teradataml DataFrame.
                The num_rows parameter specified needs to be an integer value.
                Default Value: 99999
                Types: int

        RETURNS:
            Numpy.ndarray representation of a teradataml DataFrame

        RAISES:
            TeradataMlException

        EXAMPLES:
            >>> load_example_data("dataframe","admissions_train")
            >>> df1 = DataFrame.from_table('admissions_train')
            >>> df1
               masters   gpa     stats programming admitted
            id
            15     yes  4.00  Advanced    Advanced        1
            7      yes  2.33    Novice      Novice        1
            22     yes  3.46    Novice    Beginner        0
            17      no  3.83  Advanced    Advanced        1
            13      no  4.00  Advanced      Novice        1
            38     yes  2.65  Advanced    Beginner        1
            26     yes  3.57  Advanced    Advanced        1
            5       no  3.44    Novice      Novice        0
            34     yes  3.85  Advanced    Beginner        0
            40     yes  3.95    Novice    Beginner        0

            # Retrieve all values from the teradataml DataFrame

            >>> vals = df1.get_values()
            >>> vals
            array([['yes', 4.0, 'Advanced', 'Advanced', 1],
                   ['yes', 3.45, 'Advanced', 'Advanced', 0],
                   ['yes', 3.5, 'Advanced', 'Beginner', 1],
                   ['yes', 4.0, 'Novice', 'Beginner', 0],
                                 . . .
                   ['no', 3.68, 'Novice', 'Beginner', 1],
                   ['yes', 3.5, 'Beginner', 'Advanced', 1],
                   ['yes', 3.79, 'Advanced', 'Novice', 0],
                   ['no', 3.0, 'Advanced', 'Novice', 0],
                   ['yes', 1.98, 'Advanced', 'Advanced', 0]], dtype=object)

            # Retrieve values for a given number of rows from the teradataml DataFrame

            >>> vals = df1.get_values(num_rows = 3)
            >>> vals
            array([['yes', 4.0, 'Advanced', 'Advanced', 1],
                   ['yes', 3.45, 'Advanced', 'Advanced', 0],
                   ['yes', 3.5, 'Advanced', 'Beginner', 1]], dtype=object)

            # Access specific values from the entire set received as per below:
            # Retrieve all values from an entire row (for example, the first row):

            >>> vals[0]
            array(['yes', 4.0, 'Advanced', 'Advanced', 1], dtype=object)

            # Alternatively, specify a range to retrieve values from  a subset of rows (For example, first 3 rows):

            >>> vals[0:3]
            array([['yes', 4.0, 'Advanced', 'Advanced', 1],
            ['yes', 3.45, 'Advanced', 'Advanced', 0],
            ['yes', 3.5, 'Advanced', 'Beginner', 1]], dtype=object)

            # Alternatively, retrieve all values from an entire column (For example, the first column):

            >>> vals[:, 0]
            array(['yes', 'yes', 'yes', 'yes', 'yes', 'no', 'yes', 'yes', 'yes',
                   'yes', 'no', 'no', 'yes', 'yes', 'no', 'yes', 'no', 'yes', 'no',
                   'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'no', 'no', 'yes',
                   'yes', 'yes', 'no', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no',
                   'yes'], dtype=object)

            # Alternatively, retrieve a single value from a given row and column (For example, 3rd row, and 2nd column):
            >>> vals[2,1]
            3.5

            Note:
            1) Row and column indexing starts from 0, so the first column = index 0, second column = index 1, and so on...

            2) When a Pandas DataFrame is saved to Teradata Vantage & retrieved back as a teradataml DataFrame, the get_values()
               method on a Pandas DataFrame and the corresponding teradataml DataFrames have the following type differences:
                   - teradataml DataFrame get_values() retrieves 'bool' type Pandas DataFrame values (True/False) as BYTEINTS (1/0)
                   - teradataml DataFrame get_values() retrieves 'Timedelta' type Pandas DataFrame values as equivalent values in seconds.

        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["num_rows", num_rows, True, (int)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        # Validate n is a positive int.
        _Validators._validate_positive_int(num_rows, "num_rows")

        return self.to_pandas(self._index_label, num_rows).values

    @property
    def shape(self):
        """
        Returns a tuple representing the dimensionality of the DataFrame.

        PARAMETERS:
            None

        RETURNS:
            Tuple representing the dimensionality of this DataFrame.

        EXAMPLES:
            >>> load_example_data("dataframe","sales")
            >>> df = DataFrame.from_table('sales')
            >>> df
                          Feb   Jan   Mar   Apr  datetime
            accounts
            Orange Inc  210.0  None  None   250  04/01/2017
            Yellow Inc   90.0  None  None  None  04/01/2017
            Red Inc     200.0   150   140  None  04/01/2017
            Blue Inc     90.0    50    95   101  04/01/2017
            Jones LLC   200.0   150   140   180  04/01/2017
            Alpha Co    210.0   200   215   250  04/01/2017
            >>> df.shape
            (6, 6)
            >>>

        RAISES:
            TeradataMlException (TDMLDF_INFO_ERROR)
        """
        try:
            # To know the number of rows in a DF, we need to execute the node
            # Generate/Execute AED nodes
            self.__execute_node_and_set_table_name(self._nodeid)

            # The dimension of the DF is (# of rows, # of columns)
            return df_utils._get_row_count(self._table_name), len(self._column_names_and_types)

        except TeradataMlException:
            raise

        except Exception as err:
            raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_INFO_ERROR),
                                      MessageCodes.TDMLDF_INFO_ERROR) from err

    @property
    def size(self):
        """
        Returns a value representing the number of elements in the DataFrame.

        PARAMETERS:
            None

        RETURNS:
            Value representing the number of elements in the DataFrame.

        EXAMPLES:
            >>> load_example_data("dataframe","sales")
            >>> df = DataFrame.from_table('sales')
            >>> df
                          Feb   Jan   Mar   Apr  datetime
            accounts
            Orange Inc  210.0  None  None   250  04/01/2017
            Yellow Inc   90.0  None  None  None  04/01/2017
            Red Inc     200.0   150   140  None  04/01/2017
            Blue Inc     90.0    50    95   101  04/01/2017
            Jones LLC   200.0   150   140   180  04/01/2017
            Alpha Co    210.0   200   215   250  04/01/2017
            >>> df.size
            36

        RAISES:
            None
        """
        dimension = self.shape
        return dimension[0] * dimension[1]

    @collect_queryband(queryband="DF_merge")
    def merge(self, right, on=None, how="inner", left_on=None, right_on=None, use_index=False,
              lsuffix=None, rsuffix=None):
        """
        DESCRIPTION:
            Merges two teradataml DataFrames together.
         
            Supported merge operations are:
                - cross: Returns cartesian product between the two dataframes.
                - inner: Returns only matching rows, non-matching rows are eliminated.
                - left: Returns all matching rows plus non-matching rows from the left teradataml DataFrame.
                - right: Returns all matching rows plus non-matching rows from the right teradataml DataFrame.
                - full: Returns all rows from both teradataml DataFrames, including non matching rows.
            Note:
                * This method does not support operations on array columns.

        PARAMETERS:
         
            right:
                Required Argument.
                Specifies right teradataml DataFrame on which merge is to be performed.
                Types: teradataml DataFrame
            
            on:
                Optional Argument.
                Specifies list of conditions that indicate the columns used for the merge.
                When no arguments are provided for this condition, the merge is performed using the indexes
                of the teradataml DataFrames. Both teradataml DataFrames are required to have index labels to
                perform a merge operation when no arguments are provided for this condition.
                When either teradataml DataFrame does not have a valid index label in the above case,
                an exception is thrown.
                 String comparisons, in the form of "col1 <= col2", where col1 is
                  the column of left DataFrame df1 and col2 is the column of right
                  DataFrame df2.
                  Examples:
                    1. ["a","b"] indicates df1.a = df2.a and df1.b = df2.b.
                    2. ["a = b", "c = d"] indicates df1.a = df2.b and df1.c = df2.d
                    3. ["a <= b", "c > d"] indicates df1.a <= df2.b and df1.c > df2.d.
                    4. ["a < b", "c >= d"] indicates df1.a < df2.b and df1.c >= df2.d.
                    5. ["a <> b"] indicates df1.a != df2.b. Same is the case for ["a != b"].
                 Column comparisons, in the form of df1.col1 <= df2.col2, where col1
                  is the column of left DataFrame df1 and col2 is the column of right
                  DataFrame df2.
                  Examples:
                    1. [df1.a == df2.a, df1.b == df2.b] indicates df1.a = df2.a and df1.b = df2.b.
                    2. [df1.a == df2.b, df1.c == df2.d] indicates df1.a = df2.b and df1.c = df2.d.
                    3. [df1.a <= df2.b and df1.c > df2.d] indicates df1.a <= df2.b and df1.c > df2.d.
                    4. [df1.a < df2.b and df1.c >= df2.d] indicates df1.a < df2.b and df1.c >= df2.d.
                    5. df1.a != df2.b indicates df1.a != df2.b.
                 The combination of both string comparisons and comparisons as column expressions.
                  Examples:
                    1. ["a", df1.b == df2.b] indicates df1.a = df2.a and df1.b = df2.b.
                    2. [df1.a <= df2.b, "c > d"] indicates df1.a <= df2.b and df1.c > df2.d.
                Default Value: None
                Types: str or ColumnExpression or List of strings(str) or ColumnExpressions

            how:
                Optional Argument.
                Specifies the type of merge to perform. Supports inner, left, right, full and cross merge operations.
                When how is "cross", the arguments on, left_on, right_on and use_index are ignored.
                Default Value: "inner".
                Types: str
                      
            left_on:
                Optional Argument.
                Specifies column to merge on, in the left teradataml DataFrame.
                When both the 'on' and 'left_on' parameters are unspecified, the index columns
                of the teradataml DataFrames are used to perform the merge operation.
                Default Value: None.
                Types: str or ColumnExpression or List of strings(str) or ColumnExpressions
                      
            right_on:
                Optional Argument.
                Specifies column to merge on, in the right teradataml DataFrame.
                When both the 'on' and 'right_on' parameters are unspecified, the index columns
                of the teradataml DataFrames are used to perform the merge operation.
                Default Value: None.
                Types: str or ColumnExpression or List of strings(str) or ColumnExpressions
                       
            use_index:
                Optional Argument.
                Specifies whether (or not) to use index from the teradataml DataFrames as the merge key(s).
                When False, and 'on', 'left_on', and 'right_on' are all unspecified, the index columns
                of the teradataml DataFrames are used to perform the merge operation.
                Default value: False.
                Types: bool
                         
            lsuffix:
                Optional Argument.
                Specifies suffix to be added to the left table columns.
                Default Value: None.
                Types: str
                         
                Note: A suffix is required if teradataml DataFrames being merged have columns
                      with the same name.
                      
            rsuffix:
                Optional Argument.
                Specifies suffix to be added to the right table columns.
                Default Value: None.
                Types: str
                     
                Note: A suffix is required if teradataml DataFrames being merged have columns
                      with the same name.

        RAISES:
            TeradataMlException

        RETURNS:
            teradataml DataFrame

        EXAMPLES:
            
            # Example set-up teradataml DataFrames for merge
            >>> from datetime import datetime, timedelta
            >>> dob = datetime.strptime('31101991', '%d%m%Y').date()
            
            >>> df1 = pd.DataFrame(data={'col1': [1, 2,3],
                           'col2': ['teradata','analytics','platform'],
                           'col3': [1.3, 2.3, 3.3],
                           'col5': ['a','b','c'],
                            'col 6': [dob, dob + timedelta(1), dob + timedelta(2)],
                            "'col8'":[3,4,5]})
            
            >>> df2 = pd.DataFrame(data={'col1': [1, 2, 3],
                                'col4': ['teradata', 'analytics', 'are you'],
                                'col3': [1.3, 2.3, 4.3],
                                 'col7':['a','b','d'],
                                 'col 6': [dob, dob + timedelta(1), dob + timedelta(3)],
                                 "'col8'": [3, 4, 5]})
            >>> # Persist the Pandas DataFrames in Vantage.
            >>> copy_to_sql(df1, "table1", primary_index="col1")
            >>> copy_to_sql(df2, "table2", primary_index="col1")
            >>> df1 = DataFrame("table1")
            >>> df2 = DataFrame("table2")
            >>> df1
                 'col8'       col 6       col2  col3 col5
            col1                                         
            2         4  1991-11-01  analytics   2.3    b
            1         3  1991-10-31   teradata   1.3    a
            3         5  1991-11-02   platform   3.3    c
            >>> df2
                 'col8'       col 6  col3       col4 col7
            col1                                         
            2         4  1991-11-01   2.3  analytics    b
            1         3  1991-10-31   1.3   teradata    a
            3         5  1991-11-03   4.3    are you    d            
            
            >>> # 1) Specify both types of 'on' conditions and DataFrame indexes as merge keys:
            >>> df1.merge(right = df2, how = "left", on = ["col3","col2=col4"], use_index = True, lsuffix = "t1", rsuffix = "t2")
            
              t2_col1 col5    t2_col 6 t1_col1 t2_'col8'  t1_col3       col4  t2_col3  col7       col2    t1_col 6 t1_'col8'
            0       2    b  1991-11-01       2         4      2.3  analytics      2.3     b  analytics  1991-11-01         4
            1       1    a  1991-10-31       1         3      1.3   teradata      1.3     a   teradata  1991-10-31         3
            2    None    c        None       3      None      3.3       None      NaN  None   platform  1991-11-02         5

            >>> # 2) Specify 'on' conditions as ColumnExpression and DataFrame indexes as merge keys:
            >>> df1.merge(right = df2, how = "left", on = [df1.col1, df1.col3], use_index = True, lsuffix = "t1", rsuffix = "t2")

              t1_col1  t2_col1       col2  t1_col3  t2_col3 col5    t1_col 6    t2_col 6  t1_'col8'  t2_'col8'       col4  col7
            0        2      2.0  analytics      2.3      2.3    b  1991-06-23  1991-06-23          4        4.0  analytics     b
            1        1      1.0   teradata      1.3      1.3    a  1991-06-22  1991-06-22          3        3.0   teradata     a
            2        3      NaN   platform      3.3      NaN    c  1991-06-24        None          5        NaN       None  None

            
            >>> # 3) Specify left_on, right_on conditions along with DataFrame indexes as merge keys:
            >>> df1.merge(right = df2, how = "right", left_on = "col2", right_on = "col4", use_index = True, lsuffix = "t1", rsuffix = "t2")
              t1_col1 t2_col1       col2  t1_col3  t2_col3  col5    t1_col 6    t2_col 6 t1_'col8' t2_'col8'       col4 col7
            0       2       2  analytics      2.3      2.3     b  1991-11-01  1991-11-01         4         4  analytics    b
            1       1       1   teradata      1.3      1.3     a  1991-10-31  1991-10-31         3         3   teradata    a
            2    None       3       None      NaN      4.3  None        None  1991-11-03      None         5    are you    d
            
            
            >>> # 4) If teradataml DataFrames to be merged do not contain common columns, lsuffix and
                #  rsuffix are not required:
            >>> new_df1 = df1.select(['col2', 'col5'])
            >>> new_df2 = df2.select(['col4', 'col7'])
            >>> new_df1
              col5       col2
            0    b  analytics
            1    a   teradata
            2    c   platform
            >>> new_df2
              col7       col4
            0    b  analytics
            1    a   teradata
            2    d    are you
            >>> new_df1.merge(right = new_df2, how = "inner", on = "col5=col7")
              col5       col4       col2 col7
            0    b  analytics  analytics    b
            1    a   teradata   teradata    a
            
            
            >>> # 5) When no merge conditions are specified, teradataml DataFrame
                # indexes are used as merge keys.
            >>> df1.merge(right = df2, how = "full", lsuffix = "t1", rsuffix = "t2")
              t2_col1 col5    t2_col 6 t1_col1 t2_'col8'  t1_col3       col4  t2_col3 col7       col2    t1_col 6 t1_'col8'
            0       2    b  1991-11-01       2         4      2.3  analytics      2.3    b  analytics  1991-11-01         4
            1       1    a  1991-10-31       1         3      1.3   teradata      1.3    a   teradata  1991-10-31         3
            2       3    c  1991-11-03       3         5      3.3    are you      4.3    d   platform  1991-11-02         5
            
         """
        tdp = preparer(td_dialect)

        # Argument validations
        awu_matrix = []
        awu_matrix.append(["right", right, False, (DataFrame)])
        awu_matrix.append(["on", on, True, (str, ColumnExpression, list)])
        awu_matrix.append(["how", how, True, (str), False, TeradataConstants.TERADATA_JOINS.value])
        awu_matrix.append(["left_on", left_on, True, (str, ColumnExpression, list)])
        awu_matrix.append(["right_on", right_on, True, (str, ColumnExpression, list)])
        awu_matrix.append(["use_index", use_index, True, (bool)])
        awu_matrix.append(["lsuffix", lsuffix, True, (str), True])
        awu_matrix.append(["rsuffix", rsuffix, True, (str), True])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        # If self and right DataFrames are pointing to same Table object,
        # raise error.
        if self._metaexpr.t is right._metaexpr.t:
            raise TeradataMlException(Messages.get_message(MessageCodes.TDMLDF_ALIAS_REQUIRED, "merge"),
                                      MessageCodes.TDMLDF_ALIAS_REQUIRED)

        if (right_on is not None and left_on is None) or (right_on is None and left_on is not None):
            raise TeradataMlException(
                Messages.get_message(MessageCodes.MUST_PASS_ARGUMENT, "left_on", "right_on"),
                MessageCodes.MUST_PASS_ARGUMENT)

        if isinstance(on, list):
            join_conditions = on
        elif isinstance(on, (str, ColumnExpression)):
            join_conditions = [on]
        else:
            join_conditions = []

        if isinstance(left_on, list) and isinstance(right_on, list) and len(left_on) != len(right_on):
            raise TeradataMlException(
                Messages.get_message(MessageCodes.TDMLDF_UNEQUAL_NUMBER_OF_COLUMNS, "left_on", "right_on"),
                MessageCodes.TDMLDF_UNEQUAL_NUMBER_OF_COLUMNS)

        elif isinstance(left_on, list) and isinstance(right_on, (str, ColumnExpression)) and len(left_on) != 1:
            raise TeradataMlException(
                Messages.get_message(MessageCodes.TDMLDF_UNEQUAL_NUMBER_OF_COLUMNS, "left_on", "right_on"),
                MessageCodes.TDMLDF_UNEQUAL_NUMBER_OF_COLUMNS)

        elif isinstance(right_on, list) and isinstance(left_on, (str, ColumnExpression)) and len(right_on) != 1:
            raise TeradataMlException(
                Messages.get_message(MessageCodes.TDMLDF_UNEQUAL_NUMBER_OF_COLUMNS, "left_on", "right_on"),
                MessageCodes.TDMLDF_UNEQUAL_NUMBER_OF_COLUMNS)

        if left_on is not None and not isinstance(left_on, list):
            if isinstance(left_on, str):
                left_on = [left_on]
            else:
                left_on = [left_on.compile()]

        if right_on is not None and not isinstance(right_on, list):
            if isinstance(right_on, str):
                right_on = [right_on]
            else:
                right_on = [right_on.compile()]

        if isinstance(left_on, list):
            for index in range(len(left_on)):
                if isinstance(left_on[index], ColumnExpression):
                    left_on[index] = left_on[index].compile()

        if isinstance(right_on, list):
            for index in range(len(right_on)):
                if isinstance(right_on[index], ColumnExpression):
                    right_on[index] = right_on[index].compile()

        if left_on is not None and right_on is not None:
            for left_column, right_column in zip(left_on, right_on):
                join_conditions.append("{} = {}".format(tdp.quote(left_column), tdp.quote(right_column)))

        # If user did not pass any arguments which form join conditions,
        # Merge is performed using index columns of TeradataML DataFrames
        if on is None and left_on is None and right_on is None and not use_index:
            # DataFrames created on OTF table will not have index.
            if self._datalake is not None or right._datalake is not None:
                msg_code = MessageCodes.EXECUTION_FAILED
                emsg = "Either 'on' argument or both 'left_on' and 'right_on' arguments" \
                       " must be provided to merge DataFrames when they are created on" \
                       " OTF table(s)."
                error_msg = Messages.get_message(msg_code, "merge dataframes", emsg)
                raise TeradataMlException(error_msg, msg_code)

            if self._index_label is None or right._index_label is None:
                raise TeradataMlException(
                    Messages.get_message(MessageCodes.TDMLDF_INDEXES_ARE_NONE), MessageCodes.TDMLDF_INDEXES_ARE_NONE)
            else:
                use_index = True

        if use_index:
            if self._datalake is not None or right._datalake is not None:
                msg_code = MessageCodes.EXECUTION_FAILED
                emsg = "Can not use Index to merge DataFrames when they are created on OTF table(s)."
                error_msg = Messages.get_message(msg_code, "merge dataframes", emsg)
                raise TeradataMlException(error_msg, msg_code)

            if self._index_label is None or right._index_label is None:
                raise TeradataMlException(
                    Messages.get_message(MessageCodes.TDMLDF_INDEXES_ARE_NONE), MessageCodes.TDMLDF_INDEXES_ARE_NONE)

            left_index_labels = self._index_label
            right_index_labels = right._index_label
            if not isinstance(self._index_label, list):
                left_index_labels = [left_index_labels]
            if not isinstance(right._index_label, list):
                right_index_labels = [right_index_labels]

            for left_index_label, right_index_label in zip(left_index_labels, right_index_labels):
                join_conditions.append("{} = {}".format(tdp.quote(left_index_label), tdp.quote(right_index_label)))

        return self.join(other=right, on=join_conditions, how=how, lsuffix=lsuffix, rsuffix=rsuffix)

    @collect_queryband(queryband="DF_squeeze")
    def squeeze(self, axis=None):
        """
        DESCRIPTION:
            Squeeze one-dimensional axis objects into scalars.
            teradataml DataFrames with a single element are squeezed to a scalar.
            teradataml DataFrames with a single column are squeezed to a Series.
            Otherwise the object is unchanged.

            Note: Currently only '1' and 'None' are supported for axis.
                  For now with axis = 0, the teradataml DataFrame is returned.

        PARAMETERS:
            axis:
                Optional Argument.
                A specific axis to squeeze. By default, all axes with
                length equals one are squeezed.
                Permitted Values: 0 or 'index', 1 or 'columns', None
                Default: None

        RETURNS:
            teradataml DataFrame, teradataml Series, or scalar,
            the projection after squeezing 'axis' or all the axes.

        RAISES:
            TeradataMlException

        EXAMPLES:
            >>> load_example_data("dataframe", "admissions_train")
            >>> df = DataFrame("admissions_train")
            >>> df
               masters   gpa     stats programming admitted
            id
            22     yes  3.46    Novice    Beginner        0
            36      no  3.00  Advanced      Novice        0
            15     yes  4.00  Advanced    Advanced        1
            38     yes  2.65  Advanced    Beginner        1
            5       no  3.44    Novice      Novice        0
            17      no  3.83  Advanced    Advanced        1
            34     yes  3.85  Advanced    Beginner        0
            13      no  4.00  Advanced      Novice        1
            26     yes  3.57  Advanced    Advanced        1
            19     yes  1.98  Advanced    Advanced        0

            >>> gpa = df.select(["gpa"])
            >>> gpa.squeeze()
            0    4.00
            1    2.33
            2    3.46
            3    3.83
            4    4.00
            5    2.65
            6    3.57
            7    3.44
            8    3.85
            9    3.95
            Name: gpa, dtype: float64
            >>> gpa.squeeze(axis = 1)
            0    3.46
            1    3.00
            2    4.00
            3    2.65
            4    3.44
            5    3.83
            6    3.85
            7    4.00
            8    3.57
            9    1.98
            Name: gpa, dtype: float64
            >>> gpa.squeeze(axis = 0)
                gpa
            0  3.46
            1  3.00
            2  4.00
            3  2.65
            4  3.44
            5  3.83
            6  3.85
            7  4.00
            8  3.57
            9  1.98

            >>> df = DataFrame.from_query('select gpa, stats from admissions_train where gpa=2.33')
            >>> s = df.squeeze()
            >>> s
                gpa   stats
            0  2.33  Novice

            >>> single_gpa = DataFrame.from_query('select gpa from admissions_train where gpa=2.33')
            >>> single_gpa
                gpa
            0  2.33
            >>> single_gpa.squeeze()
            2.33
            >>> single_gpa.squeeze(axis = 1)
            0    2.33
            Name: gpa, dtype: float64
            >>> single_gpa.squeeze(axis = 0)
                gpa
            0  2.33
        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["axis", axis, True, (int, str), False, [0, 1, 'index', 'columns']])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        # Let's begin
        num_row, num_col = self.shape

        # Check if the number of elements in DF = 1
        if (num_row, num_col) == (1, 1) and axis is None:
            # To get the single row/column value in the DF, we need to execute the node
            # Generate/Execute AED nodes
            self.__execute_node_and_set_table_name(self._nodeid)
            return df_utils._get_scalar_value(self._table_name)

        if axis is None:
            if num_col == 1:
                axis = 1
            elif num_row == 1:
                axis = 0
            else:
                return self
        else:
            if isinstance(axis, str):
                # Set the integer value to use further for based on the string value
                if axis == "index":
                    axis = 0
                else:
                    axis = 1

            if (axis == 0 and num_row != 1) or \
               (axis == 1 and num_col != 1):
                return self

        if axis == 1:
            return Series._from_dataframe(self, axis=1)
        else:
            # TODO : Research and add capabilities to handle rowexpression based return objects
            # For now, returning the DataFrame as is
            return self

    @collect_queryband(queryband="DF_sortIndex")
    def sort_index(self, axis=0, ascending=True, kind='quicksort'):
        """
        DESCRIPTION:
            Gets sorted object by labels (along an axis) in either ascending or
            descending order for a teradataml DataFrame.
                
        PARAMETERS:
            axis:
                Optional Argument.
                Specifies the value to direct sorting on index or columns. 
                Values can be either 0 ('rows') OR 1 ('columns'), value as 0 will sort on index (if no index is present then parent DataFrame will be returned)
                and value as 1 will sort on columns names (if no index is present then parent DataFrame will be returned with sorted columns) for the DataFrame. 
                Default value: 0
                Types: int
                
            ascending:
                Optional Argument.
                Specifies a flag to sort columns in either ascending (True) or descending (False).
                Default value: True
                Types: bool
            
            kind:
                Optional Argument.
                Specifies a desired algorithm to be used.
                Permitted values: 'quicksort', 'mergesort' or 'heapsort'
                Default value: 'quicksort'
                Types: str

        RETURNS:
            teradataml DataFrame
        
        RAISES:
            TeradataMlException

        EXAMPLES:
            >>> load_example_data("dataframe","scale_housing_test")
            >>> df = DataFrame.from_table('scale_housing_test')
            >>> df
                      id    price  lotsize  bedrooms  bathrms  stories
            types                                                     
            classic   14  36000.0   2880.0       3.0      1.0      1.0
            bungalow  11  90000.0   7200.0       3.0      2.0      1.0
            classic   15  37000.0   3600.0       2.0      1.0      1.0
            classic   13  27000.0   1700.0       3.0      1.0      2.0
            classic   12  30500.0   3000.0       2.0      1.0      1.0
            
            >>> df.sort_index()
                      id    price  lotsize  bedrooms  bathrms  stories
            types                                                     
            bungalow  11  90000.0   7200.0       3.0      2.0      1.0
            classic   13  27000.0   1700.0       3.0      1.0      2.0
            classic   12  30500.0   3000.0       2.0      1.0      1.0
            classic   14  36000.0   2880.0       3.0      1.0      1.0
            classic   15  37000.0   3600.0       2.0      1.0      1.0
            
            >>> df.sort_index(0)
                      id    price  lotsize  bedrooms  bathrms  stories
            types                                                     
            bungalow  11  90000.0   7200.0       3.0      2.0      1.0
            classic   13  27000.0   1700.0       3.0      1.0      2.0
            classic   12  30500.0   3000.0       2.0      1.0      1.0
            classic   14  36000.0   2880.0       3.0      1.0      1.0
            classic   15  37000.0   3600.0       2.0      1.0      1.0
            
            >>> df.sort_index(1, False) # here 'False' means DESCENDING for respective axis
                      stories    price  lotsize  id  bedrooms  bathrms
            types                                                     
            classic       1.0  36000.0   2880.0  14       3.0      1.0
            bungalow      1.0  90000.0   7200.0  11       3.0      2.0
            classic       1.0  37000.0   3600.0  15       2.0      1.0
            classic       2.0  27000.0   1700.0  13       3.0      1.0
            classic       1.0  30500.0   3000.0  12       2.0      1.0
            
            >>> df.sort_index(1, True, 'mergesort')
                      bathrms  bedrooms  id  lotsize    price  stories
            types                                                     
            classic       1.0       3.0  14   2880.0  36000.0      1.0
            bungalow      2.0       3.0  11   7200.0  90000.0      1.0
            classic       1.0       2.0  15   3600.0  37000.0      1.0
            classic       1.0       3.0  13   1700.0  27000.0      2.0
            classic       1.0       2.0  12   3000.0  30500.0      1.0

        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["axis", axis, True, (int, str), False, [0, 1, 'columns', 'rows']])
        awu_matrix.append(["ascending", ascending, True, (bool)])
        awu_matrix.append(["kind", kind, True, (str), False, ['quicksort', 'mergesort', 'heapsort']])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        try:
            if axis in (0, 'rows'):
                # For NoPI objects
                if self._index_label is None:
                    return self
                else:
                    return self.sort(self._index_label, ascending)
            else:
                colnames_list, coltypes_list = df_utils._get_column_names_and_types_from_metaexpr(self._metaexpr)
                colnames_list = self.__get_sorted_list(colnames_list, ascending=ascending, kind=kind)
                return self.select(colnames_list)
        except TeradataMlException:
            raise

    @collect_queryband(queryband="DF_concat")
    def concat(self, other, join='OUTER', allow_duplicates=True, sort=False, ignore_index=False):
        """
        DESCRIPTION:
            Concatenates two teradataml DataFrames along the index axis.
            Note:
                * This method does not support operations on array columns.

        PARAMETERS:
            other:
                Required Argument.
                Specifies the other teradataml DataFrame with which the concatenation is
                to be performed.
                Types: teradataml DataFrame

            join:
                Optional Argument.
                Specifies how to handle indexes on columns axis.
                Supported values are:
                    * 'OUTER': It instructs the function to project all columns from both
                               the DataFrames. Columns not present in either DataFrame will
                               have a SQL NULL value.
                    * 'INNER': It instructs the function to project only the columns common
                               to both DataFrames.
                Default value: 'OUTER'
                Permitted values: 'INNER', 'OUTER'
                Types: str

            allow_duplicates:
                Optional Argument.
                Specifies if the result of concatenation can have duplicate rows.
                Default value: True
                Types: bool

            sort:
                Optional Argument.
                Specifies a flag to sort the columns axis if it is not already aligned when the join argument is set to 'outer'.
                Default value: False
                Types: bool
                
            ignore_index:
                Optional argument.
                Specifies whether to ignore the index columns in resulting DataFrame or not.
                If True, then index columns will be ignored in the concat operation.
                Default value: False
                Types: bool

        RETURNS:
            teradataml DataFrame

        RAISES:
            TeradataMlException

        EXAMPLES:
            >>> from teradataml import load_example_data
            >>> load_example_data("dataframe", "admissions_train")
            >>>
            >>> # Default options
            >>> df = DataFrame('admissions_train')
            >>> df1 = df[df.gpa == 4].select(['id', 'stats', 'masters', 'gpa'])
            >>> df1
                   stats masters  gpa
            id
            13  Advanced      no  4.0
            29    Novice     yes  4.0
            15  Advanced     yes  4.0
            >>> df2 = df[df.gpa < 2].select(['id', 'stats', 'programming', 'admitted'])
            >>> df2
                   stats programming admitted
            id
            24  Advanced      Novice        1
            19  Advanced    Advanced        0
            >>>
            >>> cdf = df1.concat(df2)
            >>> cdf
                   stats masters  gpa programming admitted
            id
            19  Advanced    None  NaN    Advanced        0
            24  Advanced    None  NaN      Novice        1
            13  Advanced      no  4.0        None     None
            29    Novice     yes  4.0        None     None
            15  Advanced     yes  4.0        None     None
            >>>
            >>> # join = 'inner'
            >>> cdf = df1.concat(df2, join='inner')
            >>> cdf
                   stats
            id
            19  Advanced
            24  Advanced
            13  Advanced
            29    Novice
            15  Advanced
            >>>
            >>> # allow_duplicates = True (default)
            >>> cdf = df1.concat(df2)
            >>> cdf
                   stats masters  gpa programming admitted
            id
            19  Advanced    None  NaN    Advanced        0
            24  Advanced    None  NaN      Novice        1
            13  Advanced      no  4.0        None     None
            29    Novice     yes  4.0        None     None
            15  Advanced     yes  4.0        None     None
            >>> cdf = cdf.concat(df2)
            >>> cdf
                   stats masters  gpa programming admitted
            id
            19  Advanced    None  NaN    Advanced        0
            13  Advanced      no  4.0        None     None
            24  Advanced    None  NaN      Novice        1
            24  Advanced    None  NaN      Novice        1
            19  Advanced    None  NaN    Advanced        0
            29    Novice     yes  4.0        None     None
            15  Advanced     yes  4.0        None     None
            >>>
            >>> # allow_duplicates = False
            >>> cdf = cdf.concat(df2, allow_duplicates=False)
            >>> cdf
                   stats masters  gpa programming admitted
            id
            19  Advanced    None  NaN    Advanced        0
            29    Novice     yes  4.0        None     None
            24  Advanced    None  NaN      Novice        1
            15  Advanced     yes  4.0        None     None
            13  Advanced      no  4.0        None     None
            >>>
            >>> # sort = True
            >>> cdf = df1.concat(df2, sort=True)
            >>> cdf
               admitted  gpa masters programming     stats
            id
            19        0  NaN    None    Advanced  Advanced
            24        1  NaN    None      Novice  Advanced
            13     None  4.0      no        None  Advanced
            29     None  4.0     yes        None    Novice
            15     None  4.0     yes        None  Advanced
            >>> 
            >>> # ignore_index = True
            >>> cdf = df1.concat(df2, ignore_index=True)
            >>> cdf
                  stats masters  gpa programming  admitted
            0  Advanced     yes  4.0        None       NaN
            1  Advanced    None  NaN    Advanced       0.0
            2    Novice     yes  4.0        None       NaN
            3  Advanced    None  NaN      Novice       1.0
            4  Advanced      no  4.0        None       NaN

        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["other", other, False, (DataFrame)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        return concat([self, other], join, allow_duplicates, sort, ignore_index)

    def __validate_sum_of_list_for_sample_api(self, samples, arg_name):
        """
        Function to verify whether the given samples is
        having float elements and sum of list is greater than 1.

        PARAMETERS:
            samples:
                Required Argument.
                Specifies the list to validate.

            arg_name:
                Required Argument.
                Specifies the name of parameter to be validated.

        RETURNS:
            True, if the sum of elements of samples is less than 1.

        RAISES:
            TeradataMLException

        EXAMPLES:
            then_list = [0.5, 0.2, 0.1]
            __validate_sum_of_list_for_sample_api(then_list, "case_when_then")
        """

        # Raise exception if all elements in list are float and sum of list is 
        # greater than 1.
        if isinstance(samples, float) and samples > 1:
            raise TeradataMlException(
                Messages.get_message(MessageCodes.INVALID_ARG_VALUE, str(samples), arg_name,
                                     "greater than 0 and less than or equal to 1"),
                MessageCodes.INVALID_ARG_VALUE)
        if isinstance(samples, list) and all(isinstance(item, float) for item in samples) \
                and sum(samples) > 1:
            raise TeradataMlException(
                Messages.get_message(MessageCodes.INVALID_ARG_VALUE, str(samples), arg_name,
                                     "a list having sum of all elements greater than 0 and less than or equal to 1"),
                MessageCodes.INVALID_ARG_VALUE)

        return True

    def __validate_len_of_list_for_sample_api(self, samples, arg_name):
        """
        Function to verify whether the given samples is
        having length greater than 16.

        PARAMETERS:
            samples:
                Required Argument.
                Specifies the list to validate.

            arg_name:
                Required Argument.
                Specifies the name of parameter to be validated.

        RETURNS:
            True, if the length of samples is less than 16.

        RAISES:
            TeradataMLException

        EXAMPLES:
            then_list = [0.5, 0.2, 0.1]
            __validate_len_of_list_for_sample_api(then_list, "case_when_then")
        """

        # Raise exception if the length of list is greater than 16.
        if len(samples) > 16:
            raise TeradataMlException(
                Messages.get_message(MessageCodes.INVALID_ARG_VALUE, str(samples), arg_name,
                                     "a list having less than or equal to 16 samples"),
                MessageCodes.INVALID_ARG_VALUE)

        return True

    def __validate_number_of_rows_for_sample_api(self, samples, arg_name):
        """
        Function to verify whether the argument 'samples' is
        negative by itself or it has any negative numbers if
        the 'samples' is a list and to check if 'samples' specified
        as fractions has 0.0.

        PARAMETERS:
            samples:
                Required Argument.
                Specifies the parameter to validate.

            arg_name:
                Required Argument.
                Specifies the name of parameter to be validated.

        RETURNS:
            True, if samples itself is positive or all the elements in it
            are positive if 'samples' is a list and 'samples' is not 0.0 
            by itself or does not contain 0.0 if it is a list.

        RAISES:
            TeradataMLException

        EXAMPLES:
            then_list = [-0.5, 0.2, -0.1]
            __validate_number_of_rows_for_sample_api(then_list, "case_when_then")
        """

        # Raise exception if number of rows given are negative.
        if isinstance(samples, (int, float)) and samples < 0 or isinstance(samples, list) \
                and any(item < 0 for item in samples):
            raise TeradataMlException(
                Messages.get_message(MessageCodes.INVALID_ARG_VALUE, str(samples), arg_name,
                                     "greater than 0"),
                MessageCodes.INVALID_ARG_VALUE)

        # Raise exception if fractions specified as 0.
        if isinstance(samples, float) and samples == 0 or (isinstance(samples, list) \
                                                           and all(isinstance(item, float) for item in samples)
                                                           and any(item == 0 for item in samples)):
            raise TeradataMlException(
                Messages.get_message(MessageCodes.INVALID_ARG_VALUE, str(samples), arg_name,
                                     "greater than 0"),
                MessageCodes.INVALID_ARG_VALUE)

        return True

    @collect_queryband(queryband="DF_sample")
    def sample(self, n=None, frac=None, replace=False, randomize=False, case_when_then=None, case_else=None,
               stratify_column=None, seed=None, id_column=None):
        """
        DESCRIPTION:
            Allows to sample few rows from dataframe directly or based on conditions.
            Creates a new column 'sampleid' which has a unique id for each sample
            sampled, it helps to uniquely identify each sample.
            Note:
                * This method does not support operations on array columns.

        PARAMETERS:
            n:
                Required Argument, if neither of 'frac' and 'case_when_then' are specified.
                Specifies a set of positive integer constants that specifies the number of 
                rows to be sampled from the teradataml DataFrame.
                Example:
                    n = 10 or n = [10] or n = [10, 20, 30, 40]
                Default Value: None
                Types: int or list of ints.
                Note:
                    1. You should use only one of the following arguments: 'n', 'frac' and 'case_when_then'.
                    2. No more than 16 samples can be requested per count description.

            frac:
                Required Argument, if neither of 'n' and 'case_when_then' are specified.
                Specifies any set of unsigned floating point constant numbers in the half
                opened interval (0,1] that means greater than 0 and less than or equal to 1.
                It specifies the percentage of rows to be sampled from the teradataml DataFrame.
                Example:
                    frac = 0.4 or frac = [0.4] or frac = [0.2, 0.5]
                Default Value: None
                Types: float or list of floats.
                Note:
                    1. You should use only one of the following arguments: 'n', 'frac' and 'case_when_then'.
                    2. No more than 16 samples can be requested per count description.
                    3. Sum of elements in list should not be greater than 1 as total percentage cannot be
                       more than 100% and should not be less than or equal to 0.
                    4. Stratifying data sample is supported only when "stratify_column" 
                       is used with "frac" argument.
                    5. List sizes must include a minimum of one float element and a maximum of two elements
                       when data sampled with stratification. The train data sample percentage 
                       corresponds to the first element, whereas the test data sample percentage is
                       associated with the second element.
                    6. The remaining fraction is considered for sampling the data when "frac" has 
                       only one fraction for data sampling with stratification.
            replace:
                Optional Argument.
                Specifies if sampling should be done with replacement or not.
                Default Value: False
                Types: bool

            randomize:
                Optional Argument.
                Specifies if sampling should be done across AMPs in Teradata or per AMP.
                Default Value: False
                Types: bool

            case_when_then :
                Required Argument, if neither of 'frac' and 'n' are specified.
                Specifies condition and number of samples to be sampled as key value pairs.
                Keys should be of type ColumnExpressions.
                Values should be either of type int, float, list of ints or list of floats.
                The following usage of key is not allowed:
                    case_when_then = {"gpa" > 2 : 2}
                The following operators are supported:
                      comparison: ==, !=, <, <=, >, >=
                      boolean: & (and), | (or), ~ (not), ^ (xor)
                Example :
                      case_when_then = {df.gpa > 2 : 2}
                      case_when_then = {df.gpa > 2 & df.stats == 'Novice' : [0.2, 0.3],
                                       df.programming == 'Advanced' : [10,20,30]}
                Default Value: None
                Types: dictionary
                Note:
                    1. You should use only one of the following arguments: 'n', 'frac' and 'case_when_then'.
                    2. No more than 16 samples can be requested per fraction description or count description.
                    3. If any value in dictionary is specified as list of floats then
                       sum of elements in list should not be greater than 1 as total percentage cannot be
                       more than 100% and should not be less than or equal to 0.

            case_else :
                Optional Argument.
                Specifies number of samples to be sampled from rows where none of the conditions in
                'case_when_then' are met.
                Example :
                    case_else = 10
                    case_else = [10,20]
                    case_else = [0.5]
                    case_else = [0.2,0.4]
                Default Value: None
                Types: int or float or list of ints or list of floats
                Note:
                    1. This argument can only be used with 'case_when_then'. 
                       If used otherwise, below error will raised.
                           'case_else' can only be used when 'case_when_then' is specified.
                    2. No more than 16 samples can be requested per fraction description 
                       or count description.
                    3. If case_else is list of floats then sum of elements in list should not be 
                       greater than 1 as total percentage cannot be more than 100% and should not 
                       be less than or equal to 0.

            stratify_column:
                Optional Argument.
                Specifies column name that contains the labels indicating
                which data needs to be stratified.
                Notes:
                    1. Must be used with "frac" argument for stratifying data.
                    2. seed is supported for stratify column.
                    3. Arguments "stratify_column", "seed", "id_column" are supported only 
                       for stratifying the data.
                Types: str OR Feature
            
            seed:
                Optional Argument.
                Specifies the seed value which controls the data sample. The sample remains 
                same as long as the seed remains same. Use this argument to get the 
                deterministic samples. "seed" must be greater than or equal to 0 and 
                less than or equal to 2147483647.
                Notes:
                    1. Random seed is generated internally when argument 
                       is not specified.
                    2. Seed is supported only when only when "stratify_column" is used. 
                       Ignored otherwise. 
                    3. Arguments "stratify_column", "seed", "id_column" are supported only 
                       for stratifying the data.
                Types: int
            
            id_column:
                Required when "seed" is used. Optional otherwise.
                Specifies the input data column name that has the
                unique identifier for each row in the input.
                Notes:
                    1. Arguments "stratify_column", "seed", "id_column" are supported only 
                       for stratifying the data.
                    2. "id_column" is supported only when "stratify_column" is used. 
                       Ignored otherwise.
                Types: str OR Feature

        RETURNS:
            teradataml DataFrame

        RAISES:
            1. ValueError - When columns of different dataframes are given in ColumnExpression.
                             or
                            When columns are given in string format and not ColumnExpression.
            2. TeradataMlException - If types of input parameters are mismatched.
            3. TypeError

        Examples:
            >>> from teradataml import *
            >>> load_example_data("dataframe", "admissions_train")
            >>> df = DataFrame("admissions_train")

            # Print dataframe.
            >>> df
                  masters   gpa     stats programming admitted
               id
               13      no  4.00  Advanced      Novice        1
               26     yes  3.57  Advanced    Advanced        1
               5       no  3.44    Novice      Novice        0
               19     yes  1.98  Advanced    Advanced        0
               15     yes  4.00  Advanced    Advanced        1
               40     yes  3.95    Novice    Beginner        0
               7      yes  2.33    Novice      Novice        1
               22     yes  3.46    Novice    Beginner        0
               36      no  3.00  Advanced      Novice        0
               38     yes  2.65  Advanced    Beginner        1

            # Sample with only n argument.
            # Randomly samples 2 rows from the teradataml DataFrame.
            # As there is only 1 sample 'sampleid' is 1.
            >>> df.sample(n = 2)
                  masters   gpa     stats programming admitted SampleId
               id
               18     yes  3.81  Advanced    Advanced        1        1
               19     yes  1.98  Advanced    Advanced        0        1

            # Sample with multiple sample values for n.
            # Creates 2 samples with 2 and 1 rows each respectively.
            # There are 2 values(1,2) for 'sampleid' each for one sample.
            >>> df.sample(n = [2, 1])
                  masters   gpa     stats programming admitted SampleId
               id
               1      yes  3.95  Beginner    Beginner        0        1
               10      no  3.71  Advanced    Advanced        1        1
               11      no  3.13  Advanced    Advanced        1        2

            # Sample with only frac parameter.
            # Randomly samples 20% of total rows present in teradataml DataFrame.
            >>> df.sample(frac = 0.2)
                  masters   gpa     stats programming admitted SampleId
               id
               18     yes  3.81  Advanced    Advanced        1        1
               15     yes  4.00  Advanced    Advanced        1        1
               14     yes  3.45  Advanced    Advanced        0        1
               35      no  3.68    Novice    Beginner        1        1
               27     yes  3.96  Advanced    Advanced        0        1
               25      no  3.96  Advanced    Advanced        1        1
               10      no  3.71  Advanced    Advanced        1        1
               9       no  3.82  Advanced    Advanced        1        1

            # Sample with multiple sample values for frac.
            # Creates 2 samples each with 4% and 2% of total rows in teradataml DataFrame.
            >>> df.sample(frac = [0.04, 0.02])
                  masters   gpa     stats programming admitted SampleId
               id
               29     yes  4.00    Novice    Beginner        0        1
               19     yes  1.98  Advanced    Advanced        0        2
               11      no  3.13  Advanced    Advanced        1        1

            # Sample with n and replace and randomization.
            # Creates 2 samples with 2 and 1 rows respectively with possible redundant
            # sampling as replace is True and also selects rows from different AMPS as
            # randomize is True.
            >>> df.sample(n = [2, 1], replace = True, randomize = True)
                  masters   gpa     stats programming admitted SampleId
               id
               12      no  3.65    Novice      Novice        1        1
               39     yes  3.75  Advanced    Beginner        0        1
               20     yes  3.90  Advanced    Advanced        1        2

            # Sample with frac and replace and randomization.
            # Creates 2 samples with 4% and 2% of total rows in teradataml DataFrame
            # respectively with possible redundant sampling and also selects rows from different AMPS.
            >>> df.sample(frac = [0.04, 0.02], replace = True, randomize = True)
                  masters   gpa     stats programming admitted SampleId
               id
               7      yes  2.33    Novice      Novice        1        2
               30     yes  3.79  Advanced      Novice        0        1
               33      no  3.55    Novice      Novice        1        1

            # Sample with case_when_then.
            # Creates 2 samples with 1, 2 rows respectively from rows which satisfy df.gpa < 2
            # and 2.5% of rows from rows which satisfy df.stats == 'Advanced'.
            >>> df.sample(case_when_then={df.gpa < 2 : [1, 2], df.stats == 'Advanced' : 0.025})
                  masters   gpa     stats programming admitted SampleId
               id
               19     yes  1.98  Advanced    Advanced        0        1
               24      no  1.87  Advanced      Novice        1        1
               11      no  3.13  Advanced    Advanced        1        3

            # Sample with case_when_then and replace, randomize.
            # Creates 2 samples with 1, 2 rows respectively from rows which satisfy df.gpa < 2
            # and 2.5% of rows from rows which satisfy df.stats == 'Advanced' and selects rows
            # from different AMPs with replacement.
            >>> df.sample(replace = True, randomize = True, case_when_then={df.gpa < 2 : [1, 2],
                                                                           df.stats == 'Advanced' : 0.025})
                  masters   gpa     stats programming admitted SampleId
               id
               24      no  1.87  Advanced      Novice        1        1
               24      no  1.87  Advanced      Novice        1        2
               24      no  1.87  Advanced      Novice        1        2
               24      no  1.87  Advanced      Novice        1        2
               24      no  1.87  Advanced      Novice        1        2
               24      no  1.87  Advanced      Novice        1        1
               31     yes  3.50  Advanced    Beginner        1        3

            # Sample with case_when_then and case_else.
            # Creates 7 samples 2 with 1, 3 rows from rows which satisfy df.gpa > 2.
            # 1 sample with 5 rows from rows which satisify df.programming == 'Novice'.
            # 1 sample with 5 rows from rows which satisify df.masters == 'no'.
            # 1 sample with 1 row from rows which does not meet all above conditions.
            >>> df.sample(case_when_then = {df.gpa > 2 : [1, 3], df.stats == 'Novice' : [1, 2],
                                           df.programming == 'Novice' : 5, df.masters == 'no': 5}, case_else = 1)
                  masters   gpa     stats programming admitted SampleId
               id
               24      no  1.87  Advanced      Novice        1        5
               2      yes  3.76  Beginner    Beginner        0        1
               12      no  3.65    Novice      Novice        1        2
               38     yes  2.65  Advanced    Beginner        1        2
               36      no  3.00  Advanced      Novice        0        2
               19     yes  1.98  Advanced    Advanced        0        7

            # Sample with case_when_then and case_else
            # Creates 4 samples 2 with 1, 3 rows from rows which satisfy df.gpa > 2.
            # 2 samples with 2.5%, 5% of rows from all the rows which does not
            # meet condition df.gpa < 2.
            >>> df.sample(case_when_then = {df.gpa < 2 : [1, 3]}, case_else = [0.025, 0.05])
                  masters   gpa     stats programming admitted SampleId
               id
               9       no  3.82  Advanced    Advanced        1        4
               24      no  1.87  Advanced      Novice        1        1
               26     yes  3.57  Advanced    Advanced        1        4
               13      no  4.00  Advanced      Novice        1        3
               19     yes  1.98  Advanced    Advanced        0        1

            # Sample with case_when_then, case_else, replace, randomize
            # Creates 4 samples 2 with 1, 3 rows from rows which satisfy df.gpa > 2 and
            # 2 samples with 2.5%, 5% of rows from all the rows which does not
            # meet condition df.gpa < 2  with possible redundant replacement
            # and also selects rows from different AMPs
            >>> df.sample(case_when_then = {df.gpa < 2 : [1, 3]}, replace = True,
                        randomize = True, case_else = [0.025, 0.05])
                  masters   gpa     stats programming admitted SampleId
               id
               19     yes  1.98  Advanced    Advanced        0        1
               19     yes  1.98  Advanced    Advanced        0        2
               19     yes  1.98  Advanced    Advanced        0        2
               19     yes  1.98  Advanced    Advanced        0        2
               19     yes  1.98  Advanced    Advanced        0        2
               40     yes  3.95    Novice    Beginner        0        3
               3       no  3.70    Novice    Beginner        1        4
               19     yes  1.98  Advanced    Advanced        0        2
               19     yes  1.98  Advanced    Advanced        0        2
               19     yes  1.98  Advanced    Advanced        0        1
        """
        try:
            if n is not None and frac is not None:
                raise TeradataMlException(Messages.get_message(MessageCodes.EITHER_THIS_OR_THAT_ARGUMENT,
                                                               "n", "frac"),
                                          MessageCodes.EITHER_THIS_OR_THAT_ARGUMENT)
            if n is not None and case_when_then is not None:
                raise TeradataMlException(Messages.get_message(MessageCodes.EITHER_THIS_OR_THAT_ARGUMENT,
                                                               "n", "case_when_then"),
                                          MessageCodes.EITHER_THIS_OR_THAT_ARGUMENT)
            if frac is not None and case_when_then is not None:
                raise TeradataMlException(Messages.get_message(MessageCodes.EITHER_THIS_OR_THAT_ARGUMENT,
                                                               "frac", "case_when_then"),
                                          MessageCodes.EITHER_THIS_OR_THAT_ARGUMENT)
            _Validators._validate_dependent_argument("case_else", case_else, "case_when_then", case_when_then)
            if n is None and frac is None and case_when_then is None:
                raise TeradataMlException(Messages.get_message(MessageCodes.EITHER_THIS_OR_THAT_ARGUMENT,
                                                               "n or frac", "case_when_then"),
                                          MessageCodes.EITHER_THIS_OR_THAT_ARGUMENT)

            # Argument validations
            awu_matrix = []
            awu_matrix.append(["n", n, True, (int, list)])
            awu_matrix.append(["frac", frac, True, (float, list)])
            awu_matrix.append(["replace", replace, True, (bool)])
            awu_matrix.append(["randomize", randomize, True, (bool)])
            awu_matrix.append(["case_when_then", case_when_then, True, (dict)])
            awu_matrix.append(["case_else", case_else, True, (int, float, list)])

            # Validate argument types
            _Validators._validate_function_arguments(awu_matrix)

            if self._metaexpr is None:
                msg = Messages.get_message(MessageCodes.TDMLDF_INFO_ERROR)
                raise TeradataMlException(msg, MessageCodes.TDMLDF_INFO_ERROR)

            list_of_fracs = []
            case_else_var = []
            if n is not None:
                # Processing by number of samples
                n = [n] if isinstance(n, int) else n
                # Let's perform validations for following:
                # Teradata Advanced SQL Engine Sample does not allows samples more than 16.
                # Check for the same for 'n'.
                self.__validate_len_of_list_for_sample_api(n, "n")

                # Number of rows specified to sample cannot be negative.
                # Check for the same for 'n'.
                self.__validate_number_of_rows_for_sample_api(n, "n")
                case_when_then = {}
                list_of_fracs = n

            elif frac is not None:
                # Processing for sampling by percentages.
                frac = [frac] if isinstance(frac, float) else frac
                # Validations for samples.
                self.__validate_len_of_list_for_sample_api(frac, "frac")

                # Teradata Advanced SQL Engine Sample does not allow sum of samples specified as
                # percentages to be greater than 1(because it exceeds 100%) or equals to 0.
                # Check for same for 'frac'.
                self.__validate_sum_of_list_for_sample_api(frac, "frac")
                self.__validate_number_of_rows_for_sample_api(frac, "frac")
                case_when_then = {}
                list_of_fracs = frac

                # When stratify column is passed for sample or when seed is passed for 
                # reproducibilty of result then 
                # perform TrainTestSplit for data sampling. 
                if stratify_column is not None or seed is not None:
                    # Local import TrainTestSplit function.
                    from teradataml.analytics.sqle import TrainTestSplit

                    # For statify column Train Test split size must sum up to 1.
                    if len(list_of_fracs) == 1:
                        list_of_fracs.append(1 - list_of_fracs[0])

                    # Call TrainTestSplit and return the result dataframe.
                    TrainTestSplit_out = TrainTestSplit(data=self,
                                                        id_column=id_column,
                                                        train_size=list_of_fracs[0],
                                                        test_size=list_of_fracs[1],
                                                        stratify_column=stratify_column,
                                                        seed=seed,
                                                        persist=True,
                                                        display_table_name=False)
                    
                    # Retrieve the table name from TrainTestSplit_out object.
                    table_name = TrainTestSplit_out.result._table_name

                    # Add the table to garbage collector.
                    table_added = GarbageCollector._add_to_garbagecollector(table_name)

                    # Retrieve the sampled result and updated the column name and values
                    # for backward compatibility.
                    _sampled_df = TrainTestSplit_out.result
                    # Column name "TD_IsTrainRow" renamed to "sampleid".
                    return _sampled_df.assign(sampleid=case([
                        (_sampled_df.TD_IsTrainRow == 0, 2)],
                        else_=1)).drop("TD_IsTrainRow", axis=1)



            else:
                # Creating OrderDict for 'case_when_then' so that order of keys doesn't change after
                # modifying while we are traversing dictionary.
                case_when_then = OrderedDict(case_when_then)
                if len(case_when_then) > 16:
                    raise TeradataMlException(
                        Messages.get_message(MessageCodes.TDML_SAMPLE_INVALID_NUMBER_OF_SAMPLES, "case_when_then"),
                        MessageCodes.TDML_SAMPLE_INVALID_NUMBER_OF_SAMPLES)

                transformed_case_when_then = OrderedDict()
                for when_condition, then_sample_number in case_when_then.items():
                    # Validate conditions in case_when_then dictionary.
                    if not isinstance(when_condition, ColumnExpression):
                        raise ValueError("Condition in case_when_then should not be in "
                                         "string format for sample operation.")

                    # Make sure that ColumnExpression is not constituted from multiple DataFrames.
                    if when_condition.get_flag_has_multiple_dataframes():
                        raise ValueError("Combining Columns from different dataframes is "
                                         "unsupported for sample operation.")

                    # Validating values in the dict.
                    if isinstance(then_sample_number, int) or (isinstance(then_sample_number, list) \
                                                               and isinstance(then_sample_number[0], int)):
                        _Validators._validate_function_arguments([["Values in case_when_then", then_sample_number,
                                                                   True, (int, list)]])
                    else:
                        _Validators._validate_function_arguments([["Values in case_when_then", then_sample_number,
                                                                   True, ((float, list))]])

                    if isinstance(then_sample_number, list):
                        self.__validate_len_of_list_for_sample_api(then_sample_number, "case_when_then")

                    if not isinstance(then_sample_number, int):
                        self.__validate_sum_of_list_for_sample_api(then_sample_number, "case_when_then")

                    self.__validate_number_of_rows_for_sample_api(then_sample_number, "case_when_then")

                    clause_exp = when_condition.compile()
                    transformed_case_when_then[clause_exp] = case_when_then[when_condition]

                case_when_then = dict(transformed_case_when_then)

                # Processing case_else argument if given.
                if case_else is not None:
                    if isinstance(case_else, (int, float)):
                        case_else = [case_else]

                    case_else_awu_matrix = []
                    if isinstance(case_else[0], int):
                        case_else_awu_matrix.append(['Number of rows or fractions in case_else',
                                                     case_else, True, (int, list)])
                    else:
                        case_else_awu_matrix.append(['Number of rows or fractions in case_else',
                                                     case_else, True, (float, list)])

                    # Validating argument values for 'case_else'.
                    _Validators._validate_function_arguments(case_else_awu_matrix)

                    self.__validate_len_of_list_for_sample_api(case_else, "case_else")
                    self.__validate_sum_of_list_for_sample_api(case_else, "case_else")
                    self.__validate_number_of_rows_for_sample_api(case_else, "case_else")
                    case_else_var = case_else

            # Sample id column
            sample_column = "sampleid"
            # quote column names except "sampleid" column.
            get_quoted_column = lambda col: UtilFuncs._teradata_quote_arg(col, "\"",
                                                                          False) if col != sample_column else col
            selected_columns = [get_quoted_column(col) for col in self.columns]

            if sample_column in selected_columns:
                selected_columns.remove(sample_column)
            selected_columns.append("{} as \"{}\"".format(sample_column, sample_column))
            df_columns_types = df_utils._get_required_columns_types_from_metaexpr(self._metaexpr)

            new_metaexpr_columns_types = OrderedDict()
            for column in self.columns:
                self.__add_column_type_item_to_dict(new_metaexpr_columns_types, column,
                                                    column, df_columns_types)

            # As we are creating new column name, adding it to new metadata dict
            new_metaexpr_columns_types[sample_column] = INTEGER()
            sample_node_id = self._aed_utils._aed_sample(self._nodeid, ",".join(selected_columns),
                                             list_of_fracs, replace, randomize, case_when_then, case_else_var)

            column_info = ((col_name, col_type) for col_name, col_type in
                                                new_metaexpr_columns_types.items())
            # Get new metaexpr for sample_node_id
            new_metaexpr = UtilFuncs._get_metaexpr_using_columns(sample_node_id, column_info, is_persist=True,
                                                                 datalake=self._metaexpr.datalake)

            # Make this non-lazy. Added this in order to fix https://teradata-pe.atlassian.net/browse/ELE-6368
            # Cannot use __execute_node_and_set_table_name because self points to original df.
            # Hence, setting the _table_name with _execute_node_return_db_object_name.

            df = self._create_dataframe_from_node(sample_node_id, new_metaexpr, self._index_label)
            df._table_name = df_utils._execute_node_return_db_object_name(sample_node_id, new_metaexpr)

            return df

        except TeradataMlException:
            raise

        except ValueError:
            raise

        except TypeError:
            raise

        except Exception as err:
            errcode = MessageCodes.TDMLDF_INFO_ERROR
            msg = Messages.get_message(errcode)
            raise TeradataMlException(msg, errcode) from err

    @collect_queryband(queryband="DF_showQuery")
    def show_query(self, full_query=False):
        """
        DESCRIPTION:
            Function returns underlying SQL for the teradataml DataFrame. It is the same 
            SQL that is used to view the data for a teradataml DataFrame.

        PARAMETERS:
            full_query:
                Optional Argument.
                Specifies if the complete query for the dataframe should be returned.
                When this parameter is set to True, query for the dataframe is returned
                with respect to the base dataframe's table (from_table() or from_query()) or from the
                output tables of analytical functions (if there are any in the workflow).
                This query may or may not be directly used to retrieve data for the dataframe upon
                which the function is called.
                When this parameter is not used, string returned is the query already used
                or will be used to retrieve data for the teradataml DataFrame.
                Default Value: False
                Types: bool

        RETURNS:
            String representing the underlying SQL query for the teradataml DataFrame.

        EXAMPLES:
            >>> load_example_data("dataframe", "admissions_train")
            >>> load_example_data("NaiveBayes", "nb_iris_input_train")
            >>> df = DataFrame.from_table("admissions_train")

            # Example 1: Show query on base (from_table) dataframe, with default option
            >>> df.show_query()
            'select * from "admissions_train"'

            # Example 2: Show query on base (from_query) dataframe, with default option
            >>> df_from_query = DataFrame.from_query("select masters, gpa from admissions_train")
            >>> df_from_query.show_query()
            'select masters, gpa from admissions_train'

            # Example 3: Show query on base (from_table) dataframe, with full_query option
            #            This will return same query as with default option because workflow
            #            only has one dataframe.
            >>> df.show_query(full_query = True)
            'select * from "admissions_train"'

            # Example 4: Show query on base (from_query) dataframe, with full_query option
            #            This will return same query as with default option because workflow
            #            only has one dataframe.
            >>> df_from_query = DataFrame.from_query("select masters, gpa from admissions_train")
            >>> df_from_query.show_query(full_query = True)
            'select masters, gpa from admissions_train'

            # Example 5: Show query used in a workflow demonstrating default and full_query options.

            # Workflow Step-1: Assign operation on base dataframe
            >>> df1 = df.assign(temp_column=admissions_train_df.gpa + admissions_train_df.admitted)

            # Workflow Step-2: Selecting columns from assign's result
            >>> df2 = df1.select(["masters", "gpa", "programming", "admitted"])

            # Workflow Step-3: Filtering on top of select's result
            >>> df3 = df2[df2.admitted > 0]

            # Workflow Step-4: Sampling 90% rows from filter's result
            >>> df4 = df3.sample(frac=0.9)

            # Show query with full_query option on df4. 
            # This will give full query upto base dataframe(df)
            >>> df4.show_query(full_query = True)
            'select masters,gpa,stats,programming,admitted,sampleid as "sampleid" from (
             select * from (select masters,gpa,stats,programming,admitted from (select id AS 
             id, masters AS masters, gpa AS gpa, stats AS stats, programming AS programming, 
             admitted AS admitted, gpa + admitted AS temp_column from "admissions_train") as
             temp_table) as temp_table where admitted > 0) as temp_table SAMPLE 0.9'

            # Show query with default option on df4. This will give same query as give in above case.
            >>> df4.show_query()
            'select masters,gpa,stats,programming,admitted,sampleid as "sampleid" from (select * 
             from (select masters,gpa,stats,programming,admitted from (select id AS id, masters 
             AS masters, gpa AS gpa, stats AS stats, programming AS programming, admitted AS admitted, 
             gpa + admitted AS temp_column from "admissions_train") as temp_table) as temp_table 
             where admitted > 0) as temp_table SAMPLE 0.9'

            # Executing intermediate dataframe df3
            >>> df2
              masters   gpa programming  admitted
            0      no  4.00      Novice         1
            1     yes  3.57    Advanced         1
            2      no  3.44      Novice         0
            3     yes  1.98    Advanced         0
            4     yes  4.00    Advanced         1
            5     yes  3.95    Beginner         0
            6     yes  2.33      Novice         1
            7     yes  3.46    Beginner         0
            8      no  3.00      Novice         0
            9     yes  2.65    Beginner         1

            # Show query with default option on df4. This will give query with respect 
            # to view/table created by the latest executed dataframe in the workflow (df2 in this scenario).
            # This is the query teradataml internally uses to retrieve data for dataframe df4, if executed
            # at this point.
            >>> df4.show_query()
            'select masters,gpa,stats,programming,admitted,sampleid as "sampleid" from (select * from
            "ALICE"."ml__select__1585722211621282" where admitted > 0) as temp_table SAMPLE 0.9'

            # Show query with full_query option on df4. This will still give the same full query upto base dataframe(df)
            >>> df4.show_query(full_query = True)
            'select masters,gpa,stats,programming,admitted,sampleid as "sampleid" from (select *
             from (select masters,gpa,stats,programming,admitted from (select id AS id, masters
             AS masters, gpa AS gpa, stats AS stats, programming AS programming, admitted AS admitted,
             gpa + admitted AS temp_column from "admissions_train") as temp_table) as temp_table
             where admitted > 0) as temp_table SAMPLE 0.9'

        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["full_query", full_query, False, (bool)])
        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        try:
            return self.__generate_aed_query(full_query)
        except TeradataMlException:
            raise

        except TypeError:
            raise

        except Exception as err:
            errcode = MessageCodes.TDMLDF_INFO_ERROR
            msg = Messages.get_message(errcode)
            raise TeradataMlException(msg, errcode) from err        

    @collect_queryband(queryband="DF_mapRow")
    def map_row(self,
                user_function,
                exec_mode='IN-DB',
                chunk_size=1000,
                num_rows=1000,
                **kwargs):
        """
        DESCRIPTION:
            Function to apply a user defined function to each row in the
            teradataml DataFrame, leveraging Vantage's Script Table Operator.
            Notes:
                * The function requires to use same Python version in both Vantage and local environment.
                * Teradata recommends to use "dill" package with same version in both Vantage and
                   local environment.
                * This method does not support DataFrame that contain array columns.

        PARAMETERS:
            user_function:
                Required Argument.
                Specifies the user defined function to apply to each row in
                the teradataml DataFrame.
                Types: function or functools.partial

                Notes:
                    * This can be either a lambda function, a regular python
                      function, or an object of functools.partial.
                    * The first argument (positional) to the user defined
                      function must be a row in a pandas DataFrame corresponding
                      to the teradataml DataFrame to which it is to be applied.
                    * A non-lambda function can be passed only when the user
                      defined function does not accept any arguments other than
                      the mandatory input - the input row.
                      A user can also use functools.partial and lambda functions
                      for the same, which are especially handy when:
                          * there is a need to pass positional and/or keyword
                            arguments (lambda).
                          * there is a need to pass keyword arguments only
                            (functool.partial).
                    * The return type of the user defined function must be one
                      of the following:
                          * numpy ndarray
                              * For a one-dimensional array, it is expected that
                                it has as many values as the number of expected
                                output columns.
                              * For a two-dimensional array, it is expected that
                                every array contained in the outer array has as
                                many values as the number of expected output
                                columns.
                          * pandas Series
                                This represents a row in the output, and the
                                number of values in it must be the same as the
                                number of expected output columns.
                          * pandas DataFrame
                                It is expected that a pandas DataFrame returned
                                by the "user_function" has the same number of
                                columns as the number of expected output columns.
                    * The return objects will be printed to the standard output
                      as required by Script using the 'quotechar' and 'delimiter'
                      values.
                    * The user function can also print the required output to
                      the standard output in the delimited (and possibly quoted)
                      format instead of returning an object of supported type.

            exec_mode:
                Optional Argument.
                Specifies the mode of execution for the user defined function.
                Permitted values:
                    * IN-DB: Execute the function on data in the teradataml
                             DataFrame in Vantage.
                    * LOCAL: Execute the function locally on sample data (at
                             most "num_rows" rows) from the teradataml
                             DataFrame.
                Default value: 'IN-DB'
                Types: str

            chunk_size:
                Optional Argument.
                Specifies the number of rows to be read in a chunk in each
                iteration using an iterator to apply the user defined function
                to each row in the chunk.
                Varying the value passed to this argument affects the
                performance and the memory utilization.
                Default value: 1000
                Types: int

            num_rows:
                Optional Argument.
                Specifies the maximum number of sample rows to use from the
                teradataml DataFrame to apply the user defined function to when
                "exec_mode" is 'LOCAL'.
                Default value: 1000
                Types: int

            returns:
                Optional Argument.
                Specifies the output column definition corresponding to the
                output of "user_function".
                When not specified, the function assumes that the names and
                types of the output columns are same as that of the input.
                Note:
                    Teradata reserved keywords should not be provided as column names unless
                    the column names of output dataframe are an exact match of input dataframe.
                    User can find the list or check if the string is a reserved keyword or not
                    using list_td_reserved_keywords() function.
                Types: Dictionary specifying column name to
                       teradatasqlalchemy type mapping.

            delimiter:
                Optional Argument.
                Specifies a delimiter to use when reading columns from a row and
                writing result columns.
                Default value: '\t'
                Types: str with one character
                Notes:
                    * This argument cannot be same as "quotechar" argument.
                    * This argument cannot be a newline character i.e., '\n'.

            quotechar:
                Optional Argument.
                Specifies a character that forces all input and output of the
                user function to be quoted using this specified character.
                Using this argument enables the Advanced SQL Engine to
                distinguish between NULL fields and empty strings.
                A string with length zero is quoted, while NULL fields are not.
                If this character is found in the data, it will be escaped by a
                second quote character.
                Types: str with one character
                Notes:
                    * This argument cannot be same as "delimiter" argument.
                    * This argument cannot be a newline character i.e., '\n'.

            auth:
                Optional Argument.
                Specifies an authorization to use when running the
                "user_function".
                Types: str

            charset:
                Optional Argument.
                Specifies the character encoding for data.
                Permitted values: 'utf-16', 'latin'
                Types: str

            data_order_column:
                Optional Argument.
                Specifies the Order By columns for the teradataml DataFrame.
                Values to this argument can be provided as a list, if multiple
                columns are used for ordering.
                This argument is used in both cases:
                "is_local_order = True" and "is_local_order = False".
                Types: str OR list of Strings (str)
                Note:
                    "is_local_order" must be set to 'True' when
                    "data_order_column" is used with "data_hash_column".

            is_local_order:
                Optional Argument.
                Specifies a boolean value to determine whether the input data is
                to be ordered locally or not.
                "data_order_column" with "is_local_order" set to 'False'
                specifies the order in which the values in a group, or
                partition, are sorted.
                When this argument is set to 'True', qualified rows on each AMP
                are ordered in preparation to be input to a table function.
                This argument is ignored, if "data_order_column" is None.
                Default value: False
                Types: bool
                Notes:
                    * "is_local_order" cannot be specified along with
                      "data_partition_column".
                    * When "is_local_order" is set to True, "data_order_column"
                      should be specified, and the columns specified in
                      "data_order_column" are used for local ordering.

            nulls_first:
                Optional Argument.
                Specifies a boolean value to determine whether NULLS are listed
                first or last during ordering.
                This argument is ignored, if "data_order_column" is None.
                NULLS are listed first when this argument is set to 'True', and
                last when set to 'False'.
                Default value: True
                Types: bool

            sort_ascending:
                Optional Argument.
                Specifies a boolean value to determine if the result set is to
                be sorted on the "data_order_column" column in ascending or
                descending order.
                The sorting is ascending when this argument is set to 'True',
                and descending when set to 'False'.
                This argument is ignored, if "data_order_column" is None.
                Default Value: True
                Types: bool

            debug:
                Optional Argument.
                Specifies whether to display the script file path generated during function execution or not. This
                argument helps in debugging when there are any failures during function execution. When set
                to True, function displays the path of the script and does not remove the file from local file system.
                Otherwise, file is removed from the local file system.
                Default Value: False
                Types: bool

        RETURNS:
            1. teradataml DataFrame if exec_mode is "IN-DB".
            2. Pandas DataFrame if exec_mode is "LOCAL".

        RAISES:
             TypeError, TeradataMlException.

        EXAMPLES:
            >>> # This example uses the 'admissions_train' dataset, to increase
            >>> # the 'gpa' by a give percentage.
            >>> # Load the example data.
            >>> load_example_data("dataframe", "admissions_train")
            >>> df = DataFrame('admissions_train')
            >>> print(df)
               masters   gpa     stats programming  admitted
            id
            22     yes  3.46    Novice    Beginner         0
            36      no  3.00  Advanced      Novice         0
            15     yes  4.00  Advanced    Advanced         1
            38     yes  2.65  Advanced    Beginner         1
            5       no  3.44    Novice      Novice         0
            17      no  3.83  Advanced    Advanced         1
            34     yes  3.85  Advanced    Beginner         0
            13      no  4.00  Advanced      Novice         1
            26     yes  3.57  Advanced    Advanced         1
            19     yes  1.98  Advanced    Advanced         0

            >>> # Example 1:
            >>> # Create the user defined function to increase the 'gpa' by the
            >>> # percentage provided. Note that the input to and the output
            >>> # from the function is a pandas Series object.
            >>> def increase_gpa(row, p=20):
            ...     row['gpa'] = row['gpa'] + row['gpa'] * p/100
            ...     return row
            ...
            >>>
            >>> # Apply the user defined function to the DataFrame.
            >>> # Note that since the output of the user defined function
            >>> # expects the same columns with the same types, we can skip
            >>> # passing the 'returns' argument.
            >>> increase_gpa_20 = df.map_row(increase_gpa)
            >>>
            >>> # Print the result.
            >>> print(increase_gpa_20)
               masters    gpa     stats programming  admitted
            id
            22     yes  4.152    Novice    Beginner         0
            36      no  3.600  Advanced      Novice         0
            15     yes  4.800  Advanced    Advanced         1
            38     yes  3.180  Advanced    Beginner         1
            5       no  4.128    Novice      Novice         0
            17      no  4.596  Advanced    Advanced         1
            34     yes  4.620  Advanced    Beginner         0
            13      no  4.800  Advanced      Novice         1
            26     yes  4.284  Advanced    Advanced         1
            19     yes  2.376  Advanced    Advanced         0

            >>> # Example 2:
            >>> # Use the same user defined function with a lambda notation to
            >>> # pass the percentage, 'p = 40'.
            >>> increase_gpa_40 = df.map_row(lambda row: increase_gpa(row,
            ...                                                       p = 40))
            >>>
            >>> print(increase_gpa_40)
               masters    gpa     stats programming  admitted
            id
            22     yes  4.844    Novice    Beginner         0
            36      no  4.200  Advanced      Novice         0
            15     yes  5.600  Advanced    Advanced         1
            38     yes  3.710  Advanced    Beginner         1
            5       no  4.816    Novice      Novice         0
            17      no  5.362  Advanced    Advanced         1
            34     yes  5.390  Advanced    Beginner         0
            13      no  5.600  Advanced      Novice         1
            26     yes  4.998  Advanced    Advanced         1
            19     yes  2.772  Advanced    Advanced         0

            >>> # Example 3:
            >>> # Use the same user defined function with functools.partial to
            >>> # pass the percentage, 'p = 50'.
            >>> from functools import partial
            >>> increase_gpa_50 = df.map_row(partial(increase_gpa, p = 50))
            >>>
            >>> print(increase_gpa_50)
               masters    gpa     stats programming  admitted
            id
            13      no  6.000  Advanced      Novice         1
            26     yes  5.355  Advanced    Advanced         1
            5       no  5.160    Novice      Novice         0
            19     yes  2.970  Advanced    Advanced         0
            15     yes  6.000  Advanced    Advanced         1
            40     yes  5.925    Novice    Beginner         0
            7      yes  3.495    Novice      Novice         1
            22     yes  5.190    Novice    Beginner         0
            36      no  4.500  Advanced      Novice         0
            38     yes  3.975  Advanced    Beginner         1

            >>> # Example 4:
            >>> # Use a lambda function to increase the 'gpa' by 50 percent, and
            >>> # return numpy ndarray.
            >>> from numpy import asarray
            >>> inc_gpa_lambda = lambda row, p=20: asarray([row['id'],
            ...                                row['masters'],
            ...                                row['gpa'] + row['gpa'] * p/100,
            ...                                row['stats'],
            ...                                row['programming'],
            ...                                row['admitted']])
            >>> increase_gpa_100 = df.map_row(lambda row: inc_gpa_lambda(row,
            ...                                                          p=100))
            >>>
            >>> print(increase_gpa_100)
               masters   gpa     stats programming  admitted
            id
            13      no  8.00  Advanced      Novice         1
            26     yes  7.14  Advanced    Advanced         1
            5       no  6.88    Novice      Novice         0
            19     yes  3.96  Advanced    Advanced         0
            15     yes  8.00  Advanced    Advanced         1
            40     yes  7.90    Novice    Beginner         0
            7      yes  4.66    Novice      Novice         1
            22     yes  6.92    Novice    Beginner         0
            36      no  6.00  Advanced      Novice         0
            38     yes  5.30  Advanced    Beginner         1

        """
        # Input validation.
        # With 'apply', 'returns' and 'data' are optional, and 'exec-mode'
        # may have different values.
        arg_info_matrix = []
        arg_info_matrix.append(["data", self, False, (DataFrame)])
        arg_info_matrix.append(["exec_mode", exec_mode, True, (str), True,
                                TableOperatorConstants.EXEC_MODE.value])
        arg_info_matrix.append(["chunk_size", chunk_size, True, (int)])
        arg_info_matrix.append(["num_rows", num_rows, True, (int)])

        returns = kwargs.pop('returns', OrderedDict(zip(self.columns,
                                                        [col.type for col in
                                                         self._metaexpr.c])))

        # Add the "returns" for validation.
        arg_info_matrix.append(["returns", returns, False, (dict)])

        # The following arguments are specific to Script, and will be validated
        # by Script itself.
        delimiter = kwargs.pop('delimiter', '\t')
        quotechar = kwargs.pop('quotechar', None)
        data_order_column = kwargs.pop('data_order_column', None)
        is_local_order = kwargs.pop('is_local_order', False)
        nulls_first = kwargs.pop('nulls_first', True)
        sort_ascending = kwargs.pop('sort_ascending', True)
        auth = kwargs.pop('auth', None)
        charset = kwargs.pop('charset', None)
        debug = kwargs.pop('debug', False)

        # Check for other extra/unknown arguments.
        unknown_args = list(kwargs.keys())
        if len(unknown_args) > 0:
            raise TypeError(Messages.get_message(MessageCodes.UNKNOWN_ARGUMENT,
                                                 "map_row", unknown_args[0]))

        tbl_op_util = _TableOperatorUtils(arg_info_matrix, self, "map_row",
                                          user_function, exec_mode,
                                          chunk_size=chunk_size,
                                          data_partition_column=None,
                                          data_hash_column=None,
                                          data_order_column=data_order_column,
                                          is_local_order=is_local_order,
                                          nulls_first=nulls_first,
                                          sort_ascending=sort_ascending,
                                          returns=returns, delimiter=delimiter,
                                          quotechar=quotechar, auth=auth,
                                          charset=charset, num_rows=num_rows, debug=debug)

        return tbl_op_util.execute()

    @collect_queryband(queryband="DF_mapPartition")
    def map_partition(self,
                      user_function,
                      exec_mode='IN-DB',
                      chunk_size=1000,
                      num_rows=1000,
                      data_partition_column=None,
                      data_hash_column=None,
                      **kwargs):
        """
        DESCRIPTION:
            Function to apply a user defined function to a group or partition of rows
            in the teradataml DataFrame, leveraging Vantage's Script Table Operator.
            Notes:
                * The function requires to use same Python version in both Vantage and local environment.
                * Teradata recommends to use "dill" package with same version in both Vantage and
                   local environment.
                * This method does not support DataFrame that contain array columns.

        PARAMETERS:
            user_function:
                Required Argument.
                Specifies the user defined function to apply to each group or partition of
                rows in the teradataml DataFrame.
                Types: function or functools.partial

                Notes:
                    * This can be either a lambda function, a regular python
                      function, or an object of functools.partial.
                    * The first argument (positional) to the user defined
                      function must be an iterator on the partition of rows
                      from the teradataml DataFrame represented as a pandas
                      DataFrame to which it is to be applied.
                    * A non-lambda function can be passed only when the user
                      defined function does not accept any arguments other than
                      the mandatory input - the iterator on the partition of
                      rows.
                      A user can also use functools.partial and lambda functions
                      for the same, which are especially handy when:
                          * there is a need to pass positional and/or keyword
                            arguments (lambda).
                          * there is a need to pass keyword arguments only
                            (functool.partial).
                    * The return type of the user defined function must be one
                      of the following:
                          * numpy ndarray
                              * For a one-dimensional array, it is expected that
                                it has as many values as the number of expected
                                output columns.
                              * For a two-dimensional array, it is expected that
                                every array contained in the outer array has as
                                many values as the number of expected output
                                columns.
                          * pandas Series
                                This represents a row in the output, and the
                                number of values in it must be the same as the
                                number of expected output columns.
                          * pandas DataFrame
                                It is expected that a pandas DataFrame returned
                                by the "user_function" has the same number of
                                columns as the number of expected output columns.
                    * The return objects will be printed to the standard output
                      as required by Script using the 'quotechar' and 'delimiter'
                      values.
                    * The user function can also print the required output to
                      the standard output in the delimited (and possibly quoted)
                      format instead of returning an object of supported type.

            exec_mode:
                Optional Argument.
                Specifies the mode of execution for the user defined function.
                Permitted values:
                    * IN-DB: Execute the function on data in the teradataml
                             DataFrame in Vantage.
                    * LOCAL: Execute the function locally on sample data (at
                             most "num_rows" rows) from the teradataml
                             DataFrame.
                Default value: 'IN-DB'
                Types: str

            chunk_size:
                Optional Argument.
                Specifies the number of rows to be read in a chunk in each
                iteration using the iterator that will be passed to the user
                defined function.
                Varying the value passed to this argument affects the
                performance and the memory utilization.
                Default value: 1000
                Types: int

            num_rows:
                Optional Argument.
                Specifies the maximum number of sample rows to use from the
                teradataml DataFrame to apply the user defined function to when
                "exec_mode" is 'LOCAL'.
                Default value: 1000
                Types: int

            data_partition_column:
                Optional Argument.
                Specifies the Partition By columns for the teradataml DataFrame.
                Values to this argument can be provided as a list, if multiple
                columns are used for partition.
                Types: str OR list of Strings (str)
                Note:
                    * "data_partition_column" cannot be specified along with
                      "data_hash_column".
                    * "data_partition_column" cannot be specified along with
                      "is_local_order = True".

            data_hash_column:
                Optional Argument.
                Specifies the column to be used for hashing.
                The rows in the teradataml DataFrame are redistributed to AMPs
                based on the hash value of the column specified.
                The "user_function" then runs once on each AMP.
                If there is no "data_partition_column", then the entire result
                set, delivered by the function, constitutes a single group or
                partition.
                Types: str
                Note:
                    * "data_hash_column" cannot be specified along with
                      "data_partition_column".
                    * "is_local_order" must be set to 'True' when
                      "data_order_column" is used with "data_hash_column".

            returns:
                Optional Argument.
                Specifies the output column definition corresponding to the
                output of "user_function".
                When not specified, the function assumes that the names and
                types of the output columns are same as that of the input.
                Note:
                    Teradata reserved keywords should not be provided as column names unless
                    the column names of output dataframe are an exact match of input dataframe.
                    User can find the list or check if the string is a reserved keyword or not
                    using list_td_reserved_keywords() function.
                Types: Dictionary specifying column name to
                       teradatasqlalchemy type mapping.

            delimiter:
                Optional Argument.
                Specifies a delimiter to use when reading columns from a row and
                writing result columns.
                Default value: '\t'
                Types: str with one character
                Notes:
                    * This argument cannot be same as "quotechar" argument.
                    * This argument cannot be a newline character i.e., '\n'.

            quotechar:
                Optional Argument.
                Specifies a character that forces all input and output of the
                user function to be quoted using this specified character.
                Using this argument enables the Advanced SQL Engine to
                distinguish between NULL fields and empty strings. A string with
                length zero is quoted, while NULL fields are not.
                If this character is found in the data, it will be escaped by a
                second quote character.
                Types: str with one character
                Notes:
                    * This argument cannot be same as "delimiter" argument.
                    * This argument cannot be a newline character i.e., '\n'.

            auth:
                Optional Argument.
                Specifies an authorization to use when running the
                "user_function".
                Types: str

            charset:
                Optional Argument.
                Specifies the character encoding for data.
                Permitted values: 'utf-16', 'latin'
                Types: str

            data_order_column:
                Optional Argument.
                Specifies the Order By columns for the teradataml DataFrame.
                Values to this argument can be provided as a list, if multiple
                columns are used for ordering.
                This argument is used in both cases:
                "is_local_order = True" and "is_local_order = False".
                Types: str OR list of Strings (str)
                Note:
                    "is_local_order" must be set to 'True' when
                    "data_order_column" is used with "data_hash_column".

            is_local_order:
                Optional Argument.
                Specifies a boolean value to determine whether the input data
                is to be ordered locally or not.
                "data_order_column" with "is_local_order" set to 'False'
                specifies the order in which the values in a group, or
                partition, are sorted.
                When this argument is set to 'True', qualified rows on each AMP
                are ordered in preparation to be input to a table function.
                This argument is ignored, if "data_order_column" is None.
                Default value: False
                Types: bool
                Notes:
                    * "is_local_order" cannot be specified along with
                      "data_partition_column".
                    * When "is_local_order" is set to True, "data_order_column"
                      should be specified, and the columns specified in
                      "data_order_column" are used for local ordering.

            nulls_first:
                Optional Argument.
                Specifies a boolean value to determine whether NULLS are listed
                first or last during ordering.
                This argument is ignored, if "data_order_column" is None.
                NULLS are listed first when this argument is set to 'True', and
                last when set to 'False'.
                Default value: True
                Types: bool

            sort_ascending:
                Optional Argument.
                Specifies a boolean value to determine if the result set is to
                be sorted on the "data_order_column" column in ascending or
                descending order.
                The sorting is ascending when this argument is set to 'True',
                and descending when set to 'False'.
                This argument is ignored, if "data_order_column" is None.
                Default Value: True
                Types: bool

            debug:
                Optional Argument.
                Specifies whether to display the script file path generated during function execution or not. This
                argument helps in debugging when there are any failures during function execution. When set
                to True, function displays the path of the script and does not remove the file from local file system.
                Otherwise, file is removed from the local file system.
                Default Value: False
                Types: bool

        RETURNS:
            1. teradataml DataFrame if exec_mode is "IN-DB".
            2. Pandas DataFrame if exec_mode is "LOCAL".

        RAISES:
             TypeError, TeradataMlException.

        EXAMPLES:
            >>> # This example uses the 'admissions_train' dataset, calculates
            >>> # the average 'gpa' per partition based on the value in
            >>> # 'admitted' column.
            >>> # Load the example data.
            >>> load_example_data("dataframe", "admissions_train")
            >>> df = DataFrame('admissions_train')
            >>> print(df)
               masters   gpa     stats programming  admitted
            id
            22     yes  3.46    Novice    Beginner         0
            36      no  3.00  Advanced      Novice         0
            15     yes  4.00  Advanced    Advanced         1
            38     yes  2.65  Advanced    Beginner         1
            5       no  3.44    Novice      Novice         0
            17      no  3.83  Advanced    Advanced         1
            34     yes  3.85  Advanced    Beginner         0
            13      no  4.00  Advanced      Novice         1
            26     yes  3.57  Advanced    Advanced         1
            19     yes  1.98  Advanced    Advanced         0

            >>> # Example 1:
            >>> # Create the user defined function to calculate the average
            >>> # 'gpa', by reading data in chunks.
            >>> # Note that the function accepts a TextFileReader object to
            >>> # iterate on data in chunks.
            >>> # The return type of the function is a numpy ndarray.
            >>>
            >>> from numpy import asarray
            >>> def grouped_gpa_avg(rows):
            ...     admitted = None
            ...     row_count = 0
            ...     gpa = 0
            ...     for chunk in rows:
            ...         for _, row in chunk.iterrows():
            ...             row_count += 1
            ...             gpa += row['gpa']
            ...             if admitted is None:
            ...                 admitted = row['admitted']
            ...     if row_count > 0:
            ...         return asarray([admitted, gpa/row_count])
            ...
            >>>
            >>> # Apply the user defined function to the DataFrame.
            >>> from teradatasqlalchemy.types import INTEGER, FLOAT
            >>> returns = OrderedDict([('admitted', INTEGER()),
            ...                        ('avg_gpa', FLOAT())])
            >>> avg_gpa_1 = df.map_partition(grouped_gpa_avg,
            ...                              returns = returns,
            ...                              data_partition_column = 'admitted')
            >>>
            >>> # Print the result.
            >>> print(avg_gpa_1)
               admitted   avg_gpa
            0         1  3.533462
            1         0  3.557143

            >>> # Example 2:
            >>> # Create the user defined function to calculate the average
            >>> # 'gpa', by reading data at once into a pandas DataFrame.
            >>> # Note that the function accepts a TextFileReader object to
            >>> # iterate on data in chunks.
            >>> # The return type of the function is a pandas Series.
            >>> def grouped_gpa_avg_2(rows):
            ...     pdf = rows.read()
            ...     if pdf.shape[0] > 0:
            ...         return pdf[['admitted', 'gpa']].mean()
            ...
            >>> avg_gpa_2 = df.map_partition(grouped_gpa_avg_2,
            ...                              returns = returns,
            ...                              data_partition_column = 'admitted')
            >>>
            >>> print(avg_gpa_2)
               admitted   avg_gpa
            0         0  3.557143
            1         1  3.533462

            >>> # Example 3:
            >>> # The following example demonstrates using a lambda function to
            >>> # achieve the same result.
            >>> # Note that the the function is written to accept an iterator
            >>> # (TextFileReader object), and return the result which is of
            >>> # type pandas Series.
            >>> avg_gpa_3 = df.map_partition(lambda rows: grouped_gpa_avg(rows),
            ...                              returns = returns,
            ...                              data_partition_column = 'admitted')
            >>>
            >>> print(avg_gpa_3)
               admitted   avg_gpa
            0         0  3.557143
            1         1  3.533462

            >>> # Example 4:
            >>> # The following example demonstrates using a function that
            >>> # returns the input data.
            >>> # Note that the the function is written to accept an iterator
            >>> # (TextFileReader object), and returns the result which is of
            >>> # type pandas DataFrame.
            >>> def echo(rows):
            ...     pdf = rows.read()
            ...     if pdf is not None:
            ...         return pdf
            ...
            >>> echo_out = df.map_partition(echo,
            ...                             data_partition_column = 'admitted')
            >>> print(echo_out)
               masters   gpa     stats programming  admitted
            id
            5       no  3.44    Novice      Novice         0
            7      yes  2.33    Novice      Novice         1
            22     yes  3.46    Novice    Beginner         0
            19     yes  1.98  Advanced    Advanced         0
            15     yes  4.00  Advanced    Advanced         1
            17      no  3.83  Advanced    Advanced         1
            34     yes  3.85  Advanced    Beginner         0
            13      no  4.00  Advanced      Novice         1
            36      no  3.00  Advanced      Novice         0
            40     yes  3.95    Novice    Beginner         0

        """
        # Input validation.
        # With 'apply', 'returns' and 'data' are optional, and 'exec-mode' may
        # have different values.
        arg_info_matrix = []
        arg_info_matrix.append(["data", self, False, (DataFrame)])
        arg_info_matrix.append(["exec_mode", exec_mode, True, (str), True,
                                TableOperatorConstants.EXEC_MODE.value])
        arg_info_matrix.append(["chunk_size", chunk_size, True, (int)])
        arg_info_matrix.append(["num_rows", num_rows, True, (int)])

        returns = kwargs.pop('returns', OrderedDict(zip(self.columns,
                                                        [col.type for col in
                                                         self._metaexpr.c])))
        # Add the "returns" for validation.
        arg_info_matrix.append(["returns", returns, False, (dict)])

        # Exactly one of 'data_partition_column' or 'data_hash_column' must be
        # provided.
        if (data_hash_column is not None and
            data_partition_column is not None) or \
                (data_hash_column is None and
                 data_partition_column is None):
            raise TeradataMlException(
                Messages.get_message(MessageCodes.EITHER_THIS_OR_THAT_ARGUMENT,
                                     "data_hash_column", "data_partition_column"
                                     ),
                MessageCodes.EITHER_THIS_OR_THAT_ARGUMENT)

        # The following arguments are specific to Script, and will be validated
        # by Script itself.
        delimiter = kwargs.pop('delimiter', '\t')
        quotechar = kwargs.pop('quotechar', None)
        data_order_column = kwargs.pop('data_order_column', None)
        is_local_order = kwargs.pop('is_local_order', False)
        nulls_first = kwargs.pop('nulls_first', True)
        sort_ascending = kwargs.pop('sort_ascending', True)
        auth = kwargs.pop('auth', None)
        charset = kwargs.pop('charset', None)
        debug = kwargs.pop('debug', False)

        # Check for other extra/unknown arguments.
        unknown_args = list(kwargs.keys())
        if len(unknown_args) > 0:
            raise TypeError(Messages.get_message(MessageCodes.UNKNOWN_ARGUMENT,
                                                 "map_partition", unknown_args[0]))

        tbl_op_util = _TableOperatorUtils(arg_info_matrix, self, "map_partition",
                                          user_function, exec_mode,
                                          chunk_size=chunk_size,
                                          data_partition_column=data_partition_column,
                                          data_hash_column=data_hash_column,
                                          data_order_column=data_order_column,
                                          is_local_order=is_local_order,
                                          nulls_first=nulls_first,
                                          sort_ascending=sort_ascending,
                                          returns=returns, delimiter=delimiter,
                                          quotechar=quotechar, auth=auth,
                                          charset=charset, num_rows=num_rows, debug=debug)

        return tbl_op_util.execute()

    @collect_queryband(queryband="DF_apply")
    def apply(self,
              user_function,
              exec_mode='REMOTE',
              chunk_size=1000,
              num_rows=1000,
              **kwargs):
        """
        DESCRIPTION:
            Function to apply a user-defined function(UDF) to each row in the
            teradataml DataFrame, leveraging Apply Table Operator of Open
            Analytics Framework.
            Notes:
                * The function requires to use same Python version in both remote environment and local environment.
                * Teradata recommends to use "dill" package with same version in both remote environment and
                   local environment.
                * This method does not support DataFrame that contain array columns.

        PARAMETERS:
            user_function:
                Required Argument.
                Specifies the user defined function to apply to each row in
                the teradataml DataFrame.
                Types: function or functools.partial

                Notes:
                    * This can be either a lambda function, a regular python
                      function, or an object of functools.partial.
                    * The first argument (positional) to the UDF must be a row
                      in a Pandas DataFrame corresponding to the teradataml
                      DataFrame to which it is to be applied.
                    * A non-lambda function can be passed only when the UDF
                      does not accept any arguments other than
                      the mandatory input - the input row.
                      A user can also use functools.partial and lambda functions
                      for the same, which are especially handy when:
                          * there is a need to pass positional and/or keyword
                            arguments (lambda).
                          * there is a need to pass keyword arguments only
                            (functool.partial).
                    * The return type of the UDF must be one
                      of the following:
                          * numpy ndarray
                              * when one-dimensional, having the same number of
                                values as output columns.
                              * when two-dimensional, every array contained in
                                the outer array having the same number of values
                                as output columns.
                          * pandas Series
                          * pandas DataFrame

                    * To use apply on the UDF, packages dill and pandas must be
                      installed in remote user environment using install_lib function
                      of UserEnv class. Refer to help(UserEnv.install_lib).

            exec_mode:
                Optional Argument.
                Specifies the mode of execution for the UDF.
                Default value: 'REMOTE'
                Types: str

            chunk_size:
                Optional Argument.
                Specifies the number of rows to be read in a chunk in each
                iteration using an iterator to apply the UDF
                to each row in the chunk.
                Varying the value passed to this argument affects the
                performance and the memory utilized by the function.
                Default value: 1000
                Types: int

            num_rows:
                For future use.

            env_name:
                Required Argument.
                Specifies the name of the remote user environment or an object of
                class UserEnv.
                Types: str or oject of class UserEnv.

            returns:
                Optional Argument.
                Specifies the output column definition corresponding to the
                output of "user_function".
                When not specified, the function assumes that the names and
                types of the output columns are same as that of the input.
                Types: Dictionary specifying column name to
                       teradatasqlalchemy type mapping.

            delimiter:
                Optional Argument.
                Specifies a delimiter to use when reading columns from a row and
                writing result columns.
                Default value: ','
                Types: str with one character
                Notes:
                    * This argument cannot be same as "quotechar" argument.
                    * This argument cannot be a newline character ('\n').

            quotechar:
                Optional Argument.
                Specifies a character that forces all input and output of the
                user function to be quoted using this specified character.
                Using this argument enables the Advanced SQL Engine to
                distinguish between NULL fields and empty strings.
                A string with length zero is quoted, while NULL fields are not.
                If this character is found in the data, it will be escaped by a
                second quote character.
                Types: str with one character
                Notes:
                    * This argument cannot be same as "delimiter" argument.
                    * This argument cannot be a newline character ('\n').

            data_partition_column:
                Optional Argument.
                Specifies the Partition By columns for the teradataml DataFrame.
                Values to this argument can be provided as a list, if multiple
                columns are used for partition. If there is no "data_partition_column",
                then the entire result set delivered by the function, constitutes a single
                group or partition.
                Default Value: ANY
                Types: str OR list of Strings (str)
                Notes:
                    1) "data_partition_column" can not be specified along with "data_hash_column".
                    2) "data_partition_column" can not be specified along with "is_local_order = True".

            data_hash_column:
                Optional Argument.
                Specifies the column to be used for hashing.
                The rows in the teradataml DataFrame are redistributed to AMPs
                based on the hash value of the column specified.
                If there is no data_hash_column, then the entire result
                set, delivered by the function, constitutes a single group or
                partition.
                Types: str
                Notes:
                    1. "data_hash_column" can not be specified along with "data_partition_column".
                    2. "data_hash_column" can not be specified along with "is_local_order=False" and
                       "data_order_column".

            data_order_column:
                Optional Argument.
                Specifies the Order By columns for the teradataml DataFrame.
                Values to this argument can be provided as a list, if multiple
                columns are used for ordering.
                This argument is used with in both cases:
                "is_local_order = True" and "is_local_order = False".
                Types: str OR list of Strings (str)
                Note:
                    "data_order_column" can not be specified along with "data_hash_column".

            is_local_order:
                Optional Argument.
                Specifies a boolean value to determine whether the input data is
                to be ordered locally or not.
                "data_order_column" with "is_local_order" set to 'False'
                specifies the order in which the values in a group, or
                partition, are sorted.
                When this argument is set to 'True', qualified rows on each AMP
                are ordered in preparation to be input to a table function.
                This argument is ignored, if "data_order_column" is None.
                Default value: False
                Types: bool
                Note:
                    When "is_local_order" is set to 'True', "data_order_column" should be
                    specified, and the columns specified in "data_order_column"
                    are used for local ordering.


            nulls_first:
                Optional Argument.
                Specifies a boolean value to determine whether NULLS are listed
                first or last during ordering.
                This argument is ignored, if "data_order_column" is None.
                NULLS are listed first when this argument is set to 'True', and
                last when set to 'False'.
                Default value: True
                Types: bool

            sort_ascending:
                Optional Argument.
                Specifies a boolean value to determine if the result set is to
                be sorted on the "data_order_column" column in ascending or
                descending order.
                The sorting is ascending when this argument is set to 'True',
                and descending when set to 'False'.
                This argument is ignored, if "data_order_column" is None.
                Default Value: True
                Types: bool

            style:
                Optional Argument.
                Specifies how input is passed to and output is generated by the 'apply_command'
                respectively.
                Note:
                    This clause only supports 'csv' value for Apply.
                Default value: "csv"
                Types: str

            debug:
                Optional Argument.
                Specifies whether to display the script file path generated during function execution or not. This
                argument helps in debugging when there are any failures during function execution. When set
                to True, function displays the path of the script and does not remove the file from local file system.
                Otherwise, file is removed from the local file system.
                Default Value: False
                Types: bool

        RETURNS:
            teradataml DataFrame.

        RAISES:
             TypeError, TeradataMlException.

        EXAMPLES:

            # Create a Python 3.7.9 environment with given name and description in Vantage.
            >>> env = create_env('testenv', 'python_3.7.9', 'Test environment')
            User environment testenv created.

            # Packages dill and pandas must be installed in remote user environment.
            >>> env.install_lib(['pandas','dill'])
            Request to install libraries initiated successfully in the remote user environment demo_env. Check the status using status() with the claim id 'ef255030-1be2-4d4a-9d47-12cd4365a003'.

            # Check the status of installation.
            >>> env.status('ef255030-1be2-4d4a-9d47-12cd4365a003')
                                           Claim Id     File/Libs  Method Name     Stage             Timestamp Additional Details
            0  ef255030-1be2-4d4a-9d47-12cd4365a003  pandas, dill  install_lib   Started  2022-08-04T04:27:56Z
            1  ef255030-1be2-4d4a-9d47-12cd4365a003  pandas, dill  install_lib  Finished  2022-08-04T04:29:12Z
            >>>

            # This example uses the 'admissions_train' dataset, to increase
            # the 'gpa' by a give percentage.
            # Load the example data.
            >>> load_example_data("dataframe", "admissions_train")
            >>> df = DataFrame('admissions_train')
            >>> print(df)
               masters   gpa     stats programming  admitted
            id
            22     yes  3.46    Novice    Beginner         0
            36      no  3.00  Advanced      Novice         0
            15     yes  4.00  Advanced    Advanced         1
            38     yes  2.65  Advanced    Beginner         1
            5       no  3.44    Novice      Novice         0
            17      no  3.83  Advanced    Advanced         1
            34     yes  3.85  Advanced    Beginner         0
            13      no  4.00  Advanced      Novice         1
            26     yes  3.57  Advanced    Advanced         1
            19     yes  1.98  Advanced    Advanced         0

            # Example 1:
            # Create the user defined function to increase the 'gpa' by the
            # percentage provided. Note that the input to and the output
            # from the function is a Pandas Series object.
            >>> def increase_gpa(row, p=20):
            ...     row['gpa'] = row['gpa'] + row['gpa'] * p/100
            ...     return row
            ...
            >>>
            # Apply the user defined function to the DataFrame.
            # Note that since the output of the user defined function
            # expects the same columns with the same types, we can skip
            # passing the 'returns' argument.
            >>> increase_gpa_20 = df.apply(increase_gpa, env_name='testenv')
            >>>
            >>> # Print the result.
            >>> print(increase_gpa_20)
               masters    gpa     stats programming  admitted
            id
            22     yes  4.152    Novice    Beginner         0
            36      no  3.600  Advanced      Novice         0
            15     yes  4.800  Advanced    Advanced         1
            38     yes  3.180  Advanced    Beginner         1
            5       no  4.128    Novice      Novice         0
            17      no  4.596  Advanced    Advanced         1
            34     yes  4.620  Advanced    Beginner         0
            13      no  4.800  Advanced      Novice         1
            26     yes  4.284  Advanced    Advanced         1
            19     yes  2.376  Advanced    Advanced         0

            # Example 2:
            # Use the same user defined function with a lambda notation to
            # pass the percentage, 'p = 40'.
            >>> increase_gpa_40 = df.apply(lambda row: increase_gpa(row,
            ...                                                     p = 40),
            ...                            env_name='testenv')
            >>>
            >>> print(increase_gpa_40)
               masters    gpa     stats programming  admitted
            id
            22     yes  4.844    Novice    Beginner         0
            36      no  4.200  Advanced      Novice         0
            15     yes  5.600  Advanced    Advanced         1
            38     yes  3.710  Advanced    Beginner         1
            5       no  4.816    Novice      Novice         0
            17      no  5.362  Advanced    Advanced         1
            34     yes  5.390  Advanced    Beginner         0
            13      no  5.600  Advanced      Novice         1
            26     yes  4.998  Advanced    Advanced         1
            19     yes  2.772  Advanced    Advanced         0

            # Example 3:
            # Use the same user defined function with functools.partial to
            # pass the percentage, 'p = 50'.
            >>> from functools import partial
            >>> increase_gpa_50 = df.apply(partial(increase_gpa, p = 50),
            ...                            env_name='testenv')
            >>>
            >>> print(increase_gpa_50)
               masters    gpa     stats programming  admitted
            id
            13      no  6.000  Advanced      Novice         1
            26     yes  5.355  Advanced    Advanced         1
            5       no  5.160    Novice      Novice         0
            19     yes  2.970  Advanced    Advanced         0
            15     yes  6.000  Advanced    Advanced         1
            40     yes  5.925    Novice    Beginner         0
            7      yes  3.495    Novice      Novice         1
            22     yes  5.190    Novice    Beginner         0
            36      no  4.500  Advanced      Novice         0
            38     yes  3.975  Advanced    Beginner         1

            # Example 4:
            # Use a lambda function to double 'gpa', and
            # return numpy ndarray.
            >>> from numpy import asarray
            >>> inc_gpa_lambda = lambda row, p=20: asarray([row['id'],
            ...                                row['masters'],
            ...                                row['gpa'] + row['gpa'] * p/100,
            ...                                row['stats'],
            ...                                row['programming'],
            ...                                row['admitted']])

            >>> increase_gpa_100 = df.apply(lambda row: inc_gpa_lambda(row,
            ...                                                        p=100),
            ...                             env_name='testenv')
            >>>
            >>> print(increase_gpa_100)
               masters   gpa     stats programming  admitted
            id
            13      no  8.00  Advanced      Novice         1
            26     yes  7.14  Advanced    Advanced         1
            5       no  6.88    Novice      Novice         0
            19     yes  3.96  Advanced    Advanced         0
            15     yes  8.00  Advanced    Advanced         1
            40     yes  7.90    Novice    Beginner         0
            7      yes  4.66    Novice      Novice         1
            22     yes  6.92    Novice    Beginner         0
            36      no  6.00  Advanced      Novice         0
            38     yes  5.30  Advanced    Beginner         1
        """
        # Input validation.
        arg_info_matrix = []

        # Validate only arguments specific to this apply function.
        # All other arguments will be validated by Apply class.
        arg_info_matrix.append(["data", self, False, (DataFrame)])
        arg_info_matrix.append(["exec_mode", exec_mode, True, (str), True, ['REMOTE']])
        arg_info_matrix.append(["chunk_size", chunk_size, True, (int)])
        arg_info_matrix.append(["num_rows", num_rows, True, (int)])

        # When returns argument is not specified, assume output schema
        # is same as input table schema.
        default_returns = OrderedDict(zip(self.columns,
                                          [col.type for col in
                                           self._metaexpr.c]))
        returns = kwargs.pop('returns', default_returns)
        arg_info_matrix.append(["returns", returns, False, (dict)])

        # Following arguments are specific to Apply, and will be validated
        # by Apply class itself.
        env_name = kwargs.pop('env_name', None)
        if isinstance(env_name, UserEnv):
            env_name = env_name.env_name

        style = kwargs.pop('style', 'CSV')
        delimiter = kwargs.pop('delimiter', ',')
        quotechar = kwargs.pop('quotechar', None)
        data_partition_column = kwargs.pop('data_partition_column', None)
        data_hash_column = kwargs.pop('data_hash_column', None)
        data_order_column = kwargs.pop('data_order_column', None)
        is_local_order = kwargs.pop('is_local_order', False)
        nulls_first = kwargs.pop('nulls_first', True)
        sort_ascending = kwargs.pop('sort_ascending', True)
        debug = kwargs.pop('debug', False)

        # Check for other extra/unknown arguments.
        unknown_args = list(kwargs.keys())
        if len(unknown_args) > 0:
            raise TypeError(Messages.get_message(MessageCodes.UNKNOWN_ARGUMENT,
                                                 "apply", unknown_args[0]))

        tbl_op_util = _TableOperatorUtils(arg_info_matrix, self, "apply",
                                          user_function, exec_mode,
                                          chunk_size=chunk_size,
                                          data_partition_column=data_partition_column,
                                          data_hash_column=data_hash_column,
                                          data_order_column=data_order_column,
                                          is_local_order=is_local_order,
                                          nulls_first=nulls_first,
                                          sort_ascending=sort_ascending,
                                          returns=returns, delimiter=delimiter,
                                          quotechar=quotechar,
                                          auth=None,
                                          charset=None,
                                          num_rows=num_rows,
                                          env_name=env_name,
                                          style=style,
                                          debug=debug)

        return tbl_op_util.execute()

    @collect_queryband(queryband="DF_window")
    def window(self,
               partition_columns=None,
               order_columns=None,
               sort_ascending=True,
               nulls_first=None,
               window_start_point=None,
               window_end_point=None,
               ignore_window=False
               ):
        """
        DESCRIPTION:
            This function generates Window object on a teradataml DataFrame to run
            window aggregate functions.
            Function allows user to specify window for different types of
            computations:
                * Cumulative
                * Group
                * Moving
                * Remaining
            By default, window with Unbounded Preceding and Unbounded Following
            is considered for calculation.
            Notes:
                * If both "partition_columns" and "order_columns" are None and
                  original DataFrame has BLOB and CLOB type of columns, then
                    * Window aggregate operation on CLOB and BLOB type of
                      columns is omitted.
                    * Resultant DataFrame does not contain the BLOB and CLOB
                      type of columns from original DataFrame.
                * This method does not support operations on array columns.

        PARAMETERS:
            partition_columns:
                Optional Argument.
                Specifies the name(s) of the column(s) over which the ordered
                aggregate function executes by partitioning the rows. Such a
                grouping is static.
                Notes:
                     1. If this argument is not specified, then the entire data
                        from teradataml DataFrame, constitutes a single
                        partition, over which the ordered aggregate function
                        executes.
                     2. "partition_columns" does not support CLOB and BLOB type
                        of columns.
                        Refer 'DataFrame.tdtypes' to get the types of the
                        columns of a teradataml DataFrame.
                     3. "partition_columns" supports only columns specified in
                        groupby function, if window is initiated on DataFrameGroupBy.
                Types: str OR list of Strings (str) OR ColumnExpression OR list of ColumnExpressions

            order_columns:
                Optional Argument.
                Specifies the name(s) of the column(s) to order the rows in a
                partition, which determines the sort order of the rows over
                which the function is applied.
                Notes:
                    1. "order_columns" does not support CLOB and BLOB type
                       of columns.
                       Refer 'DataFrame.tdtypes' to get the types of the
                       columns of a teradataml DataFrame.
                    2. "order_columns" supports only columns specified in
                        groupby, if window is initiated on DataFrameGroupBy.
                    3. When ColumnExpression(s) is(are) passed to "order_columns", then the
                       corresponding expression takes precedence over arguments
                       "sort_ascending" and "nulls_first". Say, ColumnExpression is col1, then
                       1. col1.asc() or col.desc() is effective irrespective of "sort_ascending".
                       2. col1.nulls_first() or col.nulls_last() is effective irrespective of "nulls_first".
                       3. Any combination of above two take precedence over "sort_ascending" and "nulls_first".
                Types: str OR list of Strings (str) OR ColumnExpression OR list of ColumnExpressions

            sort_ascending:
                Optional Argument.
                Specifies whether column ordering should be in ascending or
                descending order.
                Default Value: True (ascending)
                Notes:
                     * When "order_columns" argument is not specified, this argument
                       is ignored.
                     * When ColumnExpression(s) is(are) passed to "order_columns", then the
                       argument is ignored.
                Types: bool

            nulls_first:
                Optional Argument.
                Specifies whether null results are to be listed first or last
                or scattered.
                Default Value: None
                Notes:
                     * When "order_columns" argument is not specified, this argument
                       is ignored.
                     * When "order_columns" is a ColumnExpression(s), this argument
                       is ignored.
                Types: bool

            window_start_point:
                Optional Argument.
                Specifies a starting point for a window. Based on the integer
                value, n, starting point of the window is decided.
                    * If 'n' is negative, window start point is n rows
                      preceding the current row/data point.
                    * If 'n' is positive, window start point is n rows
                      following the current row/data point.
                    * If 'n' is 0, window start at current row itself.
                    * If 'n' is None, window start as Unbounded preceding,
                      i.e., all rows before current row/data point are
                      considered.
                Notes:
                     1. Value passed to this should always satisfy following condition:
                        window_start_point <= window_end_point
                     2. Following functions does not require any window to
                        perform window aggregation. So, "window_start_point" is
                        insignificant for below functions:
                        * cume_dist
                        * rank
                        * dense_rank
                        * percent_rank
                        * row_number
                        * lead
                        * lag
                Default Value: None
                Types: int

            window_end_point:
                Optional Argument.
                Specifies an end point for a window. Based on the integer value,
                n, starting point of the window is decided.
                    * If 'n' is negative, window end point is n rows preceding
                      the current row/data point.
                    * If 'n' is positive, window end point is n rows following
                      the current row/data point.
                    * If 'n' is 0, window end's at current row itself.
                    * If 'n' is None, window end's at Unbounded Following,
                      i.e., all rows before current row/data point are
                      considered.
                Notes:
                     1. Value passed to this should always satisfy following condition:
                        window_start_point <= window_end_point
                     2. Following functions does not require any window to
                        perform window aggregation. So, "window_end_point" is
                        insignificant for below functions:
                        * cume_dist
                        * rank
                        * dense_rank
                        * percent_rank
                        * row_number
                        * lead
                        * lag
                Default Value: None
                Types: int

            ignore_window:
                Optional Argument.
                Specifies a flag to ignore parameters related to creating
                window ("window_start_point", "window_end_point") and use other
                arguments, if specified.
                When set to True, window is ignored, i.e., ROWS clause is not
                included.
                When set to False, window will be created, which is specified
                by "window_start_point" and "window_end_point" parameters.
                Default Value: False
                Types: bool

        RAISES:
            TypeError, ValueError

        RETURNS:
            An object of type Window.

        EXAMPLES:
            # Example 1: Create a window on a teradataml DataFrame.
            >>> load_example_data("dataframe","sales")
            >>> df = DataFrame.from_table('sales')
            >>> window = df.window()
            >>>

            # Example 2: Create a cumulative (expanding) window with rows
            #            between unbounded preceding and 3 preceding with
            #            "partition_columns" and "order_columns" argument with
            #            default sorting.
            >>> window = df.window(partition_columns=df.Feb,
            ...                    order_columns=[df.Feb, "datetime"],
            ...                    window_start_point=None,
            ...                    window_end_point=-3)
            >>>

            # Example 3: Create a moving (rolling) window with rows between
            #            current row and 3 following with sorting done on 'Feb'
            #            in ascending order, datetime' columns in descending order
            #            and "partition_columns" argument.
            >>> window = df.window(partition_columns=df.Feb,
            ...                    order_columns=[df.Feb.asc(), df.datetime.desc()],
            ...                    window_start_point=0,
            ...                    window_end_point=3)
            >>>

            # Example 4: Create a remaining (contracting) window with rows
            #            between current row and unbounded following with
            #            sorting done on 'Feb', 'datetime' columns in ascending
            #            order and NULL values in 'Feb', 'datetime'
            #            columns appears at last.
            >>> window = df.window(partition_columns=df.Feb,
            ...                    order_columns=[df.Feb.nulls_first(), df.datetime.nulls_first()],
            ...                    window_start_point=0,
            ...                    window_end_point=None)
            >>>

            # Example 5: Create a grouping window, with sorting done on 'Feb',
            #            'datetime' columns in ascending order with NULL values
            #            in 'Feb' column appears at first and 'datetime' column
            #            appears at last.
            >>> window = df.window(partition_columns="Feb",
            ...                    order_columns=[df.Feb.nulls_first(), df.datetime.nulls_last()],
            ...                    window_start_point=None,
            ...                    window_end_point=None)
            >>>

            # Example 6: Create a window on a teradataml DataFrame, which
            #            ignores all the parameters while creating window.
            >>> window = df.window(partition_columns=df.Feb,
            ...                    order_columns=[df.Feb.desc().nulls_last(), df.datetime.desc().nulls_last()]
            ...                    ignore_window=True)
            >>>

            # Example 7: Perform sum on every valid column in DataFrame.
            >>> window = df.window(partition_columns="Feb",
            ...                    order_columns=["Feb", "datetime"],
            ...                    sort_ascending=False,
            ...                    nulls_first=False,
            ...                    ignore_window=True)
            >>> window.sum()
                          Feb    Jan    Mar    Apr    datetime  Apr_sum  Feb_sum  Jan_sum  Mar_sum
            accounts
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017      781   1000.0      550      590
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017      781   1000.0      550      590
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017      781   1000.0      550      590
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017      781   1000.0      550      590
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017      781   1000.0      550      590
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017      781   1000.0      550      590

            # Example 8: Perform count on every valid column in DataFrame.
            >>> window = df.window()
            >>> window.count()
                          Feb    Jan    Mar    Apr    datetime  Apr_count  Feb_count  Jan_count  Mar_count  accounts_count  datetime_count
            accounts
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017          4          6          4          4               6               6
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017          4          6          4          4               6               6
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017          4          6          4          4               6               6
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017          4          6          4          4               6               6
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017          4          6          4          4               6               6
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017          4          6          4          4               6               6
            >>>

            # Example 9: Perform count of all the valid columns in teradataml
            # DataFrame, which is grouped by 'accounts'.
            >>> window = df.groupby("accounts").window()
            >>> window.count()
                 accounts  accounts_count
            0   Jones LLC               6
            1     Red Inc               6
            2  Yellow Inc               6
            3  Orange Inc               6
            4    Blue Inc               6
            5    Alpha Co               6
            >>>
        """   
        return Window(object=self,
                      partition_columns=partition_columns,
                      order_columns=order_columns,
                      sort_ascending=sort_ascending,
                      nulls_first=nulls_first,
                      window_start_point=window_start_point,
                      window_end_point=window_end_point,
                      ignore_window=ignore_window)

    @collect_queryband(queryband="DF_dropDuplicate")
    def drop_duplicate(self, column_names=None):
        """
        DESCRIPTION:
            Function drops the duplicate rows, i.e., returns the distinct values from teradataml DataFrame.
            Note:
                * This method does not support operations on array columns.

        PARAMETERS:
            column_names:
                Optional argument.
                Specifies the name(s) of the column(s) to drop the duplicates, i.e., to get the
                distinct values. If not specified, all columns in the DataFrame are considered for
                the operation.
                Types: str OR list of Strings (str)

        RETURNS:
            teradataml DataFrame

        RAISES:
            TeradataMlException

        EXAMPLES:

            # Create a teradataml DataFrame.
            >>> load_example_data("dataframe","admissions_train")
            >>> df = DataFrame('admissions_train')
            >>> df
                masters   gpa     stats programming  admitted
            id
            13      no  4.00  Advanced      Novice         1
            26     yes  3.57  Advanced    Advanced         1
            5       no  3.44    Novice      Novice         0
            19     yes  1.98  Advanced    Advanced         0
            15     yes  4.00  Advanced    Advanced         1
            40     yes  3.95    Novice    Beginner         0
            7      yes  2.33    Novice      Novice         1
            22     yes  3.46    Novice    Beginner         0
            36      no  3.00  Advanced      Novice         0
            38     yes  2.65  Advanced    Beginner         1

            # Example 1: Get the distinct rows of values for the column programming.
            >>> df.drop_duplicate('programming')
              programming
            0    Beginner
            1    Advanced
            2      Novice

            # Example 2: Get the distinct rows of values for the columns programming and admitted.
            >>> df.drop_duplicate(['programming','admitted'])
              programming  admitted
            0    Advanced         1
            1      Novice         0
            2      Novice         1
            3    Beginner         1
            4    Advanced         0
            5    Beginner         0
        """
        arg_info_matrix = []
        arg_info_matrix.append(["column_names", column_names, True, (str, list), True])
        _Validators._validate_function_arguments(arg_info_matrix)

        column_names = UtilFuncs._as_list(column_names)

        if column_names is None or column_names[0] is None:
            column_names = self.columns
        else:
            _Validators._validate_column_exists_in_dataframe(column_names, self._metaexpr,
                                                             False)
            column_names = list(dict.fromkeys(column_names))

        if list_td_reserved_keywords(column_names) or UtilFuncs._is_non_ascii(column_names):
            column_names = UtilFuncs._teradata_quote_arg(column_names, "\"", False)

        col_names_types = df_utils._get_required_columns_types_from_metaexpr(self._metaexpr, column_names)
        sel_nodeid = self._aed_utils._aed_select(self._nodeid, ','.join(column_names), True)
        new_metaexpr = UtilFuncs._get_metaexpr_using_columns(sel_nodeid, col_names_types.items(),
                                                             datalake=self._metaexpr.datalake)

        return self._create_dataframe_from_node(sel_nodeid, new_metaexpr, self._index_label)

    @collect_queryband(queryband="DF_toCsv")
    def to_csv(self, csv_file,
               num_rows=99999,
               all_rows=False,
               fastexport=False,
               sep=",",
               quotechar="\"",
               catch_errors_warnings=False,
               **kwargs):
        """
        DESCRIPTION:
            Export data to CSV from teradataml DataFrame with or
            without FastExport protocol.

        PARAMETERS:
            csv_file:
                Required Argument.
                Specifies the name of CSV file to export the data into.
                Types: str

            num_rows:
                Optional Argument.
                Specifies the number of rows to export.
                Note:
                    This argument is ignored if "all_rows" is set to True.
                Default Value: 99999
                Types: int

            all_rows:
                Optional Argument.
                Specifies whether all rows should be exported to CSV or not.
                Default Value: False
                Types: bool

            fastexport:
                Optional Argument.
                Specifies whether FastExport protocol should be used or not while
                exporting data. When set to True, data is exported using FastExport
                protocol, otherwise FastExport protocol is not used, which is default.
                When set to None, the approach is decided based on the number of rows
                to be exported.
                Notes:
                    1. Teradata recommends to use FastExport when number of rows
                       in teradataml DataFrame are atleast 100,000. To extract
                       lesser rows ignore this option and go with regular
                       approach. FastExport opens multiple data transfer connections
                       to the database.
                    2. FastExport does not support all Teradata Database data types.
                       For example, tables with BLOB and CLOB type columns cannot
                       be extracted.
                    3. FastExport cannot be used to extract data from a
                       volatile or temporary table.
                    4. For best efficiency, do not use DataFrame.groupby() and
                       DataFrame.sort() with FastExport.

                For additional information about FastExport protocol through
                teradatasql driver, please refer to FASTEXPORT section of
                https://pypi.org/project/teradatasql/#FastExport driver documentation.
                Default Value: False
                Types: bool

            sep:
                Optional Argument.
                Specifies a single character string used to separate fields in a CSV file.
                Default Value: ","
                Notes:
                    1. "sep" cannot be line feed ('\\n') or carriage return ('\\r').
                    2. "sep" should not be same as "quotechar".
                    3. Length of "sep" argument should be 1.
                Types: String

            quotechar:
                Optional Argument.
                Specifies a single character string used to quote fields in a CSV file.
                Default Value: "\""
                Notes:
                    1. "quotechar" cannot be line feed ('\\n') or carriage return ('\\r').
                    2. "quotechar" should not be same as "sep".
                    3. Length of "quotechar" argument should be 1.
                Types: String

            catch_errors_warnings:
                Optional Argument.
                Specifies whether to catch errors/warnings (if any) raised by
                FastExport protocol while exporting data.
                Note:
                    This argument is ignored if "fastexport" is set to False.
                Default Value: False
                Types: bool

            kwargs:
                Optional Argument.
                Specifies keyword arguments. Argument "open_sessions" can be
                passed as keyword arguments.
                    * "open_sessions" specifies the number of Teradata data transfer
                      sessions to be opened for fastexport. This argument is only
                      applicable in fastexport mode.

                Note:
                    If "open_sessions" argument is not provided, the default value
                       is the smaller of 8 or the number of AMPs avaialble.
                       For additional information about number of Teradata data-transfer
                       sessions opened during fastexport, please refer to:
                       https://pypi.org/project/teradatasql/#FastExport


        RETURNS:
            When FastExport protocol is used and "catch_errors_warnings" is set to True,
            then the function returns a tuple containing:
                a. Errors, if any, thrown by fastexport in a list of strings.
                b. Warnings, if any, thrown by fastexport in a list of strings.

        RAISES:
            TeradataMlException

        EXAMPLES:

            # Create a teradataml DataFrame.
            >>> load_example_data("dataframe","admissions_train")
            >>> df = DataFrame("admissions_train")
            >>> df
               masters   gpa     stats programming admitted
            id
            22     yes  3.46    Novice    Beginner        0
            37      no  3.52    Novice      Novice        1
            35      no  3.68    Novice    Beginner        1
            12      no  3.65    Novice      Novice        1
            4      yes  3.50  Beginner      Novice        1
            38     yes  2.65  Advanced    Beginner        1
            27     yes  3.96  Advanced    Advanced        0
            39     yes  3.75  Advanced    Beginner        0
            7      yes  2.33    Novice      Novice        1
            40     yes  3.95    Novice    Beginner        0
            ...

            # Example 1: Export data from teradataml DataFrame into CSV,
            #            with only required argument.
            >>> df.to_csv("export_to_csv_1.csv")
               Data is successfully exported into export_to_csv_1.csv

            # Example 2: Export all rows from teradataml DataFrame into CSV
            #            using FastExport protocol.
            >>> df.to_csv("export_to_csv_2.csv", all_rows=True, fastexport=True)
               Data is successfully exported into export_to_csv_2.csv

            # Example 3: Export 20 rows from teradataml DataFrame into CSV.
            >>> df.to_csv("export_to_csv_3.csv", num_rows=20)
               Data is successfully exported into export_to_csv_3.csv

            # Example 4: Export data from teradataml DataFrame into CSV using
            #            FastExport protocol by opening one Teradata data
            #            transfer session. Save errors and warnings
            #            thrown by fastexport.
            >>> err, warn = df.to_csv("export_to_csv_4.csv", fastexport=True,
                                      catch_errors_warnings=True, open_sessions=1 )
               Data is successfully exported into export_to_csv_4.csv
            >>>err
               []
            >>>warn
               []

            # Example 5: Export data from teradataml DataFrame into CSV
            #            file with '|' as field separator and single quote(')
            #            as field quote character.
            >>> df.to_csv("export_to_csv_5.csv", sep="|", quotechar="'" )
               Data is successfully exported into export_to_csv_5.csv

        """

        try:
            awu_matrix = []
            awu_matrix.append(["csv_file", csv_file, False, str, True])
            awu_matrix.append(["fastexport", fastexport, True, (bool)])

            # Get "open_sessions" argument.
            open_sessions = kwargs.get("open_sessions", None)

            # Validate 'fastexport' argument as it is unique to to_csv() function.
            # Remaining args are validated as part of _DataTransferUtils
            # class constructor.
            _Validators._validate_function_arguments(awu_matrix)
            dt_obj = _DataTransferUtils(df=self, num_rows=num_rows,
                                        all_rows=all_rows,
                                        catch_errors_warnings=
                                        catch_errors_warnings,
                                        open_sessions=open_sessions,
                                        sep=sep,
                                        quotechar=quotechar
                                        )

            if not csv_file.lower().endswith(".csv"):
                raise TeradataMlException(
                    Messages.get_message(MessageCodes.INVALID_ARG_VALUE, csv_file,
                                         "csv_file", "file with csv format"),
                    MessageCodes.INVALID_ARG_VALUE)

            # If fastexport is set to True, call _get_csv() in fastexport mode.
            if fastexport:
                return dt_obj._get_csv(require_fastexport=True,
                                       csv_file_name=csv_file,
                                       **kwargs)
            # If fastexport is None & num_rows greater than or equal to 100,000
            #  call _get_csv() in fastexport mode.
            elif fastexport is None and (not all_rows and num_rows >= 100000):
                return dt_obj._get_csv(require_fastexport=False,
                                       csv_file_name=csv_file,
                                       **kwargs)
            # If fastexport is not specified/required fallback to regular
            # approach.
            else:
                # Execute node if un-executed.
                self.__execute_node_and_set_table_name(self._nodeid,
                                                       self._metaexpr)
                return dt_obj._get_csv(require_fastexport=None,
                                       csv_file_name=csv_file)
        except TeradataMlException:
            raise
        except TypeError:
            raise
        except ValueError:
            raise
        except Exception as err:
            raise TeradataMlException(
                Messages.get_message(MessageCodes.DATA_EXPORT_FAILED, "to_csv",
                                     "CSV file", str(err)),
                MessageCodes.DATA_EXPORT_FAILED)

    @collect_queryband(queryband="DF_pivot")
    def pivot(self,
              columns=None,
              aggfuncs=None,
              limit_combinations=False,
              margins=None,
              returns=None,
              all_columns=False,
              **kwargs):
        """
        DESCRIPTION:
            Rotate data from rows into columns to create easy-to-read DataFrames. 
            The function is useful for reporting purposes, as it allows to 
            aggregate and rotate the data.  
            Note:
                * This method does not support DataFrames that contain array columns

        PARAMETERS:
            columns:
                Required when keyword arguments are not specified, optional otherwise.
                Specifies the column(s) or dictionary of column(s) with distinct column 
                value(s) used for pivoting the data.
                * When specified as a column(s), function automatically extracts 
                  the distinct values for the column.
                  For example:
                    columns = df.qtr # or columns = [df.qtr, dr.yr]
                  Note that, distinct values 'Q1', 'Q2', 'Q3', ... are 
                  automatically extracted from column 'qtr' for pivoting.
                * When specified as dictionary:
                    - key is a column expression representing a column
                    - value is a literal or list of literals, i.e., values in a column 
                      or a teradataml DataFrame with only one column.
                      Note:
                          When value is a teradataml DataFrame, the order of output pivoted columns
                          are not guarenteed. Hence value is a teradataml DataFrame, teradata recommonds
                          to not specify "returns" argument so column names are generated properly
                          according to the order of records.
                  When dictionary value contains literals then pivot is done based on the each 
                  combination for the values specified for each key, if "limit_combinations" is 
                  set to False, otherwise combinations are restricted based on the index values 
                  of the list.
                  Notes:
                       * All the values in dictionary should have same length when "limit_combinations"
                          is set to True.
                          Take a look at the examples below to understand:
                            - columns={df.qtr: ['Q1', 'Q2'], df.year: [2001, 2002]}
                              Pivot is based on all the combinations of values as specified
                              below for columns 'qtr' and 'yr'.
                                If "limit_combinations" is set to False.
                                    (Q1, 2001), (Q1, 2002), (Q2, 2001), and (Q2, 2002).
                                If "limit_combinations" is set to True.
                                    (Q1, 2001) and (Q2, 2002).
                            - columns={df.qtr: quarter_df, df.year: [2001, 2002]}
                              Note that, value passed to df.qtr is a teradataml DataFrame
                              with one column. In such cases, function extracts the distinct
                              values from the dataframe and Pivot is based on all the combinations
                              of values or limited combination of values based on argument
                              "limit_combinations".
                      * String type values in dictionary are not case sensitive. Refer to example 2
                        for more details.
                Types: ColumnExpression OR list of ColumnExpression or dict

            aggfuncs:
                Required Argument.
                Specifies the column aggregate function(s) to be used for pivoting the data.
                For example:
                    To pivot total of 'sales' column and average of 'cogs' + 'sales' columns
                    in DataFrame (df), specify "aggfuncs" as:
                        aggfuncs=[df.sales.sum(), (df.cogs+df.sales).avg()]
                Types: ColumnExpression OR list of ColumnExpression
            
            limit_combinations:
                Optional Argument. 
                Specifies whether to limit the number of combinations when "columns" argument
                is passed as a dictionary.
                When set to True, function limits the combinations one-to-one
                based on index of the values in list, hence all dictionary values 
                should be a list of same length or a single literal.
                For example:
                    df.pivot(columns={df.qtr: ['Q1', 'Q2'], df.year: [2001, 2002]}, 
                             limit_combinations=True,....)
                Pivot will be based on columns 'qtr' and 'yr' for values 
                (Q1, 2001) and (Q2, 2002) only.
                Note:
                    Argument is ignored when "columns" is a ColumnExpression or a list of ColumnExpressions.
                Default Value: False
                Types: bool
            
            margins:
                Optional Argument.
                Specifies the aggregate operation to be performed on output columns.
                Aggregate operation to be performed should be passed as dictionary where:
                    * Key is a string specifying aggregate operation to be performed.
                    * Value is a tuple or list of tuples specifying the output column names as 
                      string. Columns specified in the tuple are used in aggregate operation 
                      performed.
                For example, if for the year 2001 following three aggregate columns are needed
                in the output:
                    1. Sum of sales in Q1 and Q2
                    2. Sum of sales in Q2 and Q3
                    3. Average of cogs for first three quarters
                "margins" can be sepcified as:
                    margins={"SUM": [("Q12001_sales", "Q22001_sales"), 
                                     ("Q22001_sales", "Q32001_sales")],
                             "AVG": ("Q12001_cogs", "Q22001_cogs", "Q32001_cogs")}
                Notes:
                     * Supported aggregate functions are SUM, AVG, MIN and MAX.
                     * If "returns" is specified, column names for margins are considered
                       from returns clause.
                Types: dict
            
            returns:
                Optional Argument.
                Specifies the custom column name(s) to be returned in the output DataFrame.
                Notes:
                    * If not specified, function internally generate the output column name(s).
                    * Number of column names should be same as columns generated by pivot.
                      For example: 
                        If columns={df.qtr: ['Q1', 'Q2'], df.year: [2001, 2002]},
                           aggfuncs=[df.sales.sum(), df.cogs.sum()] and
                           limit_combination=False, then
                        number of new columns in output will be:
                            len(aggfuncs) * len(columns[df.year]) * len(columns[df.qtr])
                            or 2 * 2 * 2 = 8
                        Hence, "returns" should be list of string with 8 elements.
                        If limit_combination is set to 'True' in above example,
                        number of new columns is output will be:
                            len(aggfuncs) * (len(columns[df.year]) OR len(columns[df.qtr]))
                            or or 2 * 2 = 4
                        Hence, "returns" should be list of string with 4 elements.
                    * All columns including columns in DataFrame which do not participate in pivot
                      must be specified if "all_columns" is set to True.
                Types: str OR list of Strings (str)
            
            all_columns:
                Optional Argument.
                Specifies whether "returns" argument should include only the names of pivot 
                columns or all columns.
                When set to True, all columns including columns in DataFrame which do not
                participate in pivot must be specified.
                When set to False, only output columns excluding columns in DataFrame which do not
                participate in pivot must be specified.
                Default Value: False
                Types: bool
            
            **kwargs:
                Specifies the keyword argument to accept column name and column value(s)
                as named arguments.
                For example:
                    col1=df.year, col1_value=2001, col2=df.qtr, col2_value=['Q1', 'Q2', 'Q3']
                Notes:
                    * Either use "columns" argument or keyword arguments.
                    * Format for column name arguments should be col1, col2,.. colN with
                      values of type ColumnExpression.
                    * Format for column value argument should be col1_value, col2_value,.. colN_value.

        RETURNS:
                teradataml DataFrame
                Notes:
                    * The columns which are not participating in pivoting are always aligned to 
                      left most in the output DataFrame. 
                    * Order of pivot columns in output DataFrame follows the same order as the 
                      values specified in argument "columns" or the order in keyword arguments. 
                    * The name of the output columns are according to the name specified in 
                      "returns" argument.
                      If not specified, column names are generated based on aggregate functions
                      and values in "columns" or keyword arguments for column values.
                      For Example:
                        When columns={df.qtr:["Q1", "Q2"]} and aggfuncs=[df.col1.sum(), df.col2.max()]
                        Column names are:
                            'sum_col1_q1', 'max_col2_q1', 'sum_col1_Q2', and 'max_col2_Q2'.

        RAISES:
            TypeError, ValueError, TeradataMLException


        EXAMPLES:
            # Create a teradataml DataFrame.
            >>> load_example_data("teradataml", "star1")
            >>> df = DataFrame("star1")
            >>> print(df)
                    state    yr qtr  sales  cogs
            country
            USA        NY  2001  Q1   45.0  25.0
            CANADA     ON  2001  Q2   10.0   0.0
            CANADA     BC  2001  Q3   10.0   0.0
            USA        CA  2002  Q2   50.0  20.0
            USA        CA  2002  Q1   30.0  15.0

            # Example 1: Pivot data using all values in column 'qtr' and aggregate using 
            #            sum of column 'sales'.
            >>> pivot_df = df.pivot(columns=df.qtr,
            ...                     aggfuncs=df.sales.sum())
            >>> print(pivot_df)
              country state    yr  cogs  sum_sales_q2  sum_sales_q1  sum_sales_q3
            0     USA    NY  2001  25.0           NaN          45.0           NaN
            1  CANADA    ON  2001   0.0          10.0           NaN           NaN
            2  CANADA    BC  2001   0.0           NaN           NaN          10.0
            3     USA    CA  2002  20.0          50.0           NaN           NaN
            4     USA    CA  2002  15.0           NaN          30.0           NaN

            # Example 2: Pivot data using columns 'yr' and 'qtr', aggregate using sum of
            #            'sales' and median of 'cogs' column. Limit combination of 'Q1' with '2001' and
            #            'Q2' with '2002'.
            >>> pivot_df = df.pivot(columns={df.yr: [2001, 2002], df.qtr: ['Q1', 'Q2']},
            ...                     aggfuncs=[df.sales.sum(), df.cogs.median()],
            ...                     limit_combinations=True)
            >>> print(pivot_df)
              country state  sum_sales_2001_q1  median_cogs_2001_q1  sum_sales_2002_q2  median_cogs_2002_q2
            0  CANADA    ON                NaN                  NaN                NaN                  NaN
            1     USA    CA                NaN                  NaN               50.0                 20.0
            2     USA    NY               45.0                 25.0                NaN                  NaN
            3  CANADA    BC                NaN                  NaN                NaN                  NaN

            # Example 3: Get all possible column combinations when pivoting using 'yr'and 'qtr',
            #            aggreagation using 'sales' and 'cogs' column.
            >>> pivot_df = df.pivot(columns={df.yr: [2001, 2002], df.qtr: ['q1', 'q2']},
            ...                     aggfuncs=[df.sales.sum(), df.cogs.max()])
            >>> print(pivot_df)
              country state  sum_sales_2001_q1  max_cogs_2001_q1  sum_sales_2001_q2  max_cogs_2001_q2  sum_sales_2002_q1  max_cogs_2002_q1  sum_sales_2002_q2  max_cogs_2002_q2
            0  CANADA    BC                NaN               NaN                NaN               NaN                NaN               NaN                NaN               NaN
            1  CANADA    ON                NaN               NaN               10.0               0.0                NaN               NaN                NaN               NaN
            2     USA    NY               45.0              25.0                NaN               NaN                NaN               NaN                NaN               NaN
            3     USA    CA                NaN               NaN                NaN               NaN               30.0              15.0               50.0              20.0

            # Example 4: Custom name the returned columns using "returns" argument.
            >>> pivot_df = df.pivot(columns={df.yr:2001, df.qtr:['Q1', 'Q2']},
            ...                     aggfuncs=[df.sales.sum(), (2*df.sales-1).max()],
            ...                     returns=["Q1_2001_total_sales", "Q1_2001_total_cogs",
            ...                              "Q2_2001_total_sales", "Q2_2001_total_cogs"])
            >>> print(pivot_df)
              country state  cogs  Q1_2001_total_sales  Q1_2001_total_cogs  Q2_2001_total_sales  Q2_2001_total_cogs
            0     USA    NY  25.0                 45.0                89.0                  NaN                 NaN
            1  CANADA    ON   0.0                  NaN                 NaN                 10.0                19.0
            2  CANADA    BC   0.0                  NaN                 NaN                  NaN                 NaN
            3     USA    CA  20.0                  NaN                 NaN                  NaN                 NaN
            4     USA    CA  15.0                  NaN                 NaN                  NaN                 NaN

            # Example 5: Custom name all output columns using "returns" and "all_columns" argument.
            >>> pivot_df = df.pivot(columns={df.yr:2001, df.qtr:['Q1', 'Q2']},
            ...                     aggfuncs=[df.sales.avg(), df.cogs.median()],
            ...                     returns=["con", "st", "Q1_2001_total_sales", "Q1_2001_total_cogs",
            ...                              "Q2_2001_total_sales", "Q2_2001_total_cogs"],
            ...                     all_columns=True)
            >>> print(pivot_df)
                  con  st  Q1_2001_total_sales  Q1_2001_total_cogs  Q2_2001_total_sales  Q2_2001_total_cogs
            0  CANADA  ON                  NaN                 NaN                 10.0                 0.0
            1     USA  CA                  NaN                 NaN                  NaN                 NaN
            2     USA  NY                 45.0                25.0                  NaN                 NaN
            3  CANADA  BC                  NaN                 NaN                  NaN                 NaN

            # Example 6: Use keyword arguments to specify columns and values instead of "columns" argument.
            #            Note since "returns" is not specifies, column names are function generated.
            >>> pivot_df = df.pivot(aggfuncs=[df.sales.avg(), ((2*df.sales)+(df.cogs/2)).min()],
            ...                     col1=df.yr,
            ...                     col1_values=2001,
            ...                     col2=df.qtr,
            ...                     col2_values=['Q1', 'Q2'])
            >>> print(pivot_df)
              country state  avg_sales_2001_q1  min_sales_2_cogs_2_2001_q1  avg_sales_2001_q2  min_sales_2_cogs_2_2001_q2
            0  CANADA    BC                NaN                         NaN                NaN                         NaN
            1  CANADA    ON                NaN                         NaN               10.0                        20.0
            2     USA    NY               45.0                       102.5                NaN                         NaN
            3     USA    CA                NaN                         NaN                NaN                         NaN

            # Example 7: Find the median sales and mean cogs for first three quarters of
            #            year 2001 using "margins".
            >>> pivot_df = df.pivot(columns={df.qtr: ['q1', 'q2', 'q3'], df.yr: [2001]},
            ...                     aggfuncs=df.sales.median(),
            ...                     margins={"SUM": [("median_sales_q1_2001", "median_sales_q2_2001"),
            ...                                      ("median_sales_q3_2001", "median_sales_q2_2001")],
            ...                              "AVG": [("median_sales_q1_2001", "median_sales_q2_2001"),
            ...                                      ("median_sales_q2_2001", "median_sales_q1_2001")]})
            >>> print(pivot_df)
              country state  cogs  median_sales_q1_2001  median_sales_q2_2001  median_sales_q3_2001  sum_median_sales_q1_2001_median_sales_q2_2001  sum_median_sales_q3_2001_median_sales_q2_2001  avg_median_sales_q1_2001_median_sales_q2_2001  avg_median_sales_q2_2001_median_sales_q1_2001
            0  CANADA    BC   0.0                   NaN                   NaN                  10.0                                            NaN                                           10.0                                            NaN                                            NaN
            1     USA    NY  25.0                  45.0                   NaN                   NaN                                           45.0                                            NaN                                           45.0                                           45.0
            2  CANADA    ON   0.0                   NaN                  10.0                   NaN                                           10.0                                           10.0                                           10.0                                           10.0
            3     USA    CA  20.0                   NaN                   NaN                   NaN                                            NaN                                            NaN                                            NaN                                            NaN
            4     USA    CA  15.0                   NaN                   NaN                   NaN                                            NaN                                            NaN                                            NaN                                            NaN

            # Example 8: Find the min of sales and average of cogs for year '2001' in
            #            quater 'Q1', 'Q2'. Name the aggregated column as 'margins'.
            >>> pivot_df = df.pivot(columns={df.qtr: ['q1', 'q2'], df.yr: [2001]},
            ...                      aggfuncs=[df.cogs.min(), df.sales.avg()],
            ...                      margins={"MIN": ("cogs_min_q12001", "sales_var_samp_q22001")},
            ...                      all_columns=True,
            ...                      returns=["con",
            ...                               "st",
            ...                               "cogs_min_q12001",
            ...                               "sales_var_samp_q12001",
            ...                               "cogs_min_q22001",
            ...                               "sales_var_samp_q22001",
            ...                               "margins"])
            >>> print(pivot_df)
              con  st  cogs_min_q12001  sales_var_samp_q12001  cogs_min_q22001  sales_var_samp_q22001  margins
            0  CANADA  BC              NaN                    NaN              NaN                    NaN      NaN
            1  CANADA  ON              NaN                    NaN              0.0                   10.0     10.0
            2     USA  NY             25.0                   45.0              NaN                    NaN     25.0
            3     USA  CA              NaN                    NaN              NaN                    NaN      NaN

            # Example 9: Specify teradataml DataFrame with one column for values of 'qtr' column and pivot
            #            the data. Note this allows user to implicitly use all the distinct values for
            #            the pivot operation and not specify those explicitly.
            >>> quarters_df = df.drop(columns=['country', 'state', 'yr', 'sales', 'cogs'])
            >>> print(quarters_df)
              qtr
            0  Q1
            1  Q2
            2  Q3
            3  Q2
            4  Q1
            >>>
            >>> pivot_df = df.pivot(columns={df.qtr: quarters_df},
            ...                     aggfuncs=[df.sales.sum(), df.cogs.avg()])
            >>> print(pivot_df)
              country state    yr  sum_sales_q2  avg_cogs_q2  sum_sales_q1  avg_cogs_q1  sum_sales_q3  avg_cogs_q3
            0  CANADA    ON  2001          10.0          0.0           NaN          NaN           NaN          NaN
            1  CANADA    BC  2001           NaN          NaN           NaN          NaN          10.0          0.0
            2     USA    NY  2001           NaN          NaN          45.0         25.0           NaN          NaN
            3     USA    CA  2002          50.0         20.0          30.0         15.0           NaN          NaN
        """

        arg_info_matrix = []
        col1 = kwargs.get("col1", None)
        _Validators._validate_mutually_exclusive_arguments(columns, "columns", col1, "col1")
        # If keyword column argument col1 is provided, "columns" is not required.
        columns_arg_req = False if col1 is not None else True
        # Argument type validation.
        arg_info_matrix.append(["aggfuncs", aggfuncs, False, (ColumnExpression, list)])
        arg_info_matrix.append(["limit_combinations", limit_combinations, True, (bool)])
        arg_info_matrix.append(["margins", margins, True, (dict)])
        arg_info_matrix.append(["returns", returns, True, (str, list), True])
        arg_info_matrix.append(["all_columns", all_columns, True, (bool)])

        arg_name = "columns"
        if columns is None:
            columns = {}
            for i in range(1, int(len(kwargs) / 2) + 1):
                # Get the values of colN where N is in range(1, half the length of kwargs + 1).
                col = kwargs.get("col{}".format(i))
                col_val = kwargs.get("col{}_values".format(i))
                arg_info_matrix.append(["col{}".format(i), col, not columns_arg_req, (ColumnExpression)])
                columns[col] = col_val

            arg_name = "kwargs"
        arg_info_matrix.append(["columns", columns, columns_arg_req, (ColumnExpression, _ListOf(ColumnExpression), dict)])
        _Validators._validate_function_arguments(arg_info_matrix)

        if columns_arg_req and isinstance(columns, dict):
            expected_value_types = (int, float, str, _ListOf(int), _ListOf(float), _ListOf(str), DataFrame)
            _Validators._validate_dict_argument_key_value(arg_name="columns", arg_dict=columns,
                                                          key_types=(ColumnExpression,),
                                                          value_types=expected_value_types)
        if margins:
            _Validators._validate_dict_argument_key_value(arg_name="margins", arg_dict=margins,
                                                          key_types=str, value_types=(tuple, list),
                                                          key_permitted_values=["SUM", "AVG", "MIN", "MAX"])

        # _table_name will be populated only if query is executed on Vantage.
        # Since most of the operations are lazy operations, there are chances that
        # _table_name may not be populated for some operation. Hence, if _table_name
        # is not populated, execute that node using AED and get _table_name.
        if self._table_name is None:
            self._table_name = df_utils._execute_node_return_db_object_name(self._nodeid, self._metaexpr)

        # Generating FOR clause.
        participating_columns, aggfunc_columns, aggr_func_expression = [], [], []

        # Variable to store values of FOR clause.
        columns_values = []

        # Variable to store the names of additional columns.
        for_clause_column_names = []

        for func in UtilFuncs._as_list(aggfuncs):
            aggr_func_expression.append(func.compile())
            aggfunc_columns = aggfunc_columns + func._all_columns

        # User can pass a single ColumnExpression i.e., df.pivot(columns=df.qtr, aggfuncs=df.sales.sum(),..)
        # Convert it to a list.
        if isinstance(columns, ColumnExpression):
            columns = UtilFuncs._as_list(columns)

        is_column_values_passed = isinstance(columns, dict)

        # Input can be a list or a dictionary.
        _prev_arg, _prev_val = None, None
        for column in columns:
            participating_columns.append(column.name)

            if is_column_values_passed:

                if isinstance(columns[column], DataFrame):
                    _v_df = columns[column]
                    if len(_v_df.columns) != 1:
                        err_ = Messages.get_message(MessageCodes.FUNC_EXECUTION_FAILED,
                                                    "'pivot'",
                                                    "DataFrame specified as value in 'columns' argument "
                                                    "should have only one column.")
                        raise ValueError(err_)
                    _column_value = [*(i[0] for i in _v_df.drop_duplicate().get_values())]
                else:
                    # We are allowing users to pass an int, str, float or list of int, float, str.
                    # Convert it to list, if it is not a list.
                    _column_value = UtilFuncs._as_list(columns[column])

                # Perform limit_combinations validation.
                #   If limit_combinations is True, all the values for "columns" should be of same length.
                if _prev_arg and limit_combinations:
                    if len(_prev_val) != len(_column_value):
                        err_ = Messages.get_message(MessageCodes.INVALID_LENGTH_ARG,
                                                    "values in the dictionary",
                                                    arg_name,
                                                    "same when 'limit_combinations' is True")
                        raise ValueError(err_)

                columns_values.append(_column_value)

                _prev_arg = column.name
                _prev_val = _column_value

        if is_column_values_passed:
            # If limit_combinations is True and column_values = [(1, 2, 3), (4, 5, 6)],
            #   then distinct_values should be [(1, 4), (2, 5), (3,6)]
            # If limit_combinations is False and column_values = [(1, 2, 3), (4, 5, 6)],
            #   then distinct_values should be [(1, 4), (1, 5), (1, 6), (2, 4), (2, 5),
            #                                   (2, 6), (3, 4), (3, 5), (3, 6)]
            distinct_values = map(tuple, zip(*columns_values)) if limit_combinations else \
                itertools.product(*columns_values)
        # If user do not pass values for FOR clause, retrieve it.
        else:
            # Get the values from drop_duplicates DataFrame.
            distinct_values = (tuple(rec) for rec in self.drop_duplicate(
                column_names=participating_columns).itertuples(name=None))

        ### IN clause generation.
        # Get the values and construct the string for IN clause.
        #   IN clause example1 - FOR IN (('VAL'))
        #   IN clause example2 - FOR IN (('VAL1', 'VAL2'), ('VAL3', 'VAL4'))

        # Variable to hold the IN clause string.
        for_clause_values = "("
        seperator = ""

        # Iterator of tuple's. Every tuple represents value for IN Clause.
        for _value in distinct_values:

            for_clause_value = '{}'.format(_value if len(_value) > 1 else "('{}')".format(_value[0]))
            for_clause_values = for_clause_values + seperator + for_clause_value

            # Generate the column names. Incase returns is not sourced, use this in derived table clause.
            #   Example - if user specifies aggfuncs as [df.Sales.sum(), df.cogs.avg()] and
            #             distinct values are [('VAL1', 'VAL2'), ('VAL3', 'VAL4')], then teradataml
            #             generates the names as SUM_SALES_VAL1_VAL3, SUM_SALES_VAL1_VAL4, SUM_SALES_VAL2_VAL3,
            #             SUM_SALES_VAL2_VAL4, AVG_SALES_VAL1_VAL3, AVG_SALES_VAL1_VAL4, AVG_SALES_VAL2_VAL3,
            #             AVG_SALES_VAL2_VAL4.
            for _agg_func in aggr_func_expression:
                for_clause_column_names.append((_agg_func + "_".join(map(str, _value))).lower())

            seperator = ","

        # Raise error if no data in distinct values.
        if len(for_clause_column_names) == 0:
            err_ = Messages.get_message(MessageCodes.FUNC_EXECUTION_FAILED,
                                        "'pivot'",
                                        "No data in DataFrame.")
            raise ValueError(err_)

        for_clause_values = for_clause_values + ")"

        ### FOR IN clause generation.
        for_clause = "{} FOR ({}) IN {}".format(", ".join(aggr_func_expression),
                                                ", ".join(participating_columns),
                                                for_clause_values)

        participating_columns = participating_columns + list(set(aggfunc_columns))
        non_participating_columns = [col for col in self.columns if col not in participating_columns]

        # Generating WITH clause.
        with_clause, seperator = "", ""
        with_clause_column_names = []
        if margins:
            # margins will be a dict. Key is analytic function name. Value can be a tuple or list of tuple.
            # Case 1: {"SUM": ("col1", "col2"), "AVG": ("col1", "col3")}
            # Case 2: {"SUM": [("col1", "col2"), ("col3", "col4")],  "AVG": ("col1", "col3")}
            for agg_func, columns in margins.items():
                columns = (c for c in UtilFuncs._as_list(columns))
                for func, cols in itertools.product([agg_func.lower()], columns):
                    with_clause = with_clause + seperator + "{}({})".format(func, ", ".join(cols))
                    seperator = ", "
                    with_clause_column_names.append("_".join((func, *cols)))
            with_clause = "WITH {}".format(with_clause)

        expected_columns = len(for_clause_column_names) + len(with_clause_column_names)
        # If all_columns is True, then user passes all the columns of output pivot DataFrame.
        if all_columns:
            expected_columns = len(non_participating_columns) + expected_columns

        if returns:
            _Validators._validate_arg_length(arg_name="returns",
                                             arg_value=returns,
                                             length=expected_columns,
                                             op="EQ")

        # Generate derived table clause.
        if returns:
            if all_columns:
                cols = returns
            else:
                cols = non_participating_columns + returns
        else:
            # Replace special characters. If returns is not specified, teradataml should generate the
            # column names. Column name generation is based on aggfuncs and distinct values of columns.
            # Example - If user passes aggfuncs as (df.x+(df.y*df.Col)).sum(), then compiled expression
            #           returns SUM(x+(y*"Col")). While generating a name for a column using this, it
            #           should be generated as SUM_x_y_Col. The below snippet converts SUM(x+(y*"Col"))
            #           to SUM_x_y_Col.
            for_clause_column_names = [UtilFuncs._replace_special_chars(c) for c in for_clause_column_names]
            cols = non_participating_columns + for_clause_column_names + with_clause_column_names

        tmp_clause = 'Tmp({})'.format(", ".join(('"{}"'.format(c) for c in cols)))

        sql = "SELECT * FROM {} PIVOT ({for_clause} {with_clause}) {derived_table_clause}".format(
            self._table_name, for_clause=for_clause, with_clause=with_clause, derived_table_clause=tmp_clause)
        return DataFrame.from_query(sql)

    @collect_queryband(queryband="DF_unpivot")
    def unpivot(self,
                columns=None,
                transpose_column=None,
                measure_columns=None,
                exclude_nulls=True,
                returns=None,
                all_columns=False,
                **kwargs):
        """
        DESCRIPTION:
            Rotate data from columns into rows to create easy-to-read DataFrames.  
            Note:
                * This method does not support DataFrame that contain array columns.

        PARAMETERS:
            columns:
                Required when keyword arguments are not specified, optional otherwise.
                Specifies the dictonary of column(s) with distinct column 
                value(s) used for unpivoting the data.
                Notes:
                    * key is a column expression or tuple of column expressions.
                    * value is a literal to be taken by column when transposed 
                      into "transpose_column". If not specified, value is generated
                      by the function based on column names.
                    * Number of columns specified in each key should be equal.
                For example:
                    columns={(df.Q101Sales, df.Q101Cogs): "2001_Q1",
                             (df.Q201Sales, df.Q201Cogs): None,
                             (df.Q301Sales, df.Q301Cogs): "2001_Q3"},
                    transpose_column="yr_qtr",
                    This example transposes columns 'Q101Sales' and 'Q101Cogs' 
                    into a row where 'yr_qtr' column value would be '2001_Q1'.
                    Similarly, 'Q301Sales' and 'Q301Cogs' into row 
                    with '2001_Q3' value in 'yr_qtr' column.
                    For 'Q201Sales' and 'Q201Cogs' value of 'yr_qtr' is
                    function generated.
                Types: dict

            transpose_column:
                Required Argument.
                Specifies the name of the column in the output DataFrame, which holds 
                the data for columns specified in keys of "columns" argument.
                Types: str
            
            measure_columns:
                Required Argument. 
                Specifies the name(s) of output column(s) to unpivot the data in the columns
                specified in "columns" argument.
                Notes:
                    * Number of columns specified in "measure_columns" should be equal 
                      to number of columns in specified in each key of "columns".
                    * One exception for above is when all the columns is to be unpivoted
                      into a single column. In such case, "measure_columns" should be
                      specified as a string or list of string with one value.
                Types: str or list of str (string)
            
            exclude_nulls:
                Optional Argument.
                Specifies whether to exclude NULL(None) values while unpivoting the data.
                When set to True, excludes NULL(None), otherwise includes NULL.
                Default Value: True
                Types: bool
            
            returns:
                Optional Argument.
                Specifies the custom column name(s) to be returned in the output DataFrame.
                Notes:
                    * Number of column names should be equal to one greater than number in
                      "measure_column". 
                    * All columns including columns in DataFrame which do not participate 
                      in unpivot must be specified if "all_columns" is set to True.
                Types: str OR list of Strings (str)
            
            all_columns:
                Optional Argument.
                Specifies whether "returns" argument should include the names of only unpivot 
                columns or all columns.
                When set to True, all columns including columns in DataFrame which do not
                participate in unpivot must be specified.
                When set to False, only output columns excluding columns in DataFrame which do not
                participate in unpivot must be specified.
                Default Value: False
                Types: bool
            
            **kwargs:
                Specifies the keyword argument to accept column name and column value(s)
                as named arguments.
                For example:
                    col1=df.year, col1_value=2001, col2=df.qtr, col2_value='Q1'
                    or 
                    col1=(df.Q101Sales, df.Q101Cogs), col1_value="2001_Q1",
                    col2=(df.Q201Sales, df.Q201Cogs), col2_value=None,
                    col3=(df.Q301Sales, df.Q301Cogs), col3_value="2001_Q3"
                Notes:
                    * Either use "columns" argument or keyword arguments.
                    * Format for column name arguments should be col1, col2,.. colN with
                      values of type ColumnExpression.
                    * Format for column value argument should be col1_value, col2_value,.. colN_value.
                      
            
        RETURNS:
                teradataml DataFrame
                Notes:
                    * The columns which are not participating in unpivoting always aligned to 
                      left most in the output DataFrame. 
                    * Order of unpivot columns in output DataFrame follows the same order as the 
                      values specified in argument "columns" or the order in keyword arguments. 
                    * The output columns are named according to the values specified in 
                      "transpose_column" and "measure_columns" or "returns" argument.


        RAISES:
            TypeError, ValueError, TeradataMLException


        EXAMPLES:
            # Create a teradataml DataFrame.
            >>> load_example_data("teradataml", "star1")
            >>> df = DataFrame("star1")
            >>> print(df)
                    state    yr qtr  sales  cogs
            country
            USA        NY  2001  Q1   45.0  25.0
            CANADA     ON  2001  Q2   10.0   0.0
            CANADA     BC  2001  Q3   10.0   0.0
            USA        CA  2002  Q2   50.0  20.0
            USA        CA  2002  Q1   30.0  15.0

            # Create a pivot DataFrame. 
            >>> df = df.pivot(columns={df.qtr: ["Q1", "Q2", "Q3"], df.yr: ["2001"]},
            ...               aggfuncs=[df.sales.sum(), df.cogs.sum()],
            ...               returns=["Q101Sales", "Q201Sales", "Q301Sales",
            ...                        "Q101Cogs", "Q201Cogs", "Q301Cogs"])
            >>> print(df)
              country state  Q101Sales  Q201Sales  Q301Sales  Q101Cogs  Q201Cogs  Q301Cogs
            0  CANADA    BC        NaN        NaN        NaN       NaN      10.0       0.0
            1  CANADA    ON        NaN        NaN       10.0       0.0       NaN       NaN
            2     USA    NY       45.0       25.0        NaN       NaN       NaN       NaN
            3     USA    CA        NaN        NaN        NaN       NaN       NaN       NaN
            
            # Example 1: Unpivot quarterly sales data to 'sales' column and quarterly 
            #            cogs data to 'cogs' columns using "columns" argument.
            >>> unpivot_df = df.unpivot(columns={(df.Q101Sales, df.Q101Cogs): "2001_Q1",
            ...                                  (df.Q201Sales, df.Q201Cogs): None,
            ...                                  (df.Q301Sales, df.Q301Cogs): "2001_Q3"},
            ...                         transpose_column="yr_qtr",
            ...                         measure_columns=["sales", "cogs"])
            >>> print(unpivot_df)
              country state              yr_qtr  sales  cogs
            0  CANADA    BC  Q201Sales_Q201Cogs    NaN  10.0
            1  CANADA    ON             2001_Q1    NaN   0.0
            2  CANADA    ON             2001_Q3   10.0   NaN
            3  CANADA    BC             2001_Q3    NaN   0.0
            4     USA    NY  Q201Sales_Q201Cogs   25.0   NaN
            5     USA    NY             2001_Q1   45.0   NaN

            # Example 2: Unpivot 'sales' and 'cogs' in to a single column 'sales_cogs'.
            >>> unpivot_df = df.unpivot(columns={(df.Q101Sales, df.Q101Cogs,
            ...                                  df.Q201Sales, df.Q201Cogs,
            ...                                  df.Q301Sales, df.Q301Cogs): None},
            ...                         transpose_column="yr_qtr",
            ...                         measure_columns="sales_cogs")
            >>> print(unpivot_df)
              country state     yr_qtr  sales_cogs
            0  CANADA    BC   Q201Cogs        10.0
            1  CANADA    ON   Q101Cogs         0.0
            2  CANADA    ON  Q301Sales        10.0
            3  CANADA    BC   Q301Cogs         0.0
            4     USA    NY  Q201Sales        25.0
            5     USA    NY  Q101Sales        45.0

            # Example 3: Unpivot quarterly sales data to 'sales' column and quarterly 
            #            cogs data to 'cogs' columns using keyword arguments.    
            >>> unpivot_df = df.unpivot(transpose_column="yr_qtr",
            ...                         measure_columns=["sales", "cogs"],
            ...                         col1 = (df.Q101Sales, df.Q101Cogs),
            ...                         col1_value = "Q101",
            ...                         col2 = (df.Q201Sales, df.Q201Cogs),
            ...                         col2_value = None,
            ...                         col3 = (df.Q301Sales, df.Q301Cogs),
            ...                         col3_value = "Q103")
            >>> print(unpivot_df)
              country state              yr_qtr  sales  cogs
            0  CANADA    BC  Q201Sales_Q201Cogs    NaN  10.0
            1  CANADA    ON                Q101    NaN   0.0
            2  CANADA    ON                Q103   10.0   NaN
            3  CANADA    BC                Q103    NaN   0.0
            4     USA    NY  Q201Sales_Q201Cogs   25.0   NaN
            5     USA    NY                Q101   45.0   NaN


            # Example 4: Unpivot quarterly sales data to 'sales' column and quarterly
            #            cogs data to 'cogs' columns and rename column of the
            #            output using "returns" argument.
            >>> unpivot_df = df.unpivot(columns={(df.Q101Sales, df.Q101Cogs): "Q101",
            ...                                  (df.Q201Sales, df.Q201Cogs): None,
            ...                                  (df.Q301Sales, df.Q301Cogs): "Q301"},
            ...                         transpose_column="yr_qtr",
            ...                         measure_columns=["sales", "cogs"],
            ...                         returns=["year_quarter", "sales_data", "cogs_data"])
            >>> print(unpivot_df)
              country state        year_quarter  sales_data  cogs_data
            0  CANADA    BC  Q201Sales_Q201Cogs         NaN       10.0
            1  CANADA    ON                Q101         NaN        0.0
            2  CANADA    ON                Q301        10.0        NaN
            3  CANADA    BC                Q301         NaN        0.0
            4     USA    NY  Q201Sales_Q201Cogs        25.0        NaN
            5     USA    NY                Q101        45.0        NaN

            # Example 5: Unpivot quarterly sales data to 'sales' column and quarterly
            #            cogs data to 'cogs' columns and rename each column of the
            #            output using "returns" and "all_columns" argument.
            >>> unpivot_df = df.unpivot(columns={(df.Q101Sales, df.Q101Cogs): "Q101",
            ...                                  (df.Q201Sales, df.Q201Cogs): None,
            ...                                  (df.Q301Sales, df.Q301Cogs): "Q301"},
            ...                         transpose_column="yr_qtr",
            ...                         measure_columns=["sales", "cogs"],
            ...                         returns=["con", "st", "year_quarter",
            ...                                  "sales_data", "cogs_data"],
            ...                         all_columns=True)
            >>> print(unpivot_df)
                  con  st        year_quarter  sales_data  cogs_data
            0  CANADA  BC  Q201Sales_Q201Cogs         NaN       10.0
            1  CANADA  ON                Q101         NaN        0.0
            2  CANADA  ON                Q301        10.0        NaN
            3  CANADA  BC                Q301         NaN        0.0
            4     USA  NY  Q201Sales_Q201Cogs        25.0        NaN
            5     USA  NY                Q101        45.0        NaN

            # Example 6: Unpivot quarterly sales data to 'sales' column and quarterly
            #            cogs data to 'cogs' columns by including NULLs.
            >>> unpivot_df = df.unpivot(columns={(df.Q101Sales, df.Q101Cogs): "2001_Q1",
            ...                                  (df.Q201Sales, df.Q201Cogs): None,
            ...                                  (df.Q301Sales, df.Q301Cogs): "2001_Q3"},
            ...                         transpose_column="yr_qtr",
            ...                         measure_columns=["sales", "cogs"],
            ...                         exclude_nulls=False)
            >>> print(unpivot_df)
              country state              yr_qtr  sales  cogs
            0     USA    CA             2001_Q3    NaN   NaN
            1     USA    NY  Q201Sales_Q201Cogs   25.0   NaN
            2     USA    NY             2001_Q3    NaN   NaN
            3  CANADA    BC             2001_Q1    NaN   NaN
            4  CANADA    BC             2001_Q3    NaN   0.0
            5  CANADA    ON             2001_Q1    NaN   0.0
            6  CANADA    BC  Q201Sales_Q201Cogs    NaN  10.0
            7     USA    NY             2001_Q1   45.0   NaN
            8     USA    CA  Q201Sales_Q201Cogs    NaN   NaN
            9     USA    CA             2001_Q1    NaN   NaN
        """
        arg_info_matrix = []
        col1 = kwargs.get("col1", None)
        columns_dict = {}
        _Validators._validate_mutually_exclusive_arguments(columns, "columns", col1, "kwargs")
        # If keyword column argument col1 is provided, "columns" is not required.
        is_columns_optional = True if col1 is not None else False
        # Argument type validation.
        arg_info_matrix.append(["columns", columns, is_columns_optional, (dict)])
        arg_info_matrix.append(["transpose_column", transpose_column, False, (str), True])
        arg_info_matrix.append(["measure_columns", measure_columns, False, (str, list), True])
        arg_info_matrix.append(["exclude_nulls", exclude_nulls, True, (bool)])
        arg_info_matrix.append(["returns", returns, True, (str, list), True])
        arg_info_matrix.append(["all_columns", all_columns, True, (bool)])
        for i in range(1, int(len(kwargs) / 2) + 1):
            # Get the values of colN where N is in range(1, half the length of kwargs + 1).
            col = kwargs.get("col{}".format(i), None)
            col_val = kwargs.get("col{}_value".format(i), None)
            arg_info_matrix.append(["col{}".format(i), col, not is_columns_optional,
                                    (ColumnExpression, _TupleOf(ColumnExpression))])
            # Dictionary to populate columns info from 'kwargs'.
            columns_dict[col] = col_val

        # Validate the function arguments.
        _Validators._validate_function_arguments(arg_info_matrix)

        # If 'columns' is not provided, use kwargs as 'columns'.
        arg_name = "columns"
        if columns is None:
            columns = columns_dict
            arg_name = "kwargs"

        # Validate 'columns' argument.
        _Validators._validate_dict_argument_key_value(
            arg_name=arg_name, arg_dict=columns,
            key_types=(ColumnExpression, _TupleOf(ColumnExpression)),
            value_types=(str, int, float, type(None)))
        # Basic type check validations done.

        # Generate FOR clause.
        participating_columns_list = []
        in_columns = ""

        # Lambda function to quote the column names.
        get_quoted_column = lambda col: UtilFuncs._teradata_quote_arg(col,
                                                                      "\"", False)

        # Check for the type of 'measure_columns', if str convert it.
        measure_columns = UtilFuncs._as_list(measure_columns)
        measure_cols_len = len(measure_columns)

        # Variable to store the previous key.
        prev_val = None
        columns_len = len(columns)

        # Iterate over 'columns' and generate the clause.
        for c_name, c_value in columns.items():
            column_names = []
            # Check type of dictionary keys are same or not.
            if prev_val and not isinstance(c_name, type(prev_val)):
                raise TeradataMlException(Messages.get_message(
                    MessageCodes.INVALID_DICT_KEYS, arg_name, "same type"),
                    MessageCodes.INVALID_DICT_KEYS)

            # If key of dict is ColumnExpression then length is always 1.
            c_name_len = 1 if isinstance(c_name, ColumnExpression) else len(c_name)

            if isinstance(c_name, tuple):
                # Check for length of dictionary tuple keys.
                if prev_val and c_name_len != len(prev_val):
                    raise TeradataMlException(Messages.get_message(
                        MessageCodes.INVALID_DICT_KEYS, arg_name, "equal length"),
                        MessageCodes.INVALID_DICT_KEYS)

                for cn in c_name:
                    column_names.append(cn.name)
            else:
                column_names.append(c_name.name)

            # Store the previous key.
            prev_val = c_name

            # If length of measure columns is not equal to length of keys and
            if measure_cols_len != c_name_len:
                # If length of dictionary is 1 and value of the key is not None,
                # raise the error. Func call to raise below error:
                # df.unpivot(columns={(df.Q101Sales, df.Q101Cogs, df.Q201Sales,
                #                      df.Q201Cogs, df.Q301Sales, df.Q301Cogs): 'Q101'},
                #            transpose_column="yr_qtr",
                #            measure_columns="sales_cogs")
                if columns_len == 1 and c_value is not None:
                    raise ValueError(Messages.get_message(
                        MessageCodes.INVALID_DICT_ARG_VALUE, c_value,
                        "value", arg_name, 'None'))

                # If measure_cols length is greater than 1 and value of dictionary is
                # None, raise error. Func call to raise below error:
                # df.unpivot(columns={df.Q101Sales: None},
                #            measure_columns=['x', 'y'],
                #              transpose_column = "yr_qtr")
                if (measure_cols_len > 1 and c_value is None) or columns_len > 1:
                    raise ValueError(Messages.get_message(
                        MessageCodes.INVALID_DICT_KEY_VALUE_LENGTH,
                        "keys", "columns", "measure_columns",
                        "if length of 'columns' is greater than 1"))

            # Quote the column names and put it in () if length of dictionary is not 1.
            _column_fmt = "{}" if columns_len == 1 else "({})"
            column_str = _column_fmt.format(", ".join(get_quoted_column(c)
                                                      for c in column_names))

            # Populate the participating columns name.
            participating_columns_list.extend(column_names)

            # If dict value is provided. Generate alias using 'AS'.
            if c_value:
                column_str = "{0} AS '{1}'".format(column_str, c_value)

            in_columns = "{0}".format(column_str) if in_columns == "" \
                else "{0}, {1}".format(in_columns, column_str)
        # For loop iteration done.

        # Validate 'returns' has expected number of column names passed.
        if returns:
            # Formula = len(measure_cols) + len(transpose_col) is to get the
            # number of columns other than non participating columns.
            # 'transpose_column' is a string, so we are adding 1 for len(transpose_col).
            output_columns_len = len(measure_columns) + 1

            # Get the non participating columns.
            non_participating_columns = [col for col in self.columns
                                         if col not in participating_columns_list]

            # If all_column=True, add non_participating_columns as well.
            if all_columns:
                output_columns_len = len(non_participating_columns) + output_columns_len

            _Validators._validate_arg_length(arg_name="returns",
                                             arg_value=returns,
                                             length=output_columns_len,
                                             op="EQ")
        # 'returns' argument validation done.

        # Generate the FOR clause.
        for_clause = "({measure_cols}) FOR {transpose_cols} IN ({in_cols})".format(
            measure_cols=", ".join(get_quoted_column(c) for c in measure_columns),
            transpose_cols=get_quoted_column(transpose_column),
            in_cols=in_columns)

        # Generate the 'Tmp' clause.
        # If 'returns' is not provided, return 'Tmp'.
        if returns is None:
            tmp_clause = "Tmp"
        # If all_columns is set to True, return Tmp clause with
        # provided column names as is.
        elif all_columns:
            tmp_clause = "Tmp({})".format(", ".join(get_quoted_column(c)
                                                    for c in returns))
        # If 'returns' is provided and 'all_columns' is False, add non participating
        # columns and returns column names.
        else:
            tmp_clause = "Tmp({})".format(", ".join(get_quoted_column(c) for c in
                                                    non_participating_columns + returns))

        # User may filter out or select only few columns from original DF to execute
        # unpivot(). Hence, we are getting table name from 'nodeid' and 'metaexpr'.
        if self._table_name is None:
            self._table_name = df_utils._execute_node_return_db_object_name(
                self._nodeid, self._metaexpr)

        # Generate the SELECT query.
        select_query = 'SELECT * FROM {tbl_name} UNPIVOT{excl_null} ({for_cl}) {tmp_cl};'. \
            format(tbl_name=self._table_name,
                   excl_null="" if exclude_nulls else " INCLUDE NULLS",
                   for_cl=for_clause,
                   tmp_cl=tmp_clause)

        # Create the teradataml dataframe from SELECT query and return the same.
        return DataFrame.from_query(select_query)

    @collect_queryband(queryband="DF_unnest")
    def unnest(self, array_col, key_col=None, ordinality=False, **kwargs):
        """
        DESCRIPTION:
            Explodes the array column to multiple rows.

        PARAMETERS:
            array_col:
                Required Argument.
                Specifies the array column to be exploded.
                Types: DataFrameColumn OR str

            key_col:
                Optional Argument.
                Specifies the column to be used as key while exploding the array column.
                Note:
                    * If 'key_col' is not specified, teradataml associate each array with temporary
                      generated unique key to identify exploded array elements.
                Types: DataFrameColumn OR str

            ordinality:
                Optional Argument.
                Specifies whether to include the position of each element in the array.
                Default Value: False
                Types: bool

            kwargs:
                array_col_alias:
                    Optional Argument.
                    Specifies the alias names for the output exploded array column.
                    Default Value: "array_col"
                    Types: str

                key_col_alias:
                    Optional Argument.
                    Specifies the alias name for the output key column.
                    Default Value: "key_col"
                    Types: str

                pos_col_alias:
                    Optional Argument.
                    Specifies the alias name for the output position column.
                    Note:
                        * If 'ordinality' is set to False, then 'pos_col_alias' will be ignored.
                    Default Value: "pos_col"
                    Types: str

        RETURNS:
            teradataml DataFrame

        RAISES:
            TeradataMlException

        EXAMPLES:
            >>> from teradataml import *
            >>> load_example_data("array", "array_table")

            # Create a DataFrame on 'array_table'.
            >>> df = DataFrame("array_table")
            >>> df
                             arr1                   arr2                        arr3
            id                                                                      
            1    (10,20,30,40,50)     (23,200,215,40,21)  ('ab','bc','cd','ab','ef')
            4   (180,28,38,48,58)  (30,NULL,NULL,250,27)  ('mn','no',NULL,'op','pq')
            2   (150,25,35,45,55)       (28,50,95,90,26)  ('xy','yz','za','ab','xy')
            6   (16,261,36,46,56)     (29,170,160,46,25)  ('uv','xy','ab','xy','yz')
            3   (12,22,320,42,52)     (25,22,140,200,23)  ('pq','ab','ab','st','tu')
            5    (14,24,34,44,54)  (26,NULL,NULL,180,24)  ('ij','jk',NULL,'kl','lm')

            # Example 1: Explode the array column 'arr1' without key column and ordinality.
            >>> res = df.unnest(array_col="arr1")
            >>> res
                id               arr1                   arr2                        arr3  key_col  array_col
            0    2  (150,25,35,45,55)       (28,50,95,90,26)  ('xy','yz','za','ab','xy')        1         45
            1    2  (150,25,35,45,55)       (28,50,95,90,26)  ('xy','yz','za','ab','xy')        1         25
            2    2  (150,25,35,45,55)       (28,50,95,90,26)  ('xy','yz','za','ab','xy')        1         35
            3    2  (150,25,35,45,55)       (28,50,95,90,26)  ('xy','yz','za','ab','xy')        1        150
            4    2  (150,25,35,45,55)       (28,50,95,90,26)  ('xy','yz','za','ab','xy')        1         55
            5    6  (16,261,36,46,56)     (29,170,160,46,25)  ('uv','xy','ab','xy','yz')        2         36
            6    6  (16,261,36,46,56)     (29,170,160,46,25)  ('uv','xy','ab','xy','yz')        2         46
            7    6  (16,261,36,46,56)     (29,170,160,46,25)  ('uv','xy','ab','xy','yz')        2        261
            8    6  (16,261,36,46,56)     (29,170,160,46,25)  ('uv','xy','ab','xy','yz')        2         16
            9    6  (16,261,36,46,56)     (29,170,160,46,25)  ('uv','xy','ab','xy','yz')        2         56

            # Example 2: Explode the array column 'arr3' with key column 'id' and ordinality.
            >>> res = df.unnest(array_col=df.arr3, key_col="id", ordinality=True)
                id               arr1                   arr2                        arr3  key_col array_col  pos_col
            0    1   (10,20,30,40,50)     (23,200,215,40,21)  ('ab','bc','cd','ab','ef')        1        ab        4
            1    1   (10,20,30,40,50)     (23,200,215,40,21)  ('ab','bc','cd','ab','ef')        1        ef        5
            2    1   (10,20,30,40,50)     (23,200,215,40,21)  ('ab','bc','cd','ab','ef')        1        bc        2
            3    1   (10,20,30,40,50)     (23,200,215,40,21)  ('ab','bc','cd','ab','ef')        1        ab        1
            4    1   (10,20,30,40,50)     (23,200,215,40,21)  ('ab','bc','cd','ab','ef')        1        cd        3
            5    2  (150,25,35,45,55)       (28,50,95,90,26)  ('xy','yz','za','ab','xy')        2        xy        5
            6    2  (150,25,35,45,55)       (28,50,95,90,26)  ('xy','yz','za','ab','xy')        2        xy        1
            7    2  (150,25,35,45,55)       (28,50,95,90,26)  ('xy','yz','za','ab','xy')        2        yz        2
            8    2  (150,25,35,45,55)       (28,50,95,90,26)  ('xy','yz','za','ab','xy')        2        ab        4
            9    2  (150,25,35,45,55)       (28,50,95,90,26)  ('xy','yz','za','ab','xy')        2        za        3

            # Example 3: Explode the array column 'arr2' with alias names specified.
            >>> res = df.unnest("arr2", df.id, True, array_col_alias="key", key_col_alias="val", pos_col_alias="pos")
            >>> res
                id               arr1                   arr2                        arr3  val    key  pos
            0    1   (10,20,30,40,50)     (23,200,215,40,21)  ('ab','bc','cd','ab','ef')    1   40.0    4
            1    1   (10,20,30,40,50)     (23,200,215,40,21)  ('ab','bc','cd','ab','ef')    1   23.0    1
            2    1   (10,20,30,40,50)     (23,200,215,40,21)  ('ab','bc','cd','ab','ef')    1  215.0    3
            3    1   (10,20,30,40,50)     (23,200,215,40,21)  ('ab','bc','cd','ab','ef')    1  200.0    2
            4    1   (10,20,30,40,50)     (23,200,215,40,21)  ('ab','bc','cd','ab','ef')    1   21.0    5
            5    2  (150,25,35,45,55)       (28,50,95,90,26)  ('xy','yz','za','ab','xy')    2   50.0    2
            6    2  (150,25,35,45,55)       (28,50,95,90,26)  ('xy','yz','za','ab','xy')    2   26.0    5
            7    2  (150,25,35,45,55)       (28,50,95,90,26)  ('xy','yz','za','ab','xy')    2   90.0    4
            8    2  (150,25,35,45,55)       (28,50,95,90,26)  ('xy','yz','za','ab','xy')    2   28.0    1
            9    2  (150,25,35,45,55)       (28,50,95,90,26)  ('xy','yz','za','ab','xy')    2   95.0    3
        """
        array_col_alias = kwargs.get("array_col_alias", "array_col")
        key_col_alias = kwargs.get("key_col_alias", "key_col")
        pos_col_alias = kwargs.get("pos_col_alias", "pos_col")

        # Argument validation matrix
        arg_info_matrix = []
        arg_info_matrix.append(["array_col", array_col, False, (str, ColumnExpression), True])
        arg_info_matrix.append(["key_col", key_col, True, (str, ColumnExpression), True])
        arg_info_matrix.append(["ordinality", ordinality, True, (bool,), True])
        arg_info_matrix.append(["array_col_alias", array_col_alias, True, (str,), True])
        arg_info_matrix.append(["key_col_alias", key_col_alias, True, (str,), True])
        arg_info_matrix.append(["pos_col_alias", pos_col_alias, True, (str,), True])

        # Validate function arguments
        _Validators._validate_function_arguments(arg_info_matrix)
        
        key_col_name = key_col.name if isinstance(key_col, ColumnExpression) else key_col
        array_col_name = array_col.name if isinstance(array_col, ColumnExpression) else array_col

        df = self
        # If key_col is not provided, create a temporary key column using ROW_NUMBER().
        if key_col is None:
            temp_key_col = f"col_{random.randint(0, 999999):06d}"
            df = df.assign(**{temp_key_col: _SQLColumnExpression('ROW_NUMBER() OVER (ORDER BY 1)')})
            key_col_name = temp_key_col
            _Validators._validate_column_exists_in_dataframe(array_col_name, self._metaexpr, True)
        else:
            _Validators._validate_column_exists_in_dataframe([key_col_name, array_col_name], self._metaexpr, True)

        # Prepare the list of columns passed to UNNEST function and output column aliases.
        # Quote the column names and aliases to handle special characters and reserved words.
        unnest_expr_cols = UtilFuncs._teradata_quote_arg([key_col_name, array_col_name], "\"", False)
        output_alias = [key_col_alias, array_col_alias] + ([pos_col_alias] if ordinality else [])
        output_alias = UtilFuncs._teradata_quote_arg(output_alias, "\"", False)

        # Generate a unique temporary table name for WITH clause.
        temp_table_name = UtilFuncs._generate_temp_table_name(gc_on_quit=False)

        # Prepare expressions for UNNEST function.
        # UNNEST syntax: 
        # SELECT * FROM TABLE (UNNEST(unnest_expr) [WITH ORDINALITY]) AS tf(key_col_alias, array_col_alias , pos_col_alias])
        unnest_query = SQLBundle._build_unnest_query(
            temp_table_name=temp_table_name,
            df_query=df.show_query(),
            unnest_expr_cols=unnest_expr_cols,
            ordinality=ordinality,
            output_alias=output_alias,
        )

        # Create and return the teradataml DataFrame from the SELECT query.
        # If key_col is not provided, drop the temporary key column from the result.
        res_df = DataFrame.from_query(unnest_query)
        return res_df if key_col else res_df.drop(columns=[temp_key_col])

    @collect_queryband(queryband="DF_plot")
    def plot(self, x, y, scale=None, kind="line", **kwargs):
        """
        DESCRIPTION:
            Generate plots on teradataml DataFrame. Following type of plots
            are supported, which can be specified using argument "kind":
                * bar plot
                * corr plot
                * line plot
                * mesh plot
                * scatter plot
                * wiggle plot
            Note:
                * This method does not support operations on array columns.

        PARAMETERS:
            x:
                Required Argument.
                Specifies a DataFrame column to use for the x-axis data.
                Types: teradataml DataFrame Column

            y:
                Required Argument.
                Specifies DataFrame column(s) to use for the y-axis data.
                Types: teradataml DataFrame Column OR list of teradataml DataFrame Columns.

            scale:
                Optional Argument.
                Specifies DataFrame column to use for scale data to
                wiggle and mesh plots.
                Note:
                    "scale" is significant for wiggle and mesh plots. Ignored for other
                    type of plots.
                Types: teradataml DataFrame Column.

            kind:
                Optional Argument.
                Specifies the kind of plot.
                Permitted Values:
                    * 'line'
                    * 'bar'
                    * 'scatter'
                    * 'corr'
                    * 'wiggle'
                    * 'mesh'
                Default Value: line
                Types: str

            ax:
                Optional Argument.
                Specifies the axis for the plot.
                Types: Axis

            cmap:
                Optional Argument.
                Specifies the name of the colormap to be used for plotting.
                Notes:
                     * Significant only when corresponding type of plot is mesh or geometry.
                     * Ignored for other type of plots.
                Permitted Values:
                    * All the colormaps mentioned in below URLs are supported.
                        * https://matplotlib.org/stable/tutorials/colors/colormaps.html
                        * https://matplotlib.org/cmocean/
                Types: str

            color:
                Optional Argument.
                Specifies the color for the plot.
                Note:
                    Hexadecimal color codes are not supported.
                Permitted Values:
                    * 'blue'
                    * 'orange'
                    * 'green'
                    * 'red'
                    * 'purple'
                    * 'brown'
                    * 'pink'
                    * 'gray'
                    * 'olive'
                    * 'cyan'
                    * Apart from above mentioned colors, the colors mentioned in
                      https://xkcd.com/color/rgb are also supported.
                Default Value: blue
                Types: str OR list of str

            figure:
                Optional Argument.
                Specifies the figure for the plot.
                Types: Figure

            figsize:
                Optional Argument.
                Specifies the size of the figure in a tuple of 2 elements. First
                element represents width of plot image in pixels and second
                element represents height of plot image in pixels.
                Default Value: (640, 480)
                Types: tuple

            figtype:
                Optional Argument.
                Specifies the type of the image to generate.
                Permitted Values:
                    * 'png'
                    * 'jpg'
                    * 'svg'
                Default Value: png
                Types: str

            figdpi:
                Optional Argument.
                Specifies the dots per inch for the plot image.
                Note:
                    * Valid range for "dpi" is: 72 <= width <= 300.
                Default Value: 100 for PNG and JPG Type image.
                Types: int

            grid_color:
                Optional Argument.
                Specifies the color of the grid.
                Note:
                    Hexadecimal color codes are not supported.
                Permitted Values:
                    * 'blue'
                    * 'orange'
                    * 'green'
                    * 'red'
                    * 'purple'
                    * 'brown'
                    * 'pink'
                    * 'gray'
                    * 'olive'
                    * 'cyan'
                    * Apart from above mentioned colors, the colors mentioned in
                      https://xkcd.com/color/rgb are also supported.
                Default Value: gray
                Types: str

            grid_format:
                Optional Argument.
                Specifies the format for the grid.
                Types: str

            grid_linestyle:
                Optional Argument.
                Specifies the line style of the grid.
                Permitted Values:
                    * -
                    * --
                    * -.
                Default Value: -
                Types: str

            grid_linewidth:
                Optional Argument.
                Specifies the line width of the grid.
                Note:
                    Valid range for "grid_linewidth" is: 0.5 <= grid_linewidth <= 10.
                Default Value: 0.8
                Types: int OR float

            heading:
                Optional Argument.
                Specifies the heading for the plot.
                Types: str

            legend:
                Optional Argument.
                Specifies the legend(s) for the Plot.
                Types: str OR list of str

            legend_style:
                Optional Argument.
                Specifies the location for legend to display on Plot image. By default,
                legend is displayed at upper right corner.
                Permitted Values:
                    * 'upper right'
                    * 'upper left'
                    * 'lower right'
                    * 'lower left'
                    * 'right'
                    * 'center left'
                    * 'center right'
                    * 'lower center'
                    * 'upper center'
                    * 'center'
                Default Value: 'upper right'
                Types: str

            linestyle:
                Optional Argument.
                Specifies the line style for the plot.
                Permitted Values:
                    * -
                    * --
                    * -.
                    * :
                Default Value: -
                Types: str OR list of str

            linewidth:
                Optional Argument.
                Specifies the line width for the plot.
                Note:
                    Valid range for "linewidth" is: 0.5 <= linewidth <= 10.
                Default Value: 0.8
                Types: int OR float OR list of int OR list of float

            marker:
                Optional Argument.
                Specifies the type of the marker to be used.
                Permitted Values:
                    All the markers mentioned in https://matplotlib.org/stable/api/markers_api.html
                    are supported.
                Types: str OR list of str

            markersize:
                Optional Argument.
                Specifies the size of the marker.
                Note:
                    Valid range for "markersize" is: 1 <= markersize <= 20.
                Default Value: 6
                Types: int OR float OR list of int OR list of float

            position:
                Optional Argument.
                Specifies the position of the axis in the figure. Accepts a tuple
                of two elements where first element represents the row and second
                element represents column.
                Default Value: (1, 1)
                Types: tuple

            span:
                Optional Argument.
                Specifies the span of the axis in the figure. Accepts a tuple
                of two elements where first element represents the row and second
                element represents column.
                For Example,
                    Span of (2, 1) specifies the Axis occupies 2 rows and 1 column
                    in Figure.
                Default Value: (1, 1)
                Types: tuple

            reverse_xaxis:
                Optional Argument.
                Specifies whether to reverse tick values on x-axis or not.
                Default Value: False
                Types: bool

            reverse_yaxis:
                Optional Argument.
                Specifies whether to reverse tick values on y-axis or not.
                Default Value: False
                Types: bool

            series_identifier:
                Optional Argument.
                Specifies the teradataml DataFrame Column which represents the
                identifier for the data. As many plots as distinct "series_identifier"
                are generated in a single Axis.
                For example:
                    consider the below data in teradataml DataFrame.
                           ID   x   y
                        0  1    1   1
                        1  1    2   2
                        2  2   10  10
                        3  2   20  20
                    If "series_identifier" is not specified, simple plot is
                    generated where every 'y' is plotted against 'x' in a
                    single plot. However, specifying "series_identifier" as 'ID'
                    generates two plots in a single axis. One plot is for ID 1
                    and another plot is for ID 2.
                Types: teradataml DataFrame Column.

            title:
                Optional Argument.
                Specifies the title for the Axis.
                Types: str

            xlabel:
                Optional Argument.
                Specifies the label for x-axis.
                Notes:
                     * When set to empty string, label is not displayed for x-axis.
                     * When set to None, name of the x-axis column is displayed as
                       label.
                Types: str

            xlim:
                Optional Argument.
                Specifies the range for xtick values.
                Types: tuple

            xtick_format:
                Optional Argument.
                Specifies whether to format tick values for x-axis or not.
                Types: str

            ylabel:
                Optional Argument.
                Specifies the label for y-axis.
                Notes:
                     * When set to empty string, label is not displayed for y-axis.
                     * When set to None, name of the y-axis column(s) is displayed as
                       label.
                Types: str

            ylim:
                Optional Argument.
                Specifies the range for ytick values.
                Types: tuple

            ytick_format:
                Optional Argument.
                Specifies whether to format tick values for y-axis or not.
                Types: str

            vmin:
                Optional Argument.
                Specifies the lower range of the color map. By default, the range
                is derived from data and color codes are assigned accordingly.
                Note:
                    "vmin" significant only for Geometry Plot.
                Types: int OR float

            vmax:
                Optional Argument.
                Specifies the upper range of the color map. By default, the range is
                derived from data and color codes are assigned accordingly.
                Note:
                    "vmax" Significant only for Geometry Plot.
                For example:
                    Assuming user wants to use colormap 'matter' and derive the colors for
                    values which are in between 1 and 100.
                    Note:
                        Colormap 'matter' starts with Pale Yellow and ends with Violet.
                    * If "colormap_range" is not specified, then range is derived from
                      existing values. Thus, colors are represented as below in the whole range:
                      * 1 as Pale Yellow.
                      * 100 as Violet.
                    * If "colormap_range" is specified as -100 and 100, the value 1 is at middle of
                      the specified range. Thus, colors are represented as below in the whole range:
                      * -100 as Pale Yellow.
                      * 1 as Orange.
                      * 100 as Violet.
                Types: int OR float

            wiggle_fill:
                Optional Argument.
                Specifies whether to fill the wiggle area or not. By default, the right
                positive half of the wiggle is not filled. If specified as True, wiggle
                area is filled.
                Note:
                    Applicable only for the wiggle plot.
                Default Value: False
                Types: bool

            wiggle_scale:
                Optional Argument.
                Specifies the scale of the wiggle. By default, the amplitude of wiggle is scaled
                relative to RMS of the first payload.  In certain cases, it can lead to excessively
                large wiggles. Use "wiggle_scale" to adjust the relative size of the wiggle.
                Note:
                    Applicable only for the wiggle and mesh plots.
                Types: int OR float

            ignore_nulls:
                Optional Argument.
                Specifies whether to delete rows with null values or not present in 'x', 'y' and
                'scale' params.
                Default Value: False
                Types: bool


        RAISES:
            TeradataMlException

        EXAMPLES:
            # Load example data.
            >>> load_example_data("movavg", "ibm_stock")
            >>> load_example_data("uaf", ["waveletTable", "us_air_pass"])
            >>> load_example_data("teradataml", "iris_input")

            # Create teradataml DataFrame objects.
            >>> ibm_stock = DataFrame("ibm_stock")
            >>> mesh = DataFrame("waveletTable")
            >>> us_air_pass = DataFrame("us_air_pass")
            >>> iris_input = DataFrame("iris_input")

            # Example 1: Line Plot - This example creates simple line plot
            # and composite line plot with 2 columns in y-axis.

            # Print the DataFrame.
            >>> print(ibm_stock)
                name    period  stockprice
            id
            244  ibm  62/05/04       475.0
            101  ibm  61/10/10       557.0
            284  ibm  62/07/02       350.0
            141  ibm  61/12/07       577.0
            120  ibm  61/11/06       592.0
            303  ibm  62/07/30       376.0
            263  ibm  62/06/01       364.0
            305  ibm  62/08/01       385.0
            122  ibm  61/11/08       596.0
            265  ibm  62/06/05       370.0

            # Simple Line Plot.
            >>> plot =  ibm_stock.plot(x=ibm_stock.period, y=ibm_stock.stockprice,
                                       title="period vs stockprice",
                                       heading="Simple Line Plot")

            # Display the plot.
            >>> plot.show()

            # Composite Line Plot.
            >>> figure = Figure(width=800, height=900, image_type="jpg",
                                heading="Composite Line Plot")

            # Print the DataFrame.
            >>> print(ibm_stock)
                name    period  stockprice
            id
            244  ibm  62/05/04       475.0
            101  ibm  61/10/10       557.0
            284  ibm  62/07/02       350.0
            141  ibm  61/12/07       577.0
            120  ibm  61/11/06       592.0
            303  ibm  62/07/30       376.0
            263  ibm  62/06/01       364.0
            305  ibm  62/08/01       385.0
            122  ibm  61/11/08       596.0
            265  ibm  62/06/05       370.0

            >>> n1 = ibm_stock.assign(n=ibm_stock.stockprice * 2)
            >>> plot =  n1.plot(x=n1.period, y=[n1.stockprice, n1.n],
                                color=['dark orange', 'sand'], figure=figure)

            # Display the plot.
            >>> plot.show()

            # Example 2: Bar Plot - This example creates simple bar plot
            # and composite bar plot with 2 columns in y-axis.

            # Print the DataFrame.
            >>> print(ibm_stock)
                name    period  stockprice
            id
            244  ibm  62/05/04       475.0
            101  ibm  61/10/10       557.0
            284  ibm  62/07/02       350.0
            141  ibm  61/12/07       577.0
            120  ibm  61/11/06       592.0
            303  ibm  62/07/30       376.0
            263  ibm  62/06/01       364.0
            305  ibm  62/08/01       385.0
            122  ibm  61/11/08       596.0
            265  ibm  62/06/05       370.0

            # Simple Bar Plot.
            >>> plot =  ibm_stock.plot(x=ibm_stock.period, y=ibm_stock.stockprice,
                                   kind='bar', xtick_format='MMM', ytick_format='9,99.99',
                                   xlabel='xlabel', ylabel='ylabel', color="orange")

            # Display the plot.
            >>> plot.show()

            # Composite Bar Plot.
            # Import Figure.
            >>> from teradataml import Figure
            >>> figure = Figure(width=800, height=900, image_type="jpg", heading="Heading")

            # Print the DataFrame.
            >>> print(us_air_pass)
              TD_TIMECODE  id  idx  international  domestic
            0    17/11/01   0   10           7.72     61.91
            1    18/01/01   0   12           8.60     55.83
            2    18/02/01   0   13           7.64     54.08
            3    17/01/01   0    0           8.51     54.11
            4    17/03/01   0    2           9.00     63.96
            5    17/04/01   0    3           9.16     61.10
            6    17/05/01   0    4           9.24     64.44
            7    17/06/01   0    5          10.26     66.75
            8    17/02/01   0    1           7.30     51.08
            9    17/12/01   0   11           8.96     61.37

            >>> plot =  us_air_pass.plot(x=us_air_pass.idx, y=[us_air_pass.international, us_air_pass.domestic],
                                         kind='bar', color=['blue', 'red'], figure=figure,
                                         heading="Composite Bar Plot", figsize=(1200, 1100))

            # Display the plot.
            >>> plot.show()

            # Example 3: Scatter Plot - This example creates simple scatter plot
            # and composite scatter plot with 2 columns in y-axis.

            # Print the DataFrame.
            >>> print(iris_input)
                 sepal_length  sepal_width  petal_length  petal_width  species
            id
            141           6.7          3.1           5.6          2.4        3
            99            5.1          2.5           3.0          1.1        2
            17            5.4          3.9           1.3          0.4        1
            139           6.0          3.0           4.8          1.8        3
            15            5.8          4.0           1.2          0.2        1
            137           6.3          3.4           5.6          2.4        3
            118           7.7          3.8           6.7          2.2        3
            120           6.0          2.2           5.0          1.5        3
            101           6.3          3.3           6.0          2.5        3
            122           5.6          2.8           4.9          2.0        3

            # Simple Scatter Plot.
            >>> plot =  plot = iris_input.plot(x=iris_input.sepal_length, y=iris_input.petal_length,
                                               kind='scatter', xlabel='sepal_length',
                                               ylabel='petal_length',
                                               color="red", grid_color='black',
                                               grid_linewidth=1, grid_linestyle="-",
                                               marker="p", marker_size=1,
                                               title="Scatter plot of sepal_length vs petal_length")
            # Display the plot.
            >>> plot.show()

            # Composite Scatter Plot.
            # Create DataFrame objects for species 1, 2, 3.
            >>> iris_sp1 = iris_input[iris_input.species == 1]
            >>> iris_sp2 = iris_input[iris_input.species == 2]
            >>> iris_sp3 = iris_input[iris_input.species == 3]

            # Print the DataFrames.
            >>> print(iris_sp1)
                sepal_length  sepal_width  petal_length  petal_width  species
            id
            38           4.9          3.6           1.4          0.1        1
            7            4.6          3.4           1.4          0.3        1
            26           5.0          3.0           1.6          0.2        1
            17           5.4          3.9           1.3          0.4        1
            34           5.5          4.2           1.4          0.2        1
            13           4.8          3.0           1.4          0.1        1
            32           5.4          3.4           1.5          0.4        1
            11           5.4          3.7           1.5          0.2        1
            15           5.8          4.0           1.2          0.2        1
            36           5.0          3.2           1.2          0.2        1

            >>> print(iris_sp2)
                sepal_length  sepal_width  petal_length  petal_width  species
            id
            59           6.6          2.9           4.6          1.3        2
            57           6.3          3.3           4.7          1.6        2
            97           5.7          2.9           4.2          1.3        2
            99           5.1          2.5           3.0          1.1        2
            51           7.0          3.2           4.7          1.4        2
            70           5.6          2.5           3.9          1.1        2
            89           5.6          3.0           4.1          1.3        2
            68           5.8          2.7           4.1          1.0        2
            53           6.9          3.1           4.9          1.5        2
            78           6.7          3.0           5.0          1.7        2

            >>> print(iris_sp3)
                 sepal_length  sepal_width  petal_length  petal_width  species
            id
            133           6.4          2.8           5.6          2.2        3
            131           7.4          2.8           6.1          1.9        3
            110           7.2          3.6           6.1          2.5        3
            122           5.6          2.8           4.9          2.0        3
            141           6.7          3.1           5.6          2.4        3
            120           6.0          2.2           5.0          1.5        3
            139           6.0          3.0           4.8          1.8        3
            118           7.7          3.8           6.7          2.2        3
            101           6.3          3.3           6.0          2.5        3
            112           6.4          2.7           5.3          1.9        3

            # Import subplots.
            >>> from teradataml import subplots

            # Function to create a figure and a set of subplots.
            # The function makes it convenient to create common layouts of subplots,
            # including the enclosing figure object.
            # This will help to create a figure with 3 subplots in 1 row.
            # fig and axes is passed to plot().
            >>> fig, axes = subplots(nrows=1, ncols=3)

            >>> p = iris_sp1.plot(x=iris_sp1.sepal_length, y=iris_sp1.petal_length,
                                  ax=axes[0], xlim=(0,10), ylim=(0,8),
                                  figure=fig, kind="scatter",
                                  title="Scatter plot of species 1: Sepal Length v/s Petal Length", color="blue", marker="*")

            >>> p = iris_sp2.plot(x=iris_sp2.sepal_length, y=iris_sp2.petal_length,
                                  ax=axes[1], xlim=(0,10), ylim=(0,8),
                                  figure=fig, kind="scatter",
                                  title="Scatter plot of species 2: Sepal Length v/s Petal Length", color="red", marker="p")

            >>> p = iris_sp3.plot(x=iris_sp3.sepal_length, y=iris_sp3.petal_length,
                                  ax=axes[2], xlim=(0,10), ylim=(0,8),
                                  figure=fig, kind="scatter",
                                  title="Scatter plot of species 3: Sepal Length v/s Petal Length", color="orange", marker="1")

            # Display the plot.
            >>> plot.show()

            # Example 4: Mesh Plot - This example creates simple mesh plot.

            # Print the DataFrame.
            >>> print(mesh)
                   x      t      y             c
            ID
            a   94.0  800.0  701.0 -2.034400e-22
            a   94.0  800.0  702.0 -4.217551e-22
            a   94.0  800.0  702.5 -5.192715e-22
            a   94.0  800.0  703.0 -5.182389e-22
            a   94.0  800.0  704.0  5.473949e-22
            a   94.0  800.0  704.5  2.389177e-21
            a   94.0  800.0  703.5 -2.592409e-22
            a   94.0  800.0  701.5 -3.051780e-22
            a   94.0  800.0  700.5 -1.266145e-22
            a   94.0  800.0  700.0 -7.378603e-23

            # Simple Mesh Plot.
            >>> plot =  mesh.plot(x=mesh.x, y=mesh.y, scale=mesh.c,
                                  kind='mesh', cmap='ice', vmin=-0.5,
                                  vmax=0.5)

            # Display the plot.
            >>> plot.show()

            # Example 5: Wiggle Plot - This example creates simple wiggle plot.

            # Simple Wiggle Plot.

            # Create teradataml DataFrame object.
            >>> wiggle = DataFrame("waveletTable")

            # Print the DataFrame.
            >>> print(wiggle)
                   x      t      y             c
            ID
            a   94.0  800.0  701.0 -2.034400e-22
            a   94.0  800.0  702.0 -4.217551e-22
            a   94.0  800.0  702.5 -5.192715e-22
            a   94.0  800.0  703.0 -5.182389e-22
            a   94.0  800.0  704.0  5.473949e-22
            a   94.0  800.0  704.5  2.389177e-21
            a   94.0  800.0  703.5 -2.592409e-22
            a   94.0  800.0  701.5 -3.051780e-22
            a   94.0  800.0  700.5 -1.266145e-22
            a   94.0  800.0  700.0 -7.378603e-23

            >>> plot =  wiggle.plot(x=wiggle.x, y=wiggle.y, scale=wiggle.c, kind='wiggle')

            # Display the plot.
            >>> plot.show()

            # Examples for subplot.
            # Example 1: This example creates a figure with subplot with scatter plots.

            # Load example data.
            >>> load_example_data("uaf", "house_values")

            # Create teradataml DataFrame objects.
            >>> house_values = DataFrame("house_values")

            # Print the DataFrame.
            >>> print(house_values)
                                   TD_TIMECODE  house_val    salary  mortgage
            cityid
            33      2020-07-01 08:00:00.000000    66000.0   29000.0     0.039
            33      2020-04-01 08:00:00.000000    80000.0   22000.0     0.029
            33      2020-05-01 08:00:00.000000   184000.0   49000.0     0.030
            33      2020-06-01 08:00:00.000000   320000.0  112000.0     0.017
            33      2020-09-01 08:00:00.000000   195000.0   72000.0     0.049
            33      2020-10-01 08:00:00.000000   134000.0   89000.0     0.045
            33      2020-11-01 08:00:00.000000   198000.0   49000.0     0.052
            33      2020-08-01 08:00:00.000000   144000.0   74000.0     0.034
            33      2020-03-01 08:00:00.000000   220000.0   76000.0     0.035
            33      2020-02-01 08:00:00.000000   144000.0   50000.0     0.040

            # Import subplots.
            >>> from teradataml import subplots

            # This will help to create a figure with 2 subplots in 1 row.
            # fig and axes is passed to plot().
            >>> fig, axes = subplots(nrows=1, ncols=2)

            # Create plot with house_val, salary and salary and mortgage.
            >>> plot =  house_values.plot(x=house_values.house_val, y=house_values.salary,
                                          ax=axes[0], figure=fig, kind="scatter",
                                          xlim=(100000,250000), ylim=(25000, 100000),
                                          title="Scatter plot of House Val v/s Salary",
                                          color="green")
            >>> plot =  house_values.plot(x=house_values.salary, y=house_values.mortgage,
                                          ax=axes[1], figure=fig, kind="scatter",
                                          title="Scatter plot of House Val v/s Mortgage",
                                          color="red")

            # Show the plot.
            >>> plot.show()

            Example 2:
            # Subplot with grid. This will generate a figure with 2 subplots in first row
            # first column and second column respectively and 1 subplot in second row.
            >>> fig, axes = subplots(grid = {(1, 1): (1, 1), (1, 2): (1, 1),
                                             (2, 1): (1, 2)})

            # Print the DataFrame.
            >>> print(house_values)
                                   TD_TIMECODE  house_val    salary  mortgage
            cityid
            33      2020-07-01 08:00:00.000000    66000.0   29000.0     0.039
            33      2020-04-01 08:00:00.000000    80000.0   22000.0     0.029
            33      2020-05-01 08:00:00.000000   184000.0   49000.0     0.030
            33      2020-06-01 08:00:00.000000   320000.0  112000.0     0.017
            33      2020-09-01 08:00:00.000000   195000.0   72000.0     0.049
            33      2020-10-01 08:00:00.000000   134000.0   89000.0     0.045
            33      2020-11-01 08:00:00.000000   198000.0   49000.0     0.052
            33      2020-08-01 08:00:00.000000   144000.0   74000.0     0.034
            33      2020-03-01 08:00:00.000000   220000.0   76000.0     0.035
            33      2020-02-01 08:00:00.000000   144000.0   50000.0     0.040

            # Create plot with house_val, salary and salary and mortgage.
            >>> plot =  house_values.plot(x=house_values.house_val, y=house_values.salary,
                                          ax=axes[0], figure=fig, kind="scatter",
                                          title="Scatter plot of House Val v/s Salary",
                                          color="green")
            >>> plot =  house_values.plot(x=house_values.salary, y=house_values.mortgage,
                                          ax=axes[1], figure=fig, kind="scatter",
                                          title="Scatter plot of Salary v/s Mortgage",
                                          color="red")
            >>> plot =  house_values.plot(x=house_values.salary, y=house_values.mortgage,
                                          ax=axes[2], figure=fig, kind="scatter",
                                          title="Scatter plot of House Val v/s Mortgage",
                                          color="blue")
            # Show the plot.
            >>> plot.show()

        """

        _plot = _Plot(x=x, y=y, scale=scale, kind=kind, **kwargs)
        # If plot is already generated, return the same plot.
        if self._plot is None:
            self._plot = _plot
            return _plot

        if self._plot == _plot:
            return self._plot
        else:
            self._plot = _plot
            return _plot

    @collect_queryband(queryband="DF_itertuples")
    def itertuples(self, name='Row', num_rows=None):
        """
        DESCRIPTION:
            Method to Iterate over teradataml DataFrame rows as namedtuples.
            Note:
                * This method does not support operations on array columns.

        PARAMETERS:
            name:
                Optional Argument.
                Specifies the name of the returned namedtuples. When set to None,
                the method returns the row in a list instead of namedtuple.
                Default Value: Row
                Types: str

            num_rows:
                Optional Argument.
                Specifies the number of rows to retrieve from the DataFrame.
                When set to None, the method retrieves all the records.
                Types: int OR NoneType

        RETURNS:
            iterator, an object to iterate over row in the DataFrame.

        RAISES:
            None

        EXAMPLES:
            # Load the data.
            >>> load_example_data("teradataml", "titanic")

            # Example 1: Retrieve all the records from the teradataml DataFrame.
            >>> df = DataFrame("titanic")
            >>> rows = df.itertuples()
            >>> next(rows)
            Row(passenger=78, survived=0, pclass=3, name='Moutal Mr. Rahamin Haim', sex='male', age=None, sibsp=0, parch=0, ticket='374746', fare=8.05, cabin=None, embarked='S')
            >>> next(rows)
            Row(passenger=93, survived=0, pclass=1, name='Chaffee Mr. Herbert Fuller', sex='male', age=46, sibsp=1, parch=0, ticket='W.E.P. 5734', fare=61.175, cabin='E31', embarked='S')
            >>> next(rows)
            Row(passenger=5, survived=0, pclass=3, name='Allen Mr. William Henry', sex='male', age=35, sibsp=0, parch=0, ticket='373450', fare=8.05, cabin=None, embarked='S')
            >>>

            # Example 2: Retrieve the first 20 records from the teradataml DataFrame in a list,
            # when DataFrame is ordered by column "passenger".
            >>> df = DataFrame("titanic")
            >>> rows = df.sort("passenger").itertuples(name=None, num_rows=20)
            >>> next(rows)
            [1, 0, 3, 'Braund, Mr. Owen Harris', 'male', 22, 1, 0, 'A/5 21171', 7.25, None, 'S']
            >>> next(rows)
            [2, 1, 1, 'Cumings, Mrs. John Bradley (Florence Briggs Thayer)', 'female', 38, 1, 0, 'PC 17599', 71.2833, 'C85', 'C']
            >>> next(rows)
            [3, 1, 3, 'Heikkinen, Miss. Laina', 'female', 26, 0, 0, 'STON/O2. 3101282', 7.925, None, 'S']
        """
        # Validate argument types
        _validation_matrix = []
        _validation_matrix.append(["name", name, True, str, True])
        _validation_matrix.append(["num_rows", num_rows, True, (int, type(None))])
        _Validators._validate_function_arguments(_validation_matrix)

        # Generate/Execute AED nodes
        self.__execute_node_and_set_table_name(self._nodeid, self._metaexpr)

        all_rows = True if num_rows is None else False
        dt_obj = _DataTransferUtils(df=self, num_rows=num_rows, all_rows=all_rows)

        query = dt_obj._generate_select_query()

        # Execute query.
        cur = execute_sql(query)

        if name:
            columns = [column[0] for column in cur.description]
            for rec in cur:
                row = _Row(columns=columns, values=rec)
                yield row
        else:
            for rec in cur:
                yield rec

    @collect_queryband(queryband="DF_replace")
    def replace(self, to_replace, value=None, subset=None):
        """
        DESCRIPTION:
            Function replaces every occurrence of "to_replace" with the "value"
            in the columns mentioned in "subset". When "subset" is not provided,
            function replaces in all columns.
            Note:
                * This method does not support operations on array columns.

        PARAMETERS:
            to_replace:
                Required Argument.
                Specifies a ColumnExpression or a literal that the function
                searches for values in the Column. Use ColumnExpression when
                you want to match the condition based on a DataFrameColumn
                function, else use literal.
                Note:
                    Only ColumnExpressions generated from DataFrameColumn
                    functions are supported. BinaryExpressions are not supported.
                    Example: Consider teradataml DataFrame has two columns COL1, COL2.
                             df.COL1.abs() is supported but df.COL1 == df.COL2 is not
                             supported.
                Supported column types: CHAR, VARCHAR, FLOAT, INTEGER, DECIMAL
                Types: ColumnExpression OR int OR float OR str OR dict

            value:
                Required argument when "to_replace" is not a dictionary. Optional otherwise.
                Specifies a ColumnExpression or a literal that replaces
                the "to_replace" in the column. Use ColumnExpression when
                you want to replace based on a DataFrameColumn function, else
                use literal.
                Notes:
                     * Argument is ignored if "to_replace" is a dictionary.
                     * Only ColumnExpressions generated from DataFrameColumn
                       functions are supported. BinaryExpressions are not supported.
                       Example: Consider teradataml DataFrame has two columns COL1, COL2.
                                df.COL1.abs() is supported but df.COL1 == df.COL2 is not
                                supported.
                Supported column types: CHAR, VARCHAR, FLOAT, INTEGER, DECIMAL
                Types: ColumnExpression OR int OR float OR str

            subset:
                Optional Argument.
                Specifies column(s) to consider for replacing the values.
                Types: ColumnExpression OR str OR list

        RAISES:
            TeradataMlException

        RETURNS:
            teradataml DataFrame

        EXAMPLES:
            # Load the data to run the example.
            >>> load_example_data("dataframe", "admissions_train")

            # Create a DataFrame on 'admissions_train' table.
            >>> df = DataFrame("admissions_train")
            >>> print(df)
               masters   gpa     stats programming  admitted
            id
            15     yes  4.00  Advanced    Advanced         1
            34     yes  3.85  Advanced    Beginner         0
            13      no  4.00  Advanced      Novice         1
            38     yes  2.65  Advanced    Beginner         1
            5       no  3.44    Novice      Novice         0
            40     yes  3.95    Novice    Beginner         0
            7      yes  2.33    Novice      Novice         1
            22     yes  3.46    Novice    Beginner         0
            26     yes  3.57  Advanced    Advanced         1
            17      no  3.83  Advanced    Advanced         1

            # Example 1: Replace the string 'Advanced' with 'Good' in columns 'stats'
            #            and 'programming'.
            >>> res = df.replace("Advanced", "Good", subset=["stats", "programming"])
            >>> print(res)
               masters   gpa   stats programming  admitted
            id
            13      no  4.00    Good      Novice         1
            36      no  3.00    Good      Novice         0
            15     yes  4.00    Good        Good         1
            40     yes  3.95  Novice    Beginner         0
            22     yes  3.46  Novice    Beginner         0
            38     yes  2.65    Good    Beginner         1
            26     yes  3.57    Good        Good         1
            5       no  3.44  Novice      Novice         0
            7      yes  2.33  Novice      Novice         1
            19     yes  1.98    Good        Good         0

            # Example 2: Replace the string 'Advanced' with 'Good' and 'Beginner' with 'starter'
            #            in columns 'stats' and 'programming'.
            >>> res = df.replace({"Advanced": "Good", "Beginner": "starter"}, subset=["stats", "programming"])
            >>> print(res)
               masters   gpa   stats programming  admitted
            id
            15     yes  4.00    Good        Good         1
            7      yes  2.33  Novice      Novice         1
            22     yes  3.46  Novice     starter         0
            17      no  3.83    Good        Good         1
            13      no  4.00    Good      Novice         1
            38     yes  2.65    Good     starter         1
            26     yes  3.57    Good        Good         1
            5       no  3.44  Novice      Novice         0
            34     yes  3.85    Good     starter         0
            40     yes  3.95  Novice     starter         0

            # Example 3: Append the string '_New' to 'stats' column when values in
            #           'programming' and 'stats' are same.
            >>> res = df.replace({df.programming: df.stats+"_New"}, subset=["stats"])
            >>> print(res)
               masters   gpa         stats programming  admitted
            id
            15     yes  4.00  Advanced_New    Advanced         1
            34     yes  3.85      Advanced    Beginner         0
            13      no  4.00      Advanced      Novice         1
            38     yes  2.65      Advanced    Beginner         1
            5       no  3.44    Novice_New      Novice         0
            40     yes  3.95        Novice    Beginner         0
            7      yes  2.33    Novice_New      Novice         1
            22     yes  3.46        Novice    Beginner         0
            26     yes  3.57  Advanced_New    Advanced         1
            17      no  3.83  Advanced_New    Advanced         1

            # Example 4: Round the values of gpa to it's nearest integer.
            >>> res = df.replace({df.gpa: df.gpa.round(0)}, subset=["gpa"])
            >>> print(res)
               masters  gpa     stats programming  admitted
            id
            15     yes  4.0  Advanced    Advanced         1
            7      yes  2.0    Novice      Novice         1
            22     yes  3.0    Novice    Beginner         0
            17      no  4.0  Advanced    Advanced         1
            13      no  4.0  Advanced      Novice         1
            38     yes  3.0  Advanced    Beginner         1
            26     yes  4.0  Advanced    Advanced         1
            5       no  3.0    Novice      Novice         0
            34     yes  4.0  Advanced    Beginner         0
            40     yes  4.0    Novice    Beginner         0

            # Example 5: Replace the value of masters with '1' if value is 'yes'
            #            and with '0' if value is no.
            >>> res = df.replace({'yes': 1, 'no': 0}, subset=["masters"])
            >>> print(res)
               masters   gpa     stats programming  admitted
            id
            15       1  4.00  Advanced    Advanced         1
            7        1  2.33    Novice      Novice         1
            22       1  3.46    Novice    Beginner         0
            17       0  3.83  Advanced    Advanced         1
            13       0  4.00  Advanced      Novice         1
            38       1  2.65  Advanced    Beginner         1
            26       1  3.57  Advanced    Advanced         1
            5        0  3.44    Novice      Novice         0
            34       1  3.85  Advanced    Beginner         0
            40       1  3.95    Novice    Beginner         0
        """
        _validation_matrix = []
        _validation_matrix.append(["to_replace", to_replace, True, (int, float, str, dict, ColumnExpression)])
        _validation_matrix.append(["value", value, False, (int, float, str, dict, type(None), ColumnExpression)])
        _validation_matrix.append(["subset", subset, False, (str, list, type(None))])
        _Validators._validate_function_arguments(_validation_matrix)

        if subset is None:
            subset = self.columns
        else:
            subset = [col.name if not isinstance(col, str) else col for col in UtilFuncs._as_list(subset)]

        if not isinstance(to_replace, dict):
            to_replace = {to_replace: value}

        new_columns = {}
        for column in self.columns:
            new_columns[column] = self[column].replace(to_replace) if column in subset else self[column]
        return self.assign(**new_columns, drop_columns=True).select(self.columns)

    @collect_queryband(queryband="DF_cube")
    def cube(self, columns, include_grouping_columns=False):
        """
        DESCRIPTION:
            cube() function creates a multi-dimensional cube for the DataFrame
            using the specified column(s), and there by running aggregates on
            it to produce the aggregations on different dimensions.
            Note:
                * This method does not support DataFrame containing array columns.

        PARAMETERS:
            columns:
                Required Argument.
                Specifies the name(s) of input teradataml DataFrame column(s).
                Types: str OR list of str(s)

            include_grouping_columns:
                Optional Argument.
                Specifies whether to include aggregations on the grouping column(s) or not.
                When set to True, the resultant DataFrame will have the aggregations on the
                columns mentioned in "columns". Otherwise, resultant DataFrame will not have
                aggregations on the columns mentioned in "columns".
                Default Value: False
                Types: bool

        RETURNS:
            teradataml DataFrameGroupBy

        RAISES:
            TeradataMlException

        EXAMPLES :
            # Load the data to run the example.
            >>> load_example_data("dataframe","admissions_train")

            # Create a DataFrame on 'admissions_train' table.
            >>> df = DataFrame("admissions_train")
            >>> df
               masters   gpa     stats programming  admitted
            id
            15     yes  4.00  Advanced    Advanced         1
            34     yes  3.85  Advanced    Beginner         0
            13      no  4.00  Advanced      Novice         1
            38     yes  2.65  Advanced    Beginner         1
            5       no  3.44    Novice      Novice         0
            40     yes  3.95    Novice    Beginner         0
            7      yes  2.33    Novice      Novice         1
            22     yes  3.46    Novice    Beginner         0
            26     yes  3.57  Advanced    Advanced         1
            17      no  3.83  Advanced    Advanced         1

            # Example 1: Find the sum of all valid columns by grouping the
            #            DataFrame columns with 'masters' and 'stats'.
            >>> df1 = df.cube(["masters", "stats"]).sum()
            >>> df1
              masters     stats  sum_id  sum_gpa  sum_admitted
            0      no  Beginner       8     3.60             1
            1    None  Advanced     555    84.21            16
            2    None  Beginner      21    18.31             3
            3     yes  Beginner      13    14.71             2
            4    None      None     820   141.67            26
            5     yes  Advanced     366    49.26             7
            6      no      None     343    63.96            16
            7    None    Novice     244    39.15             7
            8      no  Advanced     189    34.95             9
            9     yes    Novice      98    13.74             1

            # Example 2: Find the avg of all valid columns by grouping the DataFrame
            #            with columns 'masters' and 'admitted'. Include grouping columns
            #            in aggregate function 'avg'.
            >>> df1 = df.cube(["masters", "admitted"], include_grouping_columns=True).avg()
            >>> df1
              masters  admitted     avg_id   avg_gpa  avg_admitted
            0     yes       NaN  21.681818  3.532273      0.454545
            1    None       1.0  18.846154  3.533462      1.000000
            2      no       NaN  19.055556  3.553333      0.888889
            3     yes       0.0  24.083333  3.613333      0.000000
            4    None       NaN  20.500000  3.541750      0.650000
            5    None       0.0  23.571429  3.557143      0.000000
            6     yes       1.0  18.800000  3.435000      1.000000
            7      no       1.0  18.875000  3.595000      1.000000
            8      no       0.0  20.500000  3.220000      0.000000

            # Example 3: Find the avg of all valid columns by grouping the DataFrame with
            #            columns 'masters' and 'admitted'. Do not include grouping columns
            #            in aggregate function 'avg'.
            >>> df1 = df.cube(["masters", "admitted"], include_grouping_columns=False).avg()
            >>> df1
              masters  admitted     avg_id   avg_gpa
            0      no       0.0  20.500000  3.220000
            1    None       1.0  18.846154  3.533462
            2      no       NaN  19.055556  3.553333
            3     yes       0.0  24.083333  3.613333
            4    None       NaN  20.500000  3.541750
            5    None       0.0  23.571429  3.557143
            6     yes       1.0  18.800000  3.435000
            7     yes       NaN  21.681818  3.532273
            8      no       1.0  18.875000  3.595000
        """
        # Validate columns argument.
        arg_info_matrix = []
        arg_info_matrix.append(["columns", columns, False, (str, list), True])
        arg_info_matrix.append(["include_grouping_columns", include_grouping_columns, False, bool])

        # Validate argument types
        _Validators._validate_function_arguments(arg_info_matrix)

        # Checking each element in passed columns to be valid column in dataframe
        _Validators._validate_column_exists_in_dataframe(columns, self._metaexpr)

        # Query generation of cube API is same as the group by.
        # Only 'cube' is concatenated with 'group by' clause.
        return self.groupby(columns, option="cube", include_grouping_columns=include_grouping_columns)

    @collect_queryband(queryband="DF_rollup")
    def rollup(self, columns, include_grouping_columns=False):
        """
        DESCRIPTION:
            rollup() function creates a multi-dimensional rollup for the DataFrame
            using the specified column(s), and there by running aggregates on
            it to produce the aggregations on different dimensions.
            Note:
                * This method does not support DataFrame containing array columns.

        PARAMETERS:
            columns:
                Required Argument.
                Specifies the name(s) of input teradataml DataFrame column(s).
                Types: str OR list of str(s)

            include_grouping_columns:
                Optional Argument.
                Specifies whether to include aggregations on the grouping column(s) or not.
                When set to True, the resultant DataFrame will have the aggregations on the
                columns mentioned in "columns". Otherwise, resultant DataFrame will not have
                aggregations on the columns mentioned in "columns".
                Default Value: False
                Types: bool

        RETURNS:
            teradataml DataFrameGroupBy

        RAISES:
            TeradataMlException

        EXAMPLES :
            # Load the data to run the example.
            >>> load_example_data("dataframe","admissions_train")

            # Create a DataFrame on 'admissions_train' table.
            >>> df = DataFrame("admissions_train")
            >>> df
               masters   gpa     stats programming  admitted
            id
            15     yes  4.00  Advanced    Advanced         1
            34     yes  3.85  Advanced    Beginner         0
            13      no  4.00  Advanced      Novice         1
            38     yes  2.65  Advanced    Beginner         1
            5       no  3.44    Novice      Novice         0
            40     yes  3.95    Novice    Beginner         0
            7      yes  2.33    Novice      Novice         1
            22     yes  3.46    Novice    Beginner         0
            26     yes  3.57  Advanced    Advanced         1
            17      no  3.83  Advanced    Advanced         1

            # Example 1: Find the sum of all valid columns by grouping the
            #            DataFrame columns with 'masters' and 'stats'.
            >>> df1 = df.rollup(["masters", "stats"]).sum()
            >>> df1
              masters     stats  sum_id  sum_gpa  sum_admitted
            0      no      None     343    63.96            16
            1     yes      None     477    77.71            10
            2    None      None     820   141.67            26
            3      no    Novice     146    25.41             6
            4      no  Beginner       8     3.60             1
            5     yes    Novice      98    13.74             1
            6     yes  Beginner      13    14.71             2
            7     yes  Advanced     366    49.26             7
            8      no  Advanced     189    34.95             9

            # Example 2: Find the avg of all valid columns by grouping the DataFrame
            #            with columns 'masters' and 'admitted'. Include grouping columns
            #            in aggregate function 'avg'.
            >>> df1 = df.rollup(["masters", "admitted"], include_grouping_columns=True).avg()
            >>> df1
              masters  admitted     avg_id   avg_gpa  avg_admitted
            0      no       NaN  19.055556  3.553333      0.888889
            1     yes       NaN  21.681818  3.532273      0.454545
            2    None       NaN  20.500000  3.541750      0.650000
            3     yes       0.0  24.083333  3.613333      0.000000
            4      no       1.0  18.875000  3.595000      1.000000
            5     yes       1.0  18.800000  3.435000      1.000000
            6      no       0.0  20.500000  3.220000      0.000000

            # Example 3: Find the avg of all valid columns by grouping the DataFrame with
            #            columns 'masters' and 'admitted'. Do not include grouping columns
            #            in aggregate function 'avg'.
            >>> df1 = df.rollup(["masters", "admitted"], include_grouping_columns=False).avg()
            >>> df1
              masters  admitted     avg_id   avg_gpa
            0      no       NaN  19.055556  3.553333
            1     yes       NaN  21.681818  3.532273
            2      no       0.0  20.500000  3.220000
            3     yes       0.0  24.083333  3.613333
            4      no       1.0  18.875000  3.595000
            5     yes       1.0  18.800000  3.435000
            6    None       NaN  20.500000  3.541750
        """
        # Validate columns argument.
        arg_info_matrix = []
        arg_info_matrix.append(["columns", columns, False, (str, list), True])
        arg_info_matrix.append(["include_grouping_columns", include_grouping_columns, False, bool])

        # Validate argument types
        _Validators._validate_function_arguments(arg_info_matrix)

        # Checking each element in passed columns to be valid column in dataframe
        _Validators._validate_column_exists_in_dataframe(columns, self._metaexpr)

        # Query generation of cube API is same as the group by.
        # Only 'rollup' is concatenated with 'group by' clause.
        return self.groupby(columns, option="rollup", include_grouping_columns=include_grouping_columns)

    # Metadata functions for DataFrame created on datalake/OTF table.
    @property
    @collect_queryband(queryband="DF_snpsht")
    @df_utils.check_otf_dataframe()
    def snapshots(self):
        """
        DESCRIPTION:
            Gets snapshot information for a DataLake table.

        PARAMETERS:
            None

        RETURNS:
            teradataml DataFrame.

        RAISES:
            TeradataMLException.

        EXAMPLES :
            # Example 1: Get the snapshot information for datalake table.
            >>> from teradataml.dataframe.dataframe import in_schema
            >>> in_schema_tbl = in_schema(schema_name="datalake_db",
            ...                           table_name="datalake_table",
            ...                           datalake_name="datalake")
            >>> datalake_df = DataFrame(in_schema_tbl)
            >>> datalake_df.snapshots
                         snapshotId	  snapshotTimestamp	timestampMSecs	                                     manifestList	  summary
            0	6373759902296319074	2023-06-15 00:07:47	1686787667420	s3://vim-iceberg-v1/glue/metadata/snap-6373759...	{"added-data-files":"1","added-records":"5","a...}
            1	4768076782814510171	2023-06-15 00:09:01	1686787741964	s3://vim-iceberg-v1/glue/metadata/snap-4768076...	{"added-data-files":"1","added-records":"2","a...}
            2	7771482207931850214	2024-05-29 04:59:09	1716958749946	s3://vim-iceberg-v1/glue/metadata/snap-7771482...	{"deleted-data-files":"2","deleted-records":"7...}
            3	1545363077953282623	2024-05-29 05:13:39	1716959619455	s3://vim-iceberg-v1/glue/metadata/snap-1545363...	{"changed-partition-count":"0","total-records"...}
            4	2166707884289108360	2024-05-29 05:17:49	1716959869075	s3://vim-iceberg-v1/glue/metadata/snap-2166707...	{"changed-partition-count":"0","total-records"...}
            5	8934190131471882700	2024-05-29 05:21:32	1716960092422	s3://vim-iceberg-v1/glue/metadata/snap-8934190...	{"changed-partition-count":"0","total-records"...}
            6	3086605171258231948	2024-05-29 05:34:43	1716960883786	s3://vim-iceberg-v1/glue/metadata/snap-3086605...	{"changed-partition-count":"0","total-records"...}
            7	7592503716012384122	2024-05-29 06:04:48	1716962688047	s3://vim-iceberg-v1/glue/metadata/snap-7592503...	{"changed-partition-count":"0","total-records"...}
            8	2831061717890032890	2024-06-04 17:21:01	1717521661689	s3://vim-iceberg-v1/glue/metadata/snap-2831061...	{"added-data-files":"2","added-records":"7","a...}
            9	8810491341502972715	2024-10-22 23:47:22	1729640842067	s3://vim-iceberg-v1/glue/metadata/snap-8810491...	{"added-data-files":"1","added-records":"1","a...}
            10	3953136136558551163	2024-12-03 04:40:48	1733200848733	s3://vim-iceberg-v1/glue/metadata/snap-3953136...	{"added-data-files":"1","added-records":"4","a...}
            11	6034775168901969481	2024-12-03 04:40:49	1733200849966	s3://vim-iceberg-v1/glue/metadata/snap-6034775...	{"deleted-data-files":"1","deleted-records":"5...}
        """
        return self._execute_metadata_query_and_generate_dataframe("TD_SNAPSHOTS")

    @property
    @collect_queryband(queryband="DF_prttns")
    @df_utils.check_otf_dataframe()
    def partitions(self):
        """
        DESCRIPTION:
            Gets partition information for a DataLake table.

        PARAMETERS:
            None

        RETURNS:
            teradataml DataFrame.

        RAISES:
            TeradataMLException.

        EXAMPLES :
            # Example 1: Get the partition information for datalake table.
            >>> from teradataml.dataframe.dataframe import in_schema
            >>> in_schema_tbl = in_schema(schema_name="datalake_db",
            ...                           table_name="datalake_table",
            ...                           datalake_name="datalake")
            >>> datalake_df = DataFrame(in_schema_tbl)
            >>> datalake_df.partitions
                  id	name
            0   1000	  c2
            1   1001	  c3


        """
        return self._execute_metadata_query_and_generate_dataframe("TD_PARTITIONS")

    @property
    @collect_queryband(queryband="DF_mnfsts")
    @df_utils.check_otf_dataframe()
    def manifests(self):
        """
        DESCRIPTION:
            Gets manifest information for a DataLake table.

        PARAMETERS:
            None

        RETURNS:
            teradataml DataFrame.

        RAISES:
            TeradataMLException.

        EXAMPLES :
            # Example 1: Get the manifest information for datalake table.
            >>> from teradataml.dataframe.dataframe import in_schema
            >>> in_schema_tbl = in_schema(schema_name="datalake_db",
            ...                           table_name="datalake_table",
            ...                           datalake_name="datalake")
            >>> datalake_df = DataFrame(in_schema_tbl)
            >>> datalake_df.manifests
                         snapshotId	    snapshotTimestamp	                              manifestList	                              manifestFile	manifestFileLength	datafilecount	totalrowcount
            0	8068130797628952520	  2025-05-02 11:45:26	s3://vim-iceberg-v1/otftestdb/nt_sales/...	s3://vim-iceberg-v1/otftestdb/nt_sales/...	              7158	            6	            6
        """
        return self._execute_metadata_query_and_generate_dataframe("TD_MANIFESTS")

    @property
    @collect_queryband(queryband="DF_hstry")
    @df_utils.check_otf_dataframe()
    def history(self):
        """
        DESCRIPTION:
            Gets the snapshot history related to a DataLake table.

        PARAMETERS:
            None

        RETURNS:
            teradataml DataFrame.

        RAISES:
            TeradataMLException.

        EXAMPLES :
            # Example 1: Get the partition information for datalake table.
            >>> from teradataml.dataframe.dataframe import in_schema
            >>> in_schema_tbl = in_schema(schema_name="datalake_db",
            ...                           table_name="datalake_table",
            ...                           datalake_name="datalake")
            >>> datalake_df = DataFrame(in_schema_tbl)
            >>> datalake_df.history
                                 id	              timestamp
            0	8068130797628952520	    2025-05-02 11:45:26
        """
        return self._execute_metadata_query_and_generate_dataframe("TD_HISTORY")

    def _execute_metadata_query_and_generate_dataframe(self, func_name):
        """Function executes OTF metadata query and return result in DataFrame format"""
        query = SQLBundle()._get_sql_query(SQLConstants.SQL_TD_OTF_METADATA).format(func_name,
                                                                                    self._table_name)
        return DataFrame.from_query(query)

    @collect_queryband(queryband="DF_gt_snpsht")
    @df_utils.check_otf_dataframe()
    def get_snapshot(self, as_of):
        """
        DESCRIPTION:
            Gets the data from a DataLake table for the given snapshot id or timestamp string.
            Notes:
                * The snapshot id can be obtained from the 'snapshots' property of the DataFrame.
                * The time travel value represented by 'as_of' should be in the format "YYYY-MM-DD HH:MM:SS.FFFFFFF"
                  for TIMESTAMP string or "YYYY-MM-DD" for DATE string.

        PARAMETERS:
            as_of:
                Required Argument.
                Specifies the snapshot id or timestamp information for which the snapshot is to be fetched.
                Types: str or int

        RETURNS:
            teradataml DataFrame.

        RAISES:
            TeradataMLException.

        EXAMPLES:
            # DataFrame creation on OTF table.
            >>> from teradataml.dataframe.dataframe import in_schema
            >>> in_schema_tbl = in_schema(schema_name="datalake_db",
            ...                           table_name="datalake_table",
            ...                           datalake_name="datalake")
            >>> datalake_df = DataFrame(in_schema_tbl)

            # List snapshots first.
            >>> datalake_df.snapshots
                        snapshotId	  snapshotTimestamp	 timestampMSecs	                                     manifestList	  summary
               2046682612111137809	2025-06-03 13:26:15	  1748957175692	 s3://vim-iceberg-v1/datalake_db/datalake_table/metadata/snap-204...	{"added-data-files":"Red Inc","added-records"...}
                282293708812257203	2025-06-03 05:53:19	  1748929999245	 s3://vim-iceberg-v1/datalake_db/datalake_table/metadata/snap-282...    {"added-data-files":"Blue Inc","added-records"...}

            # Example 1: Get the snapshot using snapshot id.
            >>> datalake_df.get_snapshot(2046682612111137809)
                          Feb    Jan    Mar    Apr    datetime
            accounts
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017

            # Example 2: Get the snapshot using snapshot id in string format.
            >>> datalake_df.get_snapshot("2046682612111137809")
                          Feb    Jan    Mar    Apr    datetime
            accounts
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017

            # Example 3: Get the snapshot using timestamp string.
            >>> datalake_df.get_snapshot("2025-06-03 13:26:16")
                          Feb    Jan    Mar    Apr    datetime
            accounts
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017

            # Example 4: Get the snapshot using date string.
            >>> datalake_df.get_snapshot("2025-06-04")
                          Feb    Jan    Mar    Apr    datetime
            accounts
            Blue Inc     90.0   50.0   95.0  101.0  04/01/2017
            Alpha Co    210.0  200.0  215.0  250.0  04/01/2017
            Jones LLC   200.0  150.0  140.0  180.0  04/01/2017
            Yellow Inc   90.0    NaN    NaN    NaN  04/01/2017
            Orange Inc  210.0    NaN    NaN  250.0  04/01/2017
            Red Inc     200.0  150.0  140.0    NaN  04/01/2017

        """
        _Validators._validate_function_arguments([["as_of", as_of, False, (int, str)]])

        # If already int or string representation of int, return by quoting it
        if isinstance(as_of, int) or (isinstance(as_of, str) and as_of.isdigit()):
            snapshot_on = "'{}'".format(as_of)
        else:
            try:
                snapshot_on = UtilFuncs._get_time_formatted_string(as_of)
            except ValueError as e:
                raise TeradataMlException(Messages.get_message(MessageCodes.FUNC_EXECUTION_FAILED,
                                                               "get_snapshot", "Invalid value for 'as_of' argument: {}. "
                                                               "Use valid format [\"YYYY-MM-DD HH:MM:SS.FFFFFFF\", \"YYYY-MM-DD HH:MM:SS\","
                                                               "\"YYYY-MM-DD\"]".format(as_of)),
                                          MessageCodes.FUNC_EXECUTION_FAILED)

        query = SQLBundle()._get_sql_query(SQLConstants.SQL_TD_OTF_SNAPSHOT).format(self._table_name, snapshot_on)

        try:
            return DataFrame.from_query(query)
        except TeradataMlException as e:
            raise TeradataMlException(Messages.get_message(MessageCodes.FUNC_EXECUTION_FAILED,
                                                           "get_snapshot()", "Invalid value for 'as_of' argument: {}. "
                                                           "Use valid timestamp or correct snapshot id listed using 'snapshots' property.".format(as_of)),
                                      MessageCodes.FUNC_EXECUTION_FAILED)

    def as_of(self, **kwargs):
        """
        DESCRIPTION:
            Function to get DataFrame at specific time on temporal table.
            Note:
                Function is supported only on temporal tables or temporal views.

        PARAMETERS:
            kwargs:
                Specifies keyword arguments.

                valid_time:
                    Optional Argument.
                    Specifies the valid time to retrieve data from DataFrame created on either ValidTime
                    or BiTemporal table/view.
                    Notes:
                         * Either "valid_time" or "transaction_time" must be provided.
                         * Argument accepts below values:
                             * "current" - to get the current valid time data.
                             * any string other than "current" is considered as date and data will be retrieved at that of time.
                             * date object - to get the data valid on that date.
                             * datetime object - to get the data valid at that point of time.
                             * tuple - to get the data which is valid between the two valid times.
                                 * tuple should have only two elements. First element considered as starting time
                                   and second element considered as end time for a period of time.
                                   Records will be retrieved which are valid between the two valid times.
                                 * Both elements can be of date or datetime or string type. If you are using
                                   string, make sure the string represents a valid date.
                                 * Any element can be None.
                                    * If first element is None and valid time dimension column is PERIOD_DATE type,
                                      then it is considered as '0001-01-01'.
                                    * If first element is None and valid time dimension column is PERIOD_TIMESTAMP type,
                                      then it is considered as '0001-01-01 00:00:00.000000+00:00'.
                                    * If second element is None and valid time dimension column is PERIOD_DATE type,
                                      then it is considered as '9999-12-31'.
                                    * If second element is None and valid time dimension column is PERIOD_TIMESTAMP type,
                                      then it is considered as '9999-12-31 23:59:59.999999+00:00'.
                            * None - to consider the DataFrame as regular DataFrame and retrieve all the records from
                                     valid time dimension.
                    Types: date or str or tuple or NoneType

                include_valid_time_column:
                    Optional Argument.
                    Specifies whether to include the valid time dimension column in the resultant DataFrame.
                    When set to True, valid time dimension column is included in resultant DataFrame.
                    Otherwise, valid time dimension column is not included in resultant DataFrame.
                    Note:
                        Ignored when "valid_time" is either tuple or None.
                    Default Value: False
                    Types: bool

                transaction_time:
                    Optional Argument.
                    Specifies the transaction time to retrieve data from DataFrame created on either
                    TransactionTime or BiTemporal table/view.
                    Notes:
                         * Either "valid_time" or "transaction_time" must be provided.
                         * Argument accepts below values.
                            * "current" - to get the records which are valid at current time.
                            * any string other than "current" is considered as timestamp and records which are
                              valid at that of time.
                            * datetime object - to get the records which are valid at that of time.
                            * None - to consider the DataFrame as regular DataFrame and retrieve all the records
                              from transaction time dimension.
                    Types: datetime or str or NoneType

                include_transaction_time_column:
                    Optional Argument.
                    Specifies whether to include the transaction time dimension column in the resultant DataFrame.
                    When set to True, transaction time dimension column is included in resultant DataFrame.
                    Otherwise, transaction time dimension column is not included in resultant DataFrame.
                    Default Value: False
                    Types: bool

                additional_period:
                    Optional Argument.
                    Specifies the additional period to be kept in resultant DataFrame.
                    Note:
                        This is applicable only when "valid_time" is None.
                    Types: tuple of date or str

        RETURNS:
            teradataml DataFrame

        RAISES:
            TeradatamlException.

        EXAMPLES:
            # Load the data to run the example.
            >>> load_example_data("teradataml", "Employee_roles") # load valid time data.
            >>> load_example_data("teradataml", "Employee_Address") # load transaction time data.
            >>> load_example_data("teradataml", "Employee") # load bitemporal data.

            >>> df1 = DataFrame("Employee_roles")
                       EmployeeName Department  Salary      role_validity_period
            EmployeeID
            1              John Doe         IT   100.0  ('20/01/01', '24/12/31')
            2            Jane Smith         DA   200.0  ('20/01/01', '99/12/31')
            3                   Bob  Marketing   330.0  ('25/01/01', '99/12/31')
            3                   Bob      Sales   300.0  ('24/01/01', '24/12/31')

            # Example 1: Get the employee roles from DataFrame df1 which are valid at current time.
            >>> df1.as_of(valid_time="current")
                       EmployeeName Department  Salary
            EmployeeID
            2            Jane Smith         DA   200.0
            3                   Bob  Marketing   330.0

            # Example 2: Get the employee roles from DataFrame df1 which are valid at current time.
            #            Also include valid time dimension column.
            >>> df1.as_of(valid_time="current", include_valid_time_column=True)
                       EmployeeName Department  Salary      role_validity_period
            EmployeeID
            2            Jane Smith         DA   200.0  ('20/01/01', '99/12/31')
            3                   Bob  Marketing   330.0  ('25/01/01', '99/12/31')

            # Example 3: Get the employee roles from DataFrame df1 which are valid at 31st Dec 2026.
                         Include valid time dimension column.
            >>> df1.as_of(valid_time="2026-12-31", include_valid_time_column=True)
                       EmployeeName Department  Salary      role_validity_period
            EmployeeID
            2            Jane Smith         DA   200.0  ('20/01/01', '99/12/31')
            3                   Bob  Marketing   330.0  ('25/01/01', '99/12/31')

            # Example 4: Get the employee roles from DataFrame df1 which are valid at 31st Dec 2026.
            #            Also include valid time dimension column. Use date object instead of string
            #            to specify the date.
            >>> from datetime import date
            >>> d = date(2026, 12, 31)
            >>> df1.as_of(valid_time=d, include_valid_time_column=True)
                       EmployeeName Department  Salary      role_validity_period
            EmployeeID
            2            Jane Smith         DA   200.0  ('20/01/01', '99/12/31')
            3                   Bob  Marketing   330.0  ('25/01/01', '99/12/31')

            # Example 5: Get the employee roles which are valid between 20th Jan 2018 and 5th March 2024.
            #            Include valid time dimension column.
            >>> df1.as_of(valid_time=("2018-01-20", "2024-03-05"), include_valid_time_column=True)
                       EmployeeName Department  Salary                 VALIDTIME
            EmployeeID
            2            Jane Smith         DA   200.0  ('20/01/01', '24/03/05')
            1              John Doe         IT   100.0  ('20/01/01', '24/03/05')
            3                   Bob      Sales   300.0  ('24/01/01', '24/03/05')

            # Example 6: Get the employee roles which are valid between 20th Jan 2018 and 5th March 2024.
            #            Then again get the records which are valid at 1st Jan 2023. Do not include
            #            valid time dimension column since selecting valid time dimension column is ignored
            #            when "valid_time" is a tuple.
            >>> df1.as_of(valid_time=(date(2018, 1, 20), "2024-03-05")).as_of(valid_time=date(2023, 1, 1))
                       EmployeeName Department  Salary
            EmployeeID
            2            Jane Smith         DA   200.0
            1              John Doe         IT   100.0

            # Example 7: Get the employee roles which are valid between 1st Jan 0001 and 1st Jun 2024.
            >>> df1.as_of(valid_time=(None, date(2024, 3, 5)))
                       EmployeeName Department  Salary                 VALIDTIME
            EmployeeID
            2            Jane Smith         DA   200.0  ('20/01/01', '24/03/05')
            1              John Doe         IT   100.0  ('20/01/01', '24/03/05')
            3                   Bob      Sales   300.0  ('24/01/01', '24/03/05')

            # Example 8: Get the employee roles which are valid between 1st Jun 2024 and 31st Dec 9999.
            >>> df1.as_of(valid_time=("2024-06-01", None))
                       EmployeeName Department  Salary                 VALIDTIME
            EmployeeID
            1              John Doe         IT   100.0  ('24/06/01', '24/12/31')
            2            Jane Smith         DA   200.0  ('24/06/01', '99/12/31')
            3                   Bob  Marketing   330.0  ('25/01/01', '99/12/31')
            3                   Bob      Sales   300.0  ('24/06/01', '24/12/31')

            # Example 9: Consider df1 as regular DataFrame and retrieve all the records irrespective
            #            whether records are valid or not.
            >>> df1.as_of(valid_time=None)
                       EmployeeName Department  Salary
            EmployeeID
            1              John Doe         IT   100.0
            2            Jane Smith         DA   200.0
            3                   Bob  Marketing   330.0
            3                   Bob      Sales   300.0

            # Example 10. Consider df1 as regular DataFrame and retrieve all the records irrespective
            #              whether records are valid or not. Also include additional period and valid time
            #              dimension column.
            >>> df1.as_of(valid_time=None, additional_period=("2024-01-01", "2024-03-05"), include_valid_time_column=True)
                       EmployeeName Department  Salary      role_validity_period                 VALIDTIME
            EmployeeID
            1              John Doe         IT   100.0  ('20/01/01', '24/12/31')  ('24/01/01', '24/03/05')
            2            Jane Smith         DA   200.0  ('20/01/01', '99/12/31')  ('24/01/01', '24/03/05')
            3                   Bob  Marketing   330.0  ('25/01/01', '99/12/31')  ('24/01/01', '24/03/05')
            3                   Bob      Sales   300.0  ('24/01/01', '24/12/31')  ('24/01/01', '24/03/05')

            >>> df2 = DataFrame("Employee_Address")
                       EmployeeName      address                                                           validity_period
            EmployeeID
            2            Jane Smith   456 Elm St  ('2025-03-04 15:41:44.610000+00:00', '9999-12-31 23:59:59.999999+00:00')
            1              John Doe  123 Main St  ('2025-03-04 15:41:44.610000+00:00', '9999-12-31 23:59:59.999999+00:00')
            3           Bob Johnson   789 Oak St  ('2025-03-04 15:41:44.610001+00:00', '9999-12-31 23:59:59.999999+00:00')

            # Example 11: Consider df2 as regular DataFrame and retrieve all the records including historic
            #             records. Also include transaction time dimension column.
            >>> df2.as_of(transaction_time=None, include_transaction_time_column=True)
                       EmployeeName         address                                                           validity_period
            EmployeeID
            1              John Doe     123 Main St  ('2025-03-04 15:41:44.610000+00:00', '9999-12-31 23:59:59.999999+00:00')
            2            Jane Smith      456 Elm St  ('2025-03-04 15:41:44.610000+00:00', '9999-12-31 23:59:59.999999+00:00')
            3           Bob Johnson  789 Oak Street  ('2025-03-04 15:41:44.610000+00:00', '2025-03-04 15:41:44.610001+00:00')
            3           Bob Johnson      789 Oak St  ('2025-03-04 15:41:44.610001+00:00', '9999-12-31 23:59:59.999999+00:00')

            # Example 12: Get the employee address which are valid at current time from DataFrame df2.
            #             Also include transaction time dimension column.
            >>> df2.as_of(transaction_time="current", include_transaction_time_column=True)
                       EmployeeName      address                                                           validity_period
            EmployeeID
            2            Jane Smith   456 Elm St  ('2025-03-04 15:41:44.610000+00:00', '9999-12-31 23:59:59.999999+00:00')
            1              John Doe  123 Main St  ('2025-03-04 15:41:44.610000+00:00', '9999-12-31 23:59:59.999999+00:00')
            3           Bob Johnson   789 Oak St  ('2025-03-04 15:41:44.610001+00:00', '9999-12-31 23:59:59.999999+00:00')

            # Example 13: Get the employee address which are valid at current time from DataFrame df2.
            #             Do not include transaction time dimension column.
            >>> df2.as_of(transaction_time="current", include_transaction_time_column=False)
                       EmployeeName      address
            EmployeeID
            2            Jane Smith   456 Elm St
            1              John Doe  123 Main St
            3           Bob Johnson   789 Oak St

            # Example 14: Get the employee address which are valid at 2025-03-04 15:41:44.610000+00:00 from DataFrame df2.
            #             Include transaction time dimension column.
            >>> df2.as_of(transaction_time="2025-03-04 15:41:44.610000+00:00", include_transaction_time_column=True)
                       EmployeeName         address                                                           validity_period
            EmployeeID
            2            Jane Smith      456 Elm St  ('2025-03-04 15:41:44.610000+00:00', '9999-12-31 23:59:59.999999+00:00')
            1              John Doe     123 Main St  ('2025-03-04 15:41:44.610000+00:00', '9999-12-31 23:59:59.999999+00:00')
            3           Bob Johnson  789 Oak Street  ('2025-03-04 15:41:44.610000+00:00', '2025-03-04 15:41:44.610001+00:00')

            # Example 15: Get the employee address which are valid at 2025-03-04 15:41:44.610001+00:00 from DataFrame df2.
            #             Include transaction time dimension column.
            >>> from datetime import datetime, timezone, timedelta
            >>> dt = datetime(2025, 3, 4, 15, 41, 44, 610001)
            >>> dt_with_tz = dt.replace(tzinfo=timezone(timedelta(hours=0)))
            >>> df2.as_of(transaction_time=dt_with_tz, include_transaction_time_column=True)
                       EmployeeName      address                                                           validity_period
            EmployeeID
            2            Jane Smith   456 Elm St  ('2025-03-04 15:41:44.610000+00:00', '9999-12-31 23:59:59.999999+00:00')
            1              John Doe  123 Main St  ('2025-03-04 15:41:44.610000+00:00', '9999-12-31 23:59:59.999999+00:00')
            3           Bob Johnson   789 Oak St  ('2025-03-04 15:41:44.610001+00:00', '9999-12-31 23:59:59.999999+00:00')

            >>> df3 = DataFrame("Employee")
                       EmployeeName      address Department  Salary             role_validity                                                           validity_period
            EmployeeID
            1              John Doe  123 Main St         IT   100.0  ('20/01/01', '24/12/31')  ('2025-03-04 18:08:58.720000+00:00', '9999-12-31 23:59:59.999999+00:00')
            2            Jane Smith   456 Elm St         DA   200.0  ('20/01/01', '99/12/31')  ('2025-03-04 18:08:58.720000+00:00', '9999-12-31 23:59:59.999999+00:00')
            3                   Bob   789 OAK St  Marketing   330.0  ('25/01/01', '99/12/31')  ('2025-05-06 11:39:25.580000+00:00', '9999-12-31 23:59:59.999999+00:00')
            3                   Bob   789 Oak St      Sales   300.0  ('24/01/01', '24/12/31')  ('2025-03-04 18:09:08.830000+00:00', '9999-12-31 23:59:59.999999+00:00')

            # Example 16: Get all the records from DataFrame df3 by considering the DataFrame as
            #             regular DataFrame. Include both valid time and transaction time dimension columns.
            >>> df3.as_of(valid_time=None,
            ...           transaction_time=None,
            ...           include_valid_time_column=True,
            ...           include_transaction_time_column=True
            ...           )
                       EmployeeName         address Department  Salary             role_validity                                                           validity_period
            EmployeeID
            3                   Bob  789 Oak Street      Sales   300.0  ('24/01/01', '24/12/31')  ('2025-03-04 18:08:58.720000+00:00', '2025-03-04 18:09:08.830000+00:00')
            3                   Bob      789 Oak St  Marketing   330.0  ('25/01/01', '99/12/31')  ('2025-03-04 18:09:08.830000+00:00', '2025-05-06 11:39:25.580000+00:00')
            1              John Doe     123 Main St         IT   100.0  ('20/01/01', '24/12/31')  ('2025-03-04 18:08:58.720000+00:00', '9999-12-31 23:59:59.999999+00:00')
            2            Jane Smith      456 Elm St         DA   200.0  ('20/01/01', '99/12/31')  ('2025-03-04 18:08:58.720000+00:00', '9999-12-31 23:59:59.999999+00:00')
            3                   Bob  789 Oak Street  Marketing   330.0  ('25/01/01', '99/12/31')  ('2025-03-04 18:08:58.720000+00:00', '2025-03-04 18:09:08.830000+00:00')
            3                   Bob      789 OAK St  Marketing   330.0  ('25/01/01', '99/12/31')  ('2025-05-06 11:39:25.580000+00:00', '9999-12-31 23:59:59.999999+00:00')
            3                   Bob      789 Oak St      Sales   300.0  ('24/01/01', '24/12/31')  ('2025-03-04 18:09:08.830000+00:00', '9999-12-31 23:59:59.999999+00:00')

            # Example 17: Get the employee address from DataFrame df3 which are valid at 1st Jun 2024 from
            #             valid time dimension and valid at '2025-03-04 18:09:08.720001+00:00' from transaction
            #             time dimension. Include both valid time and transaction time dimension columns.
            >>> df3.as_of(valid_time="2024-06-01",
            ...           transaction_time="2025-03-04 18:09:08.720001+00:00",
            ...           include_valid_time_column=True,
            ...           include_transaction_time_column=True
            ...           )
                       EmployeeName         address Department  Salary             role_validity                                                           validity_period
            EmployeeID
            2            Jane Smith      456 Elm St         DA   200.0  ('20/01/01', '99/12/31')  ('2025-03-04 18:08:58.720000+00:00', '9999-12-31 23:59:59.999999+00:00')
            1              John Doe     123 Main St         IT   100.0  ('20/01/01', '24/12/31')  ('2025-03-04 18:08:58.720000+00:00', '9999-12-31 23:59:59.999999+00:00')
            3                   Bob  789 Oak Street      Sales   300.0  ('24/01/01', '24/12/31')  ('2025-03-04 18:08:58.720000+00:00', '2025-03-04 18:09:08.830000+00:00')

            # Example 18: Get the employee address from DataFrame df3 which are valid at 25th Jan 2024
            #             from valid time dimension and valid at current time from transaction time dimension.
            #             Include only transaction time dimension column.
            >>> df3.as_of(valid_time=date(2024, 1, 25),
            ...           transaction_time="current",
            ...           include_transaction_time_column=True)
                       EmployeeName      address Department  Salary                                                           validity_period
            EmployeeID
            2            Jane Smith   456 Elm St         DA   200.0  ('2025-03-04 18:08:58.720000+00:00', '9999-12-31 23:59:59.999999+00:00')
            1              John Doe  123 Main St         IT   100.0  ('2025-03-04 18:08:58.720000+00:00', '9999-12-31 23:59:59.999999+00:00')
            3                   Bob   789 Oak St      Sales   300.0  ('2025-03-04 18:09:08.830000+00:00', '9999-12-31 23:59:59.999999+00:00')

            # Example 19: Get the employee address from DataFrame df3 which are valid between 1st Jan 2025
            #             and 30th June 2025 from valid time dimension and valid at
            #             '2025-03-04 18:08:59.720000+00:00' from transaction time dimension.
            #             Include both valid time and transaction time dimension columns.
            >>> from datetime import datetime, timezone
            >>>df3.as_of(valid_time=("2025-01-01", date(2025, 6, 30)),
            ...          transaction_time=datetime(2025, 3, 4, 18, 8, 59, 720000).astimezone(timezone.utc),
            ...          include_transaction_time_column=True)
                       EmployeeName     address Department  Salary                                                           validity_period                 VALIDTIME
            EmployeeID
            2            Jane Smith  456 Elm St         DA   200.0  ('2025-03-04 18:08:58.720000+00:00', '9999-12-31 23:59:59.999999+00:00')  ('25/01/01', '25/06/30')
            3                   Bob  789 Oak St  Marketing   330.0  ('2025-03-04 18:09:08.830000+00:00', '2025-05-06 11:39:25.580000+00:00')  ('25/01/01', '25/06/30')

            # Example 20: Get the employee address from DataFrame df3 by considering the DataFrame as regular
            #             DataFrame from valid time dimension and valid at current time from transaction time dimension.
            #             Add additional period and include both valid time and transaction time dimension columns.
            >>> df3.as_of(valid_time=None,
            ...           transaction_time="current",
            ...           additional_period=("2024-01-01", "2024-03-05"),
            ...           include_valid_time_column=True,
            ...           include_transaction_time_column=True
            ...           )
                       EmployeeName      address Department  Salary             role_validity                                                           validity_period                 VALIDTIME
            EmployeeID
            1              John Doe  123 Main St         IT   100.0  ('20/01/01', '24/12/31')  ('2025-03-04 18:08:58.720000+00:00', '9999-12-31 23:59:59.999999+00:00')  ('24/01/01', '24/03/05')
            2            Jane Smith   456 Elm St         DA   200.0  ('20/01/01', '99/12/31')  ('2025-03-04 18:08:58.720000+00:00', '9999-12-31 23:59:59.999999+00:00')  ('24/01/01', '24/03/05')
            3                   Bob   789 OAK St  Marketing   330.0  ('25/01/01', '99/12/31')  ('2025-05-06 11:39:25.580000+00:00', '9999-12-31 23:59:59.999999+00:00')  ('24/01/01', '24/03/05')
            3                   Bob   789 Oak St      Sales   300.0  ('24/01/01', '24/12/31')  ('2025-03-04 18:09:08.830000+00:00', '9999-12-31 23:59:59.999999+00:00')  ('24/01/01', '24/03/05')
        """

        if "valid_time" not in kwargs and "transaction_time" not in kwargs:
            _Validators._validate_mutually_exclusive_arguments(
                None, "valid_time", None, "transaction_time")

        # Validate argument types.
        _validation = []
        _validation.append(["valid_time", kwargs.get("valid_time"), True, (date, datetime, str, tuple, type(None))])
        _validation.append(["transaction_time", kwargs.get("transaction_time"), True, (datetime, str, type(None))])
        _validation.append(["additional_period", kwargs.get("additional_period"), True, (tuple, type(None))])
        _validation.append(["include_valid_time_column", kwargs.get("include_valid_time_column"), True, bool])
        _validation.append(["include_transaction_time_column", kwargs.get("include_transaction_time_column"), True, bool])

        # Validate argument types
        _Validators._validate_function_arguments(_validation)

        # Validate temporal table type.
        _Validators._validate_temporal_table_type(self.df_type)

        # Extract valid_time and transaction_time from kwargs.
        valid_time = kwargs.get("valid_time")
        transaction_time = kwargs.get("transaction_time")
        additional_period = kwargs.get("additional_period")
        include_valid_time_column = kwargs.get("include_valid_time_column")
        include_transaction_time_column = kwargs.get("include_transaction_time_column")

        # Validate if user specifies valid_time for a transaction time table.
        if "valid_time" in kwargs:
            _Validators._validate_as_of_arguments(df_type=self.df_type)

        # Validate if user specifies transaction_time for a valid time table.
        if "transaction_time" in kwargs:
            _Validators._validate_as_of_arguments(df_type=self.df_type, argument_name='transaction_time')

        add_vt_period = False

        # Generate the time qualifier clause.
        if "valid_time" in kwargs and "transaction_time" not in kwargs:
            clause = self.__get_valid_time_clause(valid_time, additional_period)
        elif "transaction_time" in kwargs and "valid_time" not in kwargs:
            clause = self.__get_transaction_time_clause(transaction_time)
        else:
            # Generate both clauses.
            clause = "{} AND {}".format(self.__get_valid_time_clause(valid_time, additional_period),
                                        self.__get_transaction_time_clause(transaction_time)
                                        )

        # Exclude the time dimension columns if user is not willing to see it in output DF.
        columns_to_exclude = []
        if not include_valid_time_column and self._valid_time_column:
            columns_to_exclude.append(self._valid_time_column.name)

        if not include_transaction_time_column and self._transaction_time_column:
            columns_to_exclude.append(self._transaction_time_column.name)

        columns = [col for col in self.columns if col not in columns_to_exclude]
        col_names_types = df_utils._get_required_columns_types_from_metaexpr(self._metaexpr, columns)

        # Notes:
        #   * If valid_time is tuple, i.e., for valid time qualifier SEQUENCED VALIDTIME,
        #     add additional column VALIDTIME. This column should not be present in SELECT statement.
        #     Also, ValidTime dimension column should not be present in SELECT statement. VALIDTIME column
        #     acts as validTime dimension column here.
        #   * Time qualifier NONSEQUENCED VALIDTIME PERIOD clause also produces additional column VALIDTIME.
        #     Hence, add additional column VALIDTIME also returned in the output DataFrame. However, valid time
        #     column can exist in SELECT statement.
        if isinstance(valid_time, tuple):
            add_vt_period = True
            columns = [col for col in columns if col != self._valid_time_column.name]
            col_names_types = df_utils._get_required_columns_types_from_metaexpr(self._metaexpr, columns)
            col_names_types["VALIDTIME"] = self._valid_time_column.type
        elif (isinstance(valid_time, type(None)) and additional_period is not None):
            col_names_types = df_utils._get_required_columns_types_from_metaexpr(self._metaexpr, columns)
            col_names_types["VALIDTIME"] = self._valid_time_column.type

        # SELECT Node.
        column_expression = ", ".join(columns)
        sel_nodeid = self._aed_utils._aed_select(self._nodeid, column_expression, timestamp_expr=clause)

        # Constructing new Metadata (_metaexpr) without DB; using dummy select_nodeid and underlying table name.
        new_metaexpr = UtilFuncs._get_metaexpr_using_columns(sel_nodeid, col_names_types.items())
        df = self._create_dataframe_from_node(sel_nodeid, new_metaexpr, self._index_label)

        # If time qualifier is SEQUENCED PERIOD, then add VALIDTIME column to DataFrame
        # since it produces temporal dataset.
        if add_vt_period:
            df._valid_time_column = df['VALIDTIME']

        return df

    def __get_valid_time_clause(self, valid_time, additional_period=None):
        """
        DESCRIPTION:
            Function to get valid time clause for temporal table.

        PARAMETERS:
            valid_time:
                Required Argument.
                Specifies the valid time dimension to represent temporal data when creating the DataFrame.
                Types: date or str

            additional_period:
                Optional Argument.
                Specifies the additional period to be kept in DataFrame.
                Note:
                    This is applicable only when "valid_time" is None.
                Types: tuple of date or str

        RETURNS:
            str

        RAISES:
            None.
        """
        is_vt_dt_type = isinstance(self._valid_time_column.type, tdtypes.PERIOD_DATE)
        if valid_time == "current":
            return "CURRENT VALIDTIME"

        if isinstance(valid_time, (str, date, datetime)):
            # If valid_time is a string, then check what is the type of temporal column.
            # ValidTime dimension allows both DATE and TIMESTAMP type for ValidTime dimension
            # columns.
            if is_vt_dt_type:
                return "VALIDTIME AS OF DATE '{}'".format(valid_time)
            return "VALIDTIME AS OF TIMESTAMP '{}'".format(valid_time)

        # If valid_time is a tuple, then it is a period.
        # User can specify start and/or end time. Derive missing value.
        if isinstance(valid_time, tuple):
            start = valid_time[0]
            end = valid_time[1]
            start = ("0001-01-01" if is_vt_dt_type else '0001-01-01 00:00:00.000000+00:00') if start is None else str(
                start)
            end = ("9999-12-31" if is_vt_dt_type else '9999-12-31 23:59:59.999999+00:00') if end is None else str(end)
            return "SEQUENCED VALIDTIME PERIOD '({}, {})'".format(start, end)

        if isinstance(valid_time, type(None)) and additional_period is not None:
            return "NONSEQUENCED VALIDTIME PERIOD '({}, {})'".format(additional_period[0], additional_period[1])

        return "NONSEQUENCED VALIDTIME"

    def __get_transaction_time_clause(self, transaction_time):
        """
        DESCRIPTION:
            Function to get transaction time clause for temporal table.

        PARAMETERS:
            transaction_time:
                Required Argument.
                Specifies the transaction time dimension to represent temporal data when creating the DataFrame.
                Types: date or str

        RETURNS:
            str

        RAISES:
            None.
        """
        if transaction_time == "current":
            return "CURRENT TRANSACTIONTIME"

        if isinstance(transaction_time, type(None)):
            return "NONSEQUENCED TRANSACTIONTIME"

        return "TRANSACTIONTIME as of timestamp '{}'".format(transaction_time)

    def _generate_temporal_dataframe(self, timestamp_expr, time_column):
        """
        DESCRIPTION:
            Helper method to generate a temporal DataFrame based on the given timestamp expression.

        PARAMETERS:
            timestamp_expr:
                Required Argument.
                Specifies the timestamp expression to filter the temporal data.
                Types: str

            time_column:
                Required Argument.
                Specifies the temporal column (valid-time or transaction-time) to process.
                Types: ColumnExpression

        RAISES:
            None.

        RETURNS:
            teradataml DataFrame
        """
        col_expr = "{} as {}".format(time_column.cast(time_column.type).compile(), time_column.name)
        cols = [col.name if col.name != time_column.name else col_expr for col in self._metaexpr.c]
        column_expression = ", ".join(cols)
        sel_node_id = self._aed_utils._aed_select(self._nodeid, column_expression, timestamp_expr=timestamp_expr)
        return self._create_dataframe_from_node(sel_node_id, self._metaexpr, self._index_label)

    def historic_rows(self):
        """
        DESCRIPTION:
            Retrieves historical rows from a DataFrame created on a valid-time
            or bi-temporal table/view. Historical rows are defined as those where the
            end of the valid-time period precedes the current time.

        PARAMETERS:
            None.

        RETURNS:
            teradataml DataFrame.

        RAISES:
            TeradataMLException.

        EXAMPLES:
            # Load the data to run the example.
            >>> load_example_data("teradataml", "Employee_roles")

            # Create a DataFrame on 'Employee_roles' table.
            >>> df = DataFrame("Employee_roles")

            # Retrieve historic rows from the DataFrame.
            >>> df.historic_rows()
            EmployeeID    EmployeeName    Department    Salary          role_validity_period
                     1        John Doe            IT     100.0      ('20/01/01', '24/12/31')
                     3             Bob         Sales     300.0      ('24/01/01', '24/12/31')
        """

        from teradataml.dataframe.functions import current_date, current_timestamp
        # Validate temporal table type.
        _Validators._validate_temporal_table_type(self.df_type)
        valid_time_col = self._valid_time_column
        df = self._generate_temporal_dataframe("NONSEQUENCED VALIDTIME", valid_time_col)
        # Check the type of the ValidTime dimension column
        if isinstance(valid_time_col.type, tdtypes.PERIOD_DATE):
            # Filter records where the end of the ValidTime period is less than the current date
            return df[valid_time_col.end() < current_date()]
        return df[valid_time_col.end() < current_timestamp()]

    def future_rows(self):
        """
        DESCRIPTION:
            Retrieves future rows from a DataFrame created on a valid-
            time or bi-temporal table/view. Future rows are defined as those where the
            start of the valid-time period is greater than the current time.

        PARAMETERS:
            None.

        RETURNS:
            teradataml DataFrame.

        RAISES:
            TeradataMLException.

        EXAMPLES:
            # Load the data to run the example.
            >>> load_example_data("teradataml", "Employee_roles")

            # Create a DataFrame on 'Employee_roles' table.
            >>> df = DataFrame("Employee_roles")

            # Retrieve future rows from the DataFrame.
            >>> df.future_rows()
            EmployeeID   EmployeeName   Department   Salary         role_validity_period
                     3            Bob    Marketing    330.0     ('29/01/01', '99/12/31')
        """
        from teradataml.dataframe.functions import current_date, current_timestamp
        # Validate temporal table type.
        _Validators._validate_temporal_table_type(self.df_type)
        valid_time_col = self._valid_time_column
        df = self._generate_temporal_dataframe("NONSEQUENCED VALIDTIME", valid_time_col)
        # Check the type of the ValidTime dimension column
        if isinstance(valid_time_col.type, tdtypes.PERIOD_DATE):
            # Filter records where the start of the ValidTime period is greater than the current date
            return df[valid_time_col.begin() > current_date()]
        return df[valid_time_col.begin() > current_timestamp()]

    def open_rows(self):
        """
        DESCRIPTION:
            Retrieves open rows from a DataFrame created on a transaction-time
            or bi-temporal table/view. Open rows are defined as those where the
            end of the transaction-time period is greater than or equal to the current time.

        PARAMETERS:
            None.

        RETURNS:
            teradataml DataFrame.

        RAISES:
            TeradataMLException.

        EXAMPLES:
            # Load the data to run the example.
            >>> load_example_data("teradataml", "Employee_address")

            # Create a DataFrame on 'Employee_address' table.
            >>> df = DataFrame("Employee_address")

            # Retrieve open rows from the DataFrame.
            >>> df.open_rows()
            EmployeeID       EmployeeName        address                                                           validity_period
                     1           John Doe    123 Main St  ('2025-03-04 15:41:44.610000+00:00', '9999-12-31 23:59:59.999999+00:00')
                     2         Jane Smith     456 Elm St  ('2025-03-04 15:41:44.610000+00:00', '9999-12-31 23:59:59.999999+00:00')
        """
        from teradataml.dataframe.functions import current_timestamp
        # Validate temporal table type.
        _Validators._validate_temporal_table_type(self.df_type)
        transaction_time_col = self._transaction_time_column
        df = self._generate_temporal_dataframe("NONSEQUENCED TRANSACTIONTIME", transaction_time_col)
        return df[transaction_time_col.end() >= current_timestamp()]

    def closed_rows(self):
        """
        DESCRIPTION:
            Retrieves closed rows from a DataFrame created on a transaction-time
            or bi-temporal table/view. Closed rows are defined as those where the
            end  of the transaction-time period is less than the current time.

        PARAMETERS:
            None.

        RETURNS:
            teradataml DataFrame.

        RAISES:
            TeradataMLException.

        EXAMPLES:
            # Load the data to run the example.
            >>> load_example_data("teradataml", "Employee_address")

            # Create a DataFrame on 'Employee_address' table.
            >>> df = DataFrame("Employee_address")

            # Retrieve closed rows from the DataFrame.
            >>> df.closed_rows()
            EmployeeID    EmployeeName      address                                                           validity_period
                     1        John Doe  123 Main St  ('2025-03-04 15:41:44.610000+00:00', '2025-04-01 23:59:59.999999+00:00')
        """
        from teradataml.dataframe.functions import current_timestamp
        # Validate temporal table type.
        _Validators._validate_temporal_table_type(self.df_type)
        transaction_time_col = self._transaction_time_column
        df = self._generate_temporal_dataframe("NONSEQUENCED TRANSACTIONTIME", transaction_time_col)
        return df[transaction_time_col.end() < current_timestamp()]

    @collect_queryband(queryband="DF_create_view")
    def create_view(self, view_name, schema_name=None):
        """
        Creates a view from the DataFrame object in the specified schema.
        As teradataml creates views, internally for operations, which will be garbage
        collected during remove_context(), this function helps the user to persist the
        DataFrame as a view.
        Note:
            The persisted view can be used across sessions and can be accessed
            using the view_name and schema_name.

        PARAMETERS:
            view_name:
                Required Argument.
                Specifies the name of the view to be persisted.
                Types: str

            schema_name:
                Optional Argument.
                Specifies the schema name where the view is to be persisted.
                Note:
                    If the schema_name is not provided, the current database will be used.
                Types: str

        RETURNS:
            Persisted teradataml DataFrame.

        RAISES:
            TeradataMlException
        
        EXAMPLES:
            # Load the data to run the example.
            >>> load_example_data("antiselect", ["antiselect_input"])
            >>> antiselect_input = DataFrame.from_table("antiselect_input")
            >>> antiselect_input
                    orderid orderdate       priority  quantity       sales  discount        shipmode            custname province   region     custsegment          prodcat
            rowids
            49          293  12/10/01           high        49  10123.0200      0.07  delivery truck        barry french  nunavut  nunavut        consumer  office supplies
            97          613  11/06/17           high        12     93.5400      0.03     regular air        carl jackson  nunavut  nunavut       corporate  office supplies
            85          515  10/08/28  not specified        19    394.2700      0.08     regular air      carlos soltero  nunavut  nunavut        consumer  office supplies
            86          515  10/08/28  not specified        21    146.6900      0.05     regular air      carlos soltero  nunavut  nunavut        consumer        furniture
            1             3  10/10/13            low         6    261.5400      0.04     regular air  muhammed macintyre  nunavut  nunavut  small business  office supplies
            50          293  12/10/01           high        27    244.5700      0.01     regular air        barry french  nunavut  nunavut        consumer  office supplies
            80          483  11/07/10           high        30   4965.7595      0.08     regular air       clay rozendal  nunavut  nunavut       corporate       technology

            # Filter the data based on quantity.
            >>> anti_df = antiselect_input[antiselect_input.quantity < 30]
            >>> anti_df
                    orderid orderdate       priority  quantity   sales  discount     shipmode            custname province   region     custsegment          prodcat
            rowids
            97          613  11/06/17           high        12   93.54      0.03  regular air        carl jackson  nunavut  nunavut       corporate  office supplies
            86          515  10/08/28  not specified        21  146.69      0.05  regular air      carlos soltero  nunavut  nunavut        consumer        furniture
            85          515  10/08/28  not specified        19  394.27      0.08  regular air      carlos soltero  nunavut  nunavut        consumer  office supplies
            1             3  10/10/13            low         6  261.54      0.04  regular air  muhammed macintyre  nunavut  nunavut  small business  office supplies
            50          293  12/10/01           high        27  244.57      0.01  regular air        barry french  nunavut  nunavut        consumer  office supplies

            # Run Antiselect on filtered data. This will create temporary view which will be garbage collected.
            >>> obj = Antiselect(data=anti_df, exclude=['rowids', 'orderdate', 'discount', 'province', 'custsegment'])
            
            # Get the view name that is internally created by teradataml to store the result of Antiselect.
            >>> obj.result.db_object_name
            '"<schema_name>"."ml__td_sqlmr_out__1752582812690000"'

            # Check the output of Antiselect.
            >>> obj.result
            orderid       priority  quantity   sales     shipmode            custname   region          prodcat
            0      613           high        12   93.54  regular air        carl jackson  nunavut  office supplies
            1      515  not specified        21  146.69  regular air      carlos soltero  nunavut        furniture
            2      515  not specified        19  394.27  regular air      carlos soltero  nunavut  office supplies
            3      293           high        27  244.57  regular air        barry french  nunavut  office supplies
            4        3            low         6  261.54  regular air  muhammed macintyre  nunavut  office supplies

            # Describe the resultant DataFrame.
            >>> df = obj.result.describe() # This will create a temporary view.
            
            # Get the view name.
            >>> df.db_object_name
            '"<schema_name>"."ml__td_sqlmr_out__1752585435339977"'

            # Check the output of describe.
            >>> df
              ATTRIBUTE            StatName   StatValue
            0   orderid             MAXIMUM  613.000000
            1   orderid  STANDARD DEVIATION  245.016734
            2   orderid     PERCENTILES(25)  293.000000
            3   orderid     PERCENTILES(50)  515.000000
            4  quantity               COUNT    5.000000
            5  quantity             MINIMUM    6.000000
            6  quantity             MAXIMUM   27.000000
            7  quantity                MEAN   17.000000
            8  quantity  STANDARD DEVIATION    8.154753
            9  quantity     PERCENTILES(25)   12.000000

            # Example 1: Persist the view which can be accessed across sessions.
            >>> df_new = df.create_view(view_name="antiselect_describe_view")
            >>> df_new
              ATTRIBUTE            StatName   StatValue
            0  quantity             MAXIMUM   27.000000
            1  quantity  STANDARD DEVIATION    8.154753
            2  quantity     PERCENTILES(25)   12.000000
            3  quantity     PERCENTILES(50)   19.000000
            4     sales               COUNT    5.000000
            5     sales             MINIMUM   93.540000
            6   orderid               COUNT    5.000000
            7   orderid             MINIMUM    3.000000
            8   orderid             MAXIMUM  613.000000
            9   orderid                MEAN  387.800000

            # Get the view name.
            >>> df_new.db_object_name # "<schema_name>" is user connected database.
            '"<schema_name>"."antiselect_describe_view"'

        """
        # Argument validation
        arg_info_matrix = []
        arg_info_matrix.append(["view_name", view_name, False, (str,), True])
        arg_info_matrix.append(["schema_name", schema_name, True, (str,), True])
        _Validators._validate_missing_required_arguments(arg_info_matrix)
        _Validators._validate_function_arguments(arg_info_matrix)

        # TODO: Investigate and identify issue when volatile tables replaces views in future.

        visited = set()
        to_persist = []
        is_teradataml_temp_table = lambda x: x.startswith("ml__") or x.startswith("tdml_")
        sql_bundle = SQLBundle()

        def trace_views(table_name):
            if table_name in visited:
                return
            visited.add(table_name)
            base_name = UtilFuncs._extract_table_name(full_qualified_name=table_name)
            if is_teradataml_temp_table(base_name):
                to_persist.append(table_name)
                # Try to get the SQL for the view
                show_view_sql = sql_bundle._get_sql_query(SQLConstants.SQL_SHOW_VIEW).\
                    format(table_name)
                try:
                    result = execute_sql(show_view_sql).fetchall()
                    if result:
                        view_sql = result[0][0].replace("\r", "").replace("\n", " ")\
                            .replace("\t", " ").strip()

                        # Extract all table names from the view SQL
                        for tname in UtilFuncs.extract_table_names_from_query(view_sql):
                            trace_views(tname)
                except Exception as e:
                    # Check if error is like 'not a view', then try SHOW TABLE
                    err_msg = str(e).lower()
                    if 'not a view' in err_msg:
                        show_table_sql = sql_bundle._get_sql_query(SQLConstants.SQL_SHOW_TABLE).\
                            format(table_name)
                        try:
                            result = execute_sql(show_table_sql).fetchall()
                            if result:
                                # Table found, nothing to trace further.
                                # This table is persisted.
                                return
                        except Exception as e2:
                            # If SHOW TABLE also fails, raise the exception
                            raise e2
                    else:
                        # If error is not about 'not a view', re-raise
                        raise e

        # 1. Get the query for this DataFrame
        query = self.show_query()
        # 2. Extract all table names from the query
        for tname in UtilFuncs.extract_table_names_from_query(query):
            trace_views(tname)

        # 3.. Persist the current DataFrame as a permanent object
        # This CREATE VIEW AS SELECT ...
        # Use object_name, schema_name as needed.
        from teradataml.dbutils.dbutils import _get_quoted_object_name
        target_name = _get_quoted_object_name(schema_name=schema_name, object_name=view_name)
        
        create_sql = sql_bundle._build_create_view(view_name=target_name,
                                                    select_expression=query)
    
        # No try-except here, as we want to raise any error that occurs during execution.
        execute_sql(create_sql)

        # TODO: Add logger message that these views/tables persisted.
        # if to_persist:
        #     logger.info("to_persist: ", to_persist)

        # Remove the tables/view from GC file as we need to persist them. Removing only after
        # required object is created.
        GarbageCollector._delete_object_entry(objects_to_delete=to_persist,
                                              object_type=None,
                                              remove_entry_from_gc_list=True)

        # Return the teradataml DataFrame for the persisted object.
        if schema_name is None:
            schema_name = tdmlctx._get_current_databasename()
        return DataFrame(in_schema(schema_name=schema_name, table_name=view_name))


class DataFrameGroupBy(DataFrame):
    """
    This class integrate GroupBy clause with AED.
    Updates AED node for DataFrame groupby object.

    """

    def __init__(self, nodeid, metaexpr, column_names_and_types, columns, groupbyexpr, column_list, option=None, include_grouping_columns=False):
        """
        init() method for DataFrameGroupBy.

        PARAMETERS:
            nodeid:
                Required Argument.
                Specifies the input teradataml DataFrame nodeid.
                Types: str

            metaexpr:
                Required Argument.
                Specifies the input teradataml DataFrame metaexpr.
                Types: _MetaExpression

            column_names_and_types:
                Required Argument.
                Specifies the input teradataml DataFrame column_names_and_types.
                Types: List of tuple of column names and types

            columns:
                Required Argument.
                Specifies the input teradataml DataFrame columns.
                Types: List of Strings

            groupbyexpr:
                Required Argument.
                Specifies Group By Expression to be passed to AED API.
                Types: str

            column_list:
                Required Argument.
                Specifies list of columns provided by user to be part group by clause.
                Types: str or List of Strings

            option:
                Optional Argument.
                Specifies the groupby option.
                Permitted Values: "CUBE", "ROLLUP", None
                Types: str or NoneType

            include_grouping_columns:
                Optional Argument.
                Specifies whether to include aggregations on the grouping column(s) or not.
                When set to True, the resultant DataFrame will have the aggregations on the
                columns mentioned in "columns". Otherwise, resultant DataFrame will not have
                aggregations on the columns mentioned in "columns".
                Default Value: False
                Types: bool

        RETURNS:
            teradataml DataFrameGroupBy instance
        """
        super(DataFrameGroupBy, self).__init__()
        self._nodeid = self._aed_utils._aed_groupby(nodeid, groupbyexpr, option)
        self._metaexpr = metaexpr
        self._column_names_and_types = column_names_and_types
        self._columns = columns
        self.groupby_column_list = column_list
        self._include_grouping_columns = include_grouping_columns

    def _get_assign_allowed_types(self):
        """
        DESCRIPTION:
            Get allowed types for DataFrameGroupBy.assign() function.

        PARAMETERS:
            None.

        RETURNS:
            A tuple containing supported types for DataFrame.assign() operation.

        RAISES:
            None.

        EXAMPLES:
            allowed_types = self._get_assign_allowed_types()
        """
        from sqlalchemy.sql.functions import Function
        return (type(None), int, float, str, decimal.Decimal, Function, ColumnExpression, ClauseElement)

    def _generate_assign_metaexpr_aed_nodeid(self, drop_columns, node_id, **kwargs):
        """
        DESCRIPTION:
            Function generates the MetaExpression and AED nodeid for DataFrameGroupBy.assign()
            function based on the inputs to the function.

        PARAMETERS:
            drop_columns:
                Optional Argument.
                This argument is ignored and all columns are dropped and only new columns
                and grouping columns are returned. This is unused argument.
                Types: bool

            node_id:
                Optional Argument.
                Specifies the input nodeid for the assign operation. This is unused argument.
                Types: str

            kwargs:
                keyword, value pairs
                - keywords are the column names.
                - values can be:
                    * Column arithmetic expressions.
                    * int/float/string literals.
                    * DataFrameColumn a.k.a. ColumnExpression Functions.
                      (Visit DataFrameColumn Functions in Function reference guide for more
                      details)
                    * SQLAlchemy ClauseElements.
                      (Visit teradataml extension with SQLAlchemy in teradataml User Guide
                       and Function reference guide for more details)

        RETURNS:
            A tuple containing new MetaExpression and AED nodeid for the operation.

        RAISES:
            None.

        EXAMPLES:
            (new_meta, new_nodeid) = self._generate_assign_metaexpr_aed_nodeid(drop_columns, **kwargs)
        """
        # By default, we will drop old columns for DataFrameGroupBy.
        # Apply the assign expression.
        (new_meta, result) = self._metaexpr._assign(True, **kwargs)

        # Join the expressions in result.
        assign_expression = ', '.join(list(map(lambda x: x[1], result)))

        # Construct new MetaExpression with grouped columns in the list.
        new_column_names = []
        new_column_types = []
        for col in self.groupby_column_list:
            new_column_names.append(col)
            new_column_types.append(self[col].type)

        for col in new_meta.c:
            new_column_names.append(col.name)
            new_column_types.append(col.type)

        # No need to add group by columns to 'assign_expression'. AED will take care
        # of it while producing a query.
        new_nodeid = self._aed_utils._aed_aggregate(self._nodeid, assign_expression, "agg")

        new_meta = UtilFuncs._get_metaexpr_using_columns(new_nodeid,
                                                         zip(new_column_names,
                                                             new_column_types),
                                                         datalake=self._metaexpr.datalake)

        return (new_meta, new_nodeid)

    def _get_groupby_columns_expression(self):
        """
        Description:
            Function to get the ColumnExpression's for grouping columns.

        PARAMETERS:
            None.

        RETURNS:
            list

        EXAMPLES:
            # Create a Window from a teradataml DataFrame.
            from teradataml import *
            load_example_data("dataframe","sales")
            df = DataFrame.from_table('sales').groupby("accounts")
            df._get_groupby_columns_expression()
        """
        return [column for column in self._metaexpr.c if column.name in self.groupby_column_list]


class DataFrameGroupByTime(DataFrame):
    """
    This class integrate Group By Time clause with AED.
    Updates AED node for DataFrame GROUP BY TIME object.

    """

    def __init__(self, nodeid, metaexpr, column_names_and_types, columns, groupby_value_expr, column_list,
                 timebucket_duration,
                 value_expression=None, timecode_column=None, sequence_column=None, fill=None):
        """
        init() method for DataFrameGroupByTime.

        PARAMETERS:
            nodeid:
                Required Argument.
                Specifies the input teradataml DataFrame nodeid.
                Types: str

            metaexpr:
                Required Argument.
                Specifies the input teradataml DataFrame metaexpr.
                Types: _MetaExpression

            column_names_and_types:
                Required Argument.
                Specifies the input teradataml DataFrame column_names_and_types.
                Types: List of tuple of column names and types

            columns:
                Required Argument.
                Specifies the input teradataml DataFrame columns.
                Types: List of Strings

            groupby_value_expr:
                Required Argument.
                Specifies Group By Expression to be passed to AED API.
                Types: str

            column_list:
                Required Argument.
                Specifies list of columns provided by user to be part of GROUP BY TIME clause.
                Types: str or List of Strings

            timebucket_duration:
                Required Argument.
                Specifies the duration of each timebucket for aggregation and is used to
                assign each potential timebucket a unique number.
                Types: Str
                Example: MINUTES(23) which is equal to 23 Minutes
                         CAL_MONTHS(5) which is equal to 5 calendar months

            value_expression:
                Required Argument.
                Specifies a column or any expression involving columns (except for scalar subqueries).
                These expressions are used for grouping purposes not related to time.
                Types: str or List of Strings
                Example: col1 or ["col1", "col2"]

            timecode_column:
                Required Argument.
                Specifies a column expression that serves as the timecode for a non-PTI table.
                TD_TIMECODE is used implicitly for PTI tables, but can also be specified
                explicitly by the user with this parameter.
                Types: str

            sequence_column:
                Required Argument.
                Specifies a column expression (with an optional table name) that is the sequence number.
                For a PTI table, it can be TD_SEQNO or any other column that acts as a sequence number.
                For a non-PTI table, sequence_column is a column that plays the role of TD_SEQNO (because non-PTI tables
                do not have TD_SEQNO).
                Types: str

            fill:
                Required Argument.
                Specifies values for missing timebucket values.
                Types: str or int or float

        RETURNS:
            teradataml DataFrameGroupByTime instance

        """
        super(DataFrameGroupByTime, self).__init__()

        # Processing for GROUP BY TIME clasue.
        if groupby_value_expr is None:
            groupby_value_expr = ""

        if fill is None:
            fill = ""

        timecode_column = "" if timecode_column is None else UtilFuncs._process_for_teradata_keyword(timecode_column)
        sequence_column = "" if sequence_column is None else UtilFuncs._process_for_teradata_keyword(sequence_column)

        self._nodeid = self._aed_utils._aed_groupby_time(nodeid=nodeid, timebucket_duration=timebucket_duration,
                                                         value_expression=groupby_value_expr,
                                                         using_timecode=timecode_column, seqno_col=sequence_column,
                                                         fill=fill)

        # MetaExpression is same as that of parent.
        self._metaexpr = metaexpr
        # Columns are same as that of parent columns
        self._columns = columns
        # List of columns is GROUP BY TIME clause.
        self.groupby_column_list = column_list
        # Retrieve metadata information
        #   1. '_column_names_and_types',
        #   2. '_td_column_names_and_types' and
        #   3. '_td_column_names_and_sqlalchemy_types'
        self._get_metadata_from_metaexpr(self._metaexpr)

        # Saving attributes used while constructing Group By Time clause
        self._timebucket_duration = timebucket_duration
        self._value_expression = []
        if value_expression is not None:
            self._value_expression = value_expression
        self._timecode_column = timecode_column
        self._sequence_column = sequence_column
        self._fill = fill

    @collect_queryband(queryband="DF_bottom")
    def bottom(self, number_of_values_to_column, with_ties=False):
        """
        DESCRIPTION:
            Returns the smallest number of values in the columns for each group, with or without ties.

            Note:
                1. This function is valid only on columns with numeric types.
                2. Null values are not included in the result computation.

        PARAMETERS:
            number_of_values_to_column:
                Required Argument.
                Specifies a dictionary that accepts number of values to be selected for each column.
                Number of values is a key in the dictionary. Key should be any positive integer.
                Whereas value in the dictionary can be a column name or list of column names.
                Sometimes, value can also include a special character '*', instead of column name.
                This should be used only when one wants to return same number of values for all columns.
                Types: Dictionary
                Examples:
                    # Let's assume, a teradataml DataFrame has following columns:
                    #   col1, col2, col3, ..., colN

                    # For bottom() to return 2 values for column "col1":
                    number_of_values_to_column = {2: "col1"}

                    # For bottom() to return 2 values for column "col1" and 5 values for "col3":
                    number_of_values_to_column = {2: "col1", 5: "col3"}

                    # For bottom() to return 2 values for column "col1", "col2" and "col3":
                    number_of_values_to_column = {2: ["col1", "col2", "col3"]}

                    # Use cases for using '*' default value.
                    # For bottom() to return 2 values for all columns. In case, we need to return 2 values
                    # for each column in the DataFrame, then one can use '*'.
                    number_of_values_to_column = {2: "*"}

                    # For bottom() to return 2 values for column "col1" and "col3"
                    # and 5 values for rest of the columns:
                    number_of_values_to_column = {2: ["col1", "col3"], 5: "*"}

                    # We can use default value column character ('*') in list as well
                    # For bottom() to return 2 values for column "col1" and "col3"
                    # and 5 values for "col4" and rest of the columns:
                    number_of_values_to_column = {2: ["col1", "col3"], 5: ["col4", "*"]}

            with_ties:
                Optional Argument.
                Specifies a flag to decide whether to run bottom function with ties or not.
                BOTTOM WITH TIES implies that the rows returned include the specified number of rows in
                the ordered set for each timebucket. It includes any rows where the sort key value
                is the same as the sort key value in the last row that satisfies the specified number
                or percentage of rows. If this clause is omitted and ties are found, the earliest
                value in terms of timecode is returned.
                Types: bool

        RETURNS:
            teradataml DataFrame

        RAISES:
            TypeError - If incorrect type of values passed to input argument.
            ValueError - If invalid value passed to the the argument.
            TeradataMLException
                1. If required argument 'number_of_values_to_column' is missing or None is passed.
                2. TDMLDF_AGGREGATE_FAILED - If bottom() operation fails to
                    generate the column-wise smallest number of values for the columns.

        EXAMPLES :
            >>> # Load the example datasets.
            ... load_example_data("dataframe", ["ocean_buoys", "ocean_buoys_seq", "ocean_buoys_nonpti"])
            >>>

            >>> # Create the required DataFrames.
            ... # DataFrame on non-sequenced PTI table
            ... ocean_buoys = DataFrame("ocean_buoys")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys.columns
            ['buoyid', 'TD_TIMECODE', 'temperature', 'salinity']
            >>> ocean_buoys.head()
                                   TD_TIMECODE  temperature  salinity
            buoyid
            0       2014-01-06 08:10:00.000000        100.0        55
            0       2014-01-06 08:08:59.999999          NaN        55
            1       2014-01-06 09:01:25.122200         77.0        55
            1       2014-01-06 09:03:25.122200         79.0        55
            1       2014-01-06 09:01:25.122200         70.0        55
            1       2014-01-06 09:02:25.122200         71.0        55
            1       2014-01-06 09:03:25.122200         72.0        55
            0       2014-01-06 08:09:59.999999         99.0        55
            0       2014-01-06 08:00:00.000000         10.0        55
            0       2014-01-06 08:10:00.000000         10.0        55

            >>> # DataFrame on sequenced PTI table
            ... ocean_buoys_seq = DataFrame("ocean_buoys_seq")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys_seq.columns
            ['TD_TIMECODE', 'TD_SEQNO', 'buoyid', 'salinity', 'temperature', 'dates']
            >>> ocean_buoys_seq.head()
                                   TD_TIMECODE  TD_SEQNO  salinity  temperature       dates
            buoyid
            0       2014-01-06 08:00:00.000000        26        55         10.0  2016-02-26
            0       2014-01-06 08:08:59.999999        18        55          NaN  2015-06-18
            1       2014-01-06 09:02:25.122200        24        55         78.0  2015-12-24
            1       2014-01-06 09:01:25.122200        23        55         77.0  2015-11-23
            1       2014-01-06 09:02:25.122200        12        55         71.0  2014-12-12
            1       2014-01-06 09:03:25.122200        13        55         72.0  2015-01-13
            1       2014-01-06 09:01:25.122200        11        55         70.0  2014-11-11
            0       2014-01-06 08:10:00.000000        19        55         10.0  2015-07-19
            0       2014-01-06 08:09:59.999999        17        55         99.0  2015-05-17
            0       2014-01-06 08:10:00.000000        27        55        100.0  2016-03-27

            >>> # DataFrame on NON-PTI table
            ... ocean_buoys_nonpti = DataFrame("ocean_buoys_nonpti")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys_nonpti.columns
            ['buoyid', 'timecode', 'temperature', 'salinity']
            >>> ocean_buoys_nonpti.head()
                                        buoyid  temperature  salinity
            timecode
            2014-01-06 08:09:59.999999       0         99.0        55
            2014-01-06 08:10:00.000000       0         10.0        55
            2014-01-06 09:01:25.122200       1         70.0        55
            2014-01-06 09:01:25.122200       1         77.0        55
            2014-01-06 09:02:25.122200       1         71.0        55
            2014-01-06 09:03:25.122200       1         72.0        55
            2014-01-06 09:02:25.122200       1         78.0        55
            2014-01-06 08:10:00.000000       0        100.0        55
            2014-01-06 08:08:59.999999       0          NaN        55
            2014-01-06 08:00:00.000000       0         10.0        55

            ### Examples for bottom without ties ###
            #
            # Example 1: Executing bottom function on DataFrame created on non-sequenced PTI table.
            #
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="MINUTES(2)",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> number_of_values_to_column = {2: "temperature"}
            >>> ocean_buoys_grpby1.bottom(number_of_values_to_column).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(MINUTES(2))  buoyid  bottom2temperature
            0  ('2014-01-06 08:00:00.000000+00:00', '2014-01-...                     530161       0                10.0
            1  ('2014-01-06 08:02:00.000000+00:00', '2014-01-...                     530162       0                 NaN
            2  ('2014-01-06 08:04:00.000000+00:00', '2014-01-...                     530163       0                 NaN
            3  ('2014-01-06 08:06:00.000000+00:00', '2014-01-...                     530164       0                 NaN
            4  ('2014-01-06 08:08:00.000000+00:00', '2014-01-...                     530165       0                99.0
            5  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                     530166       0               100.0
            6  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                     530166       0                10.0
            7  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                     530191       1                70.0
            8  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                     530191       1                77.0
            9  ('2014-01-06 09:02:00.000000+00:00', '2014-01-...                     530192       1                71.0

            #
            # Example 2: Executing bottom to select 2 values for all the columns in ocean_buoys_seq DataFrame
            #            on sequenced PTI table.
            #
            >>> ocean_buoys_seq_grpby1 = ocean_buoys_seq.groupby_time(timebucket_duration="MINUTES(2)",
            ...                                                       value_expression="buoyid", fill="NULLS")
            >>> number_of_values_to_column = {2: "*"}
            >>> ocean_buoys_seq_grpby1.bottom(number_of_values_to_column).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(MINUTES(2))  buoyid  bottom2TD_SEQNO  bottom2salinity  bottom2temperature
            0  ('2014-01-06 08:00:00.000000+00:00', '2014-01-...                     530161       0             26.0             55.0                10.0
            1  ('2014-01-06 08:02:00.000000+00:00', '2014-01-...                     530162       0              NaN              NaN                 NaN
            2  ('2014-01-06 08:04:00.000000+00:00', '2014-01-...                     530163       0              NaN              NaN                 NaN
            3  ('2014-01-06 08:06:00.000000+00:00', '2014-01-...                     530164       0              NaN              NaN                 NaN
            4  ('2014-01-06 08:08:00.000000+00:00', '2014-01-...                     530165       0             17.0             55.0                99.0
            5  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                     530166       0             19.0             55.0                10.0
            6  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                     530191       1             11.0             55.0                70.0
            7  ('2014-01-06 09:02:00.000000+00:00', '2014-01-...                     530192       1             12.0             55.0                71.0
            8  ('2014-01-06 10:00:00.000000+00:00', '2014-01-...                     530221      44              4.0             55.0                43.0
            9  ('2014-01-06 10:02:00.000000+00:00', '2014-01-...                     530222      44              9.0             55.0                53.0

            #
            # Example 3: Executing bottom function on DataFrame created on NON-PTI table.
            #
            >>> ocean_buoys_nonpti_grpby1 = ocean_buoys_nonpti.groupby_time(timebucket_duration="MINUTES(2)",
            ...                                                             value_expression="buoyid",
            ...                                                             timecode_column="timecode", fill="NULLS")
            >>> number_of_values_to_column = {2: "temperature"}
            >>> ocean_buoys_nonpti_grpby1.bottom(number_of_values_to_column).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(MINUTES(2))  buoyid  bottom2temperature
            0  ('2014-01-06 08:00:00.000000+00:00', '2014-01-...                   11574961       0                10.0
            1  ('2014-01-06 08:02:00.000000+00:00', '2014-01-...                   11574962       0                 NaN
            2  ('2014-01-06 08:04:00.000000+00:00', '2014-01-...                   11574963       0                 NaN
            3  ('2014-01-06 08:06:00.000000+00:00', '2014-01-...                   11574964       0                 NaN
            4  ('2014-01-06 08:08:00.000000+00:00', '2014-01-...                   11574965       0                99.0
            5  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                   11574966       0               100.0
            6  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                   11574966       0                10.0
            7  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                   11574991       1                70.0
            8  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                   11574991       1                77.0
            9  ('2014-01-06 09:02:00.000000+00:00', '2014-01-...                   11574992       1                71.0

            ### Examples for bottom with ties ###
            #
            # Example 4: Executing bottom with ties function on DataFrame created on non-sequenced PTI table.
            #
            >>> ocean_buoys_grpby2 = ocean_buoys.groupby_time(timebucket_duration="MINUTES(2)",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> number_of_values_to_column = {2: "temperature"}
            >>> ocean_buoys_grpby2.bottom(number_of_values_to_column, with_ties=True).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(MINUTES(2))  buoyid  bottom_with_ties2temperature
            0  ('2014-01-06 08:00:00.000000+00:00', '2014-01-...                     530161       0                          10.0
            1  ('2014-01-06 08:02:00.000000+00:00', '2014-01-...                     530162       0                           NaN
            2  ('2014-01-06 08:04:00.000000+00:00', '2014-01-...                     530163       0                           NaN
            3  ('2014-01-06 08:06:00.000000+00:00', '2014-01-...                     530164       0                           NaN
            4  ('2014-01-06 08:08:00.000000+00:00', '2014-01-...                     530165       0                          99.0
            5  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                     530166       0                         100.0
            6  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                     530166       0                          10.0
            7  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                     530191       1                          77.0
            8  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                     530191       1                          70.0
            9  ('2014-01-06 09:02:00.000000+00:00', '2014-01-...                     530192       1                          71.0

            #
            # Example 5: Executing bottom with ties to select 2 values for temperature and 3 for rest of the columns in
            #            ocean_buoys DataFrame.
            #
            >>> ocean_buoys_grpby3 = ocean_buoys.groupby_time(timebucket_duration="MINUTES(2)", fill="NULLS")
            >>> number_of_values_to_column = {2: "temperature", 3:"*"}
            >>> ocean_buoys_grpby3.bottom(number_of_values_to_column, with_ties=True).sort(["TIMECODE_RANGE"])
                                                  TIMECODE_RANGE  GROUP BY TIME(MINUTES(2))  bottom_with_ties3buoyid  bottom_with_ties3salinity  bottom_with_ties2temperature
            0  ('2014-01-06 08:00:00.000000+00:00', '2014-01-...                     530161                      0.0                       55.0                          10.0
            1  ('2014-01-06 08:02:00.000000+00:00', '2014-01-...                     530162                      NaN                        NaN                           NaN
            2  ('2014-01-06 08:04:00.000000+00:00', '2014-01-...                     530163                      NaN                        NaN                           NaN
            3  ('2014-01-06 08:06:00.000000+00:00', '2014-01-...                     530164                      NaN                        NaN                           NaN
            4  ('2014-01-06 08:08:00.000000+00:00', '2014-01-...                     530165                      0.0                       55.0                          99.0
            5  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                     530166                      0.0                       55.0                          10.0
            6  ('2014-01-06 08:12:00.000000+00:00', '2014-01-...                     530167                      NaN                        NaN                           NaN
            7  ('2014-01-06 08:14:00.000000+00:00', '2014-01-...                     530168                      NaN                        NaN                           NaN
            8  ('2014-01-06 08:16:00.000000+00:00', '2014-01-...                     530169                      NaN                        NaN                           NaN
            9  ('2014-01-06 08:18:00.000000+00:00', '2014-01-...                     530170                      NaN                        NaN                           NaN
            >>>

            #
            # Example 6: Executing bottom with ties function on DataFrame created on NON-PTI table.
            #
            >>> ocean_buoys_nonpti_grpby2 = ocean_buoys_nonpti.groupby_time(timebucket_duration="MINUTES(2)",
            ...                                                             value_expression="buoyid",
            ...                                                             timecode_column="timecode", fill="NULLS")
            >>> number_of_values_to_column = {2: "temperature"}
            >>> ocean_buoys_nonpti_grpby2.bottom(number_of_values_to_column, with_ties=True).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(MINUTES(2))  buoyid  bottom_with_ties2temperature
            0  ('2014-01-06 08:00:00.000000+00:00', '2014-01-...                   11574961       0                          10.0
            1  ('2014-01-06 08:02:00.000000+00:00', '2014-01-...                   11574962       0                           NaN
            2  ('2014-01-06 08:04:00.000000+00:00', '2014-01-...                   11574963       0                           NaN
            3  ('2014-01-06 08:06:00.000000+00:00', '2014-01-...                   11574964       0                           NaN
            4  ('2014-01-06 08:08:00.000000+00:00', '2014-01-...                   11574965       0                          99.0
            5  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                   11574966       0                          10.0
            6  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                   11574966       0                         100.0
            7  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                   11574991       1                          77.0
            8  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                   11574991       1                          70.0
            9  ('2014-01-06 09:02:00.000000+00:00', '2014-01-...                   11574992       1                          71.0
            >>>
            >>>
        """
        # Set the operation
        operation = "bottom"
        if with_ties:
            operation = "bottom with ties"

        # Argument validations
        awu_matrix = []
        awu_matrix.append(["number_of_values_to_column", number_of_values_to_column, False, (dict)])
        awu_matrix.append(["with_ties", with_ties, True, (bool)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)
        _Validators._validate_missing_required_arguments(awu_matrix)

        # Check if number_of_values_to_column dict is empty or not.
        if not number_of_values_to_column:
            raise ValueError(Messages.get_message(MessageCodes.ARG_EMPTY, "number_of_values_to_column"))

        return self.__process_time_series_aggregate_with_multi_input_arguments(number_of_values_to_column, operation)

    @collect_queryband(queryband="DF_deltaT")
    def delta_t(self, start_condition, end_condition):
        """
        DESCRIPTION:
            Calculates the time difference, or DELTA_T, between a starting and an ending event.
            The calculation is performed against a time-ordered time series data set.

            Note:
                1. This is the only Time Series Aggregate function that works with timebucket_duration as "*"
                   in groupby_time(), i.e., unbounded time.
                2. When using groupby_time() with unbounded time, the following rules apply to
                   the system virtual columns:
                    a. $TD_GROUP_BY_TIME: Always has a value of 1, since there is only one timebucket.
                    b. $TD_TIMECODE_RANGE: Composed of the first and last timecode values read for the group.
                Note that the data being evaluated in the filtering conditions (for example, the minimum and
                maximum temperature observation) must belong to the timecode value present in the same row
                of data. This is the expected behavior. However, this assumption can be violated when
                joining multiple tables together. It is possible to construct a query where the result of a
                join causes specific data points (for example, a temperature reading) to be present in a
                data row with a timecode that is not indicative of when that data point occurred.
                In such a scenario, it is highly likely that the results are not as expected, or are misleading.
                Vantage does not detect these types of queries, so one must make sure to preserve the
                correlation between data points and timecodes.

        PARAMETERS:
            start_condition:
                Required Argument.
                Specifies any supported filtering condition that defines the start of the time period for which
                you are searching.
                Types: str or ColumnExpression

            end_condition:
                Required Argument.
                Specifies any supported filtering condition that defines the end of the time period for which
                you are searching.
                Types:  str or ColumnExpression

        RETURNS:
            teradataml DataFrame

            Note:
                1. Function returns a column of PERIOD(TIMESTAMP WITH TIME ZONE) type (Vantage Data type)
                   composed of the start and end timecode, i.e., timecode column used for aggregation
                   of each start-end pair.
                2. One result is returned per complete start-end pair found within the
                   GROUP BY TIME window. The start-end pair process is as follows:
                    a. If the current source data meets the start condition, the current
                       timecode is saved as the start time.
                    b. If the current source data meets the end condition, and a saved start
                       timecode already exists, the start timecode is saved with the end timecode
                       encountered as a result pair.
                3. The processing algorithm implies that multiple results may be found in each group.
                4. If no start-end pair is encountered, no result row is returned.
                5. Any result of delta_t which has a delta less than 1 microsecond (including a delta of 0,
                   in the case of a result which comes from a single point in time) is automatically
                   rounded to 1 microsecond.
                   This is strictly enforced to match Period data type semantics in Vantage which dictate that a
                   starting and ending bound of a Period type may not be equivalent. The smallest granularity
                   supported in Vantage is the microsecond, so these results are rounded accordingly.

        RAISES:
            TypeError - If incorrect type of values passed to input argument.
            ValueError - If invalid value passed to the the argument.
            TeradataMLException - In case illegal conditions are passed

        EXAMPLES :
            >>> # Load the example datasets.
            ... load_example_data("dataframe", ["ocean_buoys", "ocean_buoys_seq", "ocean_buoys_nonpti", "package_tracking_pti", "package_tracking"])
            >>>

            #
            # Example 1: Finding Time Elapsed between Shipping and Receiving an Item.
            #            Input data used for this example contains information about parcels
            #            sent by a delivery service.
            #

            #
            # Case 1: Using DataFrame on PTI Table and showcasing usage of unbounded time in grouping.
            #
            >>> # Create DataFrame on PTI table
            ... package_tracking_pti = DataFrame("package_tracking_pti")
            >>> package_tracking_pti.columns
            ['TD_TIMECODE', 'parcelnumber', 'status']
            >>> package_tracking_pti
                                         TD_TIMECODE                          status
            parcelnumber
            55            2016-10-15 10:00:00.000000       in transit to destination
            75            2016-10-15 16:30:00.000000          in transit to customer
            75            2016-10-15 08:00:00.000000         picked up from customer
            55            2016-10-15 09:10:00.000000    arrived at receiving station
            60            2016-10-15 10:45:00.000000    arrived at receiving station
            75            2016-10-15 17:00:00.000000           delivered to customer
            59            2016-10-15 08:05:00.000000         picked up from customer
            79            2016-10-15 08:05:00.000000         picked up from customer
            60            2016-10-15 09:20:00.000000         picked up from customer
            75            2016-10-15 16:10:00.000000  arrived at destination station
            >>>
            >>> # Execute groupby_time() using unbounded time for timebucket_duration.
            ... gbt = package_tracking_pti.groupby_time("*", "parcelnumber")
            >>> # Execute delta_t, with start and end conditions specified as String.
            ... start_condition = "status LIKE 'picked%up%customer'"
            >>> end_condition = "status LIKE 'delivered%customer'"
            >>> gbt.delta_t(start_condition, end_condition)
                                                  TIMECODE_RANGE  parcelnumber                                delta_t_td_timecode
            0  ('2012-01-01 00:00:00.000000+00:00', '9999-12-...            75  ('2016-10-15 08:00:00.000000-00:00', '2016-10-...
            1  ('2012-01-01 00:00:00.000000+00:00', '9999-12-...            55  ('2016-10-15 08:00:00.000000-00:00', '2016-10-...
            >>>

            #
            # Case 2: Using DataFrame on Non-PTI Table and showcasing usage of unbounded time in grouping.
            #
            >>> # Create DataFrame on Non-PTI table
            ... package_tracking = DataFrame("package_tracking")
            >>> package_tracking.columns
            ['parcelnumber', 'clock_time', 'status']
            >>> package_tracking
                                          clock_time                          status
            parcelnumber
            79            2016-10-15 08:05:00.000000         picked up from customer
            75            2016-10-15 09:10:00.000000    arrived at receiving station
            75            2016-10-15 10:00:00.000000       in transit to destination
            75            2016-10-15 16:10:00.000000  arrived at destination station
            75            2016-10-15 17:00:00.000000           delivered to customer
            80            2016-10-15 09:20:00.000000         picked up from customer
            59            2016-10-15 08:05:00.000000         picked up from customer
            75            2016-10-15 16:30:00.000000          in transit to customer
            75            2016-10-15 08:00:00.000000         picked up from customer
            60            2016-10-15 10:45:00.000000    arrived at receiving station
            >>>
            >>> # Execute groupby_time() using unbounded time for timebucket_duration.
            ... gbt = package_tracking.groupby_time("*", "parcelnumber", "clock_time")
            >>> # Execute delta_t, with start and end conditions specified as String.
            ... start_condition = "status LIKE 'picked%up%customer'"
            >>> end_condition = "status LIKE 'delivered%customer'"
            >>> gbt.delta_t(start_condition, end_condition)
                                                  TIMECODE_RANGE  parcelnumber                                delta_t_td_timecode
            0  ('1970-01-01 00:00:00.000000+00:00', '9999-12-...            75  ('2016-10-15 08:00:00.000000-00:00', '2016-10-...
            1  ('1970-01-01 00:00:00.000000+00:00', '9999-12-...            55  ('2016-10-15 08:00:00.000000-00:00', '2016-10-...
            >>>


            #
            # Example 2: Searching the Minimum and Maximum Observed Temperatures
            #            This example measures the time between minimum and maximum observed temperatures every
            #            30 minutes between 8:00 AM and 10:30 AM on each buoy.
            #

            #
            # Case 1: DataFrame on Non-sequenced PTI Table - specifying start condition and end condition as string
            #
            >>> # Create DataFrame
            ... ocean_buoys = DataFrame("ocean_buoys")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys.columns
            ['buoyid', 'TD_TIMECODE', 'temperature', 'salinity']
            >>> ocean_buoys.head()
                                   TD_TIMECODE  temperature  salinity
            buoyid
            0       2014-01-06 08:10:00.000000        100.0        55
            0       2014-01-06 08:08:59.999999          NaN        55
            1       2014-01-06 09:01:25.122200         77.0        55
            1       2014-01-06 09:03:25.122200         79.0        55
            1       2014-01-06 09:01:25.122200         70.0        55
            1       2014-01-06 09:02:25.122200         71.0        55
            1       2014-01-06 09:03:25.122200         72.0        55
            0       2014-01-06 08:09:59.999999         99.0        55
            0       2014-01-06 08:00:00.000000         10.0        55
            0       2014-01-06 08:10:00.000000         10.0        55

            >>> # Filter the data and grab all rows between  timestamp '2014-01-06 08:00:00' and '2014-01-06 10:30:00'
            ... ocean_buoys_dt = ocean_buoys[(ocean_buoys.TD_TIMECODE >= '2014-01-06 08:00:00') & (ocean_buoys.TD_TIMECODE < '2014-01-06 10:30:00')]
            >>>
            >>> # Let's get the minimum and maximum temperature within time range of 30 minutes
            ... df_min_max_temp1 = ocean_buoys_dt.groupby_time("MINUTES(30)", "buoyid", "TD_TIMECODE").agg({"temperature": ["min", "max"]})
            >>> # Join the dataframe with original 'ocean_buoys'
            ... df2_join1 = ocean_buoys.join(df_min_max_temp1, on="buoyid", how="inner", lsuffix="t1", rsuffix="t2")
            >>> gbt3 = df2_join1.groupby_time("DAYS(1)", "t1_buoyid", timecode_column="TD_TIMECODE")
            >>>
            >>> # Let's set the start and end conditions
            ... start_condition = "temperature = min_temperature"
            >>> end_condition = "temperature = max_temperature"
            >>> gbt3.delta_t(start_condition, end_condition)
                                                  TIMECODE_RANGE  GROUP BY TIME(DAYS(1))  t1_buoyid                                delta_t_td_timecode
            0  ('2014-01-06 00:00:00.000000+00:00', '2014-01-...                   16077         44  ('2014-01-06 10:00:26.122200-00:00', '2014-01-...
            1  ('2014-01-06 00:00:00.000000+00:00', '2014-01-...                   16077          1  ('2014-01-06 09:01:25.122200-00:00', '2014-01-...
            2  ('2014-01-06 00:00:00.000000+00:00', '2014-01-...                   16077          0  ('2014-01-06 08:00:00.000000-00:00', '2014-01-...
            >>>

            #
            # Case 2: Same example as that of above, just DataFrame on Sequenced PTI Table and
            #         specifying start condition and end condition as ColumnExpression.
            #
            >>> # Create DataFrame
            ... ocean_buoys_seq = DataFrame("ocean_buoys_seq")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys_seq.columns
            ['TD_TIMECODE', 'TD_SEQNO', 'buoyid', 'salinity', 'temperature', 'dates']
            >>> ocean_buoys_seq.head()
                                   TD_TIMECODE  TD_SEQNO  salinity  temperature       dates
            buoyid
            0       2014-01-06 08:00:00.000000        26        55         10.0  2016-02-26
            0       2014-01-06 08:08:59.999999        18        55          NaN  2015-06-18
            1       2014-01-06 09:02:25.122200        24        55         78.0  2015-12-24
            1       2014-01-06 09:01:25.122200        23        55         77.0  2015-11-23
            1       2014-01-06 09:02:25.122200        12        55         71.0  2014-12-12
            1       2014-01-06 09:03:25.122200        13        55         72.0  2015-01-13
            1       2014-01-06 09:01:25.122200        11        55         70.0  2014-11-11
            0       2014-01-06 08:10:00.000000        19        55         10.0  2015-07-19
            0       2014-01-06 08:09:59.999999        17        55         99.0  2015-05-17
            0       2014-01-06 08:10:00.000000        27        55        100.0  2016-03-27
            >>> # Filter the data and grab all rows between  timestamp '2014-01-06 08:00:00' and '2014-01-06 10:30:00'
            ... ocean_buoys_seq_dt = ocean_buoys_seq[(ocean_buoys_seq.TD_TIMECODE >= '2014-01-06 08:00:00') & (ocean_buoys_seq.TD_TIMECODE < '2014-01-06 10:30:00')]
            >>>
            >>> # Let's get the minimum and maximum temperature within time range of 30 minutes
            ... df_min_max_temp2 = ocean_buoys_seq_dt.groupby_time("MINUTES(30)", "buoyid", "TD_TIMECODE").agg({"temperature": ["min", "max"]})
            >>> # Join the dataframe with original 'ocean_buoys'
            ... df2_join2 = ocean_buoys_seq.join(df_min_max_temp2, on="buoyid", how="inner", lsuffix="t1", rsuffix="t2")
            >>> gbt4 = df2_join2.groupby_time("DAYS(1)", "t1_buoyid", timecode_column="TD_TIMECODE")
            >>>
            >>> # Let's set the start and end conditions
            >>> start_condition = gbt4.temperature == gbt4.min_temperature
            >>> end_condition = gbt4.temperature == gbt4.max_temperature
            >>> gbt4.delta_t(start_condition, end_condition)
                                                  TIMECODE_RANGE  GROUP BY TIME(DAYS(1))  t1_buoyid                                delta_t_td_timecode
            0  ('2014-01-06 00:00:00.000000+00:00', '2014-01-...                   16077         44  ('2014-01-06 10:00:26.122200-00:00', '2014-01-...
            1  ('2014-01-06 00:00:00.000000+00:00', '2014-01-...                   16077          1  ('2014-01-06 09:01:25.122200-00:00', '2014-01-...
            2  ('2014-01-06 00:00:00.000000+00:00', '2014-01-...                   16077          0  ('2014-01-06 08:00:00.000000-00:00', '2014-01-...
            >>>
            
        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["start_condition", start_condition, False, (str, ColumnExpression)])
        awu_matrix.append(["end_condition", end_condition, False, (str, ColumnExpression)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        # Note: Validation of start_condition and end_condition is not done here, as conditions can be anything
        #       (any SQL Expression going in where clause as str or any SQLColumnExpression). In case any issue
        #       with these arguments being incorrect, error will be raised from Vantage side.
        # Check for empty string, if arguments are of type String.
        if isinstance(start_condition, str):
            _Validators._validate_input_columns_not_empty(start_condition, "start_condition")

        if isinstance(end_condition, str):
            _Validators._validate_input_columns_not_empty(end_condition, "end_condition")

        # Set the operation
        operation = "delta_t"

        kwargs = {
            "start_condition": start_condition.compile() if isinstance(start_condition,
                                                                       ColumnExpression) else start_condition,
            "end_condition": end_condition.compile() if isinstance(end_condition, ColumnExpression) else end_condition
        }
        return self._get_dataframe_aggregate(operation=operation, **kwargs)

    @collect_queryband(queryband="DF_first")
    def first(self, columns=None):
        """
        DESCRIPTION:
            Returns the oldest value, determined by the timecode, for each group. FIRST is a single-threaded function.
            In the event of a tie, such as simultaneous timecode values for a particular group, all tied results
            are returned. If a sequence number is present with the data, it can break a tie, assuming it is unique
            across identical timecode values.

            Note:
                1. This function is valid only on columns with numeric types.
                2. Null values are not included in the result computation.

        PARAMETERS:
            columns:
                Optional Argument.
                Specifies a column name or list of column names on which first() operation
                must be run. By default oldest value is returned for all the compatible columns
                in a teradataml DataFrame.
                Types: str OR list of Strings (str)

        RETURNS:
            teradataml DataFrame object with first() operation performed.

        RAISES:
            1. TDMLDF_AGGREGATE_FAILED - If first() operation fails to
                return oldest value of columns in the teradataml DataFrame.

                Possible error message:
                Unable to perform 'first()' on the teradataml DataFrame.

            2. TDMLDF_AGGREGATE_COMBINED_ERR - If the first() operation
                doesn't support all the columns in the teradataml DataFrame.

                Possible error message:
                No results. Below is/are the error message(s):
                All selected columns [(col2 -  PERIOD_TIME), (col3 -
                BLOB)] is/are unsupported for 'first' operation.

        EXAMPLES :
            >>> # Load the example datasets.
            ... load_example_data("dataframe", ["ocean_buoys", "ocean_buoys_seq", "ocean_buoys_nonpti"])
            >>>

            #
            # Example 1: Executing first function on DataFrame created on non-sequenced PTI table.
            #
            >>> # Create the required DataFrames.
            ... # DataFrame on non-sequenced PTI table
            ... ocean_buoys = DataFrame("ocean_buoys")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys.columns
            ['buoyid', 'TD_TIMECODE', 'temperature', 'salinity']
            >>> ocean_buoys.head()
                                   TD_TIMECODE  temperature  salinity
            buoyid
            0       2014-01-06 08:10:00.000000        100.0        55
            0       2014-01-06 08:08:59.999999          NaN        55
            1       2014-01-06 09:01:25.122200         77.0        55
            1       2014-01-06 09:03:25.122200         79.0        55
            1       2014-01-06 09:01:25.122200         70.0        55
            1       2014-01-06 09:02:25.122200         71.0        55
            1       2014-01-06 09:03:25.122200         72.0        55
            0       2014-01-06 08:09:59.999999         99.0        55
            0       2014-01-06 08:00:00.000000         10.0        55
            0       2014-01-06 08:10:00.000000         10.0        55

            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cd",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.first().sort(["TIMECODE_RANGE", "buoyid"])
            /mnt/c/Users/pp186043/GitHub_Repos/pyTeradata/teradataml/common/utils.py:398: VantageRuntimeWarning: [Teradata][teradataml](TDML_2086) Following warning raised from Vantage with warning code: 4001
            [Teradata Database] [Warning 4001] Time Series Auxiliary Cache Warning: Multiple results found for one or more Time Series aggregate functions in this query, but only one result was returned. To get all results, resubmit this query with these aggregates isolated.
              VantageRuntimeWarning)
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_DAYS(2))  buoyid  first_salinity  first_temperature
            0  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369       0              55                 10
            1  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369       1              55                 70
            2  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369       2              55                 80
            3  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369      44              55                 43
            >>>

            #
            # Example 2: In Example 1, a VantageRuntimeWarning is raised as:
            #           "[Teradata Database] [Warning 4001] Time Series Auxiliary Cache Warning: Multiple results
            #            found for one or more Time Series aggregate functions in this query, but only one result
            #            was returned. To get all results, resubmit this query with these aggregates isolated."
            #
            #            This warning recommends to execute first() independently on each column, so that we will get
            #            all the results.
            #            To run first() on one single column we can pass column name as input. Let's run first()
            #            on 'temperature' column.
            #
            >>> ocean_buoys_grpby1.first('temperature')
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_DAYS(2))  buoyid  first_temperature
            0  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369       2                 80
            1  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369       0                 10
            2  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369      44                 43
            3  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369       1                 77
            4  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369       1                 70
            >>>

            #
            # Example 3: Executing first function on ocean_buoys_seq DataFrame created on sequenced PTI table.
            #            Table has few columns incompatible for first() operation 'dates' and 'TD_TIMECODE',
            #            while executing this first() incompatible columns are ignored.
            #
            >>> # DataFrame on sequenced PTI table
            ... ocean_buoys_seq = DataFrame("ocean_buoys_seq")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys_seq.columns
            ['TD_TIMECODE', 'TD_SEQNO', 'buoyid', 'salinity', 'temperature', 'dates']
            >>> ocean_buoys_seq.head()
                                   TD_TIMECODE  TD_SEQNO  salinity  temperature       dates
            buoyid
            0       2014-01-06 08:00:00.000000        26        55         10.0  2016-02-26
            0       2014-01-06 08:08:59.999999        18        55          NaN  2015-06-18
            1       2014-01-06 09:02:25.122200        24        55         78.0  2015-12-24
            1       2014-01-06 09:01:25.122200        23        55         77.0  2015-11-23
            1       2014-01-06 09:02:25.122200        12        55         71.0  2014-12-12
            1       2014-01-06 09:03:25.122200        13        55         72.0  2015-01-13
            1       2014-01-06 09:01:25.122200        11        55         70.0  2014-11-11
            0       2014-01-06 08:10:00.000000        19        55         10.0  2015-07-19
            0       2014-01-06 08:09:59.999999        17        55         99.0  2015-05-17
            0       2014-01-06 08:10:00.000000        27        55        100.0  2016-03-27

            >>> ocean_buoys_seq_grpby1 = ocean_buoys_seq.groupby_time(timebucket_duration="2cd",
            ...                                                       value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_seq_grpby1.first().sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_DAYS(2))  buoyid  first_TD_SEQNO  first_salinity  first_temperature
            0  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369       0              26              55                 10
            1  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369       1              11              55                 70
            2  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369       2              14              55                 80
            3  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369      22               1              25                 23
            4  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369      44               4              55                 43
            >>>

            #
            # Example 4: Executing first function on DataFrame created on NON-PTI table.
            #
            >>> # DataFrame on NON-PTI table
            ... ocean_buoys_nonpti = DataFrame("ocean_buoys_nonpti")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys_nonpti.columns
            ['buoyid', 'timecode', 'temperature', 'salinity']
            >>> ocean_buoys_nonpti.head()
                                        buoyid  temperature  salinity
            timecode
            2014-01-06 08:09:59.999999       0         99.0        55
            2014-01-06 08:10:00.000000       0         10.0        55
            2014-01-06 09:01:25.122200       1         70.0        55
            2014-01-06 09:01:25.122200       1         77.0        55
            2014-01-06 09:02:25.122200       1         71.0        55
            2014-01-06 09:03:25.122200       1         72.0        55
            2014-01-06 09:02:25.122200       1         78.0        55
            2014-01-06 08:10:00.000000       0        100.0        55
            2014-01-06 08:08:59.999999       0          NaN        55
            2014-01-06 08:00:00.000000       0         10.0        55

            >>> ocean_buoys_nonpti_grpby1 = ocean_buoys_nonpti.groupby_time(timebucket_duration="2cd",
            ...                                                             value_expression="buoyid",
            ...                                                             timecode_column="timecode", fill="NULLS")
            >>> ocean_buoys_nonpti_grpby1.first().sort(["TIMECODE_RANGE", "buoyid"])
            /mnt/c/Users/pp186043/GitHub_Repos/pyTeradata/teradataml/common/utils.py:398: VantageRuntimeWarning: [Teradata][teradataml](TDML_2086) Following warning raised from Vantage with warning code: 4001
            [Teradata Database] [Warning 4001] Time Series Auxiliary Cache Warning: Multiple results found for one or more Time Series aggregate functions in this query, but only one result was returned. To get all results, resubmit this query with these aggregates isolated.
              VantageRuntimeWarning)
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_DAYS(2))  buoyid  first_salinity  first_temperature
            0  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                        8039       0              55                 10
            1  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                        8039       1              55                 70
            2  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                        8039       2              55                 80
            3  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                        8039      44              55                 43
            >>>

            #
            # Example 5: Execute first() on a few select columns 'temperature' and 'salinity'.
            #
            >>> ocean_buoys_seq_grpby1.first(["temperature", "salinity"]).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_DAYS(2))  buoyid  first_temperature  first_salinity
            0  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369       0                 10              55
            1  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369       1                 70              55
            2  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369       2                 80              55
            3  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369      22                 23              25
            4  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369      44                 43              55
            >>>
        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["columns", columns, True, (str, list), True])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        # Checking each element in passed columns to be valid column in dataframe
        _Validators._validate_column_exists_in_dataframe(columns, self._metaexpr)

        return self._get_dataframe_aggregate(operation='first', columns=columns)

    @collect_queryband(queryband="DF_last")
    def last(self, columns=None):
        """
        DESCRIPTION:
            Returns the newest value, determined by the timecode, for each group. LAST is a single-threaded function.
            In the event of a tie, such as simultaneous timecode values for a particular group, all tied results
            are returned. If a sequence number is present with the data, it can break a tie, assuming it is unique
            across identical timecode values.

            Note:
                1. This function is valid only on columns with numeric types.
                2. Null values are not included in the result computation.

        PARAMETERS:
            columns:
                Optional Argument.
                Specifies a column name or list of column names on which last() operation
                must be run. By default newest value is returned for all the compatible columns
                in a teradataml DataFrame.
                Types: str OR list of Strings (str)

        RETURNS:
            teradataml DataFrame object with last() operation performed.

        RAISES:
            1. TDMLDF_AGGREGATE_FAILED - If last() operation fails to
                return newest value of columns in the teradataml DataFrame.

                Possible error message:
                Unable to perform 'last()' on the teradataml DataFrame.

            2. TDMLDF_AGGREGATE_COMBINED_ERR - If the last() operation
                doesn't support all the columns in the teradataml DataFrame.

                Possible error message:
                No results. Below is/are the error message(s):
                All selected columns [(col2 -  PERIOD_TIME), (col3 -
                BLOB)] is/are unsupported for 'last' operation.

        EXAMPLES :
            >>> # Load the example datasets.
            ... load_example_data("dataframe", ["ocean_buoys", "ocean_buoys_seq", "ocean_buoys_nonpti"])
            >>>

            #
            # Example 1: Executing last function on DataFrame created on non-sequenced PTI table.
            #
            >>> # Create the required DataFrames.
            ... # DataFrame on non-sequenced PTI table
            ... ocean_buoys = DataFrame("ocean_buoys")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys.columns
            ['buoyid', 'TD_TIMECODE', 'temperature', 'salinity']
            >>> ocean_buoys.head()
                                   TD_TIMECODE  temperature  salinity
            buoyid
            0       2014-01-06 08:10:00.000000        100.0        55
            0       2014-01-06 08:08:59.999999          NaN        55
            1       2014-01-06 09:01:25.122200         77.0        55
            1       2014-01-06 09:03:25.122200         79.0        55
            1       2014-01-06 09:01:25.122200         70.0        55
            1       2014-01-06 09:02:25.122200         71.0        55
            1       2014-01-06 09:03:25.122200         72.0        55
            0       2014-01-06 08:09:59.999999         99.0        55
            0       2014-01-06 08:00:00.000000         10.0        55
            0       2014-01-06 08:10:00.000000         10.0        55

            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.last().sort(["TIMECODE_RANGE", "buoyid"])
            /mnt/c/Users/pp186043/GitHub_Repos/pyTeradata/teradataml/common/utils.py:398: VantageRuntimeWarning: [Teradata][teradataml](TDML_2086) Following warning raised from Vantage with warning code: 4001
            [Teradata Database] [Warning 4001] Time Series Auxiliary Cache Warning: Multiple results found for one or more Time Series aggregate functions in this query, but only one result was returned. To get all results, resubmit this query with these aggregates isolated.
              VantageRuntimeWarning)
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid  last_salinity  last_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0             55               100
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1             55                72
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2             55                82
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44             55                43
            >>>

            #
            # Example 2: In Example 1, a VantageRuntimeWarning is raised as:
            #           "[Teradata Database] [Warning 4001] Time Series Auxiliary Cache Warning: Multiple results
            #            found for one or more Time Series aggregate functions in this query, but only one result
            #            was returned. To get all results, resubmit this query with these aggregates isolated."
            #
            #            This warning recommends to execute last() independently on each column, so that we will get
            #            all the results.
            #            To run last() on one single column we can pass column name as input. Let's run last()
            #            on 'temperature' column.
            #
            >>> ocean_buoys_grpby1.last("temperature")
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid  last_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2                82
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0               100
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0                10
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44                43
            4  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1                79
            5  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1                72
            >>>

            #
            # Example 3: Executing last function on ocean_buoys_seq DataFrame created on sequenced PTI table.
            #            Table has few columns incompatible for last() operation 'dates' and 'TD_TIMECODE',
            #            while executing this last() incompatible columns are ignored.
            #
            >>> # DataFrame on sequenced PTI table
            ... ocean_buoys_seq = DataFrame("ocean_buoys_seq")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys_seq.columns
            ['TD_TIMECODE', 'TD_SEQNO', 'buoyid', 'salinity', 'temperature', 'dates']
            >>> ocean_buoys_seq.head()
                                   TD_TIMECODE  TD_SEQNO  salinity  temperature       dates
            buoyid
            0       2014-01-06 08:00:00.000000        26        55         10.0  2016-02-26
            0       2014-01-06 08:08:59.999999        18        55          NaN  2015-06-18
            1       2014-01-06 09:02:25.122200        24        55         78.0  2015-12-24
            1       2014-01-06 09:01:25.122200        23        55         77.0  2015-11-23
            1       2014-01-06 09:02:25.122200        12        55         71.0  2014-12-12
            1       2014-01-06 09:03:25.122200        13        55         72.0  2015-01-13
            1       2014-01-06 09:01:25.122200        11        55         70.0  2014-11-11
            0       2014-01-06 08:10:00.000000        19        55         10.0  2015-07-19
            0       2014-01-06 08:09:59.999999        17        55         99.0  2015-05-17
            0       2014-01-06 08:10:00.000000        27        55        100.0  2016-03-27

            >>> ocean_buoys_seq_grpby1 = ocean_buoys_seq.groupby_time(timebucket_duration="2cy",
            ...                                                       value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_seq_grpby1.last().sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid  last_TD_SEQNO  last_salinity  last_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0             27             55               100
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1             25             55                79
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2             16             55                82
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      22              1             25                23
            4  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44              2             55                43
            >>>

            #
            # Example 4: Executing last function on DataFrame created on NON-PTI table.
            #
            >>> # DataFrame on NON-PTI table
            ... ocean_buoys_nonpti = DataFrame("ocean_buoys_nonpti")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys_nonpti.columns
            ['buoyid', 'timecode', 'temperature', 'salinity']
            >>> ocean_buoys_nonpti.head()
                                        buoyid  temperature  salinity
            timecode
            2014-01-06 08:09:59.999999       0         99.0        55
            2014-01-06 08:10:00.000000       0         10.0        55
            2014-01-06 09:01:25.122200       1         70.0        55
            2014-01-06 09:01:25.122200       1         77.0        55
            2014-01-06 09:02:25.122200       1         71.0        55
            2014-01-06 09:03:25.122200       1         72.0        55
            2014-01-06 09:02:25.122200       1         78.0        55
            2014-01-06 08:10:00.000000       0        100.0        55
            2014-01-06 08:08:59.999999       0          NaN        55
            2014-01-06 08:00:00.000000       0         10.0        55

            >>> ocean_buoys_nonpti_grpby1 = ocean_buoys_nonpti.groupby_time(timebucket_duration="2cy",
            ...                                                             value_expression="buoyid",
            ...                                                             timecode_column="timecode", fill="NULLS")
            >>> ocean_buoys_nonpti_grpby1.last().sort(["TIMECODE_RANGE", "buoyid"])
            /mnt/c/Users/pp186043/GitHub_Repos/pyTeradata/teradataml/common/utils.py:398: VantageRuntimeWarning: [Teradata][teradataml](TDML_2086) Following warning raised from Vantage with warning code: 4001
            [Teradata Database] [Warning 4001] Time Series Auxiliary Cache Warning: Multiple results found for one or more Time Series aggregate functions in this query, but only one result was returned. To get all results, resubmit this query with these aggregates isolated.
              VantageRuntimeWarning)
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_DAYS(2))  buoyid  last_salinity  last_temperature
            0  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                        8039       0             55               100
            1  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                        8039       1             55                79
            2  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                        8039       2             55                82
            3  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                        8039      44             55                43
            >>>

            #
            # Example 5: Executing last() on selected columns 'temperature' and 'salinity'
            #
            >>> ocean_buoys_seq_grpby1 = ocean_buoys_seq.groupby_time("2cy", 'buoyid')
            >>> ocean_buoys_seq_grpby1.last(['temperature', 'salinity'])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid  last_temperature  last_salinity
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2                82             55
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44                43             55
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      22                23             25
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1                79             55
            4  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0               100             55
            >>>
        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["columns", columns, True, (str, list), True])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        # Checking each element in passed columns to be valid column in dataframe
        _Validators._validate_column_exists_in_dataframe(columns, self._metaexpr)

        return self._get_dataframe_aggregate(operation='last', columns=columns)

    @collect_queryband(queryband="DF_mad")
    def mad(self, constant_multiplier_columns=None):
        """
        DESCRIPTION:
            Median Absolute Deviation (MAD) returns the median of the set of values defined as
            the absolute value of the difference between each value and the median of all values
            in each group.
            This is a single-threaded function.

            Formula for computing MAD is as follows:
                MAD = b * Mi(|Xi - Mj(Xj)|)

                Where,
                    b       = Some numeric constant. Default value is 1.4826.
                    Mj(Xj)  = Median of the original set of values.
                    Xi      = The original set of values.
                    Mi      = Median of absolute value of the difference between
                              each value in Xi and the Median calculated in Mj(Xj).

            Note:
                1. This function is valid only on columns with numeric types.
                2. Null values are not included in the result computation.

        PARAMETERS:
            constant_multiplier_columns:
                Optional Argument.
                Specifies a dictionary that accepts numeric values to be used as constant multiplier
                (b in the above formula) as key in the dictionary. Key should be any numeric value
                greater than or equal to 0. Whereas value in the dictionary can be a column name or
                list of column names. Sometimes, value can also include a special character '*',
                instead of column name. This should be used only when one wants to use same constant
                multiplier for all columns.
                Note:
                    For all numeric columns in teradataml DataFrame, that are not specified in this argument,
                    default value of constant_multiplier is used, which is 1.4826.
                Types: Dictionary
                Examples:
                    # Let's assume, a teradataml DataFrame has following columns:
                    #   col1, col2, col3, ..., colN

                    # Use 2 as constant multiplier for column "col1" and default for rest.
                    constant_multiplier_columns = {2: "col1"}

                    # Use 2.485 as constant multiplier for column "col1", 5 for "col3" and default for rest.
                    constant_multiplier_columns = {2.485: "col1", 5: "col3"}

                    # Use 2.485 as constant multiplier for column "col1", "col2" and "col3" and default for rest.
                    constant_multiplier_columns = {2.485: ["col1", "col2", "col3"]}

                    #
                    # Use cases for using '*' default value.
                    #
                    # Use 2.485 as constant multiplier for all columns. In this case, we do not need
                    # to specify all the columns, we can just use '*'.
                    constant_multiplier_columns = {2.485: "*"}

                    # Use 2.485 as constant multiplier for column "col1" and "col3"
                    # and 1.5 for rest of the columns:
                    constant_multiplier_columns = {2.485: ["col1", "col3"], 1.5: "*"}

                    # We can use default value column character ('*') in list as well
                    # Use 2.485 as constant multiplier for column "col1" and "col3"
                    # and 1.5 for "col4" and rest of the columns:
                    constant_multiplier_columns = {2.485: ["col1", "col3"], 1.5: ["col4", "*"]}

        RETURNS:
            teradataml DataFrame

        RAISES:
            TypeError - If incorrect type of values passed to input argument.
            ValueError - If invalid value passed to the the argument.
            TeradataMLException
                1. TDMLDF_AGGREGATE_FAILED - If mad() operation fails to
                    generate the column-wise median absolute deviation in the columns.

        EXAMPLES :
            >>> # Load the example datasets.
            ... load_example_data("dataframe", ["ocean_buoys", "ocean_buoys_seq", "ocean_buoys_nonpti"])
            >>>

            >>> # Create the required DataFrames.
            ... # DataFrame on non-sequenced PTI table
            ... ocean_buoys = DataFrame("ocean_buoys")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys.columns
            ['buoyid', 'TD_TIMECODE', 'temperature', 'salinity']
            >>> ocean_buoys.head()
                                   TD_TIMECODE  temperature  salinity
            buoyid
            0       2014-01-06 08:10:00.000000        100.0        55
            0       2014-01-06 08:08:59.999999          NaN        55
            1       2014-01-06 09:01:25.122200         77.0        55
            1       2014-01-06 09:03:25.122200         79.0        55
            1       2014-01-06 09:01:25.122200         70.0        55
            1       2014-01-06 09:02:25.122200         71.0        55
            1       2014-01-06 09:03:25.122200         72.0        55
            0       2014-01-06 08:09:59.999999         99.0        55
            0       2014-01-06 08:00:00.000000         10.0        55
            0       2014-01-06 08:10:00.000000         10.0        55

            >>> # DataFrame on sequenced PTI table
            ... ocean_buoys_seq = DataFrame("ocean_buoys_seq")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys_seq.columns
            ['TD_TIMECODE', 'TD_SEQNO', 'buoyid', 'salinity', 'temperature', 'dates']
            >>> ocean_buoys_seq.head()
                                   TD_TIMECODE  TD_SEQNO  salinity  temperature       dates
            buoyid
            0       2014-01-06 08:00:00.000000        26        55         10.0  2016-02-26
            0       2014-01-06 08:08:59.999999        18        55          NaN  2015-06-18
            1       2014-01-06 09:02:25.122200        24        55         78.0  2015-12-24
            1       2014-01-06 09:01:25.122200        23        55         77.0  2015-11-23
            1       2014-01-06 09:02:25.122200        12        55         71.0  2014-12-12
            1       2014-01-06 09:03:25.122200        13        55         72.0  2015-01-13
            1       2014-01-06 09:01:25.122200        11        55         70.0  2014-11-11
            0       2014-01-06 08:10:00.000000        19        55         10.0  2015-07-19
            0       2014-01-06 08:09:59.999999        17        55         99.0  2015-05-17
            0       2014-01-06 08:10:00.000000        27        55        100.0  2016-03-27

            >>> # DataFrame on NON-PTI table
            ... ocean_buoys_nonpti = DataFrame("ocean_buoys_nonpti")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys_nonpti.columns
            ['buoyid', 'timecode', 'temperature', 'salinity']
            >>> ocean_buoys_nonpti.head()
                                        buoyid  temperature  salinity
            timecode
            2014-01-06 08:09:59.999999       0         99.0        55
            2014-01-06 08:10:00.000000       0         10.0        55
            2014-01-06 09:01:25.122200       1         70.0        55
            2014-01-06 09:01:25.122200       1         77.0        55
            2014-01-06 09:02:25.122200       1         71.0        55
            2014-01-06 09:03:25.122200       1         72.0        55
            2014-01-06 09:02:25.122200       1         78.0        55
            2014-01-06 08:10:00.000000       0        100.0        55
            2014-01-06 08:08:59.999999       0          NaN        55
            2014-01-06 08:00:00.000000       0         10.0        55

            #
            # Example 1: Calculate Median Absolute Deviation for all columns over 1 calendar day of
            #            timebucket duration. Use default constant multiplier.
            #            No need to pass any arguments.
            #
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="1cd",value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.mad()
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_DAYS(1))  buoyid  mad_salinity  mad_temperature
            0  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         737      44           0.0           0.0000
            1  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         737       0           0.0          65.9757
            2  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         737       2           0.0           1.4826
            3  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         737       1           0.0           5.1891
            >>>

            #
            # Example 2: Calculate MAD values using 2 as constant multiplier for all the columns
            #            in ocean_buoys_seq DataFrame on sequenced PTI table.
            #
            >>> ocean_buoys_seq_grpby1 = ocean_buoys_seq.groupby_time(timebucket_duration="CAL_DAYS(2)", value_expression="buoyid", fill="NULLS")
            >>> constant_multiplier_columns = {2: "*"}
            >>> ocean_buoys_seq_grpby1.mad(constant_multiplier_columns).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_DAYS(2))  buoyid  mad2TD_SEQNO  mad2salinity  mad2temperature
            0  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369       0           4.0           0.0             89.0
            1  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369       1          12.0           0.0              7.0
            2  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369       2           2.0           0.0              2.0
            3  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369      22           0.0           0.0              0.0
            4  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369      44           6.0           0.0              0.0
            >>>

            #
            # Example 3: Calculate MAD values for all the column in teradataml DataFrame created on NON-PTI table.
            #            Use default constant multiplier while calculating MAD value for all columns except
            #            column 'temperature', where 2.485 is used as constant multiplier.
            #
            >>> ocean_buoys_nonpti_grpby1 = ocean_buoys_nonpti.groupby_time(timebucket_duration="2cdays",
            ...                                                             value_expression="buoyid",
            ...                                                             timecode_column="timecode", fill="NULLS")
            >>> constant_multiplier_columns = {2.485: "temperature"}
            >>> ocean_buoys_nonpti_grpby1.mad(constant_multiplier_columns).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_DAYS(2))  buoyid  mad2.485temperature  mad_salinity
            0  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                        8039       0             110.5825           0.0
            1  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                        8039       1               8.6975           0.0
            2  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                        8039       2               2.4850           0.0
            3  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                        8039      44               0.0000           0.0
            >>>

            #
            # Example 4: Calculate MAD values for all the column in teradataml DataFrame created on NON-PTI table.
            #            Use 3 as constant multiplier while calculating MAD value for all columns (buoyid and
            #            salinity), except column 'temperature', where 2 is used as constant multiplier.
            #
            >>> ocean_buoys_grpby3 = ocean_buoys.groupby_time(timebucket_duration="2cday", fill="NULLS")
            >>> constant_multiplier_columns = {2: "temperature", 3:"*"}
            >>> ocean_buoys_grpby3.mad(constant_multiplier_columns).sort(["TIMECODE_RANGE"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_DAYS(2))  mad3buoyid  mad3salinity  mad2temperature
            0  ('2014-01-06 00:00:00.000000-00:00', '2014-01-...                         369         6.0           0.0             27.0
            >>>
        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["constant_multiplier_columns", constant_multiplier_columns, True, (dict)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)

        if constant_multiplier_columns is None:
            constant_multiplier_columns = {}

        return self.__process_time_series_aggregate_with_multi_input_arguments(constant_multiplier_columns, 'mad')

    @collect_queryband(queryband="DF_mode")
    def mode(self):
        """
        DESCRIPTION:
            Returns the column-wise mode of all values in each group. In the event of a tie between two or more
            values from column, a row per result is returned. mode() is a single-threaded function.

            Note:
                1. This function is valid only on columns with numeric types.
                2. Null values are not included in the result computation.

        PARAMETERS:
            None.

        RETURNS:
            teradataml DataFrame object with mode() operation performed.

        RAISES:
            1. TDMLDF_AGGREGATE_FAILED - If mode() operation fails to
                return mode value of columns in the teradataml DataFrame.

                Possible error message:
                Unable to perform 'mode()' on the teradataml DataFrame.

            2. TDMLDF_AGGREGATE_COMBINED_ERR - If the mode() operation
                doesn't support all the columns in the teradataml DataFrame.

                Possible error message:
                No results. Below is/are the error message(s):
                All selected columns [(col2 -  PERIOD_TIME), (col3 -
                BLOB)] is/are unsupported for 'mode' operation.

        EXAMPLES :
            >>> # Load the example datasets.
            ... load_example_data("dataframe", ["ocean_buoys", "ocean_buoys_seq", "ocean_buoys_nonpti"])
            >>>

            #
            # Example 1: Executing mode function on DataFrame created on non-sequenced PTI table.
            #
            >>> # Create the required DataFrames.
            ... # DataFrame on non-sequenced PTI table
            ... ocean_buoys = DataFrame("ocean_buoys")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys.columns
            ['buoyid', 'TD_TIMECODE', 'temperature', 'salinity']
            >>> ocean_buoys.head()
                                   TD_TIMECODE  temperature  salinity
            buoyid
            0       2014-01-06 08:10:00.000000        100.0        55
            0       2014-01-06 08:08:59.999999          NaN        55
            1       2014-01-06 09:01:25.122200         77.0        55
            1       2014-01-06 09:03:25.122200         79.0        55
            1       2014-01-06 09:01:25.122200         70.0        55
            1       2014-01-06 09:02:25.122200         71.0        55
            1       2014-01-06 09:03:25.122200         72.0        55
            0       2014-01-06 08:09:59.999999         99.0        55
            0       2014-01-06 08:00:00.000000         10.0        55
            0       2014-01-06 08:10:00.000000         10.0        55

            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="10m",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.mode().sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(MINUTES(10))  buoyid  mode_temperature  mode_salinity
            0  ('2014-01-06 08:00:00.000000+00:00', '2014-01-...                      106033       0                99             55
            1  ('2014-01-06 08:00:00.000000+00:00', '2014-01-...                      106033       0                10             55
            2  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                      106034       0               100             55
            3  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                      106034       0                10             55
            4  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                      106039       1                79             55
            5  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                      106039       1                70             55
            6  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                      106039       1                72             55
            7  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                      106039       1                78             55
            8  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                      106039       1                71             55
            9  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                      106039       1                77             55

            #
            # Example 2: Executing mode function on ocean_buoys_seq DataFrame created on sequenced PTI table.
            #            Table has few columns incompatible for mode() operation 'dates' and 'TD_TIMECODE',
            #            while executing this mode() incompatible columns are ignored.
            #
            >>> # DataFrame on sequenced PTI table
            ... ocean_buoys_seq = DataFrame("ocean_buoys_seq")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys_seq.columns
            ['TD_TIMECODE', 'TD_SEQNO', 'buoyid', 'salinity', 'temperature', 'dates']
            >>> ocean_buoys_seq.head()
                                   TD_TIMECODE  TD_SEQNO  salinity  temperature       dates
            buoyid
            0       2014-01-06 08:00:00.000000        26        55         10.0  2016-02-26
            0       2014-01-06 08:08:59.999999        18        55          NaN  2015-06-18
            1       2014-01-06 09:02:25.122200        24        55         78.0  2015-12-24
            1       2014-01-06 09:01:25.122200        23        55         77.0  2015-11-23
            1       2014-01-06 09:02:25.122200        12        55         71.0  2014-12-12
            1       2014-01-06 09:03:25.122200        13        55         72.0  2015-01-13
            1       2014-01-06 09:01:25.122200        11        55         70.0  2014-11-11
            0       2014-01-06 08:10:00.000000        19        55         10.0  2015-07-19
            0       2014-01-06 08:09:59.999999        17        55         99.0  2015-05-17
            0       2014-01-06 08:10:00.000000        27        55        100.0  2016-03-27

            >>> ocean_buoys_seq_grpby1 = ocean_buoys_seq.groupby_time(timebucket_duration="MINUTES(10)",
            ...                                                       value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_seq_grpby1.mode().sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(MINUTES(10))  buoyid  mode_TD_SEQNO  mode_salinity  mode_temperature
            0  ('2014-01-06 08:00:00.000000+00:00', '2014-01-...                      106033       0             17             55                10
            1  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                      106034       0             19             55                10
            2  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                      106039       1             11             55                70
            3  ('2014-01-06 10:00:00.000000+00:00', '2014-01-...                      106045      44              7             55                43
            4  ('2014-01-06 10:00:00.000000+00:00', '2014-01-...                      106045      44             21             55                43
            5  ('2014-01-06 10:00:00.000000+00:00', '2014-01-...                      106045      44             20             55                43
            6  ('2014-01-06 10:00:00.000000+00:00', '2014-01-...                      106045      44              4             55                43
            7  ('2014-01-06 10:00:00.000000+00:00', '2014-01-...                      106045      44              9             55                43
            8  ('2014-01-06 10:00:00.000000+00:00', '2014-01-...                      106045      44              8             55                43
            9  ('2014-01-06 10:00:00.000000+00:00', '2014-01-...                      106045      44              5             55                43

            #
            # Example 3: Executing mode function on DataFrame created on NON-PTI table.
            #
            >>> # DataFrame on NON-PTI table
            ... ocean_buoys_nonpti = DataFrame("ocean_buoys_nonpti")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys_nonpti.columns
            ['buoyid', 'timecode', 'temperature', 'salinity']
            >>> ocean_buoys_nonpti.head()
                                        buoyid  temperature  salinity
            timecode
            2014-01-06 08:09:59.999999       0         99.0        55
            2014-01-06 08:10:00.000000       0         10.0        55
            2014-01-06 09:01:25.122200       1         70.0        55
            2014-01-06 09:01:25.122200       1         77.0        55
            2014-01-06 09:02:25.122200       1         71.0        55
            2014-01-06 09:03:25.122200       1         72.0        55
            2014-01-06 09:02:25.122200       1         78.0        55
            2014-01-06 08:10:00.000000       0        100.0        55
            2014-01-06 08:08:59.999999       0          NaN        55
            2014-01-06 08:00:00.000000       0         10.0        55

            >>> ocean_buoys_nonpti_grpby1 = ocean_buoys_nonpti.groupby_time(timebucket_duration="10minutes",
            ...                                                             value_expression="buoyid",
            ...                                                             timecode_column="timecode", fill="NULLS")
            >>> ocean_buoys_nonpti_grpby1.mode().sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(MINUTES(10))  buoyid  mode_temperature  mode_salinity
            0  ('2014-01-06 08:00:00.000000+00:00', '2014-01-...                     2314993       0                99             55
            1  ('2014-01-06 08:00:00.000000+00:00', '2014-01-...                     2314993       0                10             55
            2  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                     2314994       0                10             55
            3  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                     2314994       0               100             55
            4  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                     2314999       1                70             55
            5  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                     2314999       1                71             55
            6  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                     2314999       1                72             55
            7  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                     2314999       1                77             55
            8  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                     2314999       1                78             55
            9  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                     2314999       1                79             55

        """
        return self._get_dataframe_aggregate(operation='mode')

    @collect_queryband(queryband="DF_percentile")
    def percentile(self, percentile, distinct=False, interpolation="LINEAR"):
        """
        DESCRIPTION:
            Function returns the value which represents the desired percentile from each group.
            The result value is determined by the desired index (di) in an ordered list of values.
            The following equation is for the di:
                di = (number of values in group - 1) * percentile/100
            When di is a whole number, that value is the returned result.
            The di can also be between two data points, i and j, where i<j. In that case, the result
            is interpolated according to the value specified in interpolation argument.
            Note:
                1. This function is valid only on columns with numeric types.
                2. Null values are not included in the result computation.

        PARAMETERS:
            percentile:
                Required Argument.
                Specifies the desired percentile value to calculate.
                It should be between 0 and 1, both inclusive.
                Types: int or float

            distinct:
                Optional Argument.
                Specifies whether to exclude duplicate values while calculating
                the percentile value.
                Default Values: False
                Types: bool

            interpolation:
                Optional Argument.
                Specifies the interpolation type to use to interpolate the result value when the
                desired result lies between two data points.
                The desired result lies between two data points, i and j, where i<j. In this case,
                the result is interpolated according to the permitted values.
                Permitted Values: "LINEAR", "LOW", "HIGH", "NEAREST", "MIDPOINT"
                    * LINEAR: Linear interpolation.
                        The result value is computed using the following equation:
                            result = i + (j - i) * (di/100)MOD 1
                        Specify by passing "LINEAR" as string to this parameter.
                    * LOW: Low value interpolation.
                        The result value is equal to i.
                        Specify by passing "LOW" as string to this parameter.
                    * HIGH: High value interpolation.
                        The result value is equal to j.
                        Specify by passing "HIGH" as string to this parameter.
                    * NEAREST: Nearest value interpolation.
                        The result value is i if (di/100 )MOD 1 <= .5; otherwise, it is j.
                        Specify by passing "NEAREST" as string to this parameter.
                    * MIDPOINT: Midpoint interpolation.
                         The result value is equal to (i+j)/2.
                         Specify by passing "MIDPOINT" as string to this parameter.
                Default Values: "LINEAR"
                Types: str

        RETURNS:
            teradataml DataFrame.

        RAISES:
            TypeError - If incorrect type of values passed to input argument.
            ValueError - If invalid value passed to the the argument.
            TeradataMLException - TDMLDF_AGGREGATE_FAILED - If percentile() operation fails to
                                  generate the column-wise percentile values in the columns.

        EXAMPLES:
            >>> # Load the example datasets.
            ... load_example_data("dataframe", ["ocean_buoys", "ocean_buoys_seq", "ocean_buoys_nonpti", "admissions_train"])
            >>>

            #
            # Example 1: Executing percentile() function on DataFrame created on non-sequenced PTI table.
            #            Calculate the 25th percentile value for all numeric columns using default
            #            values, i.e., consider all rows (duplicate rows as well) and linear
            #            interpolation while computing the percentile value.
            #
            >>> # Create the required DataFrames.
            ... # DataFrame on non-sequenced PTI table
            ... ocean_buoys = DataFrame("ocean_buoys")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys.columns
            ['TD_TIMECODE', 'buoyid', 'salinity', 'temperature']
            >>> ocean_buoys.head()
                                   TD_TIMECODE  salinity  temperature
            buoyid
            0       2014-01-06 08:10:00.000000        55        100.0
            0       2014-01-06 08:08:59.999999        55          NaN
            1       2014-01-06 09:01:25.122200        55         77.0
            1       2014-01-06 09:03:25.122200        55         79.0
            1       2014-01-06 09:01:25.122200        55         70.0
            1       2014-01-06 09:02:25.122200        55         71.0
            1       2014-01-06 09:03:25.122200        55         72.0
            0       2014-01-06 08:09:59.999999        55         99.0
            0       2014-01-06 08:00:00.000000        55         10.0
            0       2014-01-06 08:10:00.000000        55         10.0
            >>>
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="10m", value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_grpby1.percentile(0.25).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(MINUTES(10))  buoyid  percentile_salinity  percentile_temperature
            0  ('2014-01-06 08:00:00.000000+00:00', '2014-01-...                      106033       0                 55.0                   32.25
            1  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                      106034       0                 55.0                   32.50
            2  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                      106039       1                 55.0                   71.25
            3  ('2014-01-06 10:00:00.000000+00:00', '2014-01-...                      106045      44                 55.0                   43.00
            4  ('2014-01-06 10:10:00.000000+00:00', '2014-01-...                      106046      44                 55.0                   43.00
            5  ('2014-01-06 10:20:00.000000+00:00', '2014-01-...                      106047      44                  NaN                     NaN
            6  ('2014-01-06 10:30:00.000000+00:00', '2014-01-...                      106048      44                 55.0                   43.00
            7  ('2014-01-06 10:40:00.000000+00:00', '2014-01-...                      106049      44                  NaN                     NaN
            8  ('2014-01-06 10:50:00.000000+00:00', '2014-01-...                      106050      44                 55.0                   43.00
            9  ('2014-01-06 21:00:00.000000+00:00', '2014-01-...                      106111       2                 55.0                   80.50
            >>>

            #
            # Example 2: Executing percentile() function on ocean_buoys_seq DataFrame created on
            #            sequenced PTI table.
            #            Calculate the 50th percentile value for all numeric columns.
            #            To calculate percentile consider all rows (duplicate rows as well) and
            #            use "MIDPOINT" interpolation while computing the percentile value.
            #
            >>> # DataFrame on sequenced PTI table
            ... ocean_buoys_seq = DataFrame("ocean_buoys_seq")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys_seq.columns
            ['TD_TIMECODE', 'TD_SEQNO', 'buoyid', 'salinity', 'temperature', 'dates']
            >>> ocean_buoys_seq.head()
                                   TD_TIMECODE  TD_SEQNO  salinity  temperature       dates
            buoyid
            0       2014-01-06 08:00:00.000000        26        55         10.0  2016-02-26
            0       2014-01-06 08:08:59.999999        18        55          NaN  2015-06-18
            1       2014-01-06 09:02:25.122200        24        55         78.0  2015-12-24
            1       2014-01-06 09:01:25.122200        23        55         77.0  2015-11-23
            1       2014-01-06 09:02:25.122200        12        55         71.0  2014-12-12
            1       2014-01-06 09:03:25.122200        13        55         72.0  2015-01-13
            1       2014-01-06 09:01:25.122200        11        55         70.0  2014-11-11
            0       2014-01-06 08:10:00.000000        19        55         10.0  2015-07-19
            0       2014-01-06 08:09:59.999999        17        55         99.0  2015-05-17
            0       2014-01-06 08:10:00.000000        27        55        100.0  2016-03-27
            >>>
            >>> ocean_buoys_seq_grpby1 = ocean_buoys_seq.groupby_time(timebucket_duration="1cy", value_expression="buoyid", fill="NULLS")
            >>> ocean_buoys_seq_grpby1.percentile(0.5, interpolation="MIDPOINT").sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(1))  buoyid  percentile_TD_SEQNO  percentile_salinity  percentile_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2015-01-...                            3       0                 22.5                 55.0                    54.5
            1  ('2014-01-01 00:00:00.000000-00:00', '2015-01-...                            3       1                 18.0                 55.0                    74.5
            2  ('2014-01-01 00:00:00.000000-00:00', '2015-01-...                            3       2                 15.5                 55.0                    81.5
            3  ('2014-01-01 00:00:00.000000-00:00', '2015-01-...                            3      22                  1.0                 25.0                    23.0
            4  ('2014-01-01 00:00:00.000000-00:00', '2015-01-...                            3      44                  7.5                 55.0                    48.0
            >>>

            #
            # Example 3: Executing percentile() function for all numeric columns in
            #            teradataml DataFrame created on NON-PTI table.
            #            Calculate the 75th percentile value, exclude duplicate rows and
            #            "LOW" as interpolation.
            #
            >>> # DataFrame on NON-PTI table
            ... ocean_buoys_nonpti = DataFrame("ocean_buoys_nonpti")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys_nonpti.columns
            ['timecode', 'buoyid', 'salinity', 'temperature']
            >>> ocean_buoys_nonpti.head()
                                        buoyid  salinity  temperature
            timecode
            2014-01-06 08:09:59.999999       0        55         99.0
            2014-01-06 08:10:00.000000       0        55        100.0
            2014-01-06 09:01:25.122200       1        55         70.0
            2014-01-06 09:01:25.122200       1        55         77.0
            2014-01-06 09:02:25.122200       1        55         71.0
            2014-01-06 09:03:25.122200       1        55         72.0
            2014-01-06 09:02:25.122200       1        55         78.0
            2014-01-06 08:10:00.000000       0        55         10.0
            2014-01-06 08:08:59.999999       0        55          NaN
            2014-01-06 08:00:00.000000       0        55         10.0
            >>>
            >>> ocean_buoys_nonpti_grpby1 = ocean_buoys_nonpti.groupby_time(timebucket_duration="1cy", value_expression="buoyid", timecode_column="timecode", fill="NULLS")
            >>> ocean_buoys_nonpti_grpby1.percentile(0.75, distinct=True, interpolation="low").sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(1))  buoyid  percentile_salinity  percentile_temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2015-01-...                           45       0                 55.0                    99.0
            1  ('2014-01-01 00:00:00.000000-00:00', '2015-01-...                           45       1                 55.0                    77.0
            2  ('2014-01-01 00:00:00.000000-00:00', '2015-01-...                           45       2                 55.0                    81.0
            3  ('2014-01-01 00:00:00.000000-00:00', '2015-01-...                           45      44                 55.0                    55.0
            >>>

            #
            # Example 4: Executing percentile() function on DataFrame created on a regular table.
            #            Calculate the 25th percentile value for all numeric columns using default
            #            values, i.e., consider all rows (duplicate rows as well) and linear
            #            interpolation while computing the percentile value.
            #
            >>> # Create the required DataFrame on non PTI table.
            ... admissions_train = DataFrame("admissions_train")
            >>> # Check DataFrame columns and let's peek at the data
            ... admissions_train.columns
            ['id', 'masters', 'gpa', 'stats', 'programming', 'admitted']
            >>> admissions_train.head()
               masters   gpa     stats programming  admitted
            id
            3       no  3.70    Novice    Beginner         1
            5       no  3.44    Novice      Novice         0
            6      yes  3.50  Beginner    Advanced         1
            7      yes  2.33    Novice      Novice         1
            9       no  3.82  Advanced    Advanced         1
            10      no  3.71  Advanced    Advanced         1
            8       no  3.60  Beginner    Advanced         1
            4      yes  3.50  Beginner      Novice         1
            2      yes  3.76  Beginner    Beginner         0
            1      yes  3.95  Beginner    Beginner         0
            >>> df = admissions_train.groupby("admitted").percentile(0.25)
            >>> df
               admitted  percentile_id  percentile_gpa
            0         0             15          3.4525
            1         1             10          3.5050
            >>>

            #
            # Example 5: Executing percentile() function on DataFrame created on a regular table.
            #            Calculate the 35th percentile value for all numeric columns using default
            #            values, i.e., consider all rows (duplicate rows as well) and no
            #            interpolation while computing the percentile value.
            #
            >>> # Create the required DataFrame on non PTI table.
            ... admissions_train = DataFrame("admissions_train")
            >>> # Check DataFrame columns and let's peek at the data
            ... admissions_train.columns
            ['id', 'masters', 'gpa', 'stats', 'programming', 'admitted']
            >>> admissions_train.head()
               masters   gpa     stats programming  admitted
            id
            3       no  3.70    Novice    Beginner         1
            5       no  3.44    Novice      Novice         0
            6      yes  3.50  Beginner    Advanced         1
            7      yes  2.33    Novice      Novice         1
            9       no  3.82  Advanced    Advanced         1
            10      no  3.71  Advanced    Advanced         1
            8       no  3.60  Beginner    Advanced         1
            4      yes  3.50  Beginner      Novice         1
            2      yes  3.76  Beginner    Beginner         0
            1      yes  3.95  Beginner    Beginner         0
            >>> df = admissions_train.groupby("admitted").percentile(0.25, interpolation=None)
            >>> df
               admitted  percentile_id  percentile_gpa
            0         0             19            3.46
            1         1             13            3.57
            >>>
        """
        # Argument validations
        awu_matrix = []
        awu_matrix.append(["percentile", percentile, False, (int, float)])
        awu_matrix.append(["distinct", distinct, True, (bool)])
        awu_matrix.append(["interpolation", interpolation, True, (str), True,
                           ["LINEAR", "LOW", "HIGH", "NEAREST", "MIDPOINT"]])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)
        _Validators._validate_missing_required_arguments(awu_matrix)

        if distinct is None:
            distinct = False

        if interpolation is None:
            interpolation = "LINEAR"

        _Validators._validate_argument_range(
            percentile, "percentile", lbound=0, ubound=1, lbound_inclusive=True, ubound_inclusive=True)

        return self._get_dataframe_aggregate(operation='percentile', percentile=percentile,
                                             distinct=distinct, interpolation=interpolation)

    @collect_queryband(queryband="DF_top")
    def top(self, number_of_values_to_column, with_ties=False):
        """
        DESCRIPTION:
            Returns the largest number of values in the columns for each group, with or without ties.
            TOP is a single-threaded function.

            Note:
                1. This function is valid only on columns with numeric types.
                2. Null values are not included in the result computation.

        PARAMETERS:
            number_of_values_to_column:
                Required Argument.
                Specifies a dictionary that accepts number of values to be selected for each column.
                Number of values is a key in the dictionary. Key should be any positive integer.
                Whereas value in the dictionary can be a column name or list of column names.
                Sometimes, value can also include a special character '*', instead of column name.
                This should be used only when one wants to return same number of values for all columns.
                Types: Dictionary
                Examples:
                    # Let's assume, a teradataml DataFrame has following columns:
                    #   col1, col2, col3, ..., colN

                    # For top() to return 2 values for column "col1":
                    number_of_values_to_column = {2: "col1"}

                    # For top() to return 2 values for column "col1" and 5 values for "col3":
                    number_of_values_to_column = {2: "col1", 5: "col3"}

                    # For top() to return 2 values for column "col1", "col2" and "col3":
                    number_of_values_to_column = {2: ["col1", "col2", "col3"]}

                    # Use cases for using '*' default value.
                    # For top() to return 2 values for all columns. In case, we need to return 2 values
                    # for each column in the DataFrame, then one can use '*'.
                    number_of_values_to_column = {2: "*"}

                    # For top() to return 2 values for column "col1" and "col3"
                    # and 5 values for rest of the columns:
                    number_of_values_to_column = {2: ["col1", "col3"], 5: "*"}

                    # We can use default value column character ('*') in list as well
                    # For top() to return 2 values for column "col1" and "col3"
                    # and 5 values for "col4" and rest of the columns:
                    number_of_values_to_column = {2: ["col1", "col3"], 5: ["col4", "*"]}

            with_ties:
                Optional Argument.
                Specifies a flag to decide whether to run top function with ties or not.
                TOP WITH TIES implies that the rows returned include the specified number of rows in
                the ordered set for each timebucket. It includes any rows where the sort key value
                is the same as the sort key value in the last row that satisfies the specified number
                or percentage of rows. If this clause is omitted and ties are found, the earliest
                value in terms of timecode is returned.
                Types: bool

        RETURNS:
            teradataml DataFrame

        RAISES:
            TypeError - If incorrect type of values passed to input argument.
            ValueError - If invalid value passed to the the argument.
            TeradataMLException
                1. If required argument 'number_of_values_to_column' is missing or None is passed.
                2. TDMLDF_AGGREGATE_FAILED - If top() operation fails to
                    generate the column-wise largest number of values in the columns.

        EXAMPLES :
            >>> # Load the example datasets.
            ... load_example_data("dataframe", ["ocean_buoys", "ocean_buoys_seq", "ocean_buoys_nonpti"])
            >>>

            >>> # Create the required DataFrames.
            ... # DataFrame on non-sequenced PTI table
            ... ocean_buoys = DataFrame("ocean_buoys")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys.columns
            ['buoyid', 'TD_TIMECODE', 'temperature', 'salinity']
            >>> ocean_buoys.head()
                                   TD_TIMECODE  temperature  salinity
            buoyid
            0       2014-01-06 08:10:00.000000        100.0        55
            0       2014-01-06 08:08:59.999999          NaN        55
            1       2014-01-06 09:01:25.122200         77.0        55
            1       2014-01-06 09:03:25.122200         79.0        55
            1       2014-01-06 09:01:25.122200         70.0        55
            1       2014-01-06 09:02:25.122200         71.0        55
            1       2014-01-06 09:03:25.122200         72.0        55
            0       2014-01-06 08:09:59.999999         99.0        55
            0       2014-01-06 08:00:00.000000         10.0        55
            0       2014-01-06 08:10:00.000000         10.0        55

            >>> # DataFrame on sequenced PTI table
            ... ocean_buoys_seq = DataFrame("ocean_buoys_seq")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys_seq.columns
            ['TD_TIMECODE', 'TD_SEQNO', 'buoyid', 'salinity', 'temperature', 'dates']
            >>> ocean_buoys_seq.head()
                                   TD_TIMECODE  TD_SEQNO  salinity  temperature       dates
            buoyid
            0       2014-01-06 08:00:00.000000        26        55         10.0  2016-02-26
            0       2014-01-06 08:08:59.999999        18        55          NaN  2015-06-18
            1       2014-01-06 09:02:25.122200        24        55         78.0  2015-12-24
            1       2014-01-06 09:01:25.122200        23        55         77.0  2015-11-23
            1       2014-01-06 09:02:25.122200        12        55         71.0  2014-12-12
            1       2014-01-06 09:03:25.122200        13        55         72.0  2015-01-13
            1       2014-01-06 09:01:25.122200        11        55         70.0  2014-11-11
            0       2014-01-06 08:10:00.000000        19        55         10.0  2015-07-19
            0       2014-01-06 08:09:59.999999        17        55         99.0  2015-05-17
            0       2014-01-06 08:10:00.000000        27        55        100.0  2016-03-27

            >>> # DataFrame on NON-PTI table
            ... ocean_buoys_nonpti = DataFrame("ocean_buoys_nonpti")
            >>> # Check DataFrame columns and let's peek at the data
            ... ocean_buoys_nonpti.columns
            ['buoyid', 'timecode', 'temperature', 'salinity']
            >>> ocean_buoys_nonpti.head()
                                        buoyid  temperature  salinity
            timecode
            2014-01-06 08:09:59.999999       0         99.0        55
            2014-01-06 08:10:00.000000       0         10.0        55
            2014-01-06 09:01:25.122200       1         70.0        55
            2014-01-06 09:01:25.122200       1         77.0        55
            2014-01-06 09:02:25.122200       1         71.0        55
            2014-01-06 09:03:25.122200       1         72.0        55
            2014-01-06 09:02:25.122200       1         78.0        55
            2014-01-06 08:10:00.000000       0        100.0        55
            2014-01-06 08:08:59.999999       0          NaN        55
            2014-01-06 08:00:00.000000       0         10.0        55

            ### Examples for top without ties ###
            #
            # Example 1: Executing top function on DataFrame created on non-sequenced PTI table.
            #
            >>> ocean_buoys_grpby1 = ocean_buoys.groupby_time(timebucket_duration="2cy",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> number_of_values_to_column = {2: "temperature"}
            >>> ocean_buoys_grpby1.top(number_of_values_to_column).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid  top2temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0              100
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0               99
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1               78
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1               79
            4  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2               82
            5  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2               81
            6  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44               55
            7  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44               56
            >>>

            #
            # Example 2: Executing top to select 2 values for all the columns in ocean_buoys_seq DataFrame
            #            on sequenced PTI table.
            #
            >>> ocean_buoys_seq_grpby1 = ocean_buoys_seq.groupby_time(timebucket_duration="CAL_YEARS(2)",
            ...                                                       value_expression="buoyid", fill="NULLS")
            >>> number_of_values_to_column = {2: "*"}
            >>> ocean_buoys_seq_grpby1.top(number_of_values_to_column).sort(["TIMECODE_RANGE", "buoyid"])
                                                    TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid  top2TD_SEQNO  top2salinity  top2temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       0            26            55               99
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       1            24            55               78
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2       2            15            55               81
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      22             1            25               23
            4  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                            2      44            21            55               55

            #
            # Example 3: Executing top function on DataFrame created on NON-PTI table.
            #
            >>> ocean_buoys_nonpti_grpby1 = ocean_buoys_nonpti.groupby_time(timebucket_duration="2cyear",
            ...                                                             value_expression="buoyid",
            ...                                                             timecode_column="timecode", fill="NULLS")
            >>> number_of_values_to_column = {2: "temperature"}
            >>> ocean_buoys_nonpti_grpby1.top(number_of_values_to_column).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(CAL_YEARS(2))  buoyid  top2temperature
            0  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                           23       0               99
            1  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                           23       0              100
            2  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                           23       1               79
            3  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                           23       1               78
            4  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                           23       2               81
            5  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                           23       2               82
            6  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                           23      44               56
            7  ('2014-01-01 00:00:00.000000-00:00', '2016-01-...                           23      44               55


            ### Examples for top with ties ###
            #
            # Example 4: Executing top with ties function on DataFrame created on non-sequenced PTI table.
            #
            >>> ocean_buoys_grpby2 = ocean_buoys.groupby_time(timebucket_duration="2m",
            ...                                               value_expression="buoyid", fill="NULLS")
            >>> number_of_values_to_column = {2: "temperature"}
            >>> ocean_buoys_grpby2.top(number_of_values_to_column, with_ties=True).sort(["TIMECODE_RANGE", "buoyid"])
                                                    TIMECODE_RANGE  GROUP BY TIME(MINUTES(2))  buoyid  top_with_ties2temperature
            0  ('2014-01-06 08:00:00.000000+00:00', '2014-01-...                     530161       0                       10.0
            1  ('2014-01-06 08:02:00.000000+00:00', '2014-01-...                     530162       0                        NaN
            2  ('2014-01-06 08:04:00.000000+00:00', '2014-01-...                     530163       0                        NaN
            3  ('2014-01-06 08:06:00.000000+00:00', '2014-01-...                     530164       0                        NaN
            4  ('2014-01-06 08:08:00.000000+00:00', '2014-01-...                     530165       0                       99.0
            5  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                     530166       0                      100.0
            6  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                     530166       0                       10.0
            7  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                     530191       1                       70.0
            8  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                     530191       1                       77.0
            9  ('2014-01-06 09:02:00.000000+00:00', '2014-01-...                     530192       1                       78.0

            #
            # Example 5: Executing top with ties to select 2 values for temperature and 3 for rest of the columns in
            #            ocean_buoys DataFrame.
            #
            >>> ocean_buoys_grpby3 = ocean_buoys.groupby_time(timebucket_duration="MINUTES(2)", fill="NULLS")
            >>> number_of_values_to_column = {2: "temperature", 3:"*"}
            >>> ocean_buoys_grpby3.top(number_of_values_to_column, with_ties=True).sort(["TIMECODE_RANGE"])
                                                  TIMECODE_RANGE  GROUP BY TIME(MINUTES(2))  top_with_ties3buoyid  top_with_ties3salinity  top_with_ties2temperature
            0  ('2014-01-06 08:00:00.000000+00:00', '2014-01-...                     530161                   0.0                    55.0                       10.0
            1  ('2014-01-06 08:02:00.000000+00:00', '2014-01-...                     530162                   NaN                     NaN                        NaN
            2  ('2014-01-06 08:04:00.000000+00:00', '2014-01-...                     530163                   NaN                     NaN                        NaN
            3  ('2014-01-06 08:06:00.000000+00:00', '2014-01-...                     530164                   NaN                     NaN                        NaN
            4  ('2014-01-06 08:08:00.000000+00:00', '2014-01-...                     530165                   0.0                    55.0                       99.0
            5  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                     530166                   0.0                    55.0                       10.0
            6  ('2014-01-06 08:12:00.000000+00:00', '2014-01-...                     530167                   NaN                     NaN                        NaN
            7  ('2014-01-06 08:14:00.000000+00:00', '2014-01-...                     530168                   NaN                     NaN                        NaN
            8  ('2014-01-06 08:16:00.000000+00:00', '2014-01-...                     530169                   NaN                     NaN                        NaN
            9  ('2014-01-06 08:18:00.000000+00:00', '2014-01-...                     530170                   NaN                     NaN                        NaN
            >>>

            #
            # Example 6: Executing top with ties function on DataFrame created on NON-PTI table.
            #
            >>> ocean_buoys_nonpti_grpby2 = ocean_buoys_nonpti.groupby_time(timebucket_duration="2mins",
            ...                                                             value_expression="buoyid",
            ...                                                             timecode_column="timecode", fill="NULLS")
            >>> number_of_values_to_column = {2: "temperature"}
            >>> ocean_buoys_nonpti_grpby2.top(number_of_values_to_column, with_ties=True).sort(["TIMECODE_RANGE", "buoyid"])
                                                  TIMECODE_RANGE  GROUP BY TIME(MINUTES(2))  buoyid  top_with_ties2temperature
            0  ('2014-01-06 08:00:00.000000+00:00', '2014-01-...                   11574961       0                       10.0
            1  ('2014-01-06 08:02:00.000000+00:00', '2014-01-...                   11574962       0                        NaN
            2  ('2014-01-06 08:04:00.000000+00:00', '2014-01-...                   11574963       0                        NaN
            3  ('2014-01-06 08:06:00.000000+00:00', '2014-01-...                   11574964       0                        NaN
            4  ('2014-01-06 08:08:00.000000+00:00', '2014-01-...                   11574965       0                       99.0
            5  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                   11574966       0                      100.0
            6  ('2014-01-06 08:10:00.000000+00:00', '2014-01-...                   11574966       0                       10.0
            7  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                   11574991       1                       70.0
            8  ('2014-01-06 09:00:00.000000+00:00', '2014-01-...                   11574991       1                       77.0
            9  ('2014-01-06 09:02:00.000000+00:00', '2014-01-...                   11574992       1                       79.0
            >>>
            >>>
        """
        # Set the operation
        operation = "top"
        if with_ties:
            operation = "top with ties"

        # Argument validations
        awu_matrix = []
        awu_matrix.append(["number_of_values_to_column", number_of_values_to_column, False, (dict)])
        awu_matrix.append(["with_ties", with_ties, True, (bool)])

        # Validate argument types
        _Validators._validate_function_arguments(awu_matrix)
        _Validators._validate_missing_required_arguments(awu_matrix)

        # Check if number_of_values_to_column dict is empty or not.
        if not number_of_values_to_column:
            raise ValueError(Messages.get_message(MessageCodes.ARG_EMPTY, "number_of_values_to_column"))

        return self.__process_time_series_aggregate_with_multi_input_arguments(number_of_values_to_column, operation)

    def __process_time_series_aggregate_with_multi_input_arguments(self, number_of_values_to_column, operation):
        """
        Internal function to process bottom(), mad() and top() time series aggregate functions.

        PARAMETERS:
            number_of_values_to_column:
                Required Argument.
                Specifies a dictionary that accepts number of values to be selected for each column.
                Number of values is a key in the dictionary. Key should be any positive integer.
                Whereas value in the dictionary can be a column name or list of column names.
                Sometimes, value can also include a special character '*', instead of column name.
                This should be used only when one wants to return same number of values for all columns.
                Types: Dictionary
                Examples:
                    # Let's assume, a teradataml DataFrame has following columns:
                    #   col1, col2, col3, ..., colN

                    # For top() to return 2 values for column "col1":
                    number_of_values_to_column = {2: "col1"}

                    # For top() to return 2 values for column "col1" and 5 values for "col3":
                    number_of_values_to_column = {2: "col1", 5: "col3"}

                    # For top() to return 2 values for column "col1", "col2" and "col3":
                    number_of_values_to_column = {2: ["col1", "col2", "col3"]}

                    # Use cases for using '*' default value.
                    # For top() to return 2 values for all columns. In case, we need to return 2 values
                    # for each column in the DataFrame, then one can use '*'.
                    number_of_values_to_column = {2: "*"}

                    # For top() to return 2 values for column "col1" and "col3"
                    # and 5 values for rest of the columns:
                    number_of_values_to_column = {2: ["col1", "col3"], 5: "*"}

                    # We can use default value column character ('*') in list as well
                    # For top() to return 2 values for column "col1" and "col3"
                    # and 5 values for "col4" and rest of the columns:
                    number_of_values_to_column = {2: ["col1", "col3"], 5: ["col4", "*"]}

            with_ties:
                Required Argument.
                Specifies a flag to decide whether to run top function with ties or not.
                TOP WITH TIES implies that the rows returned include the specified number of rows in
                the ordered set for each timebucket. It includes any rows where the sort key value
                is the same as the sort key value in the last row that satisfies the specified number
                or percentage of rows. If this clause is omitted and ties are found, the earliest
                value in terms of timecode is returned.
                Types: bool

            operation:
                Required Argument.
                Specifies the bottom or top function to be run.
                Types: str

        RETURNS:
            teradataml DataFrame

        RAISES:
            TypeError - If incorrect type of values passed to input argument.
            ValueError - If invalid value passed to the the argument.
            TeradataMLException
                1. If required argument 'number_of_values_to_column' is missing or None is passed.
                2. TDMLDF_AGGREGATE_FAILED - If top() operation fails to
                    generate the column-wise smallest(bottom)/largest(top) number of values in the columns.

        EXAMPLES :
            self.__process_time_series_aggregate_with_multi_input_arguments(number_of_values_to_column, with_ties, operation)
        """
        default_constant_for_columns = []
        argument_name = "number_of_values_to_column"
        key_types = (int)
        if operation == 'mad':
            argument_name = "constant_multiplier_columns"
            key_types = (int, float)

        # Columns explicitly asked by user to process.
        columns_processed = []

        # Default value, if any, provided by used using { 5: '*' }. Here it means, 5 is num val and that will
        # be appicable for all columns. Sometimes user can specify 'x' number of values for a specific column and
        # 'y' number of values for all other columns.
        # Example:
        #    { x: ['col1'], y: '*' } - x and y are positive integers.
        # Following are couple of variables to play with this.
        default_num_val = None
        apply_default_num_val_to_rest = False

        # Dictionary to hold column:num_value pair for further processing.
        colname_to_numvalues = {}

        # Validations for key and value in number_of_values_to_column dictionary.
        # And processing the  input argument as well.
        for num_val in number_of_values_to_column:
            # Validate each key in 'argument_name' is a of correct type.
            _Validators._validate_function_arguments([["{} keys".format(argument_name), num_val, False, key_types]])

            if operation == 'mad':
                if num_val < 0:
                    raise ValueError(Messages.get_message(
                        MessageCodes.INVALID_ARG_VALUE).format(num_val, "key: {} in {}".format(num_val, argument_name),
                                                               'greater than or equal to 0'))
            else:
                # Validate each key, i.e., requested number of values in number_of_values_to_column dictionary
                # is a positive integer.
                _Validators._validate_positive_int(num_val, "key: {} in {}".format(num_val, argument_name))

            # If number values provided is applicable just single column, which is provided as string,
            # then let's convert it to a list.
            if isinstance(number_of_values_to_column[num_val], str):
                number_of_values_to_column[num_val] = [number_of_values_to_column[num_val]]

            # Process each column in the list to provide number of values 'num_val'
            for column in number_of_values_to_column[num_val]:
                if apply_default_num_val_to_rest and column.strip() == "*":
                    # Raise error for duplicate entry for applying num val to multiple columns.
                    raise ValueError(Messages.get_message(
                        MessageCodes.INVALID_ARG_VALUE).format("several \"*\"", argument_name,
                                                               'used only once'))

                # Check whether user has asked for any default value.
                if column.strip() == "*":
                    apply_default_num_val_to_rest = True
                    default_num_val = num_val
                    continue

                colname_to_numvalues[column] = num_val
                columns_processed.append(column)

        self.__validate_time_series_aggr_columns(operation=operation, columns=columns_processed)

        # Now that we have already processed all the columns explicitly mentioned by the user.
        # Let's apply default value to remaining columns, if user has specified the same.
        if apply_default_num_val_to_rest:
            remaining_columns = list(set(self.columns) - set(columns_processed))
            unsupported_types = _Dtypes._get_unsupported_data_types_for_aggregate_operations(operation)
            for column in remaining_columns:
                if not isinstance(self._td_column_names_and_sqlalchemy_types[column.lower()], tuple(unsupported_types)):
                    # We should not involve columns used in value expression of GROUP BY TIME clause as well.
                    if column not in self._value_expression:
                        colname_to_numvalues[column] = default_num_val
        else:
            # Processing columns for default value 1.4826 with MAD function.
            # Here we shall process columns, that are not explicitly specified by user either by name or '*'.
            if operation == 'mad':
                # We shall process, if and only if, user has not used '*' while passing value to
                # 'constant_multiplier_columns' argument.
                remaining_columns = list(set(self.columns) - set(columns_processed))
                unsupported_types = _Dtypes._get_unsupported_data_types_for_aggregate_operations(operation)
                for column in remaining_columns:
                    if not isinstance(self._td_column_names_and_sqlalchemy_types[column.lower()],
                                      tuple(unsupported_types)):
                        # We should not involve columns used in value expression of GROUP BY TIME clause as well.
                        if column not in self._value_expression:
                            default_constant_for_columns.append(column)

        return self._get_dataframe_aggregate(operation=operation, colname_to_numvalues=colname_to_numvalues,
                                             default_constant_for_columns=default_constant_for_columns)

    def __validate_time_series_aggr_columns(self, operation, columns):
        """
        Function to validate columns involved in time series aggregate. Columns are validated for:
            1. Column exists or not in the input teradataml DataFrame.
            2. Column has supported types for aggregate operation or not.

        PARAMETERS:
            operation:
                Required Argument.
                Aggregate operation being performed.
                Types: str

            columns:
                Required Argument.
                Column name or list of column names to be validated.
                Types: str or List of Strings

        RAISES:
            ValueError - If column does not exist in the teradataml DataFrame.
            TeradataMlException - If column has unsupported type for the aggregate operation.

        RETURNS:
            None.

        EXAMPLES:
            columns_processed = ["col1", "col2", "col3"]
            self.__validate_time_series_aggr_columns(operation="bottom", columns=columns_processed)
        """
        # Validate each column name, i.e., values in the number_of_values_to_column dictionary, is a
        # valid column in teradataml DataFrame
        _Validators._validate_column_exists_in_dataframe(columns, self._metaexpr)

        # Check if the user provided columns has unsupported datatype for aggregate operation or not.
        _Validators._validate_aggr_operation_unsupported_datatype(operation, columns,
                                                                  self._td_column_names_and_sqlalchemy_types)


class MetaData():
    """
    This class contains the column names and types for a dataframe.
    This class is used for printing DataFrame.dtypes

    """

    def __init__(self, column_names_and_types):
        """
        Constructor for TerdataML MetaData.

        PARAMETERS:
            column_names_and_types - List containing column names and Python types.

        EXAMPLES:
            meta = MetaData([('col1', 'int'),('col2', 'str')])

        RAISES:

        """
        self._column_names_and_types = column_names_and_types

    def __repr__(self):
        """
        This is the __repr__ function for MetaData.
        Returns a string containing column names and Python types.

        PARAMETERS:

        EXAMPLES:
            meta = MetaData([('col1', 'int'),('col2', 'str')])
            print(meta)

        RAISES:

        """
        if self._column_names_and_types is not None:
            return df_utils._get_pprint_dtypes(self._column_names_and_types)
        else:
            return ""

    def _repr_html_(self):
        """ Print method for Metaexpr for iPython rich display. """
        columns_types_html = """<style type="text/css">
                                table {{border:ridge 5px;}}
                                table td {{border:inset 1px;}}
                                table tr#HeaderRow {{background-color:grey; color:white;}}
                               </style>
                               <html>
                                  <table>
                                    <tr id="HeaderRow">
                                        <th>COLUMN NAME</th>
                                        <th>TYPE</th>
                                    </tr>
                                    {}
                                  </table>
                               </html>
                            """
        rows_html = ["<tr><td>{}</td><td>{}</td></tr>".format(
            c[0], c[1]) for c in self._column_names_and_types]
        return columns_types_html.format(chr(10).join(rows_html))


class _TDUAF(DataFrame):
    """
    Base class for TDSeries and TDMatrix.
    Initializes common parameters and performs validations.
    """

    def __init__(self, data, id=None, row_index=None, row_index_style="TIMECODE", id_sequence=None,
                 payload_field=None, payload_content=None, layer=None):
        """
        DESCRIPTION:
            Constructor for _TDUAF.

        PARAMETERS:
            data:
                Required Argument.
                Specifies the teradataml Dataframe.
                Types: teradataml DataFrame

            id:
                Required when teradataml DataFrame is created on non ART table.
                Specifies the name of the column in "data" containing the
                identifier values.
                Types: str

            row_index:
                Required when teradataml DataFrame is created on non ART.
                Specifies the name of the column in "data" containing the
                row indexing values.
                Types: str

            row_index_style:
                Optional Argument.
                Specifies the style of row indexing.
                Default Value: "TIMECODE"
                Permitted Values: "TIMECODE", "SEQUENCE"
                Types: str

            id_sequence:
                Optional Argument.
                Specifies a sequence of series to plot.
                Types: str or list of str

            payload_field:
                Optional Argument.
                Specifies the names of the fields for payload.
                Types: str or list of str

            payload_content:
                Optional Argument.
                Specifies the payload content type.
                Permitted Values: "REAL", "COMPLEX", "AMPL_PHASE",
                                  "AMPL_PHASE_RADIANS", "AMPL_PHASE_DEGREES",
                                  "MULTIVAR_REAL", "MULTIVAR_COMPLEX",
                                  "MULTIVAR_ANYTYPE", "MULTIVAR_AMPL_PHASE",
                                  "MULTIVAR_AMPL_PHASE_RADIANS ",
                                  "MULTIVAR_AMPL_PHASE_DEGREES"
                Types: str

            layer:
                Optional Argument.
                Specifies the layer name of the ART table, if dataframe is
                created on ART.
                Types: str

        RAISES:
            None

        RETURNS:
            None

        EXAMPLES:
            _TDUAF(data = data, id='admitted', row_index = 'id', row_index_style="TIMECODE")
        """
        self._data = data
        self._id = id
        self._row_index = row_index
        self._row_index_style = row_index_style
        self._id_sequence = id_sequence
        self._payload_field = payload_field
        self._payload_content = payload_content
        self._layer = layer

        self._parameterised_sql = None
        self._non_parameterised_sql = None
        self._spec_header = ""

        # Variable to hold the additional clauses which are specific to Child classes.
        # This variable should be populated by the corresponding child classes which inherits this class.
        self._additional_spec = {}
        self._db_utils = DataFrameUtils()

        # Variable to perform argument validation.
        self._awu_matrix = []
        self._awu_matrix.append(["data", self._data, False, (DataFrame)])
        # Validate the data parameter in order to set the super class(DataFrame) parameters.
        _Validators._validate_missing_required_arguments(self._awu_matrix)
        _Validators._validate_function_arguments(self._awu_matrix)

        # Initialize DataFrame related parameters.
        super(_TDUAF, self).__init__()
        self._table_name = self._data._table_name
        self._nodeid = self._data._nodeid
        self._get_metadata_from_metaexpr(self._data._metaexpr)

        self._awu_matrix = []
        self._awu_matrix.append(["id", self._id, not self._id_row_index_required(), (str, list), True])
        self._awu_matrix.append(["row_index", self._row_index, not self._id_row_index_required(), (str), True])
        self._awu_matrix.append(["row_index_style", self._row_index_style, True, (str), True, ["TIMECODE", "SEQUENCE"]])
        self._awu_matrix.append(["id_sequence", self._id_sequence, True, (str, list), True])
        self._awu_matrix.append(["payload_field", self._payload_field, not self._is_payload_required(), (str, list), True])
        self._awu_matrix.append(["payload_content", self._payload_content, not self._is_payload_required(), str, True,
                                 ["REAL", "COMPLEX", "AMPL_PHASE", "AMPL_PHASE_RADIANS", "AMPL_PHASE_DEGREES",
                                  "MULTIVAR_REAL", "MULTIVAR_COMPLEX", "MULTIVAR_ANYTYPE", "MULTIVAR_AMPL_PHASE",
                                  "MULTIVAR_AMPL_PHASE_RADIANS", "MULTIVAR_AMPL_PHASE_DEGREES"]])
        self._awu_matrix.append(["layer", self._layer, True, str, True])

        # store the columns to check against the DataFrame.
        # Key represents the name of the variable and value represents the value of it.
        # The corresponding child methods should implement it.
        self._cols_to_check = {}

    def _validate(self):
        """
        DESCRIPTION:
            Function to validate the input parameters.

        RETURNS:
            bool

        RAISES:
            None

        EXAMPLES:
            _TDUAF(data)._valdiate()
        """
        # Validate missing arguments.
        _Validators._validate_missing_required_arguments(self._awu_matrix)

        # Validate argument types.
        _Validators._validate_function_arguments(self._awu_matrix)

        # Validation to check if values passed in id and row_index are present as columns in data.
        for col_arg, col_value in self._cols_to_check.items():
            _Validators._validate_dataframe_has_argument_columns(col_value, col_arg, self._data, "data")

    def _generate_spec(self):
        """
        DESCRIPTION:
            Internal function to generate the SPEC for UAF. This function generate the spec for
            both parameterised SQL and non parameterised SQL.

        PARAMETERS:
            None

        RETURNS:
            tuple OR str

        RAISES:
            None

        Examples:
            self._generate_spec()
        """
        sql_clauses, sql_values = [], []

        # Extract table name. In some cases, if AED node is not executed,
        # then table name would be a None. In such cases, execute the AED node
        # and extract table name.
        table_name = self._data._table_name
        if table_name is None:
            table_name = self._db_utils._execute_node_return_db_object_name(self._data._nodeid, self._data._metaexpr)

        # UAF Functions do not accept double quotes.
        tdp = preparer(td_dialect)
        db_name = UtilFuncs._extract_db_name(table_name)
        datalake_name = UtilFuncs._extract_datalake_name(table_name)
        if datalake_name:
            table_name = '{}.{}.{}'.format(tdp.quote(datalake_name),
                                           tdp.quote(db_name),
                                           tdp.quote(UtilFuncs._extract_table_name(table_name)))
        elif db_name:
            table_name = '{}.{}'.format(tdp.quote(db_name),
                                        tdp.quote(UtilFuncs._extract_table_name(table_name)))
        else:
            table_name = tdp.quote(UtilFuncs._extract_table_name(table_name))

        sql_clauses.append("TABLE_NAME ({})")
        sql_values.append(table_name)

        if self._row_index:
            sql_clauses.append("ROW_AXIS ({}({{}}))".format(self._row_index_style))
            sql_values.append(self._row_index)

        if self._id_sequence:
            sql_clauses.append("ID_SEQUENCE ({})")
            sql_values.append(", ".join(UtilFuncs._as_list(self._id_sequence)))

        if self._payload_field:
            sql_clauses.append("PAYLOAD (FIELDS ({}), CONTENT ({}))")
            sql_values.append(", ".join(UtilFuncs._as_list(self._payload_field)))
            sql_values.append(self._payload_content)

        if self._layer:
            sql_clauses.append("LAYER ({})")
            sql_values.append(self._layer)

        # Generate the corresponding lists for components which are specific
        # to individual child classes.
        for sql_clause, sql_value in self._additional_spec.items():
            sql_clauses.append(sql_clause)
            sql_values.append(sql_value)

        # Declare a function to return a generator object. Note that, this should be a function because,
        # generator object exhausts after the first usage. So, if it is a regular variable,
        # _non_parameterised_sql will not have any data to consume from generator object.
        get_sql_clauses = lambda: ("{}{}".format(" " * 4, c) for c in sql_clauses)

        self._parameterised_sql = self._spec_header.format(
            "\n" + ", \n".join(get_sql_clauses()).format(*(["?"]*len(sql_values)))), sql_values
        self._non_parameterised_sql = self._spec_header.format("\n" + ", \n".join(get_sql_clauses()).format(*sql_values))

    def _get_sql_repr(self, render=False):
        """
        DESCRIPTION:
            Internal function to generate the SQL Clause for TDSeries in UAF Function.
            By default, this function generates the parameterised SQL.

        PARAMETERS:
            render:
                Optional Argument.
                Specifies whether the function should generate parameterised SQL clause or not.
                When set to True, the function generates the non parameterised SQL call. Otherwise,
                it generates the parameterised SQL.
                Default Value: False
                Types: bool

        RETURNS:
            str or tuple

        RAISES:
            None.

        EXAMPLES:
            self._get_sql_repr()
        """
        if render:
            # Check whether SQL clause is generated or not.
            # If generated, return it. Otherwise, generate and return it.
            if not self._non_parameterised_sql:
                # Generate the SQL Clause.
                self._generate_spec()
            return self._non_parameterised_sql

        # Check whether SQL clause is generated or not.
        # If generated, return it. Otherwise, generate and return it.
        if not self._parameterised_sql:
            self._generate_spec()

        return self._parameterised_sql

    def _is_payload_required(self):
        """
        Internal function to specify whether payload is required or not.
        Child classes can override accordingly.
        """
        return True

    def _id_row_index_required(self):
        """
        Internal function to specify whether id and row_index is required or not.
        Child classes can override accordingly.
        """
        return True

    def _create_validate_dataframe_from_node(self, nodeid, metaexpr, index_label, undropped_columns=None):
        """
        DESCRIPTION:
            Function to create a teradataml DataFrame from node.
            Validates if the columns in _cols_to_check are present in the new DataFrame.
            Returns a tuple (True, new DataFrame) if the columns in _cols_to_check
            are present in the new DataFrame otherwise, (False, new DataFrame).

        PARAMETERS:
            nodeid:
                Required Argument.
                Specifies the nodeid for the DataFrame or GeoDataFrame.
                Types: str

            metaexpr:
                Required Argument.
                Specifies the metadata for the resultant object.
                Types: _MetaExpression

            index_label:
                Required Argument.
                Specifies list specifying index column(s) for the DataFrame.
                Types: str OR list of Strings (str)

            undropped_columns:
                Optional Argument.
                Specifies list of index column(s) to be retained as columns for printing.
                Types: list

        RETURNS:
             Returns a tuple (True, new DataFrame) if the columns in _cols_to_check
             are present in the new DataFrame otherwise, (False, new DataFrame).

        RAISES:
            None

        EXAMPLES:
            self._create_validate_dataframe_from_node(new_nodeid, new_meta,
                                                      self._index_label, undropped_columns)
        """

        new_df = DataFrame._from_node(nodeid, metaexpr, index_label, undropped_columns)
        # Check _cols_to_check is empty, this means that we are creating
        # TDSeries over an ART. As the new DataFrame is internally
        # created, we will return False, i.e., DataFrame object will be returned.
        if not self._cols_to_check.items():
            return new_df, False
        else:
            # Iterate over all the columns in _cols_to_check to check if all columns
            # are present in the new DataFrame, if yes we will return True and the
            # DataFrame will be converted to corresponding TDSeries or TDMatrix object.
            for col, col_name in self._cols_to_check.items():
                if col_name not in new_df.columns:
                    return new_df, False
            return new_df, True


class TDSeries(_TDUAF):
    """
    TDSeries class for UAF Functions.
    """

    def __init__(self, data, id, row_index, row_index_style="TIMECODE", id_sequence=None,
                 payload_field=None, payload_content=None, layer=None, interval=None):
        """
        DESCRIPTION:
            1. Create a TDSeries object from a teradataml DataFrame
               representing a SERIES in time series which is used
               as input to Unbounded Array Framework, time series functions.

               A series is a one-dimensional array. They are the basic input
               of UAF functions.
               A series is identified by its series ID, i.e., "id" argument,
               and indexed by "row_index" argument.

               Series is passed to and returned from UAF functions as wavelets.
               Wavelets are collections of rows, grouped by one or more fields,
               and ordered on the "row_index" argument.

            2. Any operations like filter, select, sum, etc. over TDSeries
               returns a teradataml DataFrame.

        PARAMETERS:
            data:
                Required Argument.
                Specifies the teradataml Dataframe.
                Types: teradataml DataFrame

            id:
                Required Argument.
                Specifies the name of the column in "data" containing the
                identifier values.
                Types: str or list of str

            row_index:
                Required Argument.
                Specifies the name of the column in "data" containing the
                row indexing values.
                Types: str

            row_index_style:
                Optional Argument.
                Specifies the style of row indexing.
                Default Value: "TIMECODE"
                Permitted Values: "TIMECODE", "SEQUENCE"
                Types: str

            id_sequence:
                Optional Argument.
                Specifies a sequence of series to plot.
                Types: str or list of str

            payload_field:
                Optional Argument.
                Specifies the names of the fields for payload.
                Types: str or list of str

            payload_content:
                Optional Argument.
                Specifies the payload content type.
                Permitted Values: "REAL", "COMPLEX", "AMPL_PHASE",
                                  "AMPL_PHASE_RADIANS", "AMPL_PHASE_DEGREES",
                                  "MULTIVAR_REAL", "MULTIVAR_COMPLEX",
                                  "MULTIVAR_ANYTYPE", "MULTIVAR_AMPL_PHASE",
                                  "MULTIVAR_AMPL_PHASE_RADIANS ",
                                  "MULTIVAR_AMPL_PHASE_DEGREES"
                Types: str

            layer:
                Optional Argument.
                Specifies the layer name of the ART table, if dataframe is
                created on ART table.
                Types: str

            interval:
                Optional Argument.
                Specifies the indicator to divide a series into a collection of
                intervals along its row-axis.
                "interval" is categorised in to 4 types:
                    * Values represent time-duration
                        * Allowed Values:
                            * CAL_YEARS
                            * CAL_MONTHS
                            * CAL_DAYS
                            * WEEKS
                            * DAYS
                            * HOURS
                            * MINUTES
                            * SECONDS
                            * MILLISECONDS
                            * MICROSECONDS
                    * Values represent time-zero
                        * Allowed Values:
                            * DATE
                            * TIMESTAMP
                            * TIMESTAMP WITH TIME ZONE
                    * Values represent an integer or floating number
                        * Allowed Values: A positive integer or float,
                         range from 1 to 32767, inclusively.
                    * sequence-zero:
                        An expression which evaluates to an INTEGER or FLOAT.
                        Used when row_index_style is SEQUENCE.
                Allowed Values:
                    Individual values or combined values from below:
                    * time-duration
                    * time-duration, time-zero
                    * integer
                    * float, integer
                    * sequence-zero
                    * float, sequence-zero
                Types: str

        RAISES:
            None

        RETURNS:
            None

        EXAMPLES:
            # Example 1: Creating TDSeries object.
            >>> from teradataml import create_context, load_example_data, DataFrame, TDSeries
            >>> con = create_context(host = host, user=user, password=passw)
            >>> load_example_data("dataframe", "admissions_train")

            # Create a DataFrame to be passed as input to TDSeries.
            >>> data = DataFrame("admissions_train")

            # Create TDSeries object which can be used as input in UAF functions.
            >>> result = TDSeries(data=data, id="admitted", row_index="admitted",
                                  payload_field="abc", payload_content="REAL")

            >>> result
               masters   gpa     stats programming  admitted
            id
            5       no  3.44    Novice      Novice         0
            34     yes  3.85  Advanced    Beginner         0
            13      no  4.00  Advanced      Novice         1
            40     yes  3.95    Novice    Beginner         0
            22     yes  3.46    Novice    Beginner         0
            19     yes  1.98  Advanced    Advanced         0
            36      no  3.00  Advanced      Novice         0
            15     yes  4.00  Advanced    Advanced         1
            7      yes  2.33    Novice      Novice         1
            17      no  3.83  Advanced    Advanced         1
        """
        self._interval = interval

        super(TDSeries, self).__init__(data, id=id, row_index=row_index,
                                       row_index_style=row_index_style,
                                       id_sequence=id_sequence,
                                       payload_field=payload_field,
                                       payload_content=payload_content,
                                       layer=layer)

        self._awu_matrix.append(["interval", self._interval, True, (str), True])
        self._cols_to_check = {"id": self._id, "row_index": self._row_index}
        # Validate arguments.
        self._validate()

        self._spec_header = "SERIES_SPEC({})"
        if self._interval:
            self._additional_spec["INTERVAL ({})"] = self._interval

        self._additional_spec["SERIES_ID ({})"] = ", ".join(UtilFuncs._as_list(self._id))

    def _create_dataframe_from_node(self, nodeid, metaexpr, index_label, undropped_columns=None):
        """
        DESCRIPTION:
            Function to call the _create_validate_dataframe_from_node which will create
            and validate the DataFrame for the columns present in _cols_to_check.
            If columns are present, TDSeries object is returned else DataFrame object.

        PARAMETERS:
            nodeid:
                Required Argument.
                Specifies the nodeid for the DataFrame or GeoDataFrame.
                Types: str

            metaexpr:
                Required Argument.
                Specifies the metadata for the resultant object.
                Types: _MetaExpression

            index_label:
                Required Argument.
                Specifies list specifying index column(s) for the DataFrame.
                Types: str OR list of Strings (str)

            undropped_columns:
                Optional Argument.
                Specifies list of index column(s) to be retained as columns for printing.
                Types: list

        RETURNS:
             TDSeries if all the columns in _cols_to_check are present.
             DataFrame, otherwise.

        RAISES:
            None

        EXAMPLES:
            self._create_dataframe_from_node(new_nodeid, new_meta,
                                            self._index_label, undropped_columns)
        """
        new_df, flag = self._create_validate_dataframe_from_node(nodeid,
                                                                 metaexpr, index_label,
                                                                 undropped_columns=None)
        if flag:
            return TDSeries(data=new_df, id=self._id, row_index=self._row_index,
                            row_index_style=self._row_index_style,
                            id_sequence=self._id_sequence,
                            payload_field=self._payload_field,
                            payload_content=self._payload_content,
                            layer=self._layer, interval=self._interval)
        else:
            return new_df


class TDAnalyticResult(_TDUAF):
    """
    TDAnalyticResult class for UAF Functions.
    """

    def __init__(self, data, id_sequence=None, payload_field=None, payload_content=None, layer=None):
        """
        DESCRIPTION:
            1. Create a TDAnalyticResult object from a teradataml Dataframe created on an
               Analytic Result Table (ART) which can be used as input to
               Unbounded Array Framework functions.

               The primary use of an analytical result table (ART) is to
               associate function results with a name label, enabling us to
               easily retrieve the result data and pass the result to another
               UAF function.

               An ART can have multiple layers.
               Each layer has its own dedicated row composition for the series
               or matrix.

            2. Any operations like filter, select, sum, etc. over TDAnalyticResult
               returns a teradataml DataFrame.

        PARAMETERS:
            data:
                Required Argument.
                Specifies the teradataml Dataframe.
                Types: teradataml DataFrame

            id_sequence:
                Optional Argument.
                Specifies a sequence of series to plot.
                Types: str or list of str

            payload_field:
                Optional Argument.
                Specifies the names of the fields for payload.
                Types: str or list of str

            payload_content:
                Optional Argument.
                Specifies the payload content type.
                Permitted Values: "REAL", "COMPLEX", "AMPL_PHASE",
                                  "AMPL_PHASE_RADIANS", "AMPL_PHASE_DEGREES",
                                  "MULTIVAR_REAL", "MULTIVAR_COMPLEX",
                                  "MULTIVAR_ANYTYPE", "MULTIVAR_AMPL_PHASE",
                                  "MULTIVAR_AMPL_PHASE_RADIANS ",
                                  "MULTIVAR_AMPL_PHASE_DEGREES"
                Types: str

            layer:
                Optional Argument.
                Specifies the layer name of the ART, if dataframe is
                created on ART.
                Types: str

        RAISES:
            None

        RETURNS:
            None

        EXAMPLES:
            # Example 1: Prepare input for UAF function using the Analytic Result Table (ART).

            # Establish connection to Vantage.
            >>> from teradataml import create_context
            >>> con = create_context(host=host, user= user, password=password)

            # Create a Analytic Result Table(ART) by executing SInfo function.
            >>> from teradataml import load_example_data, SInfo
            >>> load_example_data("uaf", ["ocean_buoys2"])

            # Create teradataml DataFrame object.
            >>> data = DataFrame.from_table("ocean_buoys2")

            # Create teradataml TDSeries object.
            >>> data_series_df = TDSeries(data=data,
                                          id=["ocean_name","buoyid"],
                                          row_index="TD_TIMECODE",
                                          row_index_style="TIMECODE",
                                          payload_field="jsoncol.Measure.salinity",
                                          payload_content="REAL")

            # Execute SInfo function and store the output in 'TSINFO_RESULTS'.
            >>> uaf_out = SInfo(data=data_series_df, output_table_name='TSINFO_RESULTS')

            # Create a teradataml dataframe on 'TSINFO_RESULTS' ART.
            >>> art_df = DataFrame('TSINFO_RESULTS')

            # Check if the DataFrame 'art_table' is created on an ART.
            >>> art_df.is_art
            True

            # Create TDAnalyticResult object which can be used as input in UAF functions.
            >>> result = TDAnalyticResult(data=art_df)

            # Check if 'result' is created on an ART.
            >>> result.is_art
            True

            >>> result
               buoyid  ROW_I      INDEX_DT                 INDEX_BEGIN                   INDEX_END  NUM_ENTRIES  DISCRETE          SAMPLE_INTERVAL CONTENT  MIN_MAG_salinity  MAX_MAG_salinity  AVG_MAG_salinity  RMS_MAG_salinity HAS_NULL_NAN_INF
            0      44      1  TIMESTAMP(6)  2014-01-06 10:00:24.000000  2014-01-06 10:52:00.000009           13         0  MICROSECONDS(258000001)    REAL              55.0              55.0              55.0              55.0                N
            1       0      1  TIMESTAMP(6)  2014-01-06 08:00:00.000000  2014-01-06 08:10:00.000000            5         0             SECONDS(150)    REAL              55.0              55.0              55.0              55.0                N
            2       2      1  TIMESTAMP(6)  2014-01-06 21:01:25.122200  2014-01-06 21:03:25.122200            3         1               MINUTES(1)    REAL              55.0              55.0              55.0              55.0                N
            3       1      1  TIMESTAMP(6)  2014-01-06 09:01:25.122200  2014-01-06 09:03:25.122200            6         0              SECONDS(24)    REAL              55.0              55.0              55.0              55.0                N
        """
        super(TDAnalyticResult, self).__init__(data,
                                               id_sequence=id_sequence,
                                               payload_field=payload_field,
                                               payload_content=payload_content,
                                               layer=layer)

        # Check if payload_field and payload_content arguments are passed together or not.
        if (payload_field is None and payload_content is not None) or \
                (payload_field is not None and payload_content is None):
            raise TeradataMlException(Messages.get_message(MessageCodes.MUST_PASS_ARGUMENT,
                                                           'payload_field',
                                                           'payload_content'),
                                      MessageCodes.MUST_PASS_ARGUMENT)

        self._cols_to_check = {}
        # Validate arguments.
        self._validate()

        self._spec_header = "ART_SPEC({})"

    def _create_dataframe_from_node(self, nodeid, metaexpr, index_label, undropped_columns=None):
        """
        DESCRIPTION:
            Function to call the _create_validate_dataframe_from_node which will create
            and validate the DataFrame for the columns present in _cols_to_check.
            If columns are present, TDSeries object is returned else DataFrame object.

        PARAMETERS:
            nodeid:
                Required Argument.
                Specifies the nodeid for the DataFrame or GeoDataFrame.
                Types: str

            metaexpr:
                Required Argument.
                Specifies the metadata for the resultant object.
                Types: _MetaExpression

            index_label:
                Required Argument.
                Specifies list specifying index column(s) for the DataFrame.
                Types: str OR list of Strings (str)

            undropped_columns:
                Optional Argument.
                Specifies list of index column(s) to be retained as columns for printing.
                Types: list

        RETURNS:
             TDSeries if all the columns in _cols_to_check are present.
             DataFrame, otherwise.

        RAISES:
            None

        EXAMPLES:
            self._create_dataframe_from_node(new_nodeid, new_meta,
                                            self._index_label, undropped_columns)
        """
        new_df, flag = self._create_validate_dataframe_from_node(nodeid,
                                                                 metaexpr, index_label,
                                                                 undropped_columns=None)
        if flag:
            return TDAnalyticResult(data=new_df,
                                    id_sequence=self._id_sequence,
                                    payload_field=self._payload_field,
                                    payload_content=self._payload_content,
                                    layer=self._layer)
        else:
            return new_df

    def _id_row_index_required(self):
        """
        Internal function to specify whether id and row_index is required or not.
        """
        return False

    def _is_payload_required(self):
        """
        Internal function to specify whether payload and payload_field is required or not.
        """
        return False


class TDMatrix(_TDUAF):
    """
    TDMatrix class for UAF Functions.
    """

    def __init__(self, data, id, row_index, column_index, row_index_style="TIMECODE", column_index_style="TIMECODE",
                 id_sequence=None, payload_field=None, payload_content=None, layer=None):
        """
        DESCRIPTION:
            Create a TDMatrix object from a teradataml DataFrame representing
            a MATRIX in time series
            which is used as input to Unbounded Array Framework,
            time series functions.

            A matrix is a two-dimensional array that has rows and columns.
            A matrix is identified by its matrix id, i.e., "id" argument,
            and is indexed by "row_index" and "column_index" arguments.
            A matrix can be a one of the following types:
                * Row-major matrix: Each row is a wavelet that is grouped by
                  its matrix id and "row_index", and ordered by its "column_index".
                * Column-major matrix: Each column is a wavelet that is grouped
                  by its matrix id and "column_index", and ordered by its "row_index".

        PARAMETERS:
            data:
                Required Argument.
                Specifies the teradataml Dataframe.
                Types: teradataml DataFrame

            id:
                Required Argument.
                Specifies the name of the column in "data" containing the
                identifier values.
                Types: str or list of str

            row_index:
                Required Argument.
                Specifies the name of the column in "data" containing the
                row indexing values.
                Types: str

            column_index:
                Required Argument.
                Specifies the name of the column in "data" containing
                the column.
                indexing values.
                Types: str

            row_index_style:
                Optional Argument.
                Specifies the style of row indexing.
                Default Value: "TIMECODE"
                Permitted Values: "TIMECODE", "SEQUENCE"
                Types: str

            column_index_style:
                Optional Argument.
                Specifies the style of column indexing.
                Default Value: "TIMECODE"
                Permitted Values: "TIMECODE", "SEQUENCE"
                Types: str

            id_sequence:
                Optional Argument.
                Specifies a sequence of series to plot.
                Types: str or list of str

            payload_field:
                Optional Argument.
                Specifies the names of the fields for payload.
                Types: str or list of str

            payload_content:
                Optional Argument.
                Specifies the payload content type.
                Permitted Values: "REAL", "COMPLEX", "AMPL_PHASE",
                                  "AMPL_PHASE_RADIANS", "AMPL_PHASE_DEGREES",
                                  "MULTIVAR_REAL", "MULTIVAR_COMPLEX",
                                  "MULTIVAR_ANYTYPE", "MULTIVAR_AMPL_PHASE",
                                  "MULTIVAR_AMPL_PHASE_RADIANS ",
                                  "MULTIVAR_AMPL_PHASE_DEGREES"
                Types: str

            layer:
                Optional Argument.
                Specifies the layer name of the ART table, if dataframe is created on ART table.
                Types: str

        RAISES:
            None

        RETURNS:
            None

        EXAMPLES:
            # Create a TDMatrix object.
            >>> from teradataml import create_context, load_example_data, DataFrame, TDMatrix
            >>> con = create_context(host = host, user=user, password=passw)
            >>> load_example_data("dataframe", "admissions_train")

            # Create a DataFrame to be passed as input to TDMatrix.
            >>> data = DataFrame("admissions_train")

            # Create a TDMatrix object to be passed as input to UAF functions.
            >>> res = TDMatrix(data=data, id='admitted', row_index='id',
                               column_index = 'admitted', row_index_style="TIMECODE",
                               payload_field='payload_field',payload_content='REAL')
            >>> res
               masters   gpa     stats programming  admitted
            id
            15     yes  4.00  Advanced    Advanced         1
            34     yes  3.85  Advanced    Beginner         0
            13      no  4.00  Advanced      Novice         1
            38     yes  2.65  Advanced    Beginner         1
            5       no  3.44    Novice      Novice         0
            40     yes  3.95    Novice    Beginner         0
            7      yes  2.33    Novice      Novice         1
            22     yes  3.46    Novice    Beginner         0
            26     yes  3.57  Advanced    Advanced         1
            17      no  3.83  Advanced    Advanced         1

        """
        self._column_index = column_index
        self._column_index_style = column_index_style

        super().__init__(data=data, id=id, row_index=row_index, row_index_style=row_index_style,
                         id_sequence=id_sequence,
                         payload_field=payload_field, payload_content=payload_content, layer=layer)
        self._awu_matrix.append(["column_index", self._column_index, False, (str), True])
        self._awu_matrix.append(
            ["column_index_style", self._column_index_style, True, (str), True, ["TIMECODE", "SEQUENCE"]])

        self._cols_to_check = {"id": self._id, "row_index": self._row_index, "column_index": self._column_index}

        # Validate arguments.
        self._validate()

        self._spec_header = "MATRIX_SPEC({})"
        if self._column_index:
            self._additional_spec["COLUMN_AXIS ({}({{}}))".format(self._column_index_style)] = self._column_index

        self._additional_spec["MATRIX_ID ({})"] = ", ".join(UtilFuncs._as_list(self._id))

    def _create_dataframe_from_node(self, nodeid, metaexpr, index_label, undropped_columns=None):
        """
            DESCRIPTION:
                Function to call the _create_validate_dataframe_from_node which will create
                and validate the DataFrame for the columns present.
                If columns are present, TDMatrix object is returned else DataFrame object.

            PARAMETERS:
                nodeid:
                    Required Argument.
                    Specifies the nodeid for the DataFrame or GeoDataFrame.
                    Types: str

                metaexpr:
                    Required Argument.
                    Specifies the metadata for the resultant object.
                    Types: _MetaExpression

                index_label:
                    Required Argument.
                    Specifies list specifying index column(s) for the DataFrame.
                    Types: str OR list of Strings (str)

                undropped_columns:
                    Optional Argument.
                    Specifies list of index column(s) to be retained as columns for printing.
                    Types: list

            RETURNS:
                 TDMatrix if all the columns in _cols_to_check are present.
                 DataFrame, otherwise

            RAISES:
                None

            EXAMPLES:
                self._create_dataframe_from_node(new_nodeid, new_meta,
                                                 self._index_label, undropped_columns)
        """
        new_df, flag = self._create_validate_dataframe_from_node(nodeid,
                                                                 metaexpr, index_label,
                                                                 undropped_columns=None)
        if flag:
            return TDMatrix(data=new_df, id=self._id, row_index=self._row_index,
                            column_index=self._column_index,
                            row_index_style=self._row_index_style,
                            column_index_style=self._column_index_style,
                            id_sequence=self._id_sequence,
                            payload_field=self._payload_field,
                            payload_content=self._payload_content,
                            layer=self._layer)
        else:
            return new_df


class TDGenSeries():
    """
    TDGenSeries class for UAF Functions.
    """

    def __init__(self, instances, data_types, start, offset, num_entries):
        """
        Generate a series to be passed to a UAF function rather than using a
        pre-existing series instance.
        It contains all the information that would have been derivable
        from a TDSeries as well as the information required to generate the series.
        The TDGenSeries can only be passed to a function that accepts a single series as input.

        Generated series have an indexing mechanism which starts at integer 0 and increments by 1 for each
        additional generated entry.

        PARAMETERS:
            instances:
                Required Argument.
                Specifies the columns and values for the generated series.
                Types: dict

            data_types:
                Required Argument.
                Specifies the data types of the identifying columns for the generated series.
                Types: teradatasqlalchemy.types

            start:
                Required Argument.
                Specifies Start value for the information about how the series payload, containing
                successive real magnitude values.
                Types: float, int

            offset:
                Required Argument.
                Specifies offset value for the information about how the series payload, containing
                successive real magnitude values.
                Types: float, int

            num_entries:
                Required Argument.
                Specifies number of entries for the information about how the series payload, containing
                successive real magnitude values.
                Types: int

        RAISES:
            None

        RETURN:
            None

        EXAMPLES:
            # Import TDGenSeries.
            >>> from teradataml import TDGenSeries

            # Import INTEGER type from teradatasqlalchemy.
            >>> from teradatasqlalchemy.types import INTEGER

            # Create a TDGenSeries object to be passed as input to UAF functions.
            >>> series = TDGenSeries(instances = {"BuoyID": 3}, data_types = INTEGER(), start=0, offset=1, num_entries=5)
        """

        self._instances = instances
        self._data_types = data_types
        self._start = start
        self._offset = offset
        self._num_entries = num_entries
        self._arg_info_matrix = []
        self._arg_info_matrix.append(["instances", self._instances, False, dict])
        self._arg_info_matrix.append(["data_types", self._data_types, False, (_TDType, list)])
        self._arg_info_matrix.append(["start", self._start, False, (float, int)])
        self._arg_info_matrix.append(["offset", self._offset, False, (float, int)])
        self._arg_info_matrix.append(["num_entries", self._num_entries, False, int])

        # Validate missing arguments.
        _Validators._validate_missing_required_arguments(self._arg_info_matrix)

        # Validate argument types.
        _Validators._validate_function_arguments(self._arg_info_matrix)

        self._parameterised_sql = None
        self._non_parameterised_sql = None
        self._spec_header = "GENSERIES_SPEC ({})"

    def _generate_spec(self):
        """
        DESCRIPTION:
            Internal function to generate the SPEC for UAF. This function generate the spec for
            both parameterised SQL and non parameterised SQL.

        PARAMETERS:
            None

        RETURNS:
            tuple OR str

        RAISES:
            None

        Examples:
            self._generate_spec()
        """
        sql_clauses, sql_values = ["INSTANCE_NAMES ({})", "DT ({})", "GEN_PAYLOAD ({})"],\
                                  ["'{}'".format(json.dumps(self._instances)),
                                   ", ".join(map(str, UtilFuncs._as_list(self._data_types))),
                                   ", ".join(map(str,[self._start, self._offset,
                                                      self._num_entries]))]

        # Declare a function to return a generator object. Note that, this should be a function because,
        # generator object exhausts after the first usage. So, if it is a regular variable,
        # _non_parameterised_sql will not have any data to consume from generator object.
        get_sql_clauses = lambda: ("{}{}".format(" " * 4, c) for c in sql_clauses)

        self._parameterised_sql = self._spec_header.format(
            "\n" + ", \n".join(get_sql_clauses()).format(*(["?"] * len(sql_values)))), sql_values
        self._non_parameterised_sql = self._spec_header.format(
            "\n" + ", \n".join(get_sql_clauses()).format(*sql_values))

    def _get_sql_repr(self, render=False):
        """
        DESCRIPTION:
            Internal function to generate the SQL Clause for GenSeries Spec in UAF Function.
            By default, this function generates the parameterised SQL.

        PARAMETERS:
            render:
                Optional Argument.
                Specifies whether the function should generate parameterised SQL clause or not.
                When set to True, the function generates the non parameterised SQL call. Otherwise,
                it generates the parameterised SQL.
                Default Value: False
                Types: bool

        RETURNS:
            str or tuple

        RAISES:
            None.

        EXAMPLES:
            self._get_sql_repr()
        """
        if render:
            # Check whether SQL clause is generated or not.
            # If generated, return it. Otherwise, generate and return it.
            if not self._non_parameterised_sql:
                # Generate the SQL Clause.
                self._generate_spec()
            return self._non_parameterised_sql

        # Check whether SQL clause is generated or not.
        # If generated, return it. Otherwise, generate and return it.
        if not self._parameterised_sql:
            self._generate_spec()

        return self._parameterised_sql
