"""
ãƒ­ã‚°å‚¾å‘åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

ãƒ­ã‚°ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‚¾å‘ã‚’åˆ†æã—ã¾ã™ã€‚
"""

import os
import json
import pickle
from datetime import datetime, timedelta


HISTORY_DIR = "data/logstats/history"
STATE_DIR = "data/logstats"


def _get_history_file(log_id: str) -> str:
    """ãƒ­ã‚°IDã‹ã‚‰å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’ç”Ÿæˆ"""
    os.makedirs(HISTORY_DIR, exist_ok=True)
    return f"{HISTORY_DIR}/{log_id}.json"


def _load_history(log_id: str) -> list:
    """å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    history_file = _get_history_file(log_id)
    if os.path.exists(history_file):
        try:
            with open(history_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            pass
    return []


def _save_history(log_id: str, history: list):
    """å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
    history_file = _get_history_file(log_id)
    try:
        with open(history_file, "w", encoding="utf-8") as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ å±¥æ­´ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")


def analyze_log_trend(log_id: str, threshold_percent: int = 30) -> str:
    """
    ãƒ­ã‚°ã®å‚¾å‘ã‚’åˆ†æã—ã¾ã™ã€‚
    
    Args:
        log_id: ãƒ­ã‚°ã®è­˜åˆ¥å­
        threshold_percent: æ€¥å¢—ã¨ã¿ãªã™å¢—åŠ ç‡ï¼ˆ%ï¼‰
        
    Returns:
        str: åˆ†æçµæœãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    """
    # ç¾åœ¨ã®è¡Œæ•°ã‚’å–å¾—ï¼ˆä»®å®Ÿè£…ï¼‰
    state_file = f"{STATE_DIR}/{log_id}.pkl"
    if not os.path.exists(state_file):
        return f"ğŸ“Š {log_id}: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼ˆåˆå›å®Ÿè¡Œï¼‰"
    
    try:
        with open(state_file, "rb") as f:
            state_data = pickle.load(f)
            # è¾æ›¸å½¢å¼ã®å ´åˆã¯last_lineã‚’å–å¾—ã€æ•°å€¤ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
            if isinstance(state_data, dict):
                current_lines = state_data.get("last_line", 0)
            else:
                current_lines = state_data
    except Exception:
        return f"ğŸ“Š {log_id}: çŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼"
    
    # å±¥æ­´ã‚’èª­ã¿è¾¼ã‚€
    history = _load_history(log_id)
    
    # å±¥æ­´ã«è¿½åŠ 
    today = datetime.now().strftime("%Y-%m-%d")
    history.append({
        "date": today,
        "lines": current_lines
    })
    
    # æœ€å¤§30æ—¥åˆ†ä¿æŒ
    if len(history) > 30:
        history = history[-30:]
    
    _save_history(log_id, history)
    
    # å‚¾å‘åˆ†æ
    if len(history) < 2:
        return f"ğŸ“Š {log_id}: ãƒ‡ãƒ¼ã‚¿è“„ç©ä¸­ï¼ˆ{len(history)}æ—¥åˆ†ï¼‰"
    
    # å‰æ—¥æ¯”
    yesterday_data = history[-2]["lines"]
    # è¾æ›¸å½¢å¼ã®å ´åˆã¯last_lineã‚’å–å¾—ã€æ•°å€¤ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
    if isinstance(yesterday_data, dict):
        yesterday_lines = yesterday_data.get("last_line", 0)
    else:
        yesterday_lines = yesterday_data
    
    increase_rate = ((current_lines - yesterday_lines) / max(yesterday_lines, 1)) * 100
    
    if increase_rate > threshold_percent:
        return f"ğŸ“Š {log_id}: å‰æ—¥æ¯” +{increase_rate:.1f}% ã®æ€¥å¢—ã®å¯èƒ½æ€§"
    else:
        return f"ğŸ“Š {log_id}: æ­£å¸¸ç¯„å›²ï¼ˆå‰æ—¥æ¯” {increase_rate:+.1f}%ï¼‰"


def detect_repeated_spikes(log_id: str, days: int = 3) -> bool:
    """
    è¤‡æ•°æ—¥ã«ã‚ãŸã‚‹æ€¥å¢—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã—ã¾ã™ã€‚
    
    Args:
        log_id: ãƒ­ã‚°ã®è­˜åˆ¥å­
        days: æ¤œå‡ºå¯¾è±¡ã®æ—¥æ•°
        
    Returns:
        bool: é€£ç¶šæ€¥å¢—ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆTrue
    """
    history = _load_history(log_id)
    
    if len(history) < days + 1:
        return False
    
    # ç›´è¿‘Næ—¥é–“ã®å¢—åŠ ç‡ã‚’ãƒã‚§ãƒƒã‚¯
    spike_count = 0
    for i in range(len(history) - days, len(history)):
        if i > 0:
            prev_data = history[i - 1]["lines"]
            curr_data = history[i]["lines"]
            
            # è¾æ›¸å½¢å¼ã®å ´åˆã¯last_lineã‚’å–å¾—ã€æ•°å€¤ã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨
            if isinstance(prev_data, dict):
                prev_lines = prev_data.get("last_line", 0)
            else:
                prev_lines = prev_data
            
            if isinstance(curr_data, dict):
                curr_lines = curr_data.get("last_line", 0)
            else:
                curr_lines = curr_data
            
            increase_rate = ((curr_lines - prev_lines) / max(prev_lines, 1)) * 100
            
            if increase_rate > 20:  # 20%ä»¥ä¸Šã®å¢—åŠ 
                spike_count += 1
    
    return spike_count >= days
