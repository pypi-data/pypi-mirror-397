"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from ._schema0 import Schema0, Schema0TypedDict
from .chatstreamoptions import ChatStreamOptions, ChatStreamOptionsTypedDict
from .message import Message, MessageTypedDict
from .providersortunion import ProviderSortUnion, ProviderSortUnionTypedDict
from .reasoningsummaryverbosity import ReasoningSummaryVerbosity
from .responseformatjsonschema import (
    ResponseFormatJSONSchema,
    ResponseFormatJSONSchemaTypedDict,
)
from .responseformattextgrammar import (
    ResponseFormatTextGrammar,
    ResponseFormatTextGrammarTypedDict,
)
from .tooldefinitionjson import ToolDefinitionJSON, ToolDefinitionJSONTypedDict
from openrouter.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
    UnrecognizedStr,
)
from openrouter.utils import get_discriminator, validate_const, validate_open_enum
import pydantic
from pydantic import Discriminator, Tag, model_serializer
from pydantic.functional_validators import AfterValidator, PlainValidator
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


ChatGenerationParamsDataCollection = Union[
    Literal[
        "deny",
        "allow",
    ],
    UnrecognizedStr,
]


Quantizations = Union[
    Literal[
        "int4",
        "int8",
        "fp4",
        "fp6",
        "fp8",
        "fp16",
        "bf16",
        "fp32",
        "unknown",
    ],
    UnrecognizedStr,
]


class ChatGenerationParamsMaxPriceTypedDict(TypedDict):
    r"""The object specifying the maximum price you want to pay for this request. USD price per million tokens, for prompt and completion."""

    prompt: NotRequired[Any]
    completion: NotRequired[Any]
    image: NotRequired[Any]
    audio: NotRequired[Any]
    request: NotRequired[Any]


class ChatGenerationParamsMaxPrice(BaseModel):
    r"""The object specifying the maximum price you want to pay for this request. USD price per million tokens, for prompt and completion."""

    prompt: Optional[Any] = None

    completion: Optional[Any] = None

    image: Optional[Any] = None

    audio: Optional[Any] = None

    request: Optional[Any] = None


class ChatGenerationParamsProviderTypedDict(TypedDict):
    allow_fallbacks: NotRequired[Nullable[bool]]
    r"""Whether to allow backup providers to serve requests
    - true: (default) when the primary provider (or your custom providers in \"order\") is unavailable, use the next best provider.
    - false: use only the primary/custom provider, and return the upstream error if it's unavailable.

    """
    require_parameters: NotRequired[Nullable[bool]]
    r"""Whether to filter providers to only those that support the parameters you've provided. If this setting is omitted or set to false, then providers will receive only the parameters they support, and ignore the rest."""
    data_collection: NotRequired[Nullable[ChatGenerationParamsDataCollection]]
    r"""Data collection setting. If no available model provider meets the requirement, your request will return an error.
    - allow: (default) allow providers which store user data non-transiently and may train on it

    - deny: use only providers which do not collect user data.
    """
    zdr: NotRequired[Nullable[bool]]
    enforce_distillable_text: NotRequired[Nullable[bool]]
    order: NotRequired[Nullable[List[Schema0TypedDict]]]
    r"""An ordered list of provider slugs. The router will attempt to use the first provider in the subset of this list that supports your requested model, and fall back to the next if it is unavailable. If no providers are available, the request will fail with an error message."""
    only: NotRequired[Nullable[List[Schema0TypedDict]]]
    r"""List of provider slugs to allow. If provided, this list is merged with your account-wide allowed provider settings for this request."""
    ignore: NotRequired[Nullable[List[Schema0TypedDict]]]
    r"""List of provider slugs to ignore. If provided, this list is merged with your account-wide ignored provider settings for this request."""
    quantizations: NotRequired[Nullable[List[Quantizations]]]
    r"""A list of quantization levels to filter the provider by."""
    sort: NotRequired[Nullable[ProviderSortUnionTypedDict]]
    r"""The sorting strategy to use for this request, if \"order\" is not specified. When set, no load balancing is performed."""
    max_price: NotRequired[ChatGenerationParamsMaxPriceTypedDict]
    r"""The object specifying the maximum price you want to pay for this request. USD price per million tokens, for prompt and completion."""
    preferred_min_throughput: NotRequired[Nullable[float]]
    preferred_max_latency: NotRequired[Nullable[float]]
    min_throughput: NotRequired[Nullable[float]]
    max_latency: NotRequired[Nullable[float]]


class ChatGenerationParamsProvider(BaseModel):
    allow_fallbacks: OptionalNullable[bool] = UNSET
    r"""Whether to allow backup providers to serve requests
    - true: (default) when the primary provider (or your custom providers in \"order\") is unavailable, use the next best provider.
    - false: use only the primary/custom provider, and return the upstream error if it's unavailable.

    """

    require_parameters: OptionalNullable[bool] = UNSET
    r"""Whether to filter providers to only those that support the parameters you've provided. If this setting is omitted or set to false, then providers will receive only the parameters they support, and ignore the rest."""

    data_collection: Annotated[
        OptionalNullable[ChatGenerationParamsDataCollection],
        PlainValidator(validate_open_enum(False)),
    ] = UNSET
    r"""Data collection setting. If no available model provider meets the requirement, your request will return an error.
    - allow: (default) allow providers which store user data non-transiently and may train on it

    - deny: use only providers which do not collect user data.
    """

    zdr: OptionalNullable[bool] = UNSET

    enforce_distillable_text: OptionalNullable[bool] = UNSET

    order: OptionalNullable[List[Schema0]] = UNSET
    r"""An ordered list of provider slugs. The router will attempt to use the first provider in the subset of this list that supports your requested model, and fall back to the next if it is unavailable. If no providers are available, the request will fail with an error message."""

    only: OptionalNullable[List[Schema0]] = UNSET
    r"""List of provider slugs to allow. If provided, this list is merged with your account-wide allowed provider settings for this request."""

    ignore: OptionalNullable[List[Schema0]] = UNSET
    r"""List of provider slugs to ignore. If provided, this list is merged with your account-wide ignored provider settings for this request."""

    quantizations: OptionalNullable[
        List[Annotated[Quantizations, PlainValidator(validate_open_enum(False))]]
    ] = UNSET
    r"""A list of quantization levels to filter the provider by."""

    sort: OptionalNullable[ProviderSortUnion] = UNSET
    r"""The sorting strategy to use for this request, if \"order\" is not specified. When set, no load balancing is performed."""

    max_price: Optional[ChatGenerationParamsMaxPrice] = None
    r"""The object specifying the maximum price you want to pay for this request. USD price per million tokens, for prompt and completion."""

    preferred_min_throughput: OptionalNullable[float] = UNSET

    preferred_max_latency: OptionalNullable[float] = UNSET

    min_throughput: OptionalNullable[float] = UNSET

    max_latency: OptionalNullable[float] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "allow_fallbacks",
            "require_parameters",
            "data_collection",
            "zdr",
            "enforce_distillable_text",
            "order",
            "only",
            "ignore",
            "quantizations",
            "sort",
            "max_price",
            "preferred_min_throughput",
            "preferred_max_latency",
            "min_throughput",
            "max_latency",
        ]
        nullable_fields = [
            "allow_fallbacks",
            "require_parameters",
            "data_collection",
            "zdr",
            "enforce_distillable_text",
            "order",
            "only",
            "ignore",
            "quantizations",
            "sort",
            "preferred_min_throughput",
            "preferred_max_latency",
            "min_throughput",
            "max_latency",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class ChatGenerationParamsPluginResponseHealingTypedDict(TypedDict):
    id: Literal["response-healing"]
    enabled: NotRequired[bool]


class ChatGenerationParamsPluginResponseHealing(BaseModel):
    ID: Annotated[
        Annotated[
            Literal["response-healing"],
            AfterValidator(validate_const("response-healing")),
        ],
        pydantic.Field(alias="id"),
    ] = "response-healing"

    enabled: Optional[bool] = None


PdfEngine = Union[
    Literal[
        "mistral-ocr",
        "pdf-text",
        "native",
    ],
    UnrecognizedStr,
]


class PdfTypedDict(TypedDict):
    engine: NotRequired[PdfEngine]


class Pdf(BaseModel):
    engine: Annotated[
        Optional[PdfEngine], PlainValidator(validate_open_enum(False))
    ] = None


class ChatGenerationParamsPluginFileParserTypedDict(TypedDict):
    id: Literal["file-parser"]
    enabled: NotRequired[bool]
    pdf: NotRequired[PdfTypedDict]


class ChatGenerationParamsPluginFileParser(BaseModel):
    ID: Annotated[
        Annotated[
            Literal["file-parser"], AfterValidator(validate_const("file-parser"))
        ],
        pydantic.Field(alias="id"),
    ] = "file-parser"

    enabled: Optional[bool] = None

    pdf: Optional[Pdf] = None


Engine = Union[
    Literal[
        "native",
        "exa",
    ],
    UnrecognizedStr,
]


class ChatGenerationParamsPluginWebTypedDict(TypedDict):
    id: Literal["web"]
    enabled: NotRequired[bool]
    max_results: NotRequired[float]
    search_prompt: NotRequired[str]
    engine: NotRequired[Engine]


class ChatGenerationParamsPluginWeb(BaseModel):
    ID: Annotated[
        Annotated[Literal["web"], AfterValidator(validate_const("web"))],
        pydantic.Field(alias="id"),
    ] = "web"

    enabled: Optional[bool] = None

    max_results: Optional[float] = None

    search_prompt: Optional[str] = None

    engine: Annotated[Optional[Engine], PlainValidator(validate_open_enum(False))] = (
        None
    )


class ChatGenerationParamsPluginModerationTypedDict(TypedDict):
    id: Literal["moderation"]


class ChatGenerationParamsPluginModeration(BaseModel):
    ID: Annotated[
        Annotated[Literal["moderation"], AfterValidator(validate_const("moderation"))],
        pydantic.Field(alias="id"),
    ] = "moderation"


ChatGenerationParamsPluginUnionTypedDict = TypeAliasType(
    "ChatGenerationParamsPluginUnionTypedDict",
    Union[
        ChatGenerationParamsPluginModerationTypedDict,
        ChatGenerationParamsPluginResponseHealingTypedDict,
        ChatGenerationParamsPluginFileParserTypedDict,
        ChatGenerationParamsPluginWebTypedDict,
    ],
)


ChatGenerationParamsPluginUnion = Annotated[
    Union[
        Annotated[ChatGenerationParamsPluginModeration, Tag("moderation")],
        Annotated[ChatGenerationParamsPluginWeb, Tag("web")],
        Annotated[ChatGenerationParamsPluginFileParser, Tag("file-parser")],
        Annotated[ChatGenerationParamsPluginResponseHealing, Tag("response-healing")],
    ],
    Discriminator(lambda m: get_discriminator(m, "id", "id")),
]


Route = Union[
    Literal[
        "fallback",
        "sort",
    ],
    UnrecognizedStr,
]


Effort = Union[
    Literal[
        "xhigh",
        "high",
        "medium",
        "low",
        "minimal",
        "none",
    ],
    UnrecognizedStr,
]


class ReasoningTypedDict(TypedDict):
    effort: NotRequired[Nullable[Effort]]
    summary: NotRequired[Nullable[ReasoningSummaryVerbosity]]


class Reasoning(BaseModel):
    effort: Annotated[
        OptionalNullable[Effort], PlainValidator(validate_open_enum(False))
    ] = UNSET

    summary: Annotated[
        OptionalNullable[ReasoningSummaryVerbosity],
        PlainValidator(validate_open_enum(False)),
    ] = UNSET

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["effort", "summary"]
        nullable_fields = ["effort", "summary"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class ChatGenerationParamsResponseFormatPythonTypedDict(TypedDict):
    type: Literal["python"]


class ChatGenerationParamsResponseFormatPython(BaseModel):
    TYPE: Annotated[
        Annotated[Literal["python"], AfterValidator(validate_const("python"))],
        pydantic.Field(alias="type"),
    ] = "python"


class ChatGenerationParamsResponseFormatJSONObjectTypedDict(TypedDict):
    type: Literal["json_object"]


class ChatGenerationParamsResponseFormatJSONObject(BaseModel):
    TYPE: Annotated[
        Annotated[
            Literal["json_object"], AfterValidator(validate_const("json_object"))
        ],
        pydantic.Field(alias="type"),
    ] = "json_object"


class ChatGenerationParamsResponseFormatTextTypedDict(TypedDict):
    type: Literal["text"]


class ChatGenerationParamsResponseFormatText(BaseModel):
    TYPE: Annotated[
        Annotated[Literal["text"], AfterValidator(validate_const("text"))],
        pydantic.Field(alias="type"),
    ] = "text"


ChatGenerationParamsResponseFormatUnionTypedDict = TypeAliasType(
    "ChatGenerationParamsResponseFormatUnionTypedDict",
    Union[
        ChatGenerationParamsResponseFormatTextTypedDict,
        ChatGenerationParamsResponseFormatJSONObjectTypedDict,
        ChatGenerationParamsResponseFormatPythonTypedDict,
        ResponseFormatJSONSchemaTypedDict,
        ResponseFormatTextGrammarTypedDict,
    ],
)


ChatGenerationParamsResponseFormatUnion = Annotated[
    Union[
        Annotated[ChatGenerationParamsResponseFormatText, Tag("text")],
        Annotated[ChatGenerationParamsResponseFormatJSONObject, Tag("json_object")],
        Annotated[ResponseFormatJSONSchema, Tag("json_schema")],
        Annotated[ResponseFormatTextGrammar, Tag("grammar")],
        Annotated[ChatGenerationParamsResponseFormatPython, Tag("python")],
    ],
    Discriminator(lambda m: get_discriminator(m, "type", "type")),
]


ChatGenerationParamsStopTypedDict = TypeAliasType(
    "ChatGenerationParamsStopTypedDict", Union[str, List[str]]
)


ChatGenerationParamsStop = TypeAliasType(
    "ChatGenerationParamsStop", Union[str, List[str]]
)


class DebugTypedDict(TypedDict):
    echo_upstream_body: NotRequired[bool]


class Debug(BaseModel):
    echo_upstream_body: Optional[bool] = None


class ChatGenerationParamsTypedDict(TypedDict):
    messages: List[MessageTypedDict]
    provider: NotRequired[Nullable[ChatGenerationParamsProviderTypedDict]]
    r"""When multiple model providers are available, optionally indicate your routing preference."""
    plugins: NotRequired[List[ChatGenerationParamsPluginUnionTypedDict]]
    r"""Plugins you want to enable for this request, including their settings."""
    route: NotRequired[Nullable[Route]]
    user: NotRequired[str]
    session_id: NotRequired[str]
    r"""A unique identifier for grouping related requests (e.g., a conversation or agent workflow) for observability. If provided in both the request body and the x-session-id header, the body value takes precedence. Maximum of 128 characters."""
    model: NotRequired[str]
    models: NotRequired[List[str]]
    frequency_penalty: NotRequired[Nullable[float]]
    logit_bias: NotRequired[Nullable[Dict[str, float]]]
    logprobs: NotRequired[Nullable[bool]]
    top_logprobs: NotRequired[Nullable[float]]
    max_completion_tokens: NotRequired[Nullable[float]]
    max_tokens: NotRequired[Nullable[float]]
    metadata: NotRequired[Dict[str, str]]
    presence_penalty: NotRequired[Nullable[float]]
    reasoning: NotRequired[ReasoningTypedDict]
    response_format: NotRequired[ChatGenerationParamsResponseFormatUnionTypedDict]
    seed: NotRequired[Nullable[int]]
    stop: NotRequired[Nullable[ChatGenerationParamsStopTypedDict]]
    stream: NotRequired[bool]
    stream_options: NotRequired[Nullable[ChatStreamOptionsTypedDict]]
    temperature: NotRequired[Nullable[float]]
    tool_choice: NotRequired[Any]
    tools: NotRequired[List[ToolDefinitionJSONTypedDict]]
    top_p: NotRequired[Nullable[float]]
    debug: NotRequired[DebugTypedDict]


class ChatGenerationParams(BaseModel):
    messages: List[Message]

    provider: OptionalNullable[ChatGenerationParamsProvider] = UNSET
    r"""When multiple model providers are available, optionally indicate your routing preference."""

    plugins: Optional[List[ChatGenerationParamsPluginUnion]] = None
    r"""Plugins you want to enable for this request, including their settings."""

    route: Annotated[
        OptionalNullable[Route], PlainValidator(validate_open_enum(False))
    ] = UNSET

    user: Optional[str] = None

    session_id: Optional[str] = None
    r"""A unique identifier for grouping related requests (e.g., a conversation or agent workflow) for observability. If provided in both the request body and the x-session-id header, the body value takes precedence. Maximum of 128 characters."""

    model: Optional[str] = None

    models: Optional[List[str]] = None

    frequency_penalty: OptionalNullable[float] = UNSET

    logit_bias: OptionalNullable[Dict[str, float]] = UNSET

    logprobs: OptionalNullable[bool] = UNSET

    top_logprobs: OptionalNullable[float] = UNSET

    max_completion_tokens: OptionalNullable[float] = UNSET

    max_tokens: OptionalNullable[float] = UNSET

    metadata: Optional[Dict[str, str]] = None

    presence_penalty: OptionalNullable[float] = UNSET

    reasoning: Optional[Reasoning] = None

    response_format: Optional[ChatGenerationParamsResponseFormatUnion] = None

    seed: OptionalNullable[int] = UNSET

    stop: OptionalNullable[ChatGenerationParamsStop] = UNSET

    stream: Optional[bool] = False

    stream_options: OptionalNullable[ChatStreamOptions] = UNSET

    temperature: OptionalNullable[float] = UNSET

    tool_choice: Optional[Any] = None

    tools: Optional[List[ToolDefinitionJSON]] = None

    top_p: OptionalNullable[float] = UNSET

    debug: Optional[Debug] = None

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "provider",
            "plugins",
            "route",
            "user",
            "session_id",
            "model",
            "models",
            "frequency_penalty",
            "logit_bias",
            "logprobs",
            "top_logprobs",
            "max_completion_tokens",
            "max_tokens",
            "metadata",
            "presence_penalty",
            "reasoning",
            "response_format",
            "seed",
            "stop",
            "stream",
            "stream_options",
            "temperature",
            "tool_choice",
            "tools",
            "top_p",
            "debug",
        ]
        nullable_fields = [
            "provider",
            "route",
            "frequency_penalty",
            "logit_bias",
            "logprobs",
            "top_logprobs",
            "max_completion_tokens",
            "max_tokens",
            "presence_penalty",
            "seed",
            "stop",
            "stream_options",
            "temperature",
            "top_p",
        ]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m
