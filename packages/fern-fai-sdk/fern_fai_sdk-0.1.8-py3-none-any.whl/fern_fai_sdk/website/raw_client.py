# This file was auto-generated by Fern from our API Definition.

import typing
from json.decoder import JSONDecodeError

from ..core.api_error import ApiError
from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.http_response import AsyncHttpResponse, HttpResponse
from ..core.jsonable_encoder import jsonable_encoder
from ..core.pydantic_utilities import parse_obj_as
from ..core.request_options import RequestOptions
from ..errors.unprocessable_entity_error import UnprocessableEntityError
from ..types.delete_all_websites_response import DeleteAllWebsitesResponse
from ..types.delete_website_response import DeleteWebsiteResponse
from ..types.get_website_response import GetWebsiteResponse
from ..types.get_website_status_response import GetWebsiteStatusResponse
from ..types.get_websites_response import GetWebsitesResponse
from ..types.http_validation_error import HttpValidationError
from ..types.index_website_response import IndexWebsiteResponse
from ..types.reindex_website_response import ReindexWebsiteResponse

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class RawWebsiteClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def index_website(
        self,
        domain: str,
        *,
        base_url: str,
        domain_filter: typing.Optional[str] = OMIT,
        path_filter: typing.Optional[str] = OMIT,
        url_pattern: typing.Optional[str] = OMIT,
        chunk_size: typing.Optional[int] = OMIT,
        chunk_overlap: typing.Optional[int] = OMIT,
        min_content_length: typing.Optional[int] = OMIT,
        max_pages: typing.Optional[int] = OMIT,
        delay: typing.Optional[float] = OMIT,
        version: typing.Optional[str] = OMIT,
        product: typing.Optional[str] = OMIT,
        authed: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[IndexWebsiteResponse]:
        """
        Start crawling and indexing a website.
        Returns a job_id to track the crawling progress.

        Parameters
        ----------
        domain : str

        base_url : str
            The base URL to start indexing from (e.g., 'https://docs.example.com')

        domain_filter : typing.Optional[str]
            Domain to filter crawling (e.g., 'docs.example.com'). Defaults to base_url domain.

        path_filter : typing.Optional[str]
            Path prefix to restrict crawling (e.g., '/docs'). Only URLs starting with this will be crawled.

        url_pattern : typing.Optional[str]
            Regex pattern to filter URLs (e.g., `https://example\.com/(docs|api)/.*`).

        chunk_size : typing.Optional[int]
            Size of text chunks for splitting documents

        chunk_overlap : typing.Optional[int]
            Overlap between consecutive chunks

        min_content_length : typing.Optional[int]
            Minimum content length to index a page

        max_pages : typing.Optional[int]
            Maximum number of pages to crawl. None means unlimited.

        delay : typing.Optional[float]
            Delay in seconds between requests

        version : typing.Optional[str]
            Version to tag all indexed pages with

        product : typing.Optional[str]
            Product to tag all indexed pages with

        authed : typing.Optional[bool]
            Whether indexed pages should be auth-gated

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[IndexWebsiteResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"sources/website/{jsonable_encoder(domain)}/index",
            method="POST",
            json={
                "base_url": base_url,
                "domain_filter": domain_filter,
                "path_filter": path_filter,
                "url_pattern": url_pattern,
                "chunk_size": chunk_size,
                "chunk_overlap": chunk_overlap,
                "min_content_length": min_content_length,
                "max_pages": max_pages,
                "delay": delay,
                "version": version,
                "product": product,
                "authed": authed,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    IndexWebsiteResponse,
                    parse_obj_as(
                        type_=IndexWebsiteResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    headers=dict(_response.headers),
                    body=typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    def get_website_status(
        self, domain: str, *, job_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> HttpResponse[GetWebsiteStatusResponse]:
        """
        Get the status of a website crawling job.

        Parameters
        ----------
        domain : str

        job_id : str
            The job ID returned from the index endpoint

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[GetWebsiteStatusResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"sources/website/{jsonable_encoder(domain)}/status",
            method="GET",
            params={
                "job_id": job_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    GetWebsiteStatusResponse,
                    parse_obj_as(
                        type_=GetWebsiteStatusResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    headers=dict(_response.headers),
                    body=typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    def get_website_by_id(
        self, domain: str, website_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> HttpResponse[GetWebsiteResponse]:
        """
        Get a single indexed website page by ID.

        Parameters
        ----------
        domain : str

        website_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[GetWebsiteResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"sources/website/{jsonable_encoder(domain)}/{jsonable_encoder(website_id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    GetWebsiteResponse,
                    parse_obj_as(
                        type_=GetWebsiteResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    headers=dict(_response.headers),
                    body=typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    def get_websites(
        self,
        domain: str,
        *,
        page: typing.Optional[int] = None,
        limit: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[GetWebsitesResponse]:
        """
        List all indexed website pages for a domain with pagination.

        Parameters
        ----------
        domain : str

        page : typing.Optional[int]
            The page number for pagination

        limit : typing.Optional[int]
            The number of sources per page

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[GetWebsitesResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"sources/website/{jsonable_encoder(domain)}",
            method="GET",
            params={
                "page": page,
                "limit": limit,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    GetWebsitesResponse,
                    parse_obj_as(
                        type_=GetWebsitesResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    headers=dict(_response.headers),
                    body=typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    def reindex_website(
        self,
        domain: str,
        *,
        base_url: str,
        domain_filter: typing.Optional[str] = OMIT,
        path_filter: typing.Optional[str] = OMIT,
        url_pattern: typing.Optional[str] = OMIT,
        chunk_size: typing.Optional[int] = OMIT,
        chunk_overlap: typing.Optional[int] = OMIT,
        min_content_length: typing.Optional[int] = OMIT,
        max_pages: typing.Optional[int] = OMIT,
        delay: typing.Optional[float] = OMIT,
        version: typing.Optional[str] = OMIT,
        product: typing.Optional[str] = OMIT,
        authed: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[ReindexWebsiteResponse]:
        """
        Re-crawl a website by starting a new crawl job. The job will delete old pages before indexing.
        Uses the configuration from the original index request.

        Parameters
        ----------
        domain : str

        base_url : str
            The base URL to re-crawl (will delete old pages and re-index)

        domain_filter : typing.Optional[str]
            Domain to filter crawling (e.g., 'docs.example.com'). If not provided, uses previous config.

        path_filter : typing.Optional[str]
            Path prefix to restrict crawling (e.g., '/docs'). If not provided, uses previous config.

        url_pattern : typing.Optional[str]
            Regex pattern to filter URLs (e.g., `https://example\.com/(docs|api)/.*`). If not provided, uses previous config.

        chunk_size : typing.Optional[int]
            Size of text chunks for splitting documents. If not provided, uses previous config.

        chunk_overlap : typing.Optional[int]
            Overlap between consecutive chunks. If not provided, uses previous config.

        min_content_length : typing.Optional[int]
            Minimum content length to index a page. If not provided, uses previous config.

        max_pages : typing.Optional[int]
            Maximum number of pages to crawl. If not provided, uses previous config.

        delay : typing.Optional[float]
            Delay in seconds between requests. If not provided, uses previous config.

        version : typing.Optional[str]
            Version to tag all indexed pages with. If not provided, uses previous config.

        product : typing.Optional[str]
            Product to tag all indexed pages with. If not provided, uses previous config.

        authed : typing.Optional[bool]
            Whether indexed pages should be auth-gated. If not provided, uses previous config.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[ReindexWebsiteResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"sources/website/{jsonable_encoder(domain)}/reindex",
            method="POST",
            json={
                "base_url": base_url,
                "domain_filter": domain_filter,
                "path_filter": path_filter,
                "url_pattern": url_pattern,
                "chunk_size": chunk_size,
                "chunk_overlap": chunk_overlap,
                "min_content_length": min_content_length,
                "max_pages": max_pages,
                "delay": delay,
                "version": version,
                "product": product,
                "authed": authed,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    ReindexWebsiteResponse,
                    parse_obj_as(
                        type_=ReindexWebsiteResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    headers=dict(_response.headers),
                    body=typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    def delete_website(
        self, domain: str, *, base_url: str, request_options: typing.Optional[RequestOptions] = None
    ) -> HttpResponse[DeleteWebsiteResponse]:
        """
        Delete all pages from a specific website by base URL.

        Parameters
        ----------
        domain : str

        base_url : str
            The base URL of the website to delete (deletes all pages from this source)

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[DeleteWebsiteResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"sources/website/{jsonable_encoder(domain)}/delete",
            method="DELETE",
            json={
                "base_url": base_url,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    DeleteWebsiteResponse,
                    parse_obj_as(
                        type_=DeleteWebsiteResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    headers=dict(_response.headers),
                    body=typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    def delete_all_websites(
        self, domain: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> HttpResponse[DeleteAllWebsitesResponse]:
        """
        Delete all indexed website pages for a domain.

        Parameters
        ----------
        domain : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[DeleteAllWebsitesResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"sources/website/{jsonable_encoder(domain)}/delete-all",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    DeleteAllWebsitesResponse,
                    parse_obj_as(
                        type_=DeleteAllWebsitesResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    headers=dict(_response.headers),
                    body=typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)


class AsyncRawWebsiteClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def index_website(
        self,
        domain: str,
        *,
        base_url: str,
        domain_filter: typing.Optional[str] = OMIT,
        path_filter: typing.Optional[str] = OMIT,
        url_pattern: typing.Optional[str] = OMIT,
        chunk_size: typing.Optional[int] = OMIT,
        chunk_overlap: typing.Optional[int] = OMIT,
        min_content_length: typing.Optional[int] = OMIT,
        max_pages: typing.Optional[int] = OMIT,
        delay: typing.Optional[float] = OMIT,
        version: typing.Optional[str] = OMIT,
        product: typing.Optional[str] = OMIT,
        authed: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[IndexWebsiteResponse]:
        """
        Start crawling and indexing a website.
        Returns a job_id to track the crawling progress.

        Parameters
        ----------
        domain : str

        base_url : str
            The base URL to start indexing from (e.g., 'https://docs.example.com')

        domain_filter : typing.Optional[str]
            Domain to filter crawling (e.g., 'docs.example.com'). Defaults to base_url domain.

        path_filter : typing.Optional[str]
            Path prefix to restrict crawling (e.g., '/docs'). Only URLs starting with this will be crawled.

        url_pattern : typing.Optional[str]
            Regex pattern to filter URLs (e.g., `https://example\.com/(docs|api)/.*`).

        chunk_size : typing.Optional[int]
            Size of text chunks for splitting documents

        chunk_overlap : typing.Optional[int]
            Overlap between consecutive chunks

        min_content_length : typing.Optional[int]
            Minimum content length to index a page

        max_pages : typing.Optional[int]
            Maximum number of pages to crawl. None means unlimited.

        delay : typing.Optional[float]
            Delay in seconds between requests

        version : typing.Optional[str]
            Version to tag all indexed pages with

        product : typing.Optional[str]
            Product to tag all indexed pages with

        authed : typing.Optional[bool]
            Whether indexed pages should be auth-gated

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[IndexWebsiteResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"sources/website/{jsonable_encoder(domain)}/index",
            method="POST",
            json={
                "base_url": base_url,
                "domain_filter": domain_filter,
                "path_filter": path_filter,
                "url_pattern": url_pattern,
                "chunk_size": chunk_size,
                "chunk_overlap": chunk_overlap,
                "min_content_length": min_content_length,
                "max_pages": max_pages,
                "delay": delay,
                "version": version,
                "product": product,
                "authed": authed,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    IndexWebsiteResponse,
                    parse_obj_as(
                        type_=IndexWebsiteResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    headers=dict(_response.headers),
                    body=typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    async def get_website_status(
        self, domain: str, *, job_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> AsyncHttpResponse[GetWebsiteStatusResponse]:
        """
        Get the status of a website crawling job.

        Parameters
        ----------
        domain : str

        job_id : str
            The job ID returned from the index endpoint

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[GetWebsiteStatusResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"sources/website/{jsonable_encoder(domain)}/status",
            method="GET",
            params={
                "job_id": job_id,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    GetWebsiteStatusResponse,
                    parse_obj_as(
                        type_=GetWebsiteStatusResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    headers=dict(_response.headers),
                    body=typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    async def get_website_by_id(
        self, domain: str, website_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> AsyncHttpResponse[GetWebsiteResponse]:
        """
        Get a single indexed website page by ID.

        Parameters
        ----------
        domain : str

        website_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[GetWebsiteResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"sources/website/{jsonable_encoder(domain)}/{jsonable_encoder(website_id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    GetWebsiteResponse,
                    parse_obj_as(
                        type_=GetWebsiteResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    headers=dict(_response.headers),
                    body=typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    async def get_websites(
        self,
        domain: str,
        *,
        page: typing.Optional[int] = None,
        limit: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[GetWebsitesResponse]:
        """
        List all indexed website pages for a domain with pagination.

        Parameters
        ----------
        domain : str

        page : typing.Optional[int]
            The page number for pagination

        limit : typing.Optional[int]
            The number of sources per page

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[GetWebsitesResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"sources/website/{jsonable_encoder(domain)}",
            method="GET",
            params={
                "page": page,
                "limit": limit,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    GetWebsitesResponse,
                    parse_obj_as(
                        type_=GetWebsitesResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    headers=dict(_response.headers),
                    body=typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    async def reindex_website(
        self,
        domain: str,
        *,
        base_url: str,
        domain_filter: typing.Optional[str] = OMIT,
        path_filter: typing.Optional[str] = OMIT,
        url_pattern: typing.Optional[str] = OMIT,
        chunk_size: typing.Optional[int] = OMIT,
        chunk_overlap: typing.Optional[int] = OMIT,
        min_content_length: typing.Optional[int] = OMIT,
        max_pages: typing.Optional[int] = OMIT,
        delay: typing.Optional[float] = OMIT,
        version: typing.Optional[str] = OMIT,
        product: typing.Optional[str] = OMIT,
        authed: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[ReindexWebsiteResponse]:
        """
        Re-crawl a website by starting a new crawl job. The job will delete old pages before indexing.
        Uses the configuration from the original index request.

        Parameters
        ----------
        domain : str

        base_url : str
            The base URL to re-crawl (will delete old pages and re-index)

        domain_filter : typing.Optional[str]
            Domain to filter crawling (e.g., 'docs.example.com'). If not provided, uses previous config.

        path_filter : typing.Optional[str]
            Path prefix to restrict crawling (e.g., '/docs'). If not provided, uses previous config.

        url_pattern : typing.Optional[str]
            Regex pattern to filter URLs (e.g., `https://example\.com/(docs|api)/.*`). If not provided, uses previous config.

        chunk_size : typing.Optional[int]
            Size of text chunks for splitting documents. If not provided, uses previous config.

        chunk_overlap : typing.Optional[int]
            Overlap between consecutive chunks. If not provided, uses previous config.

        min_content_length : typing.Optional[int]
            Minimum content length to index a page. If not provided, uses previous config.

        max_pages : typing.Optional[int]
            Maximum number of pages to crawl. If not provided, uses previous config.

        delay : typing.Optional[float]
            Delay in seconds between requests. If not provided, uses previous config.

        version : typing.Optional[str]
            Version to tag all indexed pages with. If not provided, uses previous config.

        product : typing.Optional[str]
            Product to tag all indexed pages with. If not provided, uses previous config.

        authed : typing.Optional[bool]
            Whether indexed pages should be auth-gated. If not provided, uses previous config.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[ReindexWebsiteResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"sources/website/{jsonable_encoder(domain)}/reindex",
            method="POST",
            json={
                "base_url": base_url,
                "domain_filter": domain_filter,
                "path_filter": path_filter,
                "url_pattern": url_pattern,
                "chunk_size": chunk_size,
                "chunk_overlap": chunk_overlap,
                "min_content_length": min_content_length,
                "max_pages": max_pages,
                "delay": delay,
                "version": version,
                "product": product,
                "authed": authed,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    ReindexWebsiteResponse,
                    parse_obj_as(
                        type_=ReindexWebsiteResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    headers=dict(_response.headers),
                    body=typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    async def delete_website(
        self, domain: str, *, base_url: str, request_options: typing.Optional[RequestOptions] = None
    ) -> AsyncHttpResponse[DeleteWebsiteResponse]:
        """
        Delete all pages from a specific website by base URL.

        Parameters
        ----------
        domain : str

        base_url : str
            The base URL of the website to delete (deletes all pages from this source)

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[DeleteWebsiteResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"sources/website/{jsonable_encoder(domain)}/delete",
            method="DELETE",
            json={
                "base_url": base_url,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    DeleteWebsiteResponse,
                    parse_obj_as(
                        type_=DeleteWebsiteResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    headers=dict(_response.headers),
                    body=typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    async def delete_all_websites(
        self, domain: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> AsyncHttpResponse[DeleteAllWebsitesResponse]:
        """
        Delete all indexed website pages for a domain.

        Parameters
        ----------
        domain : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[DeleteAllWebsitesResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"sources/website/{jsonable_encoder(domain)}/delete-all",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    DeleteAllWebsitesResponse,
                    parse_obj_as(
                        type_=DeleteAllWebsitesResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    headers=dict(_response.headers),
                    body=typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)
