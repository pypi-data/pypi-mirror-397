# This file was auto-generated by Fern from our API Definition.

import typing
from json.decoder import JSONDecodeError

from ..core.api_error import ApiError
from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.http_response import AsyncHttpResponse, HttpResponse
from ..core.jsonable_encoder import jsonable_encoder
from ..core.pydantic_utilities import parse_obj_as
from ..core.request_options import RequestOptions
from ..core.serialization import convert_and_respect_annotation_metadata
from ..errors.unprocessable_entity_error import UnprocessableEntityError
from ..types.chat_message import ChatMessage
from ..types.http_validation_error import HttpValidationError
from ..types.language_model import LanguageModel
from ..types.post_chat_completion_response import PostChatCompletionResponse

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class RawChatClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def post_chat_completion(
        self,
        domain: str,
        *,
        messages: typing.Sequence[ChatMessage],
        model: typing.Optional[LanguageModel] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        system_prompt: typing.Optional[str] = OMIT,
        rewrite_query: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[PostChatCompletionResponse]:
        """
        Parameters
        ----------
        domain : str

        messages : typing.Sequence[ChatMessage]
            The messages to use for the chat completion

        model : typing.Optional[LanguageModel]
            The model to use for the chat completion

        max_tokens : typing.Optional[int]
            The maximum number of tokens to generate. Note: setting a token count lower than 2000 may result in incomplete responses. You can add a custom system prompt to control the verbosity of the response.

        system_prompt : typing.Optional[str]
            The system prompt to use for the chat completion

        rewrite_query : typing.Optional[bool]
            Whether to rewrite the query using query decomposition

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[PostChatCompletionResponse]
            Successful Response
        """
        _response = self._client_wrapper.httpx_client.request(
            f"chat/{jsonable_encoder(domain)}",
            method="POST",
            json={
                "model": model,
                "max_tokens": max_tokens,
                "system_prompt": system_prompt,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessage], direction="write"
                ),
                "rewrite_query": rewrite_query,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    PostChatCompletionResponse,
                    parse_obj_as(
                        type_=PostChatCompletionResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    headers=dict(_response.headers),
                    body=typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)


class AsyncRawChatClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def post_chat_completion(
        self,
        domain: str,
        *,
        messages: typing.Sequence[ChatMessage],
        model: typing.Optional[LanguageModel] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        system_prompt: typing.Optional[str] = OMIT,
        rewrite_query: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[PostChatCompletionResponse]:
        """
        Parameters
        ----------
        domain : str

        messages : typing.Sequence[ChatMessage]
            The messages to use for the chat completion

        model : typing.Optional[LanguageModel]
            The model to use for the chat completion

        max_tokens : typing.Optional[int]
            The maximum number of tokens to generate. Note: setting a token count lower than 2000 may result in incomplete responses. You can add a custom system prompt to control the verbosity of the response.

        system_prompt : typing.Optional[str]
            The system prompt to use for the chat completion

        rewrite_query : typing.Optional[bool]
            Whether to rewrite the query using query decomposition

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[PostChatCompletionResponse]
            Successful Response
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"chat/{jsonable_encoder(domain)}",
            method="POST",
            json={
                "model": model,
                "max_tokens": max_tokens,
                "system_prompt": system_prompt,
                "messages": convert_and_respect_annotation_metadata(
                    object_=messages, annotation=typing.Sequence[ChatMessage], direction="write"
                ),
                "rewrite_query": rewrite_query,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    PostChatCompletionResponse,
                    parse_obj_as(
                        type_=PostChatCompletionResponse,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    headers=dict(_response.headers),
                    body=typing.cast(
                        HttpValidationError,
                        parse_obj_as(
                            type_=HttpValidationError,  # type: ignore
                            object_=_response.json(),
                        ),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)
