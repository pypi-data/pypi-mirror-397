# This file was auto-generated by Fern from our API Definition.

import typing

from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.request_options import RequestOptions
from ..types.delete_all_websites_response import DeleteAllWebsitesResponse
from ..types.delete_website_response import DeleteWebsiteResponse
from ..types.get_website_response import GetWebsiteResponse
from ..types.get_website_status_response import GetWebsiteStatusResponse
from ..types.get_websites_response import GetWebsitesResponse
from ..types.index_website_response import IndexWebsiteResponse
from ..types.reindex_website_response import ReindexWebsiteResponse
from .raw_client import AsyncRawWebsiteClient, RawWebsiteClient

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class WebsiteClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._raw_client = RawWebsiteClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> RawWebsiteClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawWebsiteClient
        """
        return self._raw_client

    def index_website(
        self,
        domain: str,
        *,
        base_url: str,
        domain_filter: typing.Optional[str] = OMIT,
        path_filter: typing.Optional[str] = OMIT,
        url_pattern: typing.Optional[str] = OMIT,
        chunk_size: typing.Optional[int] = OMIT,
        chunk_overlap: typing.Optional[int] = OMIT,
        min_content_length: typing.Optional[int] = OMIT,
        max_pages: typing.Optional[int] = OMIT,
        delay: typing.Optional[float] = OMIT,
        version: typing.Optional[str] = OMIT,
        product: typing.Optional[str] = OMIT,
        authed: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> IndexWebsiteResponse:
        """
        Start crawling and indexing a website.
        Returns a job_id to track the crawling progress.

        Parameters
        ----------
        domain : str

        base_url : str
            The base URL to start indexing from (e.g., 'https://docs.example.com')

        domain_filter : typing.Optional[str]
            Domain to filter crawling (e.g., 'docs.example.com'). Defaults to base_url domain.

        path_filter : typing.Optional[str]
            Path prefix to restrict crawling (e.g., '/docs'). Only URLs starting with this will be crawled.

        url_pattern : typing.Optional[str]
            Regex pattern to filter URLs (e.g., `https://example\.com/(docs|api)/.*`).

        chunk_size : typing.Optional[int]
            Size of text chunks for splitting documents

        chunk_overlap : typing.Optional[int]
            Overlap between consecutive chunks

        min_content_length : typing.Optional[int]
            Minimum content length to index a page

        max_pages : typing.Optional[int]
            Maximum number of pages to crawl. None means unlimited.

        delay : typing.Optional[float]
            Delay in seconds between requests

        version : typing.Optional[str]
            Version to tag all indexed pages with

        product : typing.Optional[str]
            Product to tag all indexed pages with

        authed : typing.Optional[bool]
            Whether indexed pages should be auth-gated

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        IndexWebsiteResponse
            Successful Response

        Examples
        --------
        from fern_fai_sdk import FernAI

        client = FernAI(
            token="YOUR_TOKEN",
        )
        client.website.index_website(
            domain="domain",
            base_url="base_url",
        )
        """
        _response = self._raw_client.index_website(
            domain,
            base_url=base_url,
            domain_filter=domain_filter,
            path_filter=path_filter,
            url_pattern=url_pattern,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            min_content_length=min_content_length,
            max_pages=max_pages,
            delay=delay,
            version=version,
            product=product,
            authed=authed,
            request_options=request_options,
        )
        return _response.data

    def get_website_status(
        self, domain: str, *, job_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> GetWebsiteStatusResponse:
        """
        Get the status of a website crawling job.

        Parameters
        ----------
        domain : str

        job_id : str
            The job ID returned from the index endpoint

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        GetWebsiteStatusResponse
            Successful Response

        Examples
        --------
        from fern_fai_sdk import FernAI

        client = FernAI(
            token="YOUR_TOKEN",
        )
        client.website.get_website_status(
            domain="domain",
            job_id="job_id",
        )
        """
        _response = self._raw_client.get_website_status(domain, job_id=job_id, request_options=request_options)
        return _response.data

    def get_website_by_id(
        self, domain: str, website_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> GetWebsiteResponse:
        """
        Get a single indexed website page by ID.

        Parameters
        ----------
        domain : str

        website_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        GetWebsiteResponse
            Successful Response

        Examples
        --------
        from fern_fai_sdk import FernAI

        client = FernAI(
            token="YOUR_TOKEN",
        )
        client.website.get_website_by_id(
            domain="domain",
            website_id="website_id",
        )
        """
        _response = self._raw_client.get_website_by_id(domain, website_id, request_options=request_options)
        return _response.data

    def get_websites(
        self,
        domain: str,
        *,
        page: typing.Optional[int] = None,
        limit: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> GetWebsitesResponse:
        """
        List all indexed website pages for a domain with pagination.

        Parameters
        ----------
        domain : str

        page : typing.Optional[int]
            The page number for pagination

        limit : typing.Optional[int]
            The number of sources per page

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        GetWebsitesResponse
            Successful Response

        Examples
        --------
        from fern_fai_sdk import FernAI

        client = FernAI(
            token="YOUR_TOKEN",
        )
        client.website.get_websites(
            domain="domain",
            page=1,
            limit=1,
        )
        """
        _response = self._raw_client.get_websites(domain, page=page, limit=limit, request_options=request_options)
        return _response.data

    def reindex_website(
        self,
        domain: str,
        *,
        base_url: str,
        domain_filter: typing.Optional[str] = OMIT,
        path_filter: typing.Optional[str] = OMIT,
        url_pattern: typing.Optional[str] = OMIT,
        chunk_size: typing.Optional[int] = OMIT,
        chunk_overlap: typing.Optional[int] = OMIT,
        min_content_length: typing.Optional[int] = OMIT,
        max_pages: typing.Optional[int] = OMIT,
        delay: typing.Optional[float] = OMIT,
        version: typing.Optional[str] = OMIT,
        product: typing.Optional[str] = OMIT,
        authed: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ReindexWebsiteResponse:
        """
        Re-crawl a website by starting a new crawl job. The job will delete old pages before indexing.
        Uses the configuration from the original index request.

        Parameters
        ----------
        domain : str

        base_url : str
            The base URL to re-crawl (will delete old pages and re-index)

        domain_filter : typing.Optional[str]
            Domain to filter crawling (e.g., 'docs.example.com'). If not provided, uses previous config.

        path_filter : typing.Optional[str]
            Path prefix to restrict crawling (e.g., '/docs'). If not provided, uses previous config.

        url_pattern : typing.Optional[str]
            Regex pattern to filter URLs (e.g., `https://example\.com/(docs|api)/.*`). If not provided, uses previous config.

        chunk_size : typing.Optional[int]
            Size of text chunks for splitting documents. If not provided, uses previous config.

        chunk_overlap : typing.Optional[int]
            Overlap between consecutive chunks. If not provided, uses previous config.

        min_content_length : typing.Optional[int]
            Minimum content length to index a page. If not provided, uses previous config.

        max_pages : typing.Optional[int]
            Maximum number of pages to crawl. If not provided, uses previous config.

        delay : typing.Optional[float]
            Delay in seconds between requests. If not provided, uses previous config.

        version : typing.Optional[str]
            Version to tag all indexed pages with. If not provided, uses previous config.

        product : typing.Optional[str]
            Product to tag all indexed pages with. If not provided, uses previous config.

        authed : typing.Optional[bool]
            Whether indexed pages should be auth-gated. If not provided, uses previous config.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ReindexWebsiteResponse
            Successful Response

        Examples
        --------
        from fern_fai_sdk import FernAI

        client = FernAI(
            token="YOUR_TOKEN",
        )
        client.website.reindex_website(
            domain="domain",
            base_url="base_url",
        )
        """
        _response = self._raw_client.reindex_website(
            domain,
            base_url=base_url,
            domain_filter=domain_filter,
            path_filter=path_filter,
            url_pattern=url_pattern,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            min_content_length=min_content_length,
            max_pages=max_pages,
            delay=delay,
            version=version,
            product=product,
            authed=authed,
            request_options=request_options,
        )
        return _response.data

    def delete_website(
        self, domain: str, *, base_url: str, request_options: typing.Optional[RequestOptions] = None
    ) -> DeleteWebsiteResponse:
        """
        Delete all pages from a specific website by base URL.

        Parameters
        ----------
        domain : str

        base_url : str
            The base URL of the website to delete (deletes all pages from this source)

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        DeleteWebsiteResponse
            Successful Response

        Examples
        --------
        from fern_fai_sdk import FernAI

        client = FernAI(
            token="YOUR_TOKEN",
        )
        client.website.delete_website(
            domain="domain",
            base_url="base_url",
        )
        """
        _response = self._raw_client.delete_website(domain, base_url=base_url, request_options=request_options)
        return _response.data

    def delete_all_websites(
        self, domain: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> DeleteAllWebsitesResponse:
        """
        Delete all indexed website pages for a domain.

        Parameters
        ----------
        domain : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        DeleteAllWebsitesResponse
            Successful Response

        Examples
        --------
        from fern_fai_sdk import FernAI

        client = FernAI(
            token="YOUR_TOKEN",
        )
        client.website.delete_all_websites(
            domain="domain",
        )
        """
        _response = self._raw_client.delete_all_websites(domain, request_options=request_options)
        return _response.data


class AsyncWebsiteClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._raw_client = AsyncRawWebsiteClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> AsyncRawWebsiteClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawWebsiteClient
        """
        return self._raw_client

    async def index_website(
        self,
        domain: str,
        *,
        base_url: str,
        domain_filter: typing.Optional[str] = OMIT,
        path_filter: typing.Optional[str] = OMIT,
        url_pattern: typing.Optional[str] = OMIT,
        chunk_size: typing.Optional[int] = OMIT,
        chunk_overlap: typing.Optional[int] = OMIT,
        min_content_length: typing.Optional[int] = OMIT,
        max_pages: typing.Optional[int] = OMIT,
        delay: typing.Optional[float] = OMIT,
        version: typing.Optional[str] = OMIT,
        product: typing.Optional[str] = OMIT,
        authed: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> IndexWebsiteResponse:
        """
        Start crawling and indexing a website.
        Returns a job_id to track the crawling progress.

        Parameters
        ----------
        domain : str

        base_url : str
            The base URL to start indexing from (e.g., 'https://docs.example.com')

        domain_filter : typing.Optional[str]
            Domain to filter crawling (e.g., 'docs.example.com'). Defaults to base_url domain.

        path_filter : typing.Optional[str]
            Path prefix to restrict crawling (e.g., '/docs'). Only URLs starting with this will be crawled.

        url_pattern : typing.Optional[str]
            Regex pattern to filter URLs (e.g., `https://example\.com/(docs|api)/.*`).

        chunk_size : typing.Optional[int]
            Size of text chunks for splitting documents

        chunk_overlap : typing.Optional[int]
            Overlap between consecutive chunks

        min_content_length : typing.Optional[int]
            Minimum content length to index a page

        max_pages : typing.Optional[int]
            Maximum number of pages to crawl. None means unlimited.

        delay : typing.Optional[float]
            Delay in seconds between requests

        version : typing.Optional[str]
            Version to tag all indexed pages with

        product : typing.Optional[str]
            Product to tag all indexed pages with

        authed : typing.Optional[bool]
            Whether indexed pages should be auth-gated

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        IndexWebsiteResponse
            Successful Response

        Examples
        --------
        import asyncio

        from fern_fai_sdk import AsyncFernAI

        client = AsyncFernAI(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.website.index_website(
                domain="domain",
                base_url="base_url",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.index_website(
            domain,
            base_url=base_url,
            domain_filter=domain_filter,
            path_filter=path_filter,
            url_pattern=url_pattern,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            min_content_length=min_content_length,
            max_pages=max_pages,
            delay=delay,
            version=version,
            product=product,
            authed=authed,
            request_options=request_options,
        )
        return _response.data

    async def get_website_status(
        self, domain: str, *, job_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> GetWebsiteStatusResponse:
        """
        Get the status of a website crawling job.

        Parameters
        ----------
        domain : str

        job_id : str
            The job ID returned from the index endpoint

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        GetWebsiteStatusResponse
            Successful Response

        Examples
        --------
        import asyncio

        from fern_fai_sdk import AsyncFernAI

        client = AsyncFernAI(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.website.get_website_status(
                domain="domain",
                job_id="job_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.get_website_status(domain, job_id=job_id, request_options=request_options)
        return _response.data

    async def get_website_by_id(
        self, domain: str, website_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> GetWebsiteResponse:
        """
        Get a single indexed website page by ID.

        Parameters
        ----------
        domain : str

        website_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        GetWebsiteResponse
            Successful Response

        Examples
        --------
        import asyncio

        from fern_fai_sdk import AsyncFernAI

        client = AsyncFernAI(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.website.get_website_by_id(
                domain="domain",
                website_id="website_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.get_website_by_id(domain, website_id, request_options=request_options)
        return _response.data

    async def get_websites(
        self,
        domain: str,
        *,
        page: typing.Optional[int] = None,
        limit: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> GetWebsitesResponse:
        """
        List all indexed website pages for a domain with pagination.

        Parameters
        ----------
        domain : str

        page : typing.Optional[int]
            The page number for pagination

        limit : typing.Optional[int]
            The number of sources per page

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        GetWebsitesResponse
            Successful Response

        Examples
        --------
        import asyncio

        from fern_fai_sdk import AsyncFernAI

        client = AsyncFernAI(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.website.get_websites(
                domain="domain",
                page=1,
                limit=1,
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.get_websites(domain, page=page, limit=limit, request_options=request_options)
        return _response.data

    async def reindex_website(
        self,
        domain: str,
        *,
        base_url: str,
        domain_filter: typing.Optional[str] = OMIT,
        path_filter: typing.Optional[str] = OMIT,
        url_pattern: typing.Optional[str] = OMIT,
        chunk_size: typing.Optional[int] = OMIT,
        chunk_overlap: typing.Optional[int] = OMIT,
        min_content_length: typing.Optional[int] = OMIT,
        max_pages: typing.Optional[int] = OMIT,
        delay: typing.Optional[float] = OMIT,
        version: typing.Optional[str] = OMIT,
        product: typing.Optional[str] = OMIT,
        authed: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ReindexWebsiteResponse:
        """
        Re-crawl a website by starting a new crawl job. The job will delete old pages before indexing.
        Uses the configuration from the original index request.

        Parameters
        ----------
        domain : str

        base_url : str
            The base URL to re-crawl (will delete old pages and re-index)

        domain_filter : typing.Optional[str]
            Domain to filter crawling (e.g., 'docs.example.com'). If not provided, uses previous config.

        path_filter : typing.Optional[str]
            Path prefix to restrict crawling (e.g., '/docs'). If not provided, uses previous config.

        url_pattern : typing.Optional[str]
            Regex pattern to filter URLs (e.g., `https://example\.com/(docs|api)/.*`). If not provided, uses previous config.

        chunk_size : typing.Optional[int]
            Size of text chunks for splitting documents. If not provided, uses previous config.

        chunk_overlap : typing.Optional[int]
            Overlap between consecutive chunks. If not provided, uses previous config.

        min_content_length : typing.Optional[int]
            Minimum content length to index a page. If not provided, uses previous config.

        max_pages : typing.Optional[int]
            Maximum number of pages to crawl. If not provided, uses previous config.

        delay : typing.Optional[float]
            Delay in seconds between requests. If not provided, uses previous config.

        version : typing.Optional[str]
            Version to tag all indexed pages with. If not provided, uses previous config.

        product : typing.Optional[str]
            Product to tag all indexed pages with. If not provided, uses previous config.

        authed : typing.Optional[bool]
            Whether indexed pages should be auth-gated. If not provided, uses previous config.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ReindexWebsiteResponse
            Successful Response

        Examples
        --------
        import asyncio

        from fern_fai_sdk import AsyncFernAI

        client = AsyncFernAI(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.website.reindex_website(
                domain="domain",
                base_url="base_url",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.reindex_website(
            domain,
            base_url=base_url,
            domain_filter=domain_filter,
            path_filter=path_filter,
            url_pattern=url_pattern,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            min_content_length=min_content_length,
            max_pages=max_pages,
            delay=delay,
            version=version,
            product=product,
            authed=authed,
            request_options=request_options,
        )
        return _response.data

    async def delete_website(
        self, domain: str, *, base_url: str, request_options: typing.Optional[RequestOptions] = None
    ) -> DeleteWebsiteResponse:
        """
        Delete all pages from a specific website by base URL.

        Parameters
        ----------
        domain : str

        base_url : str
            The base URL of the website to delete (deletes all pages from this source)

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        DeleteWebsiteResponse
            Successful Response

        Examples
        --------
        import asyncio

        from fern_fai_sdk import AsyncFernAI

        client = AsyncFernAI(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.website.delete_website(
                domain="domain",
                base_url="base_url",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.delete_website(domain, base_url=base_url, request_options=request_options)
        return _response.data

    async def delete_all_websites(
        self, domain: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> DeleteAllWebsitesResponse:
        """
        Delete all indexed website pages for a domain.

        Parameters
        ----------
        domain : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        DeleteAllWebsitesResponse
            Successful Response

        Examples
        --------
        import asyncio

        from fern_fai_sdk import AsyncFernAI

        client = AsyncFernAI(
            token="YOUR_TOKEN",
        )


        async def main() -> None:
            await client.website.delete_all_websites(
                domain="domain",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.delete_all_websites(domain, request_options=request_options)
        return _response.data
