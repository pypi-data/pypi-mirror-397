import datetime
import gc
import glob
import gzip
import io
import math
import os
from pathlib import Path
import random
import re
import sqlite3
import string
import subprocess
from tempfile import NamedTemporaryFile, TemporaryDirectory
import tempfile
import duckdb  # type: ignore
import json
import yaml  # type: ignore
import Bio.bgzf as bgzf  # type: ignore
import pandas as pd  # type: ignore
import polars as pl  # type: ignore
from pyfaidx import Fasta  # type: ignore
import numpy as np  # type: ignore
import vcf  # type: ignore
import logging as log
import fastparquet as fp  # type: ignore
import cyvcf2  # type: ignore
import pyBigWig  # type: ignore

from howard.functions.commons import (
    CODE_TYPE_MAP,
    DEFAULT_ANNOTATIONS_FOLDER,
    DEFAULT_CHUNK_SIZE,
    DEFAULT_PARQUET_FOLDER,
    DEFAULT_BCFTOOLS_FOLDER,
    DEFAULT_BIGWIG_FOLDER,
    DEFAULT_ANNOVAR_FOLDER,
    DEFAULT_ASSEMBLY,
    DEFAULT_DATABASE_FOLDER,
    DEFAULT_GENOME_FOLDER,
    DEFAULT_REFSEQ_FOLDER,
    DEFAULT_SNPEFF_FOLDER,
    DEFAULT_SPLICE_FOLDER,
    DEFAULT_TOOLS_BIN,
    DEFAULT_TOOLS_FOLDER,
    add_value_into_dict,
    annotation_file_find,
    barcode,
    cast_columns_query,
    check_docker_image_exists,
    clean_annotation_field,
    command,
    convert_markdown_to_html,
    convert_markdown_to_pdf,
    detect_column_type,
    escape_markdown_table_chars,
    extract_memory_in_go,
    find,
    find_all,
    find_file_prefix,
    find_genome,
    findbypipeline,
    full_path,
    genotype_stats,
    genotypeconcordance,
    get_bin_command,
    get_file_compressed,
    get_file_format,
    get_memory,
    get_random,
    get_tmp,
    merge_regions,
    params_string_to_dict,
    remove_if_exists,
    run_parallel_commands,
    transcripts_file_to_df,
    trio,
    vaf_normalization,
    vcf_required,
    file_format_delimiters,
    folder_config,
    code_type_map,
    comparison_map,
    code_type_map_to_sql,
    code_type_map_to_vcf,
    sort_contigs,
    choose_update_strategy_safe,
)

from howard.objects.database import Database

from howard.functions.databases import (
    databases_download_annovar,
    databases_download_exomiser,
    databases_download_snpeff,
    databases_infos,
)

from howard.functions.utils import (
    format_hgvs_name,
    get_refseq_table,
    get_transcript,
    read_transcripts,
)


class Variants:

    def __init__(
        self,
        conn=None,
        input: str = None,
        output: str = None,
        config: dict = {},
        param: dict = {},
        load: bool = False,
    ) -> None:
        """
        The function `__init__` initializes the variables, sets the input, output, config, param, connexion and
        header

        :param conn: the connection to the database
        :param input: the input file
        :param output: the output file
        :param config: a dictionary containing the configuration of the model
        :param param: a dictionary containing the parameters of the model
        """

        # Init variables
        self.init_variables()

        # Input
        self.set_input(input)

        # Config
        self.set_config(config)

        # Param
        self.set_param(param)

        # Output
        self.set_output(output)

        # connexion
        self.set_connexion(conn)

        # Header
        self.set_header()

        # Samples
        self.set_samples()

        # Load data
        if load:
            self.load_data(input)

    def load_header(
        self,
        header=None,
        table: str = None,
        drop: bool = False,
        view_name: str = "header",
    ) -> str:
        """
        Load header in a table, with INFO, FORMAT, FILTERS, SAMPLES and METADATA

        Args:
            header (vcfobject, optional): VCF object from pyVCF. Defaults to None (header of the Variants object).
            table (str, optional): Table name of the header table. Defaults to None (defined as 'header' later).
            drop (bool, optional): Drop table if exists. Defaults to False.
            view_name (str, optional): Name of the table. Defaults to 'header'.

        Returns:
            str: Name of the table, None otherwise

        """

        def create_header_table(conn):
            """
            Create header table

            Args:
                conn (conn): Database connexion.

            """

            # Columns
            columns = [
                "section VARCHAR",
                "id VARCHAR",
                "number VARCHAR",
                "type VARCHAR",
                "description VARCHAR",
            ]

            # Query create
            query_create = f"""
            CREATE OR REPLACE table {view_name} (
                {', '.join(columns)},
                PRIMARY KEY (section, id)
            );
            """

            # Execute
            conn.execute(query_create)

        def insert_header(conn, vcf_header):
            """
            Insert header into table

            Args:
                conn (conn): Database connexion.

            """

            # Init
            inserts = []

            # Add INFO section
            for info_id, info in vcf_header.infos.items():
                inserts.append(
                    (
                        "INFO",
                        info_id,
                        str(info.num if info.num is not None else "."),
                        info.type if info.type is not None else "",
                        info.desc if info.desc is not None else "",
                    )
                )

            # Add FORMAT section
            for format_id, format in vcf_header.formats.items():
                inserts.append(
                    (
                        "FORMAT",
                        format_id,
                        str(format.num if format.num is not None else "."),
                        format.type if format.type is not None else "",
                        format.desc if format.desc is not None else "",
                    )
                )

            # Add FILTER section
            for filter_id, filter in vcf_header.filters.items():
                inserts.append(
                    (
                        "FILTER",
                        filter_id,
                        "",
                        "",
                        filter.desc if filter.desc is not None else "",
                    )
                )

            # Add Samples
            for sample_id in vcf_header.samples:
                inserts.append(
                    (
                        "SAMPLE",
                        sample_id,
                        "",
                        "",
                        "",
                    )
                )

            # Add Metadata
            for key, value in vcf_header.metadata.items():
                inserts.append(
                    (
                        "METADATA",
                        key,
                        "",
                        "",
                        str(value) if value is not None else "",
                    )
                )

            # Create query of insert with parameters
            query_insert = f"""
            INSERT INTO {view_name} (section, id, number, type, description) VALUES (?, ?, ?, ?, ?);
            """
            conn.executemany(query_insert, inserts)

        # Get header is None
        if header is None:
            header = self.get_header()

        # Header table
        if table is None:
            table = "header"

        # If header is not None
        if header is not None:

            # Connexion
            conn = self.get_connexion()

            # Drop table
            if drop:
                query_drop = f"""
                DROP TABLE IF EXISTS {table}
                """
                conn.execute(query_drop)

            # Create table
            create_header_table(conn)
            insert_header(conn, header)

            return table

        else:

            return None

    def set_samples(self, samples: list = None) -> list:
        """
        The function `set_samples` sets the samples attribute of an object to a provided list or
        retrieves it from a parameter dictionary.

        :param samples: The `set_samples` method is a method of a class that takes a list of samples as
        input and sets the `samples` attribute of the class to the provided list. If no samples are
        provided, it tries to get the samples from the class's parameters using the `get_param` method
        :type samples: list
        :return: The `samples` list is being returned.
        """

        if not samples:
            samples = self.get_param().get("samples", {}).get("list", None)

        self.samples = samples

        return samples

    def get_samples(self) -> list:
        """
        This function returns a list of samples.
        :return: The `get_samples` method is returning the `samples` attribute of the object.
        """

        return self.samples

    def get_samples_check(self) -> bool:
        """
        This function returns the value of the "check" key within the "samples" dictionary retrieved
        from the parameters.
        :return: The method `get_samples_check` is returning the value of the key "check" inside the
        "samples" dictionary, which is nested inside the dictionary returned by the `get_param()`
        method. If the key "check" is not found, it will return `False`.
        """

        return self.get_param().get("samples", {}).get("check", True)

    def set_input(self, input: str = None) -> None:
        """
        The function `set_input` takes a file name as input, extracts the name and extension, and sets
        attributes in the class accordingly.

        :param input: The `set_input` method in the provided code snippet is used to set attributes
        related to the input file. Here's a breakdown of the parameters and their usage in the method:
        :type input: str
        """

        if input and not isinstance(input, str):
            try:
                self.input = input.name
            except:
                log.error(f"Input file '{input} in bad format")
                raise ValueError(f"Input file '{input} in bad format")
        else:
            self.input = input

        # Input format
        if input:
            input_name, input_extension = os.path.splitext(self.input)
            self.input_name = input_name
            self.input_extension = input_extension
            self.input_format = self.input_extension.replace(".", "")

    def set_config(self, config: dict) -> None:
        """
        The set_config function takes a config object and assigns it as the configuration object for the
        class.

        :param config: The `config` parameter in the `set_config` function is a dictionary object that
        contains configuration settings for the class. When you call the `set_config` function with a
        dictionary object as the argument, it will set that dictionary as the configuration object for
        the class
        :type config: dict
        """

        self.config = config

    def set_param(self, param: dict) -> None:
        """
        This function sets a parameter object for the class based on the input dictionary.

        :param param: The `set_param` method you provided takes a dictionary object as input and sets it
        as the `param` attribute of the class instance
        :type param: dict
        """

        self.param = param

    def init_variables(self) -> None:
        """
        This function initializes the variables that will be used in the rest of the class
        """

        self.prefix = "howard"
        self.table_variants = "variants"
        self.dataframe = None

        self.comparison_map = {
            "gt": ">",
            "gte": ">=",
            "lt": "<",
            "lte": "<=",
            "equals": "=",
            "contains": "SIMILAR TO",
        }

        self.code_type_map = {"Integer": 0, "String": 1, "Float": 2, "Flag": 3}

        self.code_type_map_to_sql = {
            "Integer": "INTEGER",
            "String": "VARCHAR",
            "Float": "FLOAT",
            "Flag": "VARCHAR",
        }

        self.index_additionnal_fields = []

    def get_indexing(self) -> bool:
        """
        It returns the value of the key "indexing" in the dictionary. If the key is not present, it
        returns False.
        :return: The value of the indexing parameter.
        """

        return self.get_param().get("indexing", False)

    def get_connexion_config(self) -> dict:
        """
        The function `get_connexion_config` returns a dictionary containing the configuration for a
        connection, including the number of threads and memory limit.
        :return: a dictionary containing the configuration for the Connexion library.
        """

        # config
        config = self.get_config()

        # Connexion config
        connexion_config = {}
        threads = self.get_threads()

        # Threads
        if threads:
            connexion_config["threads"] = threads

        # Memory
        # if config.get("memory", None):
        #     connexion_config["memory_limit"] = config.get("memory")
        if self.get_memory():
            connexion_config["memory_limit"] = self.get_memory()

        # Temporary directory
        if config.get("tmp", None):
            connexion_config["temp_directory"] = config.get("tmp")

        # Access
        if config.get("access", None):
            access = config.get("access")
            if access in ["RO"]:
                access = "READ_ONLY"
            elif access in ["RW"]:
                access = "READ_WRITE"
            connexion_db = self.get_connexion_db()
            if connexion_db in ":memory:":
                access = "READ_WRITE"
            connexion_config["access_mode"] = access

        return connexion_config

    def get_duckdb_settings(self) -> dict:
        """
        The function `get_duckdb_settings` retrieves DuckDB settings from a configuration file or a
        string.
        :return: The function `get_duckdb_settings` returns a dictionary object `duckdb_settings_dict`.
        """

        # config
        config = self.get_config()

        # duckdb settings
        duckdb_settings_dict = {}
        if config.get("duckdb_settings", None):
            duckdb_settings = config.get("duckdb_settings")
            duckdb_settings = full_path(duckdb_settings)
            # duckdb setting is a file
            if os.path.exists(duckdb_settings):
                with open(duckdb_settings) as json_file:
                    duckdb_settings_dict = yaml.safe_load(json_file)
            # duckdb settings is a string
            else:
                duckdb_settings_dict = json.loads(duckdb_settings)

        return duckdb_settings_dict

    def set_connexion_db(self) -> str:
        """
        The function `set_connexion_db` returns the appropriate database connection string based on the
        input format and connection type.
        :return: the value of the variable `connexion_db`.
        """

        # Default connexion db
        default_connexion_db = ":memory:"

        # Find connexion db
        if self.get_input_format() in ["db", "duckdb"]:
            connexion_db = self.get_input()
        elif self.get_connexion_type() in ["memory", default_connexion_db, None]:
            connexion_db = default_connexion_db
        elif self.get_connexion_type() in ["tmpfile"]:
            connexion_db = self.get_tmp_dir() + f"/howard.{get_random()}.tmp.db"
        elif self.get_connexion_type() != "":
            connexion_db = self.get_connexion_type()
        else:
            connexion_db = default_connexion_db

        # Set connexion db
        self.connexion_db = connexion_db

        return connexion_db

    def set_connexion(self, conn) -> None:
        """
        The function `set_connexion` creates a connection to a database, with options for different
        database formats and settings.

        :param conn: The `conn` parameter in the `set_connexion` method is the connection to the
        database. If a connection is not provided, a new connection to an in-memory database is created.
        The method then proceeds to set up the connection based on the specified format (e.g., duckdb or
        sqlite
        """

        # Connexion db
        connexion_db = self.set_connexion_db()

        # Connexion config
        connexion_config = self.get_connexion_config()

        # Connexion format
        connexion_format = self.get_config().get("connexion_format", "duckdb")
        # Set connexion format
        self.connexion_format = connexion_format

        # Connexion
        if not conn:
            if connexion_format in ["duckdb"]:
                conn = duckdb.connect(connexion_db, config=connexion_config)
                # duckDB settings
                duckdb_settings = self.get_duckdb_settings()
                if duckdb_settings:
                    for setting in duckdb_settings:
                        setting_value = duckdb_settings.get(setting)
                        if isinstance(setting_value, str):
                            setting_value = f"'{setting_value}'"
                        conn.execute(f"PRAGMA {setting}={setting_value};")
                # duckDB settings arrow large buffer size
                conn.execute("SET arrow_large_buffer_size=true")
                # settings = conn.execute("SELECT * FROM duckdb_settings()").df()
                # log.debug(f"DuckDB settings after connexion:\n{settings.to_string()}")
            elif connexion_format in ["sqlite"]:
                conn = sqlite3.connect(connexion_db)

        # Set connexion
        self.conn = conn

        # Log
        log.debug(f"connexion_format: {connexion_format}")
        log.debug(f"connexion_db: {connexion_db}")
        log.debug(f"connexion config: {connexion_config}")
        log.debug(f"connexion duckdb settings: {self.get_duckdb_settings()}")
        log.debug("connexion duckdb settings: arrow_large_buffer_size=true")

    def set_output(self, output: str = None) -> None:
        """
        The `set_output` function in Python sets the output file based on the input or a specified key
        in the config file, extracting the output name, extension, and format.

        :param output: The `output` parameter in the `set_output` method is used to specify the name of
        the output file. If the config file has an 'output' key, the method sets the output to the value
        of that key. If no output is provided, it sets the output to `None`
        :type output: str
        """

        if output and not isinstance(output, str):
            self.output = output.name
        else:
            self.output = output

        # Output format
        if self.output:
            output_name, output_extension = os.path.splitext(self.output)
            self.output_name = output_name
            self.output_extension = output_extension
            self.output_format = self.output_extension.replace(".", "")
        else:
            self.output_name = None
            self.output_extension = None
            self.output_format = None

    def set_header(self) -> None:
        """
        It reads the header of a VCF file and stores it as a list of strings and as a VCF object
        """

        input_file = self.get_input()
        default_header_list = [
            "##fileformat=VCFv4.2",
            "#CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO",
        ]

        # Full path
        input_file = full_path(input_file)

        if input_file:

            input_format = self.get_input_format()
            input_compressed = self.get_input_compressed()
            config = self.get_config()
            header_list = default_header_list
            if input_format in [
                "vcf",
                "hdr",
                "tsv",
                "csv",
                "psv",
                "parquet",
                "db",
                "duckdb",
                "json",
            ]:
                # header provided in param
                if config.get("header_file", None):
                    with open(config.get("header_file"), "rt") as f:
                        header_list = self.read_vcf_header(f)
                # within a vcf file format (header within input file itsself)
                elif input_format in ["vcf", "hdr"] and not os.path.isdir(input_file):
                    # within a compressed vcf file format (.vcf.gz)
                    if input_compressed:
                        with bgzf.open(input_file, "rt") as f:
                            header_list = self.read_vcf_header(f)
                    # within an uncompressed vcf file format (.vcf)
                    else:
                        with open(input_file, "rt") as f:
                            header_list = self.read_vcf_header(f)
                # header provided in default external file .hdr
                elif os.path.exists((input_file + ".hdr")):
                    with open(input_file + ".hdr", "rt") as f:
                        header_list = self.read_vcf_header(f)
                else:
                    try:  # Try to get header info fields and file columns

                        with tempfile.TemporaryDirectory() as tmpdir:

                            # Create database
                            db_for_header = Database(database=input_file)

                            # Get header columns for infos fields
                            db_header_from_columns = (
                                db_for_header.get_header_from_columns()
                            )

                            # Get real columns in the file
                            db_header_columns = db_for_header.get_columns()

                            # Write header file
                            header_file_tmp = os.path.join(tmpdir, "header")
                            f = open(header_file_tmp, "w")
                            vcf.Writer(f, db_header_from_columns)
                            f.close()

                            # Replace #CHROM line with rel columns
                            header_list = db_for_header.read_header_file(
                                header_file=header_file_tmp
                            )
                            header_list[-1] = "\t".join(db_header_columns)

                    except:

                        log.warning(
                            f"No header for file {input_file}. Set as default VCF header"
                        )
                        header_list = default_header_list

            else:  # try for unknown format ?

                log.error(f"Input file format '{input_format}' not available")
                raise ValueError(f"Input file format '{input_format}' not available")

            if not header_list:
                header_list = default_header_list

            # header as list
            self.header_list = header_list

            # header as VCF object
            self.header_vcf = vcf.Reader(io.StringIO("\n".join(header_list)))

        else:

            self.header_list = None
            self.header_vcf = None

    def optimize_table(self, table: str = None) -> None:
        """
        Optimize a database table by running the OPTIMIZE command.
        :param table: The name of the table to be optimized. If not provided, the default table
        variants will be used.
        :type table: str
        """

        # Desabled due to no efficience
        return None

        # Get connexion
        conn = self.get_connexion()

        # Get table
        if not table:
            table = self.get_table_variants()

        # Vaccum
        query_vaccum = f"""VACUUM {table};"""
        # query_vaccum = None

        # Recreate table
        temp_table_name = f"{table}_optimize_temp_" + get_random(1000000)
        query_recreate = f"""CREATE TABLE {temp_table_name} AS SELECT * FROM {table};"""
        # query_recreate = None

        # Move table
        query_drop = f"""DROP TABLE {table};"""
        query_rename = f"""ALTER TABLE {temp_table_name} RENAME TO {table};"""
        # query_drop = None
        # query_rename = None

        try:
            log.debug(f"OPTIMIZE table {table}")
            if query_vaccum is not None:
                conn.execute(query_vaccum)
            if query_recreate is not None:
                conn.execute(query_recreate)
            if query_drop is not None:
                conn.execute(query_drop)
            if query_rename is not None:
                conn.execute(query_rename)
        except Exception as e:
            log.warning(f"OPTIMIZE failed for table {table}:")
            log.warning(f"Query vaccum: {query_vaccum}")
            log.warning(f"Query recreate: {query_recreate}")
            log.warning(f"Query drop: {query_drop}")
            log.warning(f"Query rename: {query_rename}")
            log.warning(e)
            pass

        return None

    def get_query_to_df(self, query: str = "", limit: int = None) -> pd.DataFrame:
        """
        The `get_query_to_df` function takes a query as a string and returns the result as a pandas
        DataFrame based on the connection format. It supports both limited and full queries.

        :param query: The `query` parameter in the `get_query_to_df` function is a string that
        represents the SQL query you want to execute. This query will be used to fetch data from a
        database and convert it into a pandas DataFrame
        :type query: str
        :param limit: The `limit` parameter in the `get_query_to_df` function is used to specify the
        maximum number of rows to be returned in the resulting dataframe. If a limit is provided, the
        function will only fetch up to that number of rows from the database query result. If no limit
        is specified,
        :type limit: int
        :return: A pandas DataFrame is being returned by the `get_query_to_df` function.
        """

        # Connexion format
        connexion_format = self.get_connexion_format()

        # Limit in query
        if limit:

            # Panda settings
            pd.set_option("display.max_rows", limit)

            # DuckDB connexion
            if connexion_format in ["duckdb"]:

                # Deprecated code: fail when empty result and limit
                # df = (
                #     self.conn.execute(query)
                #     .fetch_record_batch(limit)
                #     .read_next_batch()
                #     .to_pandas()
                # )

                result = self.conn.execute(query).fetch_record_batch(limit)
                if result is None:
                    df = result.df()
                else:
                    try:
                        df = result.read_next_batch().to_pandas()
                    except StopIteration:
                        df = self.conn.execute(query).df()[0:limit]

            # SQLite connexion
            elif connexion_format in ["sqlite"]:
                df = next(pd.read_sql_query(query, self.conn, chunksize=limit))

        # Full query without limit
        else:

            # DuckDB connexion
            if connexion_format in ["duckdb"]:
                df = self.conn.execute(query).df()

            # SQLite connexion
            elif connexion_format in ["sqlite"]:
                df = pd.read_sql_query(query, self.conn)

        return df

    def get_overview(self) -> None:
        """
        The function prints the input, output, config, and dataframe of the current object
        """
        table_variants_from = self.get_table_variants(clause="from")
        sql_columns = self.get_header_columns_as_sql()
        sql_query_export = f"SELECT {sql_columns} FROM {table_variants_from}"
        df = self.get_query_to_df(sql_query_export)
        log.info(
            "Input:  "
            + str(self.get_input())
            + " ["
            + str(str(self.get_input_format()))
            + "]"
        )
        log.info(
            "Output: "
            + str(self.get_output())
            + " ["
            + str(str(self.get_output_format()))
            + "]"
        )
        log.info("Config: ")
        for d in str(json.dumps(self.get_config(), indent=4, sort_keys=True)).split(
            "\n"
        ):
            log.info("\t" + str(d))
        log.info("Param: ")
        for d in str(json.dumps(self.get_param(), indent=4, sort_keys=True)).split(
            "\n"
        ):
            log.info("\t" + str(d))
        log.info("Sample list: " + str(self.get_header_sample_list()))
        log.info("Dataframe: ")
        for d in str(df).split("\n"):
            log.info("\t" + str(d))

        # garbage collector
        del df
        gc.collect()

        return None

    def get_stats(
        self,
        table: str = None,
        table_view: str = None,
        annotations_stats: bool = False,
        queries: dict = None,
        queries_view: str = None,
    ) -> dict:
        """
        Calculate and return various statistics of the current object, including information about the input file,
        variants, samples, header fields, quality, and SNVs/InDels.

        :param table: The name of the table containing variant data. If not provided, the default table is used.
        :type table: str, optional
        :param table_view: The name of the table view to be used for statistics calculation. If not provided, a new view is created.
        :type table_view: str, optional
        :param annotations_stats: Whether to calculate annotation statistics. Defaults to False.
        :type annotations_stats: bool, optional
        :param queries: The `queries` parameter is a dictionary that contains queries to be executed
        and added to the statistics. The keys of the dictionary are the names of the queries, and the
        values are the SQL queries to be executed.
        :type queries: dict
        :param queries_view: The `queries_view` parameter is a string that represents the name of the
        view to be used for the queries. If no value is provided, a new view will be created.
        :type queries_view: str

        :return: A dictionary containing various statistics of the current object. The dictionary has the following structure:

            - **Infos** (*dict*): General information about the input file and header fields.
                - **Input file** (*str*): The path to the input file.
                - **Header Infos** (*list*): List of INFO fields in the header.
                - **Header Formats** (*list*): List of FORMAT fields in the header.
                - **Number of INFO fields** (*int*): Number of INFO fields in the header.
                - **Number of FORMAT fields** (*int*): Number of FORMAT fields in the header.
                - **Number of samples** (*int*): Number of samples in the dataset.
                - **Number of variants** (*int*): Total number of variants in the dataset.

            - **Variants** (*dict*): Statistics about the variants.
                - **By Chromosome** (*list*): List of dictionaries with chromosome names and variant counts.
                - **By Type** (*list*): List of dictionaries with variant types and counts.
                - **By Quality** (*list*): List of dictionaries with quality scores and counts.
                - **By Filter** (*list*): List of dictionaries with filter values and counts.

            - **Samples** (*dict*): Statistics about the samples.
                - **Variants in samples** (*list*): List of dictionaries with sample names, variant counts, and percentages.

            - **Header** (*dict*): Detailed information about the header fields.
                - **List of INFO fields** (*dict*): Dictionary with detailed information about INFO fields.
                - **List of FORMAT fields** (*dict*): Dictionary with detailed information about FORMAT fields.
                - **List of FILTER fields** (*dict*): Dictionary with detailed information about FILTER fields.

            - **Annotations** (*dict*, optional): Annotation statistics, if `annotations_stats` is True.
                - **Stats** (*dict*): Dictionary with annotation statistics.

            - **Quality** (*dict*): Quality statistics.
                - **QUAL** (*dict*): Dictionary with quality statistics (average, minimum, maximum, standard deviation, median, variance).

        :rtype: dict

        :example:

        .. code-block:: python

            stats = get_stats(table="variants_table", table_view="variants_view", annotations_stats=True)
            print(stats)
        """

        # Log
        log.info(f"Stats Calculation...")

        # table variants
        if table is None:
            table_variants_from = self.get_table_variants()
        else:
            table_variants_from = table

        # table view
        if table_view is None:
            variants_view_stats_name = "variants_view_stats_" + get_random()
        else:
            variants_view_stats_name = table_view

        # Tables to remove
        tables_to_remove = []

        # Percent_round
        percent_round = 2

        # Sample struct column name
        sample_struct_column = "SAMPLES"

        # Info struct column name
        info_struct_column = None

        # Sample struct column format needed
        if annotations_stats:
            info_prefix_column = ""
        else:
            info_prefix_column = None

        # Create view
        variants_view_stats_name = self.create_annotations_view(
            table=table_variants_from,
            view=variants_view_stats_name,
            view_type="table",
            view_mode="full",
            info_prefix_column=info_prefix_column,
            info_struct_column=info_struct_column,
            sample_struct_column=sample_struct_column,
            formats=["GT"],
            fields_needed=[
                "#CHROM",
                "POS",
                "REF",
                "ALT",
                "QUAL",
                "FILTER",
            ]
            + self.get_header_sample_list(),
        )
        tables_to_remove.append(variants_view_stats_name)

        # stats dict
        stats = {"Infos": {}}

        ### File
        input_file = self.get_input()
        stats["Infos"]["Input file"] = input_file

        # Header
        header_infos = self.get_header().infos
        header_formats = self.get_header().formats
        header_infos_list = list(header_infos)
        header_formats_list = list(header_formats)
        header_table = self.load_header()

        # Stat section
        stats["Stats"] = {}

        ### Variants

        # Variants by chr
        sql_query_nb_variant_by_chrom = f'SELECT "#CHROM" as CHROM, count(*) as count FROM {variants_view_stats_name} GROUP BY "#CHROM"'
        df_nb_of_variants_by_chrom = self.get_query_to_df(sql_query_nb_variant_by_chrom)
        nb_of_variants_by_chrom = df_nb_of_variants_by_chrom.sort_values(
            by=["CHROM"], kind="quicksort"
        )

        # Total number of variants
        nb_of_variants = nb_of_variants_by_chrom["count"].sum()

        # Calculate percentage
        nb_of_variants_by_chrom["percent"] = nb_of_variants_by_chrom["count"].apply(
            lambda x: round((x * 100 / nb_of_variants), percent_round)
        )

        # Add to stats dict the number of variants by chromosome and the total number of variants
        stats["Stats"]["Variants by chromosome"] = nb_of_variants_by_chrom.to_dict(
            orient="index"
        )

        # Add to stats dict the total number of variants
        stats["Infos"]["Number of variants"] = int(nb_of_variants)

        ### Samples

        # Init
        samples = {}
        nb_of_samples = 0

        # Check Samples
        if "GT" in header_formats_list and "FORMAT" in self.get_header_columns():
            log.debug(f"Check samples...")

            # Samples stats
            samples_stats = {}

            # Get samples stats by genotype for each sample in the header
            for sample in self.get_header_sample_list():
                sql_query_samples = f"""
                    SELECT 
                        '{sample}' as 'sample',
                        SAMPLES."{sample}".GT as 'genotype',
                        count(SAMPLES."{sample}".GT) as 'count',
                        ROUND((count(SAMPLES."{sample}".GT)*100/{nb_of_variants}), {percent_round}) as 'percent'
                    FROM {variants_view_stats_name}
                    WHERE  SAMPLES."{sample}".GT IS NOT NULL
                    GROUP BY genotype
                    ORDER BY genotype
                """

                # Get samples stats by genotype for each sample in the header
                sql_query_genotype_df = self.conn.execute(sql_query_samples).df()
                non_null_genotypes = sql_query_genotype_df[
                    sql_query_genotype_df["genotype"].str.contains(r"\d")
                ]
                sample_genotype_count = non_null_genotypes["count"].sum()

                # Add to samples dict the samples stats by genotype for each sample in the header
                if len(sql_query_genotype_df):

                    # Number of samples
                    nb_of_samples += 1

                    # Add to samples dict the samples stats by genotype for each sample in the header
                    samples[sample] = sql_query_genotype_df.to_dict(orient="index")

                    # Add to samples stats dict the samples stats by genotype for each sample in the header
                    samples_stats[sample] = {
                        "Sample": f"{sample}",
                        "count": int(sample_genotype_count),
                        "percent": round(
                            (sample_genotype_count * 100 / nb_of_variants),
                            percent_round,
                        ),
                    }

            # Add to stats dict the samples stats by genotype for each sample in the header
            stats["Samples"] = samples
            stats["Infos"]["Number of samples"] = nb_of_samples
            stats["Stats"]["Variants by sample"] = samples_stats

        else:

            samples_stats = {}

        ### INFO and FORMAT fields
        header_types_df = {}
        header_types_list = {
            "INFO": {
                "label": "List of INFO fields",
                "fields": {
                    "id": "INFO",
                    "number": "Number",
                    "type": "Type",
                    "description": "Description",
                },
            },
            "FORMAT": {
                "label": "List of FORMAT fields",
                "fields": {
                    "id": "FORMAT",
                    "number": "Number",
                    "type": "Type",
                    "description": "Description",
                },
            },
            "FILTER": {
                "label": "List of FILTER fields",
                "fields": {
                    "id": "FILTER",
                    "description": "Description",
                },
            },
        }

        # Init
        header_types_df = {}

        # Get header types for INFO and FORMAT fields
        for header_section, header_info in header_types_list.items():
            label = header_info["label"]
            fields = header_info["fields"]

            # Construire la liste des champs à sélectionner
            select_fields = ", ".join(fields.keys())

            # SQL query
            sql_query_header = f"""
                SELECT {select_fields}
                FROM {header_table}
                WHERE section = '{header_section}'
            """
            header_infos_df = self.get_query_to_df(sql_query_header)
            header_infos_dict = {}

            # Add to header_types_df the header types for INFO and FORMAT fields
            for i, row in header_infos_df.iterrows():
                header_infos_dict[i] = {
                    new if new else original: row[original]
                    for original, new in fields.items()
                }

            # Add to header_types_df the header types for INFO and FORMAT fields
            if len(header_infos_dict):

                # Add to header_types_df the header types for INFO and FORMAT fields
                header_types_df[label] = pd.DataFrame.from_dict(
                    header_infos_dict, orient="index"
                ).to_dict(orient="index")

                # Add to stats dict the number of INFO and FORMAT fields
                stats["Infos"][f"Number of {header_section} fields"] = len(
                    header_types_df[label]
                )

        # Add to stats dict the header types for INFO and FORMAT fields
        stats["Header"] = header_types_df

        # Annotations stats
        if annotations_stats:

            # Init
            sql_queries_info = []

            # Get header infos list
            for field in header_infos_list:

                # Create a table with a field by line (only for INFO section), and le number of distinct value on variants table, and the number of variants with a value
                sql_queries_info.append(
                    f"""
                        SELECT
                            '{field}' AS 'Annotation',
                            count(distinct "{field}") as 'Distinct values',
                            count("{field}") as 'Annotated Variants',
                            ROUND((count("{field}") * 100 / {nb_of_variants}), {percent_round}) as 'Percent',
                        FROM
                            {variants_view_stats_name}
                        WHERE
                            "{field}" IS NOT NULL AND TRIM(CAST("{field}" AS VARCHAR)) NOT IN ('','.')
                    """
                )

            # Join all queries
            sql_query_info = f""" UNION ALL """.join(sql_queries_info)

            # Get info stats
            info_stats = self.get_query_to_df(sql_query_info)

            # Add to stats dict the annotations stats
            stats["Annotations"] = {"Distribution": info_stats.to_dict(orient="index")}

        ### Quality stats
        log.debug(f"Quality stats...")

        ### QUAL
        log.debug(f"Quality stats: QUAL...")

        if "QUAL" in self.get_header_columns():

            # SQL query
            sql_query_qual = f"""
                    SELECT
                        avg(CAST(QUAL AS INTEGER)) AS Average,
                        min(CAST(QUAL AS INTEGER)) AS Minimum,
                        max(CAST(QUAL AS INTEGER)) AS Maximum,
                        stddev(CAST(QUAL AS INTEGER)) AS StandardDeviation,
                        median(CAST(QUAL AS INTEGER)) AS Median,
                        variance(CAST(QUAL AS INTEGER)) AS Variance
                    FROM {variants_view_stats_name}
                    WHERE CAST(QUAL AS VARCHAR) NOT IN ('.')
                    """

            # Get quality stats
            qual_stats = self.conn.execute(sql_query_qual).df().to_dict(orient="index")

        else:

            # Empty quality stats
            qual_stats = {}

        ### FILTER
        log.debug(f"Quality stats: FILTER...")

        if "FILTER" in self.get_header_columns():

            # SQL query
            sql_query_filter = f"""
                WITH split_filter AS (
                    SELECT
                        TRIM(UNNEST(STRING_SPLIT(CASE WHEN TRIM(FILTER) = '' OR FILTER IS NULL THEN '.' ELSE FILTER END, ';'))) AS filter_value
                    FROM
                        {variants_view_stats_name}
                )
                SELECT
                    filter_value,
                    COUNT(*) AS 'count',
                    ROUND((count * 100 / {nb_of_variants}), {percent_round}) AS 'percent'
                FROM
                    split_filter
                GROUP BY
                    filter_value
                ORDER BY
                    count DESC
            """

            # Get filter stats
            filter_stats = (
                self.conn.execute(sql_query_filter).df().to_dict(orient="index")
            )

        else:

            # Empty filter stats
            filter_stats = {}

        ### SNV and InDel

        # SQL query
        sql_query_snv = f"""
            
            SELECT Type, count, ROUND((count * 100 / {nb_of_variants}), {percent_round}) AS 'percent' FROM (

                    SELECT
                        'Total' AS Type,
                        count(*) AS count
                    FROM {variants_view_stats_name}
                    
                    UNION

                    SELECT
                        'SNV' AS Type,
                        count(*) AS count
                    FROM {variants_view_stats_name}
                    WHERE len(REF) = 1 AND len(ALT) = 1

                    UNION

                    SELECT
                        'MNV' AS Type,
                        count(*) AS count
                    FROM {variants_view_stats_name}
                    WHERE len(REF) > 1 AND len(ALT) > 1
                    AND len(REF) = len(ALT)

                    UNION

                    SELECT
                        'InDel' AS Type,
                        count(*) AS count
                    FROM {variants_view_stats_name}
                    WHERE len(REF) > 1 OR len(ALT) > 1
                    AND len(REF) != len(ALT)

                )

            ORDER BY 
            CASE
                WHEN Type = 'Total' THEN 1
                WHEN Type = 'SNV' THEN 2
                WHEN Type = 'MNV' THEN 3
                WHEN Type = 'InDel' THEN 4
            END

                """

        # Get SNV and InDel stats
        snv_indel = self.get_query_to_df(sql_query_snv).to_dict(orient="index")

        # Substitutions
        sql_query_snv_substitution = f"""
                SELECT
                    concat(REF, '>', ALT) AS 'Substitution',
                    count(*) AS count,
                    ROUND((count * 100 / {nb_of_variants}), {percent_round}) AS 'percent'
                FROM {variants_view_stats_name}
                WHERE len(REF) = 1 AND len(ALT) = 1
                GROUP BY REF, ALT
                ORDER BY count(*) DESC
                """
        snv_substitution = self.get_query_to_df(sql_query_snv_substitution).to_dict(
            orient="index"
        )

        # Add to stats dict the SNV and InDel stats
        stats["Stats"]["Variant types"] = snv_indel
        stats["Stats"]["Substitutions"] = snv_substitution
        stats["Stats"]["Quality"] = qual_stats
        stats["Stats"]["Filters"] = filter_stats

        # Queries
        if queries is not None:

            # Create full annotations view
            variants_view_query_name = queries_view
            variants_view_query_name = self.create_annotations_view(
                table=table_variants_from,
                view=variants_view_query_name,
                view_type="view",
                view_mode="explore",
                info_prefix_column="",
                info_struct_column="INFOS",
                sample_struct_column="SAMPLES",
                formats=None,
                fields_needed_all=True,
                drop_view=True,
            )
            if queries_view is not None:
                tables_to_remove.append(variants_view_query_name)

            # Stats queries section
            stats["Queries"] = {}

            # For each query
            for query_infos in queries.items():

                # Query name and query
                query_name = query_infos[0]
                query = query_infos[1]

                # Query cast
                query_cast = cast_columns_query(query=query, conn=self.get_connexion())

                # Query execute
                query_res = self.get_query_to_df(query_cast).to_dict(orient="index")
                stats["Queries"][query_name] = query_res

        # Remove table or view
        self.remove_tables_or_views(tables=tables_to_remove)

        return stats

    def stats_to_file(
        self,
        file: str = None,
        annotations_stats: bool = False,
        queries: dict = None,
        queries_view: str = None,
    ) -> str:
        """
        The function `stats_to_file` takes a file name as input, retrieves statistics, serializes them
        into a JSON object, and writes the JSON object to the specified file.

        :param file: The `file` parameter is a string that represents the file path where the JSON data
        will be written
        :type file: str
        :param annotations_stats: The `annotations_stats` parameter is a boolean that specifies whether
        to calculate annotation statistics. If `annotations_stats` is set to True, annotation statistics
        will be calculated. If `annotations_stats` is set to False, annotation statistics will not be
        calculated. The default value is False.
        :type annotations_stats: bool
        :param queries: The `queries` parameter is a dictionary that contains queries to be executed
        and added to the statistics. The keys of the dictionary are the names of the queries, and the
        values are the SQL queries to be executed.
        :type queries: dict
        :param queries_view: The `queries_view` parameter is a string that represents the name of the
        view to be used for the queries. If no value is provided, a new view will be created.
        :type queries_view: str

        :return: The name of the file that was written to.
        """

        # Get stats
        stats = self.get_stats(
            annotations_stats=annotations_stats,
            queries=queries,
            queries_view=queries_view,
        )

        # Serializing json
        json_object = json.dumps(stats, indent=4)

        # Writing to sample.json
        with open(file, "w") as outfile:
            outfile.write(json_object)

        return file

    def print_stats(
        self,
        stdout: bool = False,
        output_file: str = None,
        json_file: str = None,
        html_file: str = None,
        pdf_file: str = None,
        annotations_stats: bool = False,
        queries: dict = None,
        queries_view: str = None,
    ) -> None:
        """
        The `print_stats` function generates a markdown file and prints the statistics contained in a
        JSON file in a formatted manner.

        :param stdout: The `stdout` parameter is a boolean that specifies whether to print the stats
        directly to the standard output. If `stdout` is set to True, the stats will be printed to the
        standard output. If `stdout` is set to False, the stats will not be printed to the standard
        output. The default value is False.
        :type stdout: bool
        :param output_file: The `output_file` parameter is a string that specifies the path and filename
        of the output file where the stats will be printed in Markdown format. If no `output_file` is
        provided, a temporary directory will be created and the stats will be saved in a file named
        "stats.md" within that
        :type output_file: str
        :param json_file: The `json_file` parameter is a string that represents the path to the JSON
        file where the statistics will be saved. If no value is provided, a temporary directory will be
        created and a default file name "stats.json" will be used
        :type json_file: str
        :param html_file: The `html_file` parameter is a string that specifies the path and filename of
        the output file where the stats will be printed in HTML format. If no `html_file` is provided,
        a temporary directory will be created and the stats will be saved in a file named "stats.html"
        within that
        :type html_file: str
        :param pdf_file: The `pdf_file` parameter is a string that specifies the path and filename of the
        output file where the stats will be printed in PDF format. If no `pdf_file` is provided, a
        temporary directory will be created and the stats will be saved in a file named "stats.pdf"
        within that
        :type pdf_file: str
        :param annotations_stats: Whether to calculate annotation statistics. Defaults to False.
        :type annotations_stats: bool, optional
        :param queries: The `queries` parameter is a dictionary that contains queries to be executed
        and added to the statistics. The keys of the dictionary are the names of the queries, and the
        values are the SQL queries to be executed.
        :type queries: dict
        :param queries_view: The `queries_view` parameter is a string that represents the name of the
        view to be used for the queries. If no value is provided, a new view will be created.
        :type queries_view: str
        of `None`.

        :return: The function `print_stats` does not return any value. It has a return type annotation

        """

        # Full path
        output_file = full_path(output_file)
        json_file = full_path(json_file)

        # Create stats file in temporary directory
        with tempfile.TemporaryDirectory() as tmpdir:

            # Files
            if not output_file:
                output_file = os.path.join(tmpdir, "stats.md")
            if not json_file:
                json_file = os.path.join(tmpdir, "stats.json")

            # Create folders
            if not os.path.exists(os.path.dirname(output_file)):
                Path(os.path.dirname(output_file)).mkdir(parents=True, exist_ok=True)
            if not os.path.exists(os.path.dirname(json_file)):
                Path(os.path.dirname(json_file)).mkdir(parents=True, exist_ok=True)

            # Create stats JSON file
            stats_file = self.stats_to_file(
                file=json_file,
                annotations_stats=annotations_stats,
                queries=queries,
                queries_view=queries_view,
            )

            # Print stats file
            with open(stats_file) as f:
                stats = yaml.safe_load(f)

            # Output
            output_title = []
            output_index = []
            output = []

            # Title
            output_title.append("# HOWARD Stats")

            # Index
            output_index.append("## Table of context")

            # Process sections
            for section in stats:
                infos = stats.get(section)
                section_link = "#" + section.lower().replace(" ", "-")
                output.append(f"\n")
                output.append(f"## {section}")
                output_index.append(f"- [{section}]({section_link})")

                if len(infos):

                    # For each info
                    for info in infos:

                        # Check if dataframe or not
                        try:
                            df = pd.DataFrame.from_dict(infos.get(info), orient="index")
                            is_df = True
                        except:
                            try:
                                df = pd.DataFrame.from_dict(
                                    json.loads((infos.get(info))), orient="index"
                                )
                                is_df = True
                            except:
                                is_df = False

                        # If dataframe is a dataframe
                        if is_df:
                            df = df.map(escape_markdown_table_chars)
                            output.append(f"### {info}")
                            info_link = "#" + info.lower().replace(" ", "-")
                            output_index.append(f"   - [{info}]({info_link})")
                            output.append(f"{df.to_markdown(index=False)}")

                        # If not a dataframe
                        else:
                            output.append(f"- {info}: {infos.get(info)}")

                else:

                    # If no info
                    output.append(f"NA")

            # Write stats in markdown file
            with open(output_file, "w") as fp:
                for item in output_title:
                    fp.write("%s\n" % item)
                fp.write("\n")
                for item in output_index:
                    fp.write("%s\n" % item)
                fp.write("\n")
                for item in output:
                    fp.write("%s\n" % item)

            # Output stats in markdown
            if stdout:
                print("")
                print("\n\n".join(output_title))
                print("")
                print("\n\n".join(output))
                print("")

            # Generate HTML and PDF files
            if html_file:
                convert_markdown_to_html(output_file, html_file)
            if pdf_file:
                convert_markdown_to_pdf(output_file, pdf_file)

        return None

    def get_input(self) -> str:
        """
        It returns the value of the input variable.
        :return: The input is being returned.
        """
        return self.input

    def get_input_format(self, input_file: str = None) -> str:
        """
        This function returns the format of the input variable, either from the provided input file or
        by prompting for input.

        :param input_file: The `input_file` parameter in the `get_input_format` method is a string that
        represents the file path of the input file. If no `input_file` is provided when calling the
        method, it will default to `None`
        :type input_file: str
        :return: The format of the input variable is being returned.
        """

        if not input_file:
            input_file = self.get_input()
        input_format = get_file_format(input_file)
        return input_format

    def get_input_compressed(self, input_file: str = None) -> str:
        """
        The function `get_input_compressed` returns the format of the input variable after compressing
        it.

        :param input_file: The `input_file` parameter in the `get_input_compressed` method is a string
        that represents the file path of the input file. If no `input_file` is provided when calling the
        method, it will default to `None` and the method will then call `self.get_input()` to
        :type input_file: str
        :return: The function `get_input_compressed` returns the compressed format of the input
        variable.
        """

        if not input_file:
            input_file = self.get_input()
        input_compressed = get_file_compressed(input_file)
        return input_compressed

    def get_output(self) -> str:
        """
        It returns the output of the neuron.
        :return: The output of the neural network.
        """

        return self.output

    def get_output_format(self, output_file: str = None) -> str:
        """
        The function `get_output_format` returns the format of the input variable or the output file if
        provided.

        :param output_file: The `output_file` parameter in the `get_output_format` method is a string
        that represents the file path of the output file. If no `output_file` is provided when calling
        the method, it will default to the output obtained from the `get_output` method of the class
        instance. The
        :type output_file: str
        :return: The format of the input variable is being returned.
        """

        if not output_file:
            output_file = self.get_output()
        output_format = get_file_format(output_file)

        return output_format

    def get_config(self) -> dict:
        """
        It returns the config
        :return: The config variable is being returned.
        """
        return self.config

    def get_param(self) -> dict:
        """
        It returns the param
        :return: The param variable is being returned.
        """
        return self.param

    def get_connexion_db(self) -> str:
        """
        It returns the connexion_db attribute of the object
        :return: The connexion_db is being returned.
        """
        return self.connexion_db

    def get_prefix(self) -> str:
        """
        It returns the prefix of the object.
        :return: The prefix is being returned.
        """
        return self.prefix

    def get_table_variants(self, clause: str = "select") -> str:
        """
        This function returns the table_variants attribute of the object

        :param clause: the type of clause the table will be used. Either "select" or "from" (optional),
        defaults to select (optional)
        :return: The table_variants attribute of the object.
        """

        # Access
        access = self.get_config().get("access", None)

        # Clauses "select", "where", "update"
        if clause in ["select", "where", "update"]:
            table_variants = self.table_variants
        # Clause "from"
        elif clause in ["from"]:
            # For Read Only
            if self.get_input_format() in ["parquet"] and access in ["RO"]:
                input_file = self.get_input()
                table_variants = f"'{input_file}' as variants"
            # For Read Write
            else:
                table_variants = f"{self.table_variants} as variants"
        else:
            table_variants = self.table_variants
        return table_variants

    def get_tmp_dir(self) -> str:
        """
        The function `get_tmp_dir` returns the temporary directory path based on configuration
        parameters or a default path.
        :return: The `get_tmp_dir` method is returning the temporary directory path based on the
        configuration, parameters, and a default value of "/tmp".
        """

        return get_tmp(
            config=self.get_config(), param=self.get_param(), default_tmp="/tmp"
        )

    def get_connexion_type(self) -> str:
        """
        If the connexion type is not in the list of allowed connexion types, raise a ValueError

        :return: The connexion type is being returned.
        """
        return self.get_config().get("connexion_type", "memory")

    def get_connexion(self):
        """
        It returns the connection object

        :return: The connection object.
        """
        return self.conn

    def close_connexion(self) -> str:
        """
        This function closes the connection to the database.
        :return: The connection is being closed.
        """

        log.debug(f"Close connexion...")
        self.conn.close()

        connexion_db = self.get_connexion_db()

        # Remove connexion db file
        if os.path.exists(connexion_db) and connexion_db != self.get_connexion_type():
            log.debug(f"Remove connexion db file: {connexion_db}")
            remove_if_exists([connexion_db])

        log.debug(f"Connexion '{connexion_db}' closed.")

        return connexion_db

    def get_header(self, type: str = "vcf"):
        """
        This function returns the header of the VCF file as a list of strings

        :param type: the type of header you want to get, defaults to vcf (optional)
        :return: The header of the vcf file.
        """

        if self.header_vcf:
            if type == "vcf":
                return self.header_vcf
            elif type == "list":
                return self.header_list
        else:
            if type == "vcf":
                header = vcf.Reader(io.StringIO("\n".join(vcf_required)))
                return header
            elif type == "list":
                return vcf_required

    def get_header_infos_list(self) -> list:
        """
        This function retrieves a list of information fields from the header.
        :return: A list of information fields from the header.
        """

        # Init
        infos_list = []

        for field in self.get_header().infos:
            infos_list.append(field)

        return infos_list

    def get_header_length(self, file: str = None) -> int:
        """
        The function `get_header_length` returns the length of the header list, excluding the #CHROM
        line.

        :param file: The `file` parameter is an optional argument that specifies the path to a VCF
        header file. If this argument is provided, the function will read the header from the specified
        file and return the length of the header list minus 1 (to exclude the #CHROM line)
        :type file: str
        :return: the length of the header list, excluding the #CHROM line.
        """

        if file:
            return len(self.read_vcf_header_file(file=file)) - 1
        elif self.get_header(type="list"):
            return len(self.get_header(type="list")) - 1
        else:
            return 0

    def get_header_columns(self) -> str:
        """
        This function returns the header list of a VCF

        :return: The length of the header list.
        """
        if self.get_header():
            return self.get_header(type="list")[-1]
        else:
            return ""

    def get_header_columns_as_list(self) -> list:
        """
        This function returns the header list of a VCF

        :return: The length of the header list.
        """
        if self.get_header():
            return self.get_header_columns().strip().split("\t")
        else:
            return []

    def get_header_columns_as_sql(self) -> str:
        """
        This function retruns header length (without #CHROM line)

        :return: The length of the header list.
        """
        sql_column_list = []
        for col in self.get_header_columns_as_list():
            sql_column_list.append(f'"{col}"')
        return ",".join(sql_column_list)

    def get_header_sample_list(
        self, check: bool = False, samples: list = None, samples_force: bool = False
    ) -> list:
        """
        The function `get_header_sample_list` returns a list of samples from a VCF header, with optional
        checking and filtering based on input parameters.

        :param check: The `check` parameter in the `get_header_sample_list` function is a boolean
        parameter that determines whether to check if the samples in the list are properly defined as
        genotype columns. If `check` is set to `True`, the function will verify if each sample in the
        list is defined as a, defaults to False
        :type check: bool (optional)
        :param samples: The `samples` parameter in the `get_header_sample_list` function is a list that
        allows you to specify a subset of samples from the header. If you provide a list of sample
        names, the function will check if each sample is defined in the header. If a sample is not found
        in the
        :type samples: list
        :param samples_force: The `samples_force` parameter in the `get_header_sample_list` function is
        a boolean parameter that determines whether to force the function to return the sample list
        without checking if the samples are genotype columns. If `samples_force` is set to `True`, the
        function will return the sample list without performing, defaults to False
        :type samples_force: bool (optional)
        :return: The function `get_header_sample_list` returns a list of samples based on the input
        parameters and conditions specified in the function.
        """

        # Init
        samples_list = []

        if samples is None:
            samples_list = self.header_vcf.samples
        else:
            samples_checked = []
            for sample in samples:
                if sample in self.header_vcf.samples:
                    samples_checked.append(sample)
                else:
                    log.warning(f"Sample '{sample}' not defined in header")
            samples_list = samples_checked

            # Force sample list without checking if is_genotype_column
            if samples_force:
                log.warning(f"Samples {samples_list} not checked if genotypes")
                return samples_list

        if check:
            samples_checked = []
            for sample in samples_list:
                if self.is_genotype_column(column=sample):
                    samples_checked.append(sample)
                else:
                    log.warning(
                        f"Sample '{sample}' not defined as a sample (genotype not well defined)"
                    )
            samples_list = samples_checked

        # Return samples list
        return samples_list

    def sort_contigs(self) -> None:
        """
        This function sort contigs

        :return: None
        """

        # Sort contigs
        header = self.get_header()
        header = sort_contigs(header)

        # Return
        return None

    def is_genotype_column(self, column: str = None) -> bool:
        """
        This function checks if a given column is a genotype column in a database.

        :param column: The `column` parameter in the `is_genotype_column` method is a string that
        represents the column name in a database table. This method checks if the specified column is a
        genotype column in the database. If a column name is provided, it calls the `is_genotype_column`
        method of
        :type column: str
        :return: The `is_genotype_column` method is returning a boolean value. If the `column` parameter
        is not None, it calls the `is_genotype_column` method of the `Database` class with the specified
        column name and returns the result. If the `column` parameter is None, it returns False.
        """

        if column is not None:
            return Database(database=self.get_input()).is_genotype_column(column=column)
        else:
            return False

    def get_verbose(self) -> bool:
        """
        It returns the value of the "verbose" key in the config dictionary, or False if the key doesn't
        exist

        :return: The value of the key "verbose" in the config dictionary.
        """
        return self.get_config().get("verbose", False)

    def get_connexion_format(self) -> str:
        """
        It returns the connexion format of the object.
        :return: The connexion_format is being returned.
        """
        connexion_format = self.connexion_format
        if connexion_format not in ["duckdb", "sqlite"]:
            log.error(f"Unknown connexion format {connexion_format}")
            raise ValueError(f"Unknown connexion format {connexion_format}")
        else:
            return connexion_format

    def insert_file_to_table(
        self,
        file,
        columns: str,
        header_len: int = 0,
        sep: str = "\t",
        chunksize: int = 1000000,
    ) -> None:
        """
        The function reads a file in chunks and inserts each chunk into a table based on the specified
        database format.

        :param file: The `file` parameter is the file that you want to load into a table. It should be
        the path to the file on your system
        :param columns: The `columns` parameter in the `insert_file_to_table` function is a string that
        should contain the names of the columns in the table where the data will be inserted. The column
        names should be separated by commas within the string. For example, if you have columns named
        "id", "name
        :type columns: str
        :param header_len: The `header_len` parameter in the `insert_file_to_table` function specifies
        the number of lines to skip at the beginning of the file before reading the actual data. This
        parameter allows you to skip any header information present in the file before processing the
        data, defaults to 0
        :type header_len: int (optional)
        :param sep: The `sep` parameter in the `insert_file_to_table` function is used to specify the
        separator character that is used in the file being read. In this case, the default separator is
        set to `\t`, which represents a tab character. You can change this parameter to a different
        separator character if, defaults to \t
        :type sep: str (optional)
        :param chunksize: The `chunksize` parameter specifies the number of rows to read in at a time
        when processing the file in chunks. In the provided code snippet, the default value for
        `chunksize` is set to 1000000. This means that the file will be read in chunks of 1,, defaults
        to 1000000
        :type chunksize: int (optional)
        """

        # Config
        chunksize = self.get_config().get("load", {}).get("chunk", chunksize)
        connexion_format = self.get_connexion_format()

        log.debug("chunksize: " + str(chunksize))

        if chunksize:
            for chunk in pd.read_csv(
                file, skiprows=header_len, sep=sep, chunksize=chunksize, engine="c"
            ):
                if connexion_format in ["duckdb"]:
                    sql_insert_into = (
                        f"INSERT INTO variants ({columns}) SELECT {columns} FROM chunk"
                    )
                    self.conn.execute(sql_insert_into)
                elif connexion_format in ["sqlite"]:
                    chunk.to_sql("variants", self.conn, if_exists="append", index=False)

    def load_data(
        self,
        input_file: str = None,
        drop_variants_table: bool = False,
        sample_size: int = 20480,
    ) -> None:
        """
        The `load_data` function reads a VCF file and inserts it into a table, with options to drop the
        table before loading the data and specify a sample size.

        :param input_file: The path to the input file. This is the VCF file that will be loaded into the
        table
        :type input_file: str
        :param drop_variants_table: The `drop_variants_table` parameter is a boolean flag that
        determines whether the variants table should be dropped before loading the data. If set to
        `True`, the variants table will be dropped. If set to `False` (default), the variants table will
        not be dropped, defaults to False
        :type drop_variants_table: bool (optional)
        :param sample_size: The `sample_size` parameter determines the number of rows to be sampled from
        the input file. If it is set to `None`, the default value of 20480 will be used, defaults to
        20480
        :type sample_size: int (optional)
        """

        log.info("Loading...")

        # change input file
        if input_file:
            self.set_input(input_file)
            self.set_header()

        # drop variants table
        if drop_variants_table:
            self.drop_variants_table()

        # get table variants
        table_variants = self.get_table_variants()

        # Access
        access = self.get_config().get("access", None)
        log.debug(f"access: {access}")

        # Input format and compress
        input_format = self.get_input_format()
        input_compressed = self.get_input_compressed()
        log.debug(f"input_format: {input_format}")
        log.debug(f"input_compressed: {input_compressed}")

        # input_compressed_format
        if input_compressed:
            input_compressed_format = "gzip"
        else:
            input_compressed_format = "none"
        log.debug(f"input_compressed_format: {input_compressed_format}")

        # Connexion format
        connexion_format = self.get_connexion_format()

        # Sample size
        if not sample_size:
            sample_size = -1
        log.debug(f"sample_size: {sample_size}")

        # Load data
        log.debug(f"Load Data from {input_format}")

        # DuckDB connexion
        if connexion_format in ["duckdb"]:

            # Database already exists
            if input_format in ["db", "duckdb"]:

                if connexion_format in ["duckdb"]:
                    log.debug(f"Input file format '{input_format}' duckDB")
                else:
                    log.error(
                        f"Input file format '{input_format}' not compatilbe with database format '{connexion_format}'"
                    )
                    raise ValueError(
                        f"Input file format '{input_format}' not compatilbe with database format '{connexion_format}'"
                    )

            # Load from existing database format
            else:

                try:
                    # Create Table or View
                    database = Database(database=self.input)
                    sql_from = database.get_sql_from(sample_size=sample_size)

                    log.debug(f"Load Data into {table_variants}...")
                    if access in ["RO"]:
                        sql_load = (
                            f"CREATE VIEW {table_variants} AS SELECT * FROM {sql_from}"
                        )
                    else:
                        sql_load = (
                            f"CREATE TABLE {table_variants} AS SELECT * FROM {sql_from}"
                        )
                    self.conn.execute(sql_load)
                    log.debug(f"Load Data into {table_variants} - done.")

                except Exception as e:
                    # Format not available
                    msg_err = f"Load Data into {table_variants} - failed to load data: {str(e)}"
                    log.error(msg_err)
                    raise ValueError(msg_err)

        # SQLite connexion
        elif connexion_format in ["sqlite"] and input_format in [
            "vcf",
            "tsv",
            "csv",
            "psv",
        ]:

            # Main structure
            structure = {
                "#CHROM": "VARCHAR",
                "POS": "INTEGER",
                "ID": "VARCHAR",
                "REF": "VARCHAR",
                "ALT": "VARCHAR",
                "QUAL": "VARCHAR",
                "FILTER": "VARCHAR",
                "INFO": "VARCHAR",
            }

            # Strcuture with samples
            structure_complete = structure
            if self.get_header_sample_list():
                structure["FORMAT"] = "VARCHAR"
                for sample in self.get_header_sample_list():
                    structure_complete[sample] = "VARCHAR"

            # Columns list for create and insert
            sql_create_table_columns = []
            sql_create_table_columns_list = []
            for column in structure_complete:
                column_type = structure_complete[column]
                sql_create_table_columns.append(
                    f'"{column}" {column_type} default NULL'
                )
                sql_create_table_columns_list.append(f'"{column}"')

            # Create database
            log.debug(f"Create Table {table_variants}")
            sql_create_table_columns_sql = ", ".join(sql_create_table_columns)
            sql_create_table_columns_list_sql = ", ".join(sql_create_table_columns_list)
            sql_create_table = f"CREATE TABLE IF NOT EXISTS {table_variants} ({sql_create_table_columns_sql})"
            self.conn.execute(sql_create_table)

            # chunksize define length of file chunk load file
            chunksize = 100000

            # delimiter
            delimiter = file_format_delimiters.get(input_format, "\t")

            # Load the input file
            with open(self.input, "rt") as input_file:

                # Use the appropriate file handler based on the input format
                if input_compressed:
                    input_file = bgzf.open(self.input, "rt")
                if input_format in ["vcf"]:
                    header_len = self.get_header_length()
                else:
                    header_len = 0

                # Insert the file contents into a table
                self.insert_file_to_table(
                    input_file,
                    columns=sql_create_table_columns_list_sql,
                    header_len=header_len,
                    sep=delimiter,
                    chunksize=chunksize,
                )

        else:
            log.error(
                f"Connexion format '{connexion_format}' not available with format '{input_format}'"
            )
            raise ValueError(
                f"Connexion format '{connexion_format}' not available with format '{input_format}'"
            )

        # Add INFO column if not exists
        if access not in ["RO"] and "INFO" not in self.get_header_columns_as_list():
            log.debug("INFO column not found, adding it")
            # Add INFO column
            self.add_column(
                table_name=table_variants,
                column_name="INFO",
                column_type="VARCHAR",
                default_value=None,
            )

        # # Explode INFOS fields into table fields
        # if self.get_explode_infos():
        #     self.explode_infos(
        #         prefix=self.get_explode_infos_prefix(),
        #         fields=self.get_explode_infos_fields(),
        #         force=True,
        #     )

        # Create index after insertion
        self.create_indexes()

    def get_explode_infos(self) -> bool:
        """
        The function `get_explode_infos` returns the value of the "explode_infos" parameter, defaulting
        to False if it is not set.
        :return: The method is returning the value of the "explode_infos" parameter, which is a boolean
        value. If the parameter is not present, it will return False.
        """

        return self.get_param().get("explode", {}).get("explode_infos", False)

    def get_explode_infos_fields(
        self,
        explode_infos_fields: str = None,
        remove_fields_not_in_header: bool = False,
    ) -> list:
        """
        The `get_explode_infos_fields` function returns a list of exploded information fields based on
        the input parameter `explode_infos_fields`.

        :param explode_infos_fields: The `explode_infos_fields` parameter is a string that specifies the
        fields to be exploded. It can be set to "ALL" to explode all fields, or it can be a
        comma-separated list of field names to explode
        :type explode_infos_fields: str
        :param remove_fields_not_in_header: The parameter `remove_fields_not_in_header` is a boolean
        flag that determines whether to remove fields that are not present in the header. If it is set
        to `True`, any field that is not in the header will be excluded from the list of exploded
        information fields. If it is set to `, defaults to False
        :type remove_fields_not_in_header: bool (optional)
        :return: The function `get_explode_infos_fields` returns a list of exploded information fields.
        If the `explode_infos_fields` parameter is not provided or is set to None, it returns an empty
        list. If the parameter is provided and its value is "ALL", it also returns an empty list.
        Otherwise, it returns a list of exploded information fields after removing any spaces and
        splitting the string by commas.
        """

        # If no fields, get it in param
        if not explode_infos_fields:
            explode_infos_fields = (
                self.get_param().get("explode", {}).get("explode_infos_fields", None)
            )

        # If no fields, defined as all fields in header using keyword
        if not explode_infos_fields:
            explode_infos_fields = "*"

        # If fields list not empty
        if explode_infos_fields:

            # Input fields list
            if isinstance(explode_infos_fields, str):
                fields_input = explode_infos_fields.split(",")
            elif isinstance(explode_infos_fields, list):
                fields_input = explode_infos_fields
            else:
                fields_input = []

            # Fields list without * keyword
            fields_without_all = fields_input.copy()
            if "*".casefold() in (item.casefold() for item in fields_without_all):
                fields_without_all.remove("*")

            # Fields in header
            fields_in_header = sorted(list(set(self.get_header().infos)))

            # Construct list of fields
            fields_output = []
            for field in fields_input:

                # Strip field
                field = field.strip()

                # format keyword * in regex
                if field.upper() in ["*"]:
                    field = ".*"

                # Find all fields with pattern
                # Check if field is in header (to prevent special caracters in field such as '+', e.g. 'GERP++_RS')
                if field in fields_in_header:
                    fields_search = [field]
                else:
                    r = re.compile(rf"^{field}$")
                    fields_search = sorted(list(filter(r.match, fields_in_header)))

                # Remove fields input from search
                if field in fields_search:
                    fields_search = [field]
                elif fields_search != [field]:
                    fields_search = sorted(
                        list(set(fields_search).difference(fields_input))
                    )

                # If field is not in header (avoid not well formatted header)
                if not fields_search and not remove_fields_not_in_header:
                    fields_search = [field]

                # Add found fields
                for new_field in fields_search:
                    # Add field, if not already exists, and if it is in header (if asked)
                    if (
                        new_field not in fields_output
                        and (
                            not remove_fields_not_in_header
                            or new_field in fields_in_header
                        )
                        and new_field not in [".*"]
                    ):
                        fields_output.append(new_field)

            return fields_output

        else:

            return []

    def get_explode_infos_prefix(self, explode_infos_prefix: str = None) -> str:
        """
        The function `get_explode_infos_prefix` returns the value of the `explode_infos_prefix` parameter, or
        the value of `self.get_param().get("explode_infos_prefix", None)` if `explode_infos_prefix` is
        not provided.

        :param explode_infos_prefix: The parameter `explode_infos_prefix` is a string that specifies a
        prefix to be used for exploding or expanding information
        :type explode_infos_prefix: str
        :return: the value of the variable `explode_infos_prefix`.
        """

        if not explode_infos_prefix:
            explode_infos_prefix = (
                self.get_param().get("explode", {}).get("explode_infos_prefix", "")
            )

        return explode_infos_prefix

    def add_column(
        self,
        table_name,
        column_name,
        column_type,
        default_value=None,
        drop: bool = False,
    ) -> dict:
        """
        The `add_column` function adds a column to a SQLite or DuckDB table with a default value if it
        doesn't already exist.

        :param table_name: The name of the table to which you want to add a column
        :param column_name: The parameter "column_name" is the name of the column that you want to add
        to the table
        :param column_type: The `column_type` parameter specifies the data type of the column that you
        want to add to the table. It should be a string that represents the desired data type, such as
        "INTEGER", "TEXT", "REAL", etc
        :param default_value: The `default_value` parameter is an optional parameter that specifies the
        default value for the newly added column. If a default value is provided, it will be assigned to
        the column for any existing rows that do not have a value for that column
        :param drop: The `drop` parameter is a boolean flag that determines whether to drop the column
        if it already exists in the table. If `drop` is set to `True`, the function will drop the
        existing column before adding the new column. If `drop` is set to `False` (default),, defaults
        to False
        :type drop: bool (optional)
        :return: a boolean value indicating whether the column was successfully added to the table.
        """

        # added
        added = False
        dropped = False

        # Check if the column already exists in the table
        query = f""" SELECT * FROM {table_name} LIMIT 0 """
        columns = self.get_query_to_df(query).columns.tolist()
        if column_name.upper() in [c.upper() for c in columns]:
            log.debug(
                f"The {column_name} column already exists in the {table_name} table"
            )
            if drop:
                self.drop_column(table_name=table_name, column_name=column_name)
                dropped = True
            else:
                return None
        else:
            log.debug(f"The {column_name} column NOT exists in the {table_name} table")

        # Add column in table
        add_column_query = (
            f""" ALTER TABLE {table_name} ADD COLUMN "{column_name}" {column_type} """
        )
        if default_value is not None:
            add_column_query += f" DEFAULT {default_value}"
        log.debug(f"add_column_query: {add_column_query}")
        self.execute_query(add_column_query)
        added = not dropped
        log.debug(
            f"The {column_name} column was successfully added to the {table_name} table"
        )

        if added:
            added_column = {
                "table_name": table_name,
                "column_name": column_name,
                "column_type": column_type,
                "default_value": default_value,
            }
        else:
            added_column = None

        return added_column

    def drop_column(
        self, column: dict = None, table_name: str = None, column_name: str = None
    ) -> bool:
        """
        The `drop_column` function drops a specified column from a given table in a database and returns
        True if the column was successfully dropped, and False if the column does not exist in the
        table.

        :param column: The `column` parameter is a dictionary that contains information about the column
        you want to drop. It has two keys:
        :type column: dict
        :param table_name: The `table_name` parameter is the name of the table from which you want to
        drop a column
        :type table_name: str
        :param column_name: The `column_name` parameter is the name of the column that you want to drop
        from the table
        :type column_name: str
        :return: a boolean value. It returns True if the column was successfully dropped from the table,
        and False if the column does not exist in the table.
        """

        # Find column infos
        if column:
            if isinstance(column, dict):
                table_name = column.get("table_name", None)
                column_name = column.get("column_name", None)
            elif isinstance(column, str):
                table_name = self.get_table_variants()
                column_name = column
            else:
                table_name = None
                column_name = None

        if not table_name and not column_name:
            return False

        # Removed
        removed = False

        # Check if the column already exists in the table
        query = f""" SELECT * FROM {table_name} LIMIT 0 """
        columns = self.get_query_to_df(query).columns.tolist()
        if column_name in columns:
            log.debug(f"The {column_name} column exists in the {table_name} table")
        else:
            log.debug(f"The {column_name} column NOT exists in the {table_name} table")
            return False

        # Add column in table # ALTER TABLE integers DROP k
        add_column_query = f""" ALTER TABLE {table_name} DROP "{column_name}" """
        self.execute_query(add_column_query)
        removed = True
        log.debug(
            f"The {column_name} column was successfully dropped to the {table_name} table"
        )

        return removed

    def get_batch_split(
        self,
        table: str = None,
        block: int = 1000,
        nb_lines: int = None,
        use_memory: bool = True,
    ) -> int:
        """
        Calculate the batch size for processing data based on the number of rows in the table and available memory.

        Args:
            table (str, optional): The name of the table to evaluate. If None, the default variants table is used.
            block (int, optional): The block size to use for the calculation. Default is 1000.
            nb_lines (int, optional): The number of lines in the table. If None, it will be calculated.
            use_memory (bool, optional): Whether to consider available memory in the calculation to ponderate block size (memory*block). Default is True.

        Returns:
            int: The calculated batch size.
        """

        # Get table variants if no table
        if table is None:
            table = self.get_table_variants()

        # Evaluate split
        log.debug("Evaluate batch size by parameter")

        # Count numbber of variants in table variants
        if nb_lines is None:
            nb_lines = (
                self.get_connexion()
                .execute(
                    f"""
                        SELECT count(1)
                        FROM {table}
                    """
                )
                .fetchone()[0]
            )

        # Check memory
        if not use_memory:
            memory = 1
        else:
            memory = extract_memory_in_go(
                get_memory(self.get_config(), self.get_param())
            )

        # Avaluate block size using block size (e.g. 1000 viarants) and memory
        block_size = block * memory

        # Calculate batch
        batch = round(nb_lines / block_size) + 1

        # Return
        return batch

    def explode_infos(
        self,
        prefix: str = None,
        create_index: bool = False,
        fields: list = None,
        fields_just_add: list = [],
        fields_not_exists: bool = True,
        detect_type_list: bool = True,
        force: bool = False,
        proccess_all_fields_together: bool = False,
        fields_forced_as_varchar: bool = False,
        table: str = None,
        table_source: str = None,
        table_dest: str = None,
        table_key: list = None,
    ) -> list:
        """
        Explode the INFO fields of a VCF file into individual columns in a specified table.

        Args:
            prefix (str, optional): A prefix for the exploded INFO fields. If not provided, the function
                will use the value of `self.get_explode_infos_prefix()`.
            create_index (bool, optional): Whether to create indexes on the exploded INFO fields. Defaults to False.
            fields (list, optional): A list of INFO fields to explode into individual columns. If not provided,
                all INFO fields will be exploded.
            fields_just_add (list, optional): A list of INFO fields to add as individual columns without exploding values.
            fields_not_exists (bool, optional): Whether to add fields that do not exist in the table. Defaults to True.
            detect_type_list (bool, optional): Whether to detect if the field is a list type. Defaults to True.
            force (bool, optional): Whether to drop and recreate a column if it already exists in the table. Defaults to False.
            proccess_all_fields_together (bool, optional): Whether to process all INFO fields together or individually.
                Defaults to False.
            fields_forced_as_varchar (bool, optional): Whether to force all fields to be treated as VARCHAR. Defaults to False.
            table (str, optional): The name of the table where the exploded INFO fields will be added as individual columns.
            table_source (str, optional): The name of the source table containing the INFO fields.
            table_dest (str, optional): The name of the destination table where the exploded INFO fields will be added.
            table_key (list, optional): A list of keys to use for identifying rows in the table.

        Returns:
            list: A list of added columns.
        """

        # drop indexes
        self.drop_indexes()

        # connexion format
        connexion_format = self.get_connexion_format()
        if connexion_format in ["sqlite"]:
            msg_err = (
                f"Connexion format '{connexion_format}' not available for explode infos"
            )
            log.error(msg_err)
            raise ValueError(msg_err)

        # Access
        access = self.get_config().get("access", None)

        # Added columns
        added_columns = []

        if access not in ["RO"]:

            # Translate fields if patterns
            fields = self.get_explode_infos_fields(explode_infos_fields=fields)

            if fields is None or len(fields) == 0:
                return []

            # prefix
            if prefix in [None, True] or not isinstance(prefix, str):
                if self.get_explode_infos_prefix() not in [None, True]:
                    prefix = self.get_explode_infos_prefix()
                else:
                    prefix = "INFO/"

            # table variants
            if table is None:
                table = self.get_table_variants(clause="select")

            # table source
            if table_source is None:
                table_source = table

            # table dest
            if table_dest is None:
                table_dest = table

            # table key
            if table_key is None:
                table_key = ["#CHROM", "POS", "REF", "ALT"]

            # Check source table columns
            try:
                table_source_struct = self.get_columns(table=table_source)
            except:
                table_source_struct = []
            try:
                table_dest_struct = self.get_columns(table=table_dest)
            except:
                table_dest_struct = []

            if "INFO" not in table_source_struct:
                msg_err = f"Column 'INFO' not found in table '{table_source}'"
                log.warning(msg_err)
                # return None
                # raise ValueError(msg_err)

            # Header infos
            header_infos = self.get_header().infos

            log.debug(
                f"Explode INFO fields - [{len(header_infos)}] annotations fields in header"
            )

            # Create view with all fields
            view_source = "view_source_" + str(random.randint(10000, 100000))
            view_source = self.create_annotations_view(
                table=table_source,
                fields=fields,
                view=view_source,
                view_type="view",
                view_mode="explore",
                info_prefix_column=prefix,
                fields_needed=table_key,
                fields_not_exists=fields_not_exists,
                fields_forced_as_varchar=fields_forced_as_varchar,
                detect_type_list=detect_type_list,
            )

            # Describe view source
            describe_query = f"DESCRIBE {view_source}"
            res = self.execute_query(describe_query)
            description_dict = {row[0]: {"type": row[1]} for row in res.fetchall()}

            # View source structure
            view_source_struct = self.get_columns(table=view_source)

            # Set fields
            sql_info_alter_table_array = []

            for info in fields:

                info_id_sql = prefix + info

                if info_id_sql in table_dest_struct:
                    log.debug(f"Field '{info_id_sql}' already exists in table")

                if (
                    info_id_sql in view_source_struct
                    and "INFO" in table_source_struct
                    and (info_id_sql not in table_dest_struct or force)
                ):

                    if "INFO" not in table_source_struct:
                        msg_err = f"Column 'INFO' not found in table '{table_source}' - Column 'INFO' needed!!!"
                        log.error(msg_err)
                        raise ValueError(msg_err)

                    if info_id_sql in table_dest_struct and force:
                        log.debug(
                            f"Explode INFO fields - Force '{info}' annotations fields update from 'INFO' column"
                        )

                    log.debug(f"Explode INFO fields - ADD '{info}' annotations fields")

                    # Get field type
                    type_sql = description_dict.get(info_id_sql, {})["type"]

                    # Add field
                    added_column = self.add_column(
                        table_name=table_dest,
                        column_name=info_id_sql,
                        column_type=type_sql,
                        default_value="null",
                        drop=force,
                    )

                    # Added column
                    if added_column:
                        added_columns.append(added_column)
                        log.debug(
                            f"Explode INFO fields - ADD '{info}' annotations fields - added"
                        )
                    else:
                        log.debug(
                            f"Explode INFO fields - ADD '{info}' annotations fields - not added"
                        )

                    # if added_column or force: #fileds_just_add
                    if (added_column or force) and not info in fields_just_add:

                        # add field to index
                        self.index_additionnal_fields.append(info_id_sql)

                        update_info_field = f"""
                            "{info_id_sql}" = {view_source}."{info_id_sql}"
                            """

                        # Set field append
                        sql_info_alter_table_array.append(update_info_field)

            if sql_info_alter_table_array:

                # Where clause join
                where_clause_join = f"""
                    {" AND ".join([f'"{table_dest}"."{key}" = "{view_source}"."{key}"' for key in table_key])}
                """

                # Evaluate block size
                batch_split = self.get_batch_split()

                # Insert by batch
                for batch_index in range(batch_split):

                    log.debug(
                        f"Explode INFO fields - Process batch [{batch_index+1}/{batch_split}]..."
                    )

                    where_clause = where_clause_join

                    # where clause
                    if batch_split > 1:
                        where_clause += (
                            f" AND ({table_dest}.POS % {batch_split}) = {batch_index} "
                        )
                    else:
                        where_clause += ""

                    # Update table
                    if proccess_all_fields_together:
                        sql_info_alter_table_array_join = ", ".join(
                            sql_info_alter_table_array
                        )
                        if sql_info_alter_table_array_join:
                            sql_info_alter_table = f"""
                                UPDATE {table_dest}
                                SET {sql_info_alter_table_array_join}
                                FROM {view_source}
                                WHERE {where_clause}
                                """
                            log.debug(
                                f"Explode INFO fields - Explode all {len(sql_info_alter_table_array)} fields..."
                            )
                            # log.debug(sql_info_alter_table)
                            self.conn.execute(sql_info_alter_table)
                    else:
                        sql_info_alter_num = 0
                        for sql_info_alter in sql_info_alter_table_array:
                            sql_info_alter_num += 1
                            sql_info_alter_table = f"""
                                UPDATE {table_dest}
                                SET {sql_info_alter}
                                FROM {view_source}
                                WHERE {where_clause}
                                """
                            log.debug(
                                f"Explode INFO fields - Explode field {sql_info_alter_num}/{len(sql_info_alter_table_array)}..."
                            )
                            # log.debug(sql_info_alter_table)
                            self.conn.execute(sql_info_alter_table)

            # Remove view_source
            self.remove_tables_or_views(tables=[view_source])

        # create indexes
        if create_index:
            self.create_indexes()

        return added_columns

    def create_indexes(self) -> None:
        """
        Create indexes on the table after insertion
        """

        # Access
        access = self.get_config().get("access", None)

        # get table variants
        table_variants = self.get_table_variants("FROM")

        if self.get_indexing() and access not in ["RO"]:
            # Create index
            sql_create_table_index = f'CREATE INDEX IF NOT EXISTS idx_{self.get_table_variants()} ON {table_variants} ("#CHROM", "POS", "REF", "ALT")'
            self.conn.execute(sql_create_table_index)
            sql_create_table_index = f'CREATE INDEX IF NOT EXISTS idx_{self.get_table_variants()}_chrom ON {table_variants} ("#CHROM")'
            self.conn.execute(sql_create_table_index)
            sql_create_table_index = f'CREATE INDEX IF NOT EXISTS idx_{self.get_table_variants()}_pos ON {table_variants} ("POS")'
            self.conn.execute(sql_create_table_index)
            sql_create_table_index = f'CREATE INDEX IF NOT EXISTS idx_{self.get_table_variants()}_ref ON {table_variants} ( "REF")'
            self.conn.execute(sql_create_table_index)
            sql_create_table_index = f'CREATE INDEX IF NOT EXISTS idx_{self.get_table_variants()}_alt ON {table_variants} ("ALT")'
            self.conn.execute(sql_create_table_index)
            for field in self.index_additionnal_fields:
                sql_create_table_index = f""" CREATE INDEX IF NOT EXISTS "idx_{self.get_table_variants()}_{field}" ON {table_variants} ("{field}") """
                self.conn.execute(sql_create_table_index)

    def drop_indexes(self) -> None:
        """
        Create indexes on the table after insertion
        """

        # Access
        access = self.get_config().get("access", None)

        # get table variants
        table_variants = self.get_table_variants("FROM")

        # Get database format
        connexion_format = self.get_connexion_format()

        if access not in ["RO"]:
            if connexion_format in ["duckdb"]:
                sql_list_indexes = f"SELECT index_name FROM duckdb_indexes WHERE table_name='{table_variants}'"
            elif connexion_format in ["sqlite"]:
                sql_list_indexes = f"SELECT name FROM sqlite_master WHERE type='index' AND tbl_name='{table_variants}';"

            list_indexes = self.conn.execute(sql_list_indexes)
            index_names = [row[0] for row in list_indexes.fetchall()]
            for index in index_names:
                sql_drop_table_index = f""" DROP INDEX IF EXISTS "{index}" """
                self.conn.execute(sql_drop_table_index)

    def read_vcf_header(self, f) -> list:
        """
        It reads the header of a VCF file and returns a list of the header lines

        :param f: the file object
        :return: The header lines of the VCF file.
        """

        header_list = []
        for line in f:
            header_list.append(line)
            if line.startswith("#CHROM"):
                break
        return header_list

    def read_vcf_header_file(self, file: str = None) -> list:
        """
        The `read_vcf_header_file` function reads the header of a VCF file, handling both compressed and
        uncompressed files.

        :param file: The `file` parameter is a string that represents the path to the VCF header file
        that you want to read. It is an optional parameter, so if you don't provide a value, it will
        default to `None`
        :type file: str
        :return: The function `read_vcf_header_file` returns a list.
        """

        if self.get_input_compressed(input_file=file):
            with bgzf.open(file, "rt") as f:
                return self.read_vcf_header(f=f)
        else:
            with open(file, "rt") as f:
                return self.read_vcf_header(f=f)

    def execute_query(self, query: str):
        """
        It takes a query as an argument, executes it, and returns the results

        :param query: The query to be executed
        :return: The result of the query is being returned.
        """
        if query:
            return self.conn.execute(query)  # .fetchall()
        else:
            return None

    def export_output(
        self,
        output_file: str | None = None,
        output_header: str | None = None,
        export_header: bool = True,
        explode_infos: bool = True,
        header_in_output: bool = None,
        query: str | None = None,
        parquet_partitions: list | None = None,
        chunk_size: int | None = None,
        threads: int | None = None,
        sort: bool = False,
        index: bool = False,
        order_by: str | None = None,
        fields_to_rename: dict | None = None,
        force_cast_as_flat: bool = False,
    ) -> bool:
        """
        The `export_output` function exports data from a VCF file to various formats, including VCF,
        CSV, TSV, PSV, and Parquet, with options for customization such as filtering, sorting, and
        partitioning.

        :param output_file: The `output_file` parameter is a string that specifies the name of the
        output file where the exported data will be saved
        :type output_file: str | None
        :param output_header: The `output_header` parameter is a string that specifies the name of the
        file where the header of the VCF file will be exported. If this parameter is not provided, the
        header will be exported to a file with the same name as the `output_file` parameter, but with
        the extension "
        :type output_header: str | None
        :param export_header: The `export_header` parameter is a boolean flag that determines whether
        the header of a VCF file should be exported to a separate file or not. If `export_header` is
        True, the header will be exported to a file. If `export_header` is False, the header will not
        be, defaults to True
        :type export_header: bool (optional)
        :param explode_infos: The `explode_infos` parameter is a boolean flag that determines whether
        the INFO fields in the VCF file should be exploded into individual columns in the output file.
        If `explode_infos` is set to True, the INFO fields will be exploded. If `explode_infos` is set
        to False, the INFO fields will not be exploded. By default, the INFO fields are exploded
        :type explode_infos: bool (optional)
        :param query: The `query` parameter in the `export_output` function is an optional SQL query
        that can be used to filter and select specific data from the VCF file before exporting it. If
        provided, only the data that matches the query will be exported. This allows you to customize
        the exported data based on
        :type query: str | None
        :param header_in_output: The `header_in_output` parameter is a boolean flag that determines
        whether the header should be included in the output file. If `header_in_output` is set to `True`,
        the header will be included in the output file. If `header_in_output` is set to `False`, the
        header will not be included in the output file. By default, the header is included in the output
        file
        :type header_in_output: bool (optional)
        :param parquet_partitions: The `parquet_partitions` parameter is a list that specifies the
        columns to be used for partitioning the Parquet file during export. Partitioning is a way to
        organize data in a hierarchical directory structure based on the values of one or more columns.
        This can improve query performance when working with large datasets
        :type parquet_partitions: list | None
        :param chunk_size: The `chunk_size` parameter specifies the number of records in a batch when
        exporting data in Parquet format. This parameter is used for partitioning the Parquet file into
        multiple files. It helps in optimizing the export process by breaking down the data into
        manageable chunks for processing and storage
        :type chunk_size: int | None
        :param threads: The `threads` parameter in the `export_output` function specifies the number of
        threads to be used during the export process. It determines the level of parallelism and can
        improve the performance of the export operation. If this parameter is not provided, the function
        will use the default number of threads
        :type threads: int | None
        :param sort: The `sort` parameter in the `export_output` function is a boolean flag that
        determines whether the output file should be sorted based on genomic coordinates of the
        variants. If `sort` is set to `True`, the output file will be sorted. If `sort` is set to
        `False`,, defaults to False
        :type sort: bool (optional)
        :param index: The `index` parameter in the `export_output` function is a boolean flag that
        determines whether an index should be created on the output file. If `index` is set to `True`,
        an index will be created on the output file. If `index` is set to `False`, no, defaults to False
        :type index: bool (optional)
        :param order_by: The `order_by` parameter in the `export_output` function is a string that
        specifies the column(s) to use for sorting the output file. This parameter is only applicable
        when exporting data in VCF format. It allows you to specify the column(s) based on which the
        output file should be
        :type order_by: str | None
        :param fields_to_rename: The `fields_to_rename` parameter is a dictionary that specifies the
        mapping of field names to be renamed during the export process. This parameter allows you to
        customize the output field names before exporting the data. Each key-value pair in the
        dictionary represents the original field name as the key and the new field name
        :type fields_to_rename: dict | None
        :param force_cast_as_flat: Only for Parquet format. The `force_cast_as_flat` parameter is a boolean
        flag that determines whether to force the export of nested or complex data structures as flat
        structures. If `force_cast_as_flat` is set to `True`, the function will flatten any nested
        structures in the data before exporting it. If `force_cast_as_flat` is set to `False`, the
        function will preserve the original structure of the data during export. By default, it is set
        to False
        :type force_cast_as_flat: bool (optional)
        :return: The `export_output` function returns a boolean value. It checks if the output file
        exists and returns True if it does, or None if it doesn't.
        """

        # Log
        log.info("Exporting...")

        # Full path
        output_file = full_path(output_file)
        output_header = full_path(output_header)

        # Config
        config = self.get_config()

        # Param
        param = self.get_param()

        # Tmp files to remove
        tmp_to_remove = []

        # If no output, get it
        if not output_file:
            output_file = self.get_output()

        # If not threads
        if not threads:
            threads = self.get_threads()

        # Rename fields
        if not fields_to_rename:
            fields_to_rename = param.get("export", {}).get("fields_to_rename", None)
        self.rename_info_fields(fields_to_rename=fields_to_rename)

        # Force cast as flat
        force_cast_as_flat = param.get("export", {}).get(
            "force_cast_as_flat", force_cast_as_flat
        )

        # Auto header name with extension
        if export_header or output_header:
            if not output_header:
                output_header = f"{output_file}.hdr"
            # Export header
            self.export_header(output_file=output_file, query=query)

        # Switch off export header if VCF output
        output_file_type = get_file_format(output_file)
        if output_file_type in ["vcf"]:
            export_header = False
            tmp_to_remove.append(output_header)

        # Chunk size
        if not chunk_size:
            chunk_size = config.get("chunk_size", None)

        # Parquet partition
        if not parquet_partitions:
            parquet_partitions = param.get("export", {}).get("parquet_partitions", None)
        if parquet_partitions and isinstance(parquet_partitions, str):
            parquet_partitions = parquet_partitions.split(",")

        # Order by
        if not order_by:
            order_by = param.get("export", {}).get("order_by", "")

        # Header in output
        if header_in_output is None:
            header_in_output = param.get("export", {}).get("include_header", False)

        # Database
        database_source = self.get_connexion()

        # Connexion format
        connexion_format = self.get_connexion_format()

        # Explode infos
        if self.get_explode_infos() and explode_infos:
            self.explode_infos(
                prefix=self.get_explode_infos_prefix(),
                fields=self.get_explode_infos_fields(),
                force=False,
                fields_forced_as_varchar=True,
            )

        # if connexion_format in ["sqlite"] or query:
        if connexion_format in ["sqlite"]:

            # Export in Parquet
            random_tmp = "".join(
                random.choice(string.ascii_lowercase) for i in range(10)
            )
            database_source = f"""{output_file}.{random_tmp}.database_export.parquet"""
            tmp_to_remove.append(database_source)

            # Table Variants
            table_variants = self.get_table_variants()

            # Create export query
            sql_query_export_subquery = f"""
                SELECT * FROM {table_variants}
                """

            # Write source file
            fp.write(database_source, self.get_query_to_df(sql_query_export_subquery))

        # Create database
        database = Database(
            database=database_source,
            table="variants",
            header_file=output_header,
            conn_config=self.get_connexion_config(),
        )

        # Existing colomns header
        existing_columns_header = database.get_header_columns_from_database(query=query)

        # Sample list
        if output_file_type in ["vcf"]:
            get_samples = self.get_samples()
            get_samples_check = self.get_samples_check()
            samples_force = get_samples is not None
            sample_list = self.get_header_sample_list(
                check=get_samples_check,
                samples=get_samples,
                samples_force=samples_force,
            )
        else:
            sample_list = None

        # Export file
        database.export(
            output_database=output_file,
            output_header=output_header,
            existing_columns_header=existing_columns_header,
            parquet_partitions=parquet_partitions,
            chunk_size=chunk_size,
            threads=threads,
            sort=sort,
            index=index,
            header_in_output=header_in_output,
            order_by=order_by,
            query=query,
            export_header=export_header,
            sample_list=sample_list,
            force_cast_as_flat=force_cast_as_flat,
        )

        # Remove
        remove_if_exists(tmp_to_remove)

        return (os.path.exists(output_file) or None) and (
            os.path.exists(output_file) or None
        )

    def get_columns(self, table: str = None) -> list:
        """
        The `get_columns` function returns a list of columns in a specified table. If the `table`
        parameter is not provided when calling the function, it will default to using the variants table.

        Args:
            table (str, optional): The name of the table from which you want to retrieve the columns. If not provided,
                it will default to using the variants table.

        Returns:
            list: A list of columns in the specified table.
        """

        if not table:
            table = self.get_table_variants()

        # Use PRAGMA table_info for SQLite or DESCRIBE for other databases
        connexion_format = self.get_connexion_format()
        if connexion_format == "sqlite":
            query = f"PRAGMA table_info({table})"
            columns_info = self.get_query_to_df(query)
            columns = columns_info["name"].tolist()
        else:
            query = f"DESCRIBE {table}"
            columns_info = self.get_query_to_df(query)
            columns = columns_info["column_name"].tolist()

        return columns

    def get_extra_infos(self, table: str = None) -> list:
        """
        The `get_extra_infos` function returns a list of columns that are in a specified table but not
        in the header.

        :param table: The `table` parameter in the `get_extra_infos` function is used to specify the
        name of the table from which you want to retrieve the extra columns that are not present in the
        header. If the `table` parameter is not provided when calling the function, it will default to
        using the variants
        :type table: str
        :return: A list of columns that are in the specified table but not in the header of the table.
        """

        header_columns = []

        if not table:
            table = self.get_table_variants(clause="from")
            header_columns = self.get_header_columns()

        # Check all columns in the database
        query = f""" SELECT * FROM {table} LIMIT 1 """
        log.debug(f"query {query}")
        table_columns = self.get_query_to_df(query).columns.tolist()
        extra_columns = []

        # Construct extra infos (not in header)
        for column in table_columns:
            if column not in header_columns:
                extra_columns.append(column)

        return extra_columns

    def get_extra_infos_sql(self, table: str = None) -> str:
        """
        It returns a string of the extra infos, separated by commas, and each extra info is surrounded
        by double quotes

        :param table: The name of the table to get the extra infos from. If None, the default table is
        used
        :type table: str
        :return: A string of the extra infos
        """

        return ", ".join(
            ['"' + str(elem) + '"' for elem in self.get_extra_infos(table=table)]
        )

    def export_header(
        self,
        header_name: str = None,
        output_file: str = None,
        output_file_ext: str = ".hdr",
        clean_header: bool = True,
        clean_info_flag: bool = False,
        remove_chrom_line: bool = False,
        query: str | None = None,
    ) -> str:
        """
        The `export_header` function takes a VCF file, extracts the header, modifies it according to
        specified options, and writes it to a new file.

        :param header_name: The `header_name` parameter is the name of the header file to be created. If
        this parameter is not specified, the header will be written to the output file
        :type header_name: str
        :param output_file: The `output_file` parameter in the `export_header` function is used to
        specify the name of the output file where the header will be written. If this parameter is not
        provided, the header will be written to a temporary file
        :type output_file: str
        :param output_file_ext: The `output_file_ext` parameter in the `export_header` function is a
        string that represents the extension of the output header file. By default, it is set to ".hdr"
        if not specified by the user. This extension will be appended to the `output_file` name to
        create the final, defaults to .hdr
        :type output_file_ext: str (optional)
        :param clean_header: The `clean_header` parameter in the `export_header` function is a boolean
        flag that determines whether the header should be cleaned or not. When `clean_header` is set to
        `True`, the function will clean the header by modifying certain lines based on a specific
        pattern. If `clean_header`, defaults to True
        :type clean_header: bool (optional)
        :param clean_info_flag: The `clean_info_flag` parameter in the `export_header` function is a boolean
        flag that determines whether the header should be cleaned for INFO/tags that are 'Flag' type.
        When `clean_info_flag` is set to `True`, the function will replace INFO/tags 'Type' as 'String'.
        Default to False
        :type clean_info_flag: bool (optional)
        :param remove_chrom_line: The `remove_chrom_line` parameter in the `export_header` function is a
        boolean flag that determines whether the #CHROM line should be removed from the header before
        writing it to the output file. If set to `True`, the #CHROM line will be removed; if set to `,
        defaults to False
        :type remove_chrom_line: bool (optional)
        :param query: The `query` parameter in the `export_header` function is an optional SQL query
        string that can be used to filter the columns in the header. If provided, the function will
        retrieve only the columns that match the query. If not provided, all columns in the header will
        be included in the output
        :type query: str | None
        :return: The function `export_header` returns the name of the temporary header file that is
        created.
        """

        if not header_name and not output_file:
            output_file = self.get_output()

        if self.get_header():

            # Get header object
            header_obj = self.get_header()

            # Create database
            db_for_header = Database(database=self.get_input())

            # Get real columns in the file
            db_header_columns = db_for_header.get_columns(sql_query=query)

            with tempfile.TemporaryDirectory() as tmpdir:

                # Write header file
                header_file_tmp = os.path.join(tmpdir, "header")
                f = open(header_file_tmp, "w")
                vcf.Writer(f, header_obj)
                f.close()

                # Replace #CHROM line with rel columns
                header_list = db_for_header.read_header_file(
                    header_file=header_file_tmp
                )
                header_list[-1] = "\t".join(db_header_columns)

                # Remove CHROM line
                if remove_chrom_line:
                    header_list.pop()

                # Clean header
                if clean_header:
                    header_list_clean = []
                    for head in header_list:
                        # Clean head for malformed header
                        head_clean = head
                        if clean_info_flag:
                            head_clean = re.subn(
                                "##FORMAT=<ID=(.*),Number=(.*),Type=Flag",
                                r"##FORMAT=<ID=\1,Number=\2,Type=String",
                                head_clean,
                                2,
                            )[0]
                        # Write header
                        header_list_clean.append(head_clean)
                    header_list = header_list_clean

            tmp_header_name = output_file + output_file_ext

            f = open(tmp_header_name, "w")
            for line in header_list:
                f.write(line)
            f.close()

        return tmp_header_name

    def export_variant_vcf(
        self,
        vcf_file,
        remove_info: bool = False,
        add_samples: bool = True,
        list_samples: list = [],
        where_clause: str = "",
        index: bool = False,
        threads: int | None = None,
    ) -> bool | None:
        """
        The `export_variant_vcf` function exports a VCF file with specified samples, allowing options to
        remove INFO field, add samples, and control compression and indexing.

        :param vcf_file: The `vcf_file` parameter is the name of the file where the VCF data will be
        written to. It is the output file that will contain the filtered VCF data based on the specified
        parameters
        :param remove_info: The `remove_info` parameter in the `export_variant_vcf` function is a
        boolean flag that determines whether to remove the INFO field from the output VCF file. If set
        to `True`, the INFO field will be removed. If set to `False`, the INFO field will be included
        in, defaults to False
        :type remove_info: bool (optional)
        :param add_samples: The `add_samples` parameter is a boolean parameter that determines whether
        the samples should be added to the VCF file or not. If set to True, the samples will be added.
        If set to False, the samples will be removed. The default value is True, defaults to True
        :type add_samples: bool (optional)
        :param list_samples: The `list_samples` parameter is a list of samples that you want to include
        in the output VCF file. By default, all samples will be included. If you provide a list of
        samples, only those samples will be included in the output file
        :type list_samples: list
        :param index: The `index` parameter in the `export_variant_vcf` function is a boolean flag that
        determines whether or not to create an index for the output VCF file. If `index` is set to
        `True`, the output VCF file will be indexed using tabix. If `index`, defaults to False
        :type index: bool (optional)
        :param threads: The `threads` parameter in the `export_variant_vcf` function specifies the
        number of threads to use for exporting the VCF file. It determines how many parallel threads
        will be used during the export process. More threads can potentially speed up the export process
        by utilizing multiple cores of the processor. If
        :type threads: int | None
        :return: The `export_variant_vcf` function returns the result of calling the `export_output`
        method with various parameters including the output file, query, threads, sort flag, and index
        flag. The `export_output` method is responsible for exporting the VCF data based on the
        specified parameters and configurations provided in the `export_variant_vcf` function.
        """

        # Config
        config = self.get_config()

        # Extract VCF
        log.debug("Export VCF...")

        # Table variants
        table_variants = self.get_table_variants()

        # Threads
        if not threads:
            threads = self.get_threads()

        # Info fields
        if remove_info:
            if not isinstance(remove_info, str):
                remove_info = "."
            info_field = f"""'{remove_info}' as INFO"""
        else:
            info_field = "INFO"

        # Samples fields
        if add_samples:
            if not list_samples:
                list_samples = self.get_header_sample_list()
            if list_samples:
                samples_fields = " , FORMAT , " + " , ".join(
                    [f""" "{sample}" """ for sample in list_samples]
                )
            else:
                samples_fields = ""
            log.debug(f"samples_fields: {samples_fields}")
        else:
            samples_fields = ""

        # Where clause
        if where_clause is None:
            where_clause = ""

        # Columns
        existing_columns = self.get_columns(table=table_variants)
        columns_default_values = {
            "#CHROM": "'chr'",
            "POS": "0",
            "ID": "'.'",
            "REF": "'N'",
            "ALT": "'N'",
            "QUAL": "'0'",
            "FILTER": "'PASS'",
        }
        select_fields_list = []
        for column in ["#CHROM", "POS", "ID", "REF", "ALT", "QUAL", "FILTER"]:
            if column not in existing_columns:
                select_fields_list.append(
                    f"{columns_default_values.get(column, '')} AS '{column}'"
                )
            else:
                select_fields_list.append(f'"{column}"')
        select_fields = ", ".join(select_fields_list)

        # Query
        sql_query_select = f""" SELECT {select_fields}, {info_field} {samples_fields} FROM {table_variants} {where_clause} """
        log.debug(f"sql_query_select={sql_query_select}")

        return self.export_output(
            output_file=vcf_file,
            output_header=None,
            export_header=True,
            explode_infos=False,
            query=sql_query_select,
            parquet_partitions=None,
            chunk_size=config.get("chunk_size", None),
            threads=threads,
            sort=True,
            index=index,
            order_by=None,
        )

    def run_commands(self, commands: list = [], threads: int = 1) -> None:
        """
        It takes a list of commands and runs them in parallel using the number of threads specified

        :param commands: A list of commands to run
        :param threads: The number of threads to use, defaults to 1 (optional)
        """

        run_parallel_commands(commands, threads)

    def get_threads(self, default: int = 1) -> int:
        """
        This function returns the number of threads to use for a job, with a default value of 1 if not
        specified.

        :param default: The `default` parameter in the `get_threads` method is used to specify the
        default number of threads to use if no specific value is provided. If no value is provided for
        the `threads` parameter in the configuration or input parameters, the `default` value will be
        used, defaults to 1
        :type default: int (optional)
        :return: the number of threads to use for the current job.
        """

        # Config
        config = self.get_config()

        # Param
        param = self.get_param()

        # Input threads
        input_thread = param.get("threads", config.get("threads", None))

        # Check threads
        if not input_thread:
            threads = default
        elif int(input_thread) <= 0:
            threads = os.cpu_count()
        else:
            threads = int(input_thread)
        return threads

    def get_memory_system(self, type: str = "available", unit: str = "G") -> str:
        """
        This function retrieves the system memory in the system and returns it as a string.

        :param type: The `type` parameter in the `get_memory_system` function specifies the type of
        memory information to retrieve. It can take one of the following values: "total" to retrieve the
        total memory, "used" to retrieve the used memory, "percent" to retrieve the percentage of used memory,
        or "available" to retrieve the available memory. The default value is "available"
        :type type: str (optional)
        :param unit: The `unit` parameter in the `get_memory_system` function specifies the unit of
        measurement for the memory value. It can take one of the following values: "K" for kilobytes,
        "M" for megabytes, "G" for gigabytes, or "T" for terabytes. The default value is "G"
        :type unit: str (optional)

        :return: The function `get_memory_system` returns a string representation of the available
        memory in the system.
        """

        import psutil  # type: ignore

        # Check system memory
        mem = psutil.virtual_memory()

        # Get memory type
        if type == "total":
            memory = str(mem.total)
        elif type == "used":
            memory = str(mem.used)
        elif type == "percent":
            memory = str(mem.percent)
        elif type == "available":
            memory = str(mem.available)
        else:
            memory = str(mem.total)

        # Convert unit
        unit_powers = {"K": 1, "M": 2, "G": 3, "T": 4}
        if unit in unit_powers:
            power = unit_powers[unit]
            memory = str(int(memory) // (1024**power))

        # Return memory
        return f"{memory}{unit}"

    def get_memory(self, default: str = None, available: bool = False) -> str:
        """
        This function retrieves the memory value from parameters or configuration with a default value
        if not found.

        :param default: The `get_memory` function takes in a default value as a string parameter. This
        default value is used as a fallback in case the `memory` parameter is not provided in the
        `param` dictionary or the `config` dictionary. If `memory` is not found in either dictionary,
        the function
        :type default: str
        :param available: A boolean parameter that determines whether to limit the memory to the
        available system memory or not. If set to True, the function will return the minimum value
        between the input memory and the available system memory. If set to False, the function will
        return the input memory or the default value, defaults to False
        :type available: bool (optional)
        :return: The `get_memory` function returns a string value representing the memory parameter. If
        the `input_memory` is provided in the parameters, it will return that value. Otherwise, it will
        return the default value provided as an argument to the function.
        """

        # Config
        config = self.get_config()

        # Param
        param = self.get_param()

        # Input threads
        input_memory = param.get("memory", config.get("memory", None))

        # Avalable memory
        if available:
            available_memory = self.get_memory_system(type="available", unit="G")
            if available_memory is not None and input_memory is not None:
                input_memory = min(input_memory, available_memory)
            else:
                input_memory = available_memory

        # Check threads
        if input_memory:
            memory = input_memory
        else:
            memory = default

        return memory

    def update_from_vcf(
        self,
        vcf_file: str,
        update_existing_fields: bool = False,
        remove_vcf_file: bool = True,
        upper_case: bool = True,
    ) -> None:
        """
        > If the database is duckdb, then use the parquet method, otherwise use the sqlite method

        :param vcf_file: the path to the VCF file you want to update the database with
        :type vcf_file: str
        :param update_existing_fields: If True, existing fields in the INFO column will be updated
        with the values from the VCF file. If False, only new fields will be added, defaults to False
        :type update_existing_fields: bool (optional)
        :param remove_vcf_file: If True, the VCF file will be removed after the update is complete,
        defaults to True
        :type remove_vcf_file: bool (optional)
        :return: None
        """

        connexion_format = self.get_connexion_format()

        if connexion_format in ["duckdb"]:
            self.update_from_vcf_duckdb(
                vcf_file,
                update_existing_fields=update_existing_fields,
                remove_vcf_file=remove_vcf_file,
                upper_case=upper_case,
            )
        elif connexion_format in ["sqlite"]:
            self.update_from_vcf_sqlite(vcf_file)

        if remove_vcf_file:
            remove_if_exists([vcf_file])

    def update_from_vcf_duckdb(
        self,
        vcf_file: str,
        update_existing_fields: bool = False,
        remove_vcf_file: bool = True,
        upper_case: bool = True,
    ) -> None:
        """
        It takes a VCF file and updates the INFO column of the variants table in the database with the
        INFO column of the VCF file

        :param vcf_file: The path to the VCF file you want to update the database with
        :type vcf_file: str
        :param update_existing_fields: If True, existing fields in the INFO column will be updated
        with the values from the VCF file. If False, only new fields will be added, defaults to False
        :type update_existing_fields: bool (optional)
        :param remove_vcf_file: If True, the VCF file will be removed after the update is complete,
        defaults to True
        :type remove_vcf_file: bool (optional)
        :param upper_case: If True, the ALT and REF fields will be compared in uppercase, defaults to True
        :type upper_case: bool (optional)
        :return: None
        """

        # variants table
        table_variants = self.get_table_variants()

        # Connexion
        conn = self.get_connexion()

        log.info(f"Update variants table from file '{os.path.basename(vcf_file)}'...")

        with TemporaryDirectory(dir=self.get_tmp_dir()) as tmp_dir:

            log.debug(f"Create parquet files from VCF '{vcf_file}'...")

            # Create parquet from VCF
            vcf_file_parquet_path = os.path.join(tmp_dir, "vcf_file.parquet")
            vcf_file_parquet = Variants(
                input=vcf_file, load=True, config={"access": "RO"}
            )

            log.debug(f"Variants input format '{vcf_file_parquet.get_input_format()}'")

            if vcf_file_parquet.get_input_format() == "parquet":

                # list of parquet files
                vcf_file_parquet_path = vcf_file

            else:

                # Export parquet parameters
                chunk_size = self.get_config().get("chunk_size", None)
                threads = self.get_threads()

                # Export parquet files
                log.debug("Export VCF to partitioned parquet...")
                vcf_file_parquet.export_output(
                    output_file=vcf_file_parquet_path,
                    chunk_size=chunk_size,
                    threads=threads,
                    export_header=True,
                )
                log.debug(f"Parquet generated: {vcf_file_parquet_path}")

                if remove_vcf_file:
                    remove_if_exists([vcf_file])

            # Update if fields exist
            if update_existing_fields:
                # list of header columns
                header_columns = self.get_header().infos.keys()
                header_columns_vcf_file_parquet = (
                    vcf_file_parquet.get_header().infos.keys()
                )

                # columns that exist in both
                common_columns = list(
                    set(header_columns).intersection(
                        set(header_columns_vcf_file_parquet)
                    )
                )

                # Remove common columns
                if len(common_columns) > 0:
                    log.debug(f"Common columns to update/remove: {common_columns}")
                    self.rename_info_fields(
                        fields_to_rename=dict.fromkeys(common_columns, None)
                    )
                else:
                    log.debug("No common columns to update/remove")

            # Upper case function for ALT and REF
            if upper_case:
                upper_func = "upper"
            else:
                upper_func = ""

            # Create table/view from parquet files
            table_source_name = "table_parquet_" + get_random(10)
            sql_query_update = f"""
                CREATE VIEW {table_source_name}
                AS (
                    SELECT "#CHROM", POS, {upper_func}(REF) as REF, {upper_func}(ALT) as ALT, INFO
                    FROM read_parquet('{vcf_file_parquet_path}')
                    WHERE INFO NOT IN ('','.')
                )
                ;
            """
            # log.debug(f"sql_query_update: {sql_query_update}")
            conn.execute(sql_query_update)

            # Update INFO fields with update_table function
            source = {
                "table": table_source_name,
                "join_keys": ["#CHROM", "POS", "REF", "ALT"],
                "columns": {
                    "INFO": {
                        "columns": ["INFO"],
                        "mode": "append",
                        "separator": ";",
                    }
                },
            }
            self.update_table(
                dest_table=table_variants,
                sources=[source],
                physical_order=True,
                force_strategy=None,
                upper_case=upper_case,
            )

            return None

    # def update_from_vcf_duckdb_old(
    #     self,
    #     vcf_file: str,
    #     update_existing_fields: bool = False,
    #     remove_vcf_file: bool = True,
    #     upper_case: bool = True,
    # ) -> None:
    #     """
    #     It takes a VCF file and updates the INFO column of the variants table in the database with the
    #     INFO column of the VCF file

    #     :param vcf_file: The path to the VCF file you want to update the database with
    #     :type vcf_file: str
    #     :param update_existing_fields: If True, existing fields in the INFO column will be updated
    #     with the values from the VCF file. If False, only new fields will be added, defaults to False
    #     :type update_existing_fields: bool (optional)
    #     :param remove_vcf_file: If True, the VCF file will be removed after the update is complete,
    #     defaults to True
    #     :type remove_vcf_file: bool (optional)
    #     :param upper_case: If True, the ALT and REF fields will be compared in uppercase, defaults to True
    #     :type upper_case: bool (optional)
    #     :return: None
    #     """

    #     # variants table
    #     table_variants = self.get_table_variants()

    #     log.info(f"Update variants table from file '{os.path.basename(vcf_file)}'...")

    #     with TemporaryDirectory(dir=self.get_tmp_dir()) as tmp_dir:

    #         log.debug(f"Create parquet files from VCF '{vcf_file}'...")

    #         # Create parquet from VCF
    #         vcf_file_parquet_path = os.path.join(tmp_dir, "vcf_file.parquet")
    #         vcf_file_parquet = Variants(
    #             input=vcf_file, load=True, config={"access": "RO"}
    #         )

    #         log.debug(f"Variants input format {vcf_file_parquet.get_input_format()}")

    #         if vcf_file_parquet.get_input_format() == "parquet":

    #             # list of parquet files
    #             list_of_parquet = [vcf_file]

    #         else:

    #             # Export parquet parameters
    #             parquet_partitions = "None"
    #             chunk_size = self.get_config().get("chunk_size", None)
    #             threads = self.get_threads()

    #             # Export parquet files
    #             log.debug("Export VCF to partitioned parquet...")
    #             vcf_file_parquet.export_output(
    #                 output_file=vcf_file_parquet_path,
    #                 parquet_partitions=parquet_partitions,
    #                 chunk_size=chunk_size,
    #                 threads=threads,
    #                 export_header=True,
    #             )
    #             log.debug(f"Partitioned parquet generated: {vcf_file_parquet_path}")

    #             if remove_vcf_file:
    #                 remove_if_exists([vcf_file])

    #             # list of parquet files
    #             list_of_parquet = glob.glob(f"{vcf_file_parquet_path}/*.parquet")

    #         log.debug(f"List of parquet: {list_of_parquet}")

    #         # Update if fields exist
    #         if update_existing_fields:
    #             # list of header columns
    #             header_columns = self.get_header().infos.keys()
    #             header_columns_vcf_file_parquet = (
    #                 vcf_file_parquet.get_header().infos.keys()
    #             )

    #             # columns that exist in both
    #             common_columns = list(
    #                 set(header_columns).intersection(
    #                     set(header_columns_vcf_file_parquet)
    #                 )
    #             )

    #             # Remove common columns
    #             if len(common_columns) > 0:
    #                 log.debug(f"Common columns to update/remove: {common_columns}")
    #                 self.rename_info_fields(
    #                     fields_to_rename=dict.fromkeys(common_columns, None)
    #                 )
    #             else:
    #                 log.debug("No common columns to update/remove")

    #         log.debug(
    #             f"Update variants table from {len(list_of_parquet)} parquet files..."
    #         )

    #         # Upper case function for ALT and REF
    #         if upper_case:
    #             upper_func = "upper"
    #         else:
    #             upper_func = ""

    #         for parquet_file in list_of_parquet:
    #             log.debug(
    #                 f"Update variants table from parquet file: {os.path.basename(parquet_file)}..."
    #             )
    #             sql_query_update = f"""
    #             UPDATE {table_variants} as table_variants
    #                 SET INFO = concat(
    #                         CASE
    #                             WHEN INFO NOT IN ('', '.')
    #                             THEN INFO
    #                             ELSE ''
    #                         END,
    #                         (
    #                             SELECT
    #                                 concat(
    #                                     CASE
    #                                         WHEN table_variants.INFO NOT IN ('','.') AND table_parquet.INFO NOT IN ('','.')
    #                                         THEN ';'
    #                                         ELSE ''
    #                                     END
    #                                     ,
    #                                     CASE
    #                                         WHEN table_parquet.INFO NOT IN ('','.')
    #                                         THEN table_parquet.INFO
    #                                         ELSE ''
    #                                     END
    #                                 )
    #                             FROM read_parquet('{parquet_file}') as table_parquet
    #                                     WHERE CAST(table_parquet.\"#CHROM\" AS VARCHAR) = CAST(table_variants.\"#CHROM\" AS VARCHAR)
    #                                     AND table_parquet.\"POS\" = table_variants.\"POS\"
    #                                     AND {upper_func}(table_parquet.\"ALT\") = {upper_func}(table_variants.\"ALT\")
    #                                     AND {upper_func}(table_parquet.\"REF\") = {upper_func}(table_variants.\"REF\")
    #                                     AND table_parquet.INFO NOT IN ('','.')
    #                         )
    #                     )
    #                 ;
    #                 """
    #             # log.debug(f"sql_query_update: {sql_query_update}")
    #             self.conn.execute(sql_query_update)

    #         # Clean INFO fields that are empty
    #         sql_query_clean = f"""
    #             UPDATE {table_variants}
    #             SET INFO = CASE
    #                 WHEN INFO IN ('','.')
    #                 THEN '.'
    #                 ELSE INFO
    #             END
    #         """
    #         # log.debug(f"sql_query_clean: {sql_query_clean}")
    #         self.conn.execute(sql_query_clean)

    def update_from_vcf_sqlite(self, vcf_file: str) -> None:
        """
        It creates a temporary table in the SQLite database, loads the VCF file into the temporary
        table, then updates the INFO column of the variants table with the INFO column of the temporary
        table

        :param vcf_file: The path to the VCF file you want to update the database with
        """

        # Create a temporary table for the VCF
        table_vcf = "tmp_vcf"
        sql_create = (
            f"CREATE TEMPORARY TABLE {table_vcf} AS SELECT * FROM variants WHERE 0"
        )
        self.conn.execute(sql_create)

        # Loading VCF into temporaire table
        vcf_df = pd.read_csv(
            vcf_file, sep="\t", comment="#", header=None, low_memory=False
        )
        vcf_df.columns = ["#CHROM", "POS", "ID", "REF", "ALT", "QUAL", "FILTER", "INFO"]
        vcf_df.to_sql(table_vcf, self.conn, if_exists="append", index=False)

        # Update table 'variants' with VCF data
        # warning: CONCAT as || operator
        sql_query_update = f"""
            UPDATE variants as table_variants
            SET INFO = CASE
                            WHEN INFO NOT IN ('', '.')
                            THEN INFO
                            ELSE ''
                        END ||
                        (
                        SELECT 
                            CASE 
                                WHEN table_variants.INFO NOT IN ('','.') 
                                    AND table_vcf.INFO NOT IN ('','.')  
                                THEN ';' 
                                ELSE '' 
                            END || 
                            CASE 
                                WHEN table_vcf.INFO NOT IN ('','.') 
                                THEN table_vcf.INFO 
                                ELSE '' 
                            END
                        FROM {table_vcf} as table_vcf
                        WHERE table_vcf.\"#CHROM\" = table_variants.\"#CHROM\"
                            AND table_vcf.\"POS\" = table_variants.\"POS\"
                            AND table_vcf.\"ALT\" = table_variants.\"ALT\"
                            AND table_vcf.\"REF\" = table_variants.\"REF\"
                        )
        """
        self.conn.execute(sql_query_update)

        # Drop temporary table
        sql_drop = f"DROP TABLE {table_vcf}"
        self.conn.execute(sql_drop)

    def drop_variants_table(self) -> None:
        """
        > This function drops the variants table
        """

        table_variants = self.get_table_variants()
        sql_table_variants = f"DROP TABLE IF EXISTS {table_variants}"
        self.conn.execute(sql_table_variants)

    def set_variant_id(
        self, variant_id_column: str = "variant_id", force: bool = None
    ) -> str:
        """
        It adds a column to the variants table called `variant_id` and populates it with a hash of the
        `#CHROM`, `POS`, `REF`, and `ALT` columns

        :param variant_id_column: The name of the column to be created in the variants table, defaults
        to variant_id
        :type variant_id_column: str (optional)
        :param force: If True, the variant_id column will be created even if it already exists
        :type force: bool
        :return: The name of the column that contains the variant_id
        """

        # Assembly
        assembly = self.get_param().get(
            "assembly", self.get_config().get("assembly", DEFAULT_ASSEMBLY)
        )

        # INFO/Tag prefix
        prefix = self.get_explode_infos_prefix()

        # Explode INFO/SVTYPE
        added_columns = self.explode_infos(prefix=prefix, fields=["SVTYPE"])

        # variants table
        table_variants = self.get_table_variants()

        # variant_id column
        if not variant_id_column:
            variant_id_column = "variant_id"

        # Creta variant_id column
        if "variant_id" not in self.get_extra_infos() or force:

            # Create column
            self.add_column(
                table_name=table_variants,
                column_name=variant_id_column,
                column_type="UBIGINT",
                default_value="0",
            )

            # Update column
            self.conn.execute(
                f"""
                    UPDATE {table_variants}
                    SET "{variant_id_column}" = hash('{assembly}', "#CHROM", "POS", "REF", "ALT", '"{prefix}SVTYPE"')
                """
            )

        # Remove added columns
        for added_column in added_columns:
            self.drop_column(column=added_column)

        # return variant_id column name
        return variant_id_column

    def get_variant_id_column(
        self, variant_id_column: str = "variant_id", force: bool = None
    ) -> str:
        """
        This function returns the variant_id column name

        :param variant_id_column: The name of the column in the dataframe that contains the variant IDs,
        defaults to variant_id
        :type variant_id_column: str (optional)
        :param force: If True, will force the variant_id to be set to the value of variant_id_column. If
        False, will only set the variant_id if it is not already set. If None, will set the variant_id
        if it is not already set, or if it is set
        :type force: bool
        :return: The variant_id column name.
        """

        return self.set_variant_id(variant_id_column=variant_id_column, force=force)

    ###
    # Annotation
    ###

    def scan_databases(
        self,
        database_formats: list = ["parquet"],
        database_releases: list = ["current"],
    ) -> dict:
        """
        The function `scan_databases` scans for available databases based on specified formats and
        releases.

        :param database_formats: The `database_formats` parameter is a list that specifies the formats
        of the databases to be scanned. In this case, the accepted format is "parquet"
        :type database_formats: list ["parquet"]
        :param database_releases: The `database_releases` parameter is a list that specifies the
        releases of the databases to be scanned. In the provided function, the default value for
        `database_releases` is set to `["current"]`, meaning that by default, the function will scan
        databases that are in the "current"
        :type database_releases: list
        :return: The function `scan_databases` returns a dictionary containing information about
        databases that match the specified formats and releases.
        """

        # Config
        config = self.get_config()

        # Param
        param = self.get_param()

        # Param - Assembly
        assembly = param.get("assembly", config.get("assembly", None))
        if not assembly:
            assembly = DEFAULT_ASSEMBLY
            log.warning(f"Default assembly '{assembly}'")

        # Scan for availabled databases
        log.info(
            f"Annotations - Check annotation parameters - Scan existing databases - Assembly {[assembly]} - Formats {database_formats} - Releases {database_releases}..."
        )
        databases_infos_dict = databases_infos(
            database_folder_releases=database_releases,
            database_formats=database_formats,
            assembly=assembly,
            config=config,
        )
        log.info(
            f"Annotations - Check annotation parameters - Scan existing databases - {len(databases_infos_dict)} databases found"
        )

        return databases_infos_dict

    def annotation(self) -> None:
        """
        It annotates the VCF file with the annotations specified in the config file.
        """

        # Config
        config = self.get_config()

        # Param
        param = self.get_param()

        # Param - Assembly
        assembly = param.get("assembly", config.get("assembly", None))
        if not assembly:
            assembly = DEFAULT_ASSEMBLY
            log.warning(f"Default assembly '{assembly}'")

        # annotations databases folders
        annotations_databases = set(
            config.get("folders", {})
            .get("databases", {})
            .get("annotations", [DEFAULT_ANNOTATIONS_FOLDER])
            + config.get("folders", {})
            .get("databases", {})
            .get("parquet", [DEFAULT_PARQUET_FOLDER])
            + config.get("folders", {})
            .get("databases", {})
            .get("bcftools", [DEFAULT_BCFTOOLS_FOLDER])
        )

        # Get param annotations
        if param.get("annotations", None) and isinstance(
            param.get("annotations", None), str
        ):
            param_annotation_list = param.get("annotations").split(",")
        else:
            param_annotation_list = []

        # Each tools param
        if param.get("annotation_parquet", None) != None:
            if isinstance(param.get("annotation_parquet", None), list):
                param_annotation_list.append(",".join(param.get("annotation_parquet")))
            else:
                param_annotation_list.append(param.get("annotation_parquet"))
        if param.get("annotation_snpsift", None) != None:
            if isinstance(param.get("annotation_snpsift", None), list):
                param_annotation_list.append(
                    "snpsift:"
                    + "+".join(param.get("annotation_snpsift")).replace(",", "+")
                )
            else:
                param_annotation_list.append(
                    "snpsift:" + param.get("annotation_snpsift").replace(",", "+")
                )
        if param.get("annotation_snpeff", None) != None:
            param_annotation_list.append("snpeff:" + param.get("annotation_snpeff"))
        if param.get("annotation_bcftools", None) != None:
            if isinstance(param.get("annotation_bcftools", None), list):
                param_annotation_list.append(
                    "bcftools:"
                    + "+".join(param.get("annotation_bcftools")).replace(",", "+")
                )
            else:
                param_annotation_list.append(
                    "bcftools:" + param.get("annotation_bcftools").replace(",", "+")
                )
        if param.get("annotation_annovar", None) != None:
            param_annotation_list.append("annovar:" + param.get("annotation_annovar"))
        if param.get("annotation_exomiser", None) != None:
            param_annotation_list.append("exomiser:" + param.get("annotation_exomiser"))
        if param.get("annotation_splice", None) != None:
            param_annotation_list.append("splice:" + param.get("annotation_splice"))

        # Merge param annotations list
        param["annotations"] = ",".join(param_annotation_list)

        # # debug
        # log.debug(f"param_annotations={param['annotations']}")

        if param.get("annotations"):

            # Log
            # log.info("Annotations - Check annotation parameters")

            if not "annotation" in param:
                param["annotation"] = {}

            # List of annotations parameters
            annotations_list_input = {}
            if isinstance(param.get("annotations", None), str):
                annotation_file_list = list(param.get("annotations", "").split(","))
                for annotation_file in annotation_file_list:
                    annotations_list_input[annotation_file.strip()] = {"INFO": None}
            else:
                annotations_list_input = param.get("annotations", {})

            log.info(f"Quick Annotations:")
            for annotation_key in list(annotations_list_input.keys()):
                log.info(f"   {annotation_key}")

            # List of annotations and associated fields
            annotations_list = {}

            for annotation_file in annotations_list_input:

                # Explode annotations if ALL
                if (
                    annotation_file.upper() == "ALL"
                    or annotation_file.upper().startswith("ALL:")
                ):

                    # check ALL parameters (formats, releases)
                    annotation_file_split = annotation_file.split(":")
                    database_formats = "parquet"
                    database_releases = "current"
                    for annotation_file_option in annotation_file_split[1:]:
                        database_all_options_split = annotation_file_option.split("=")
                        if database_all_options_split[0] == "format":
                            database_formats = database_all_options_split[1].split("+")
                        if database_all_options_split[0] == "release":
                            database_releases = database_all_options_split[1].split("+")

                    # Scan for availabled databases
                    databases_infos_dict = self.scan_databases(
                        database_formats=database_formats,
                        database_releases=database_releases,
                    )

                    # Add found databases in annotation parameters
                    for database_infos in databases_infos_dict.keys():
                        annotations_list[database_infos] = {"INFO": None}

                else:
                    annotations_list[annotation_file] = annotations_list_input[
                        annotation_file
                    ]

            # Check each databases
            if len(annotations_list):

                log.info(
                    f"Annotations - Check annotation parameters - Check {len(annotations_list)} databases..."
                )

                for annotation_file in annotations_list:

                    # Init
                    annotations = annotations_list.get(annotation_file, None)

                    # Annotation snpEff
                    if annotation_file.startswith("snpeff"):

                        log.debug(f"Quick Annotation snpEff")

                        if "snpeff" not in param["annotation"]:
                            param["annotation"]["snpeff"] = {}

                        if "options" not in param["annotation"]["snpeff"]:
                            param["annotation"]["snpeff"]["options"] = ""

                        # snpEff options in annotations
                        param["annotation"]["snpeff"]["options"] = "".join(
                            annotation_file.split(":")[1:]
                        )

                    # Annotation Annovar
                    elif annotation_file.startswith("annovar"):

                        log.debug(f"Quick Annotation Annovar")

                        if "annovar" not in param["annotation"]:
                            param["annotation"]["annovar"] = {}

                        if "annotations" not in param["annotation"]["annovar"]:
                            param["annotation"]["annovar"]["annotations"] = {}

                        # Options
                        annotation_file_split = annotation_file.split(":")
                        for annotation_file_annotation in annotation_file_split[1:]:
                            if annotation_file_annotation:
                                param["annotation"]["annovar"]["annotations"][
                                    annotation_file_annotation
                                ] = annotations

                    # Annotation Exomiser
                    elif annotation_file.startswith("exomiser"):

                        log.debug(f"Quick Annotation Exomiser")

                        param["annotation"]["exomiser"] = params_string_to_dict(
                            annotation_file
                        )

                    # Annotation Splice
                    elif annotation_file.startswith("splice"):

                        log.debug(f"Quick Annotation Splice")

                        param["annotation"]["splice"] = params_string_to_dict(
                            annotation_file
                        )

                    # Annotation Parquet or BCFTOOLS
                    else:

                        # Tools detection
                        if annotation_file.startswith("parquet:"):
                            annotation_tool_initial = "parquet"
                            annotation_file = ":".join(annotation_file.split(":")[1:])
                        elif annotation_file.startswith("bcftools:"):
                            annotation_tool_initial = "bcftools"
                            annotation_file = ":".join(annotation_file.split(":")[1:])
                        elif annotation_file.startswith("snpsift:"):
                            annotation_tool_initial = "snpsift"
                            annotation_file = ":".join(annotation_file.split(":")[1:])
                        elif annotation_file.startswith("bigwig:"):
                            annotation_tool_initial = "bigwig"
                            annotation_file = ":".join(annotation_file.split(":")[1:])
                        else:
                            annotation_tool_initial = None

                        # list of files
                        annotation_file_list = annotation_file.replace("+", ":").split(
                            ":"
                        )

                        for annotation_file in annotation_file_list:

                            if annotation_file:

                                # Annotation tool initial
                                annotation_tool = annotation_tool_initial

                                # Find file
                                annotation_file_found = annotation_file_find(
                                    annotation_file=annotation_file,
                                    databases_folders=list(annotations_databases),
                                    assembly=assembly,
                                )

                                # Full path
                                annotation_file_found = full_path(annotation_file_found)

                                if annotation_file_found:

                                    database = Database(database=annotation_file_found)
                                    quick_annotation_format = database.get_format()
                                    quick_annotation_is_compressed = (
                                        database.is_compressed()
                                    )
                                    quick_annotation_is_indexed = os.path.exists(
                                        f"{annotation_file_found}.tbi"
                                    )
                                    bcftools_preference = False

                                    # Check Annotation Tool
                                    if not annotation_tool:
                                        if (
                                            bcftools_preference
                                            and quick_annotation_format
                                            in ["vcf", "bed"]
                                            and quick_annotation_is_compressed
                                            and quick_annotation_is_indexed
                                        ):
                                            annotation_tool = "bcftools"
                                        elif quick_annotation_format in [
                                            "vcf",
                                            "bed",
                                            "tsv",
                                            "tsv",
                                            "csv",
                                            "json",
                                            "tbl",
                                            "parquet",
                                            "duckdb",
                                        ]:
                                            annotation_tool = "parquet"
                                        elif quick_annotation_format in ["bw"]:
                                            annotation_tool = "bigwig"
                                        else:
                                            log.error(
                                                f"Quick Annotation File {annotation_file_found} - Format {quick_annotation_format} not supported yet"
                                            )
                                            raise ValueError(
                                                f"Quick Annotation File {annotation_file_found} - Format {quick_annotation_format} not supported yet"
                                            )

                                    log.debug(
                                        f"Quick Annotation File {annotation_file} - Annotation tool: {annotation_tool}"
                                    )

                                    # Annotation Tool dispatch
                                    if annotation_tool:
                                        if annotation_tool not in param["annotation"]:
                                            param["annotation"][annotation_tool] = {}
                                        if (
                                            "annotations"
                                            not in param["annotation"][annotation_tool]
                                        ):
                                            param["annotation"][annotation_tool][
                                                "annotations"
                                            ] = {}
                                        param["annotation"][annotation_tool][
                                            "annotations"
                                        ][annotation_file_found] = annotations

                                else:
                                    log.warning(
                                        f"Quick Annotation File {annotation_file} does NOT exist"
                                    )

                self.set_param(param)

        if param.get("annotation", None):
            log.info("Annotations")
            if param.get("annotation", {}).get("parquet", None):
                log.info("Annotations 'parquet'...")
                self.annotation_parquet()
            if param.get("annotation", {}).get("bcftools", None):
                log.info("Annotations 'bcftools'...")
                self.annotation_bcftools()
            if param.get("annotation", {}).get("snpsift", None):
                log.info("Annotations 'snpsift'...")
                self.annotation_snpsift()
            if param.get("annotation", {}).get("bigwig", None):
                log.info("Annotations 'bigwig'...")
                self.annotation_bigwig()
            if param.get("annotation", {}).get("annovar", None):
                log.info("Annotations 'annovar'...")
                self.annotation_annovar()
            if param.get("annotation", {}).get("snpeff", None):
                log.info("Annotations 'snpeff'...")
                self.annotation_snpeff()
            if param.get("annotation", {}).get("exomiser", None) is not None:
                log.info("Annotations 'exomiser'...")
                self.annotation_exomiser()
            if param.get("annotation", {}).get("splice", None) is not None:
                log.info("Annotations 'splice' ...")
                self.annotation_splice()

        # # Explode INFOS fields into table fields
        # if self.get_explode_infos():
        #     self.explode_infos(
        #         prefix=self.get_explode_infos_prefix(),
        #         fields=self.get_explode_infos_fields(),
        #         force=True,
        #     )

    def annotation_bigwig(self, threads: int = None) -> None:
        """
        The function `annotation_bigwig` annotates variants in a VCF file using bigwig databases.

        :param threads: The `threads` parameter in the `annotation_bigwig` method is used to specify the
        number of threads to be used for parallel processing during the annotation process. If the
        `threads` parameter is not provided, the method will attempt to determine the optimal number of
        threads to use based on the system configuration
        :type threads: int
        :return: True
        """

        # DEBUG
        log.debug("Start annotation with bigwig databases")

        # # Threads
        # if not threads:
        #     threads = self.get_threads()
        # log.debug("Threads: " + str(threads))

        # Config
        config = self.get_config()
        log.debug("Config: " + str(config))

        # Config - BCFTools databases folders
        databases_folders = set(
            self.get_config()
            .get("folders", {})
            .get("databases", {})
            .get("annotations", [DEFAULT_ANNOTATIONS_FOLDER])
            + self.get_config()
            .get("folders", {})
            .get("databases", {})
            .get("bigwig", [DEFAULT_BIGWIG_FOLDER])
        )
        log.debug("Databases annotations: " + str(databases_folders))

        # Param
        annotations = (
            self.get_param()
            .get("annotation", {})
            .get("bigwig", {})
            .get("annotations", None)
        )
        log.debug("Annotations: " + str(annotations))

        # Assembly
        assembly = self.get_param().get(
            "assembly", self.get_config().get("assembly", DEFAULT_ASSEMBLY)
        )

        # Data
        table_variants = self.get_table_variants()

        # Check if not empty
        log.debug("Check if not empty")
        sql_query_chromosomes = (
            f"""SELECT count(*) as count FROM {table_variants} as table_variants"""
        )
        sql_query_chromosomes_df = self.get_query_to_df(sql_query_chromosomes)
        if not sql_query_chromosomes_df["count"][0]:
            log.info(f"VCF empty")
            return

        # VCF header
        vcf_reader = self.get_header()
        log.debug("Initial header: " + str(vcf_reader.infos))

        # Existing annotations
        for vcf_annotation in self.get_header().infos:

            vcf_annotation_line = self.get_header().infos.get(vcf_annotation)
            log.debug(
                f"Existing annotations in VCF: {vcf_annotation} [{vcf_annotation_line}]"
            )

        if annotations:

            with TemporaryDirectory(dir=self.get_tmp_dir()) as tmp_dir:

                # Export VCF file
                tmp_vcf_name = os.path.join(tmp_dir, "input.vcf.gz")

                # annotation_bigwig_config
                annotation_bigwig_config_list = []

                for annotation in annotations:
                    annotation_fields = annotations[annotation]

                    # Annotation Name
                    annotation_name = os.path.basename(annotation)

                    if not annotation_fields:
                        annotation_fields = {"INFO": None}

                    log.debug(f"Annotation '{annotation_name}'")
                    log.debug(
                        f"Annotation '{annotation_name}' - fields: {annotation_fields}"
                    )

                    # Create Database
                    database = Database(
                        database=annotation,
                        databases_folders=databases_folders,
                        assembly=assembly,
                    )

                    # Find files
                    db_file = database.get_database()
                    db_file = full_path(db_file)
                    db_hdr_file = database.get_header_file()
                    db_hdr_file = full_path(db_hdr_file)
                    db_file_type = database.get_format()

                    # If db_file is http ?
                    if database.get_database().startswith("http"):

                        # Datbase is HTTP URL
                        db_file_is_http = True

                        # DB file keep as URL
                        db_file = database.get_database()
                        log.warning(
                            f"Annotations 'bigwig' database '{db_file}' - is an HTTP URL (experimental)"
                        )

                        # Retrieve automatic annotation field name
                        annotation_field = clean_annotation_field(
                            os.path.basename(db_file).replace(".bw", "")
                        )
                        log.debug(
                            f"Create header file with annotation field '{annotation_field}' is an HTTP URL"
                        )

                        # Create automatic header file
                        db_hdr_file = os.path.join(tmp_dir, "header.hdr")
                        with open(db_hdr_file, "w") as f:
                            f.write("##fileformat=VCFv4.2\n")
                            f.write(
                                f"""##INFO=<ID={annotation_field},Number=.,Type=Float,Description="{annotation_field} annotation from {db_file}">\n"""
                            )
                            f.write(f"#CHROM	START	END	{annotation_field}\n")

                    else:

                        # Datbase is NOT HTTP URL
                        db_file_is_http = False

                    # Check index - try to create if not exists
                    if (
                        db_file is None
                        or db_hdr_file is None
                        or (not os.path.exists(db_file) and not db_file_is_http)
                        or not os.path.exists(db_hdr_file)
                        or not db_file_type in ["bw"]
                    ):
                        # if False:
                        log.error("Annotation failed: database not valid")
                        log.error(f"Annotation annotation file: {db_file}")
                        log.error(f"Annotation annotation file type: {db_file_type}")
                        log.error(f"Annotation annotation header: {db_hdr_file}")
                        raise ValueError(
                            f"Annotation failed: database not valid - annotation file {db_file} / annotation file type {db_file_type} / annotation header {db_hdr_file}"
                        )
                    else:

                        # Log
                        log.debug(
                            f"Annotation '{annotation}' - file: "
                            + str(db_file)
                            + " and "
                            + str(db_hdr_file)
                        )

                        # Load header as VCF object
                        db_hdr_vcf = Variants(input=db_hdr_file)
                        db_hdr_vcf_header_infos = db_hdr_vcf.get_header().infos
                        log.debug(
                            "Annotation database header: "
                            + str(db_hdr_vcf_header_infos)
                        )

                        # For all fields in database
                        annotation_fields_full = False
                        if "ALL" in annotation_fields or "INFO" in annotation_fields:
                            annotation_fields = {
                                key: key for key in db_hdr_vcf_header_infos
                            }
                            log.debug(
                                "Annotation database header - All annotations added: "
                                + str(annotation_fields)
                            )
                            annotation_fields_full = True

                        # Init
                        cyvcf2_header_rename_dict = {}
                        cyvcf2_header_list = []
                        cyvcf2_header_indexes = {}

                        # process annotation fields
                        for annotation_field in annotation_fields:

                            # New annotation name
                            annotation_field_new = annotation_fields[annotation_field]

                            # Check annotation field and index in header
                            if (
                                annotation_field
                                in db_hdr_vcf.get_header_columns_as_list()
                            ):
                                annotation_field_index = (
                                    db_hdr_vcf.get_header_columns_as_list().index(
                                        annotation_field
                                    )
                                    - 3
                                )
                                cyvcf2_header_indexes[annotation_field_new] = (
                                    annotation_field_index
                                )
                            else:
                                msg_err = f"Database '{db_file}' does NOT contain annotation field '{annotation_field}'"
                                log.error(msg_err)
                                raise ValueError(msg_err)

                            # Append annotation field in cyvcf2 header list
                            cyvcf2_header_rename_dict[annotation_field_new] = (
                                db_hdr_vcf_header_infos[annotation_field].id
                            )
                            cyvcf2_header_list.append(
                                {
                                    "ID": annotation_field_new,
                                    "Number": db_hdr_vcf_header_infos[
                                        annotation_field
                                    ].num,
                                    "Type": db_hdr_vcf_header_infos[
                                        annotation_field
                                    ].type,
                                    "Description": db_hdr_vcf_header_infos[
                                        annotation_field
                                    ].desc,
                                }
                            )

                            # Add header on VCF
                            vcf_reader.infos[annotation_field_new] = vcf.parser._Info(
                                annotation_field_new,
                                db_hdr_vcf_header_infos[annotation_field].num,
                                db_hdr_vcf_header_infos[annotation_field].type,
                                db_hdr_vcf_header_infos[annotation_field].desc,
                                "HOWARD BigWig annotation",
                                "unknown",
                                self.code_type_map[
                                    db_hdr_vcf_header_infos[annotation_field].type
                                ],
                            )

                        # Load bigwig database
                        bw_db = pyBigWig.open(db_file)
                        if bw_db.isBigWig():
                            log.debug(f"Database '{db_file}' is in 'BigWig' format")
                        else:
                            msg_err = f"Database '{db_file}' is NOT in 'BigWig' format"
                            log.error(msg_err)
                            raise ValueError(msg_err)

                        annotation_bigwig_config_list.append(
                            {
                                "db_file": db_file,
                                "bw_db": bw_db,
                                "cyvcf2_header_rename_dict": cyvcf2_header_rename_dict,
                                "cyvcf2_header_list": cyvcf2_header_list,
                                "cyvcf2_header_indexes": cyvcf2_header_indexes,
                            }
                        )

                # Annotate
                if annotation_bigwig_config_list:

                    # Annotation config
                    log.debug(
                        f"annotation_bigwig_config={annotation_bigwig_config_list}"
                    )

                    # Export VCF file
                    self.export_variant_vcf(
                        vcf_file=tmp_vcf_name,
                        remove_info=True,
                        add_samples=False,
                        index=True,
                    )

                    # Load input tmp file
                    input_vcf = cyvcf2.VCF(tmp_vcf_name)

                    # Add header in input file
                    for annotation_bigwig_config in annotation_bigwig_config_list:
                        for cyvcf2_header_field in annotation_bigwig_config.get(
                            "cyvcf2_header_list", []
                        ):
                            log.info(
                                f"Annotations 'bigwig' database '{os.path.basename(annotation_bigwig_config.get('db_file'))}' - annotation field '{annotation_bigwig_config.get('cyvcf2_header_rename_dict',{}).get(cyvcf2_header_field.get('ID','Unknown'))}' -> '{cyvcf2_header_field.get('ID')}'"
                            )
                            input_vcf.add_info_to_header(cyvcf2_header_field)

                    # Create output VCF file
                    output_vcf_file = os.path.join(tmp_dir, "output.vcf.gz")
                    output_vcf = cyvcf2.Writer(output_vcf_file, input_vcf)

                    # Fetch variants
                    log.info(f"Annotations 'bigwig' start...")
                    for variant in input_vcf:

                        for annotation_bigwig_config in annotation_bigwig_config_list:

                            # DB and indexes
                            bw_db = annotation_bigwig_config.get("bw_db", None)
                            bw_db_file = annotation_bigwig_config.get("db_file", None)
                            cyvcf2_header_indexes = annotation_bigwig_config.get(
                                "cyvcf2_header_indexes", None
                            )

                            # Retrieve value from chrom pos
                            res = bw_db.values(
                                variant.CHROM, variant.POS - 1, variant.POS
                            )

                            # For each annotation fields (and indexes)
                            for cyvcf2_header_index in cyvcf2_header_indexes:

                                # If value is NOT nNone
                                if not np.isnan(
                                    res[cyvcf2_header_indexes[cyvcf2_header_index]]
                                ):
                                    variant.INFO[cyvcf2_header_index] = res[
                                        cyvcf2_header_indexes[cyvcf2_header_index]
                                    ]

                        # Add record in output file
                        output_vcf.write_record(variant)

                    # Close bw db
                    for annotation_bigwig_config in annotation_bigwig_config_list:

                        # DB and indexes
                        bw_db = annotation_bigwig_config.get("bw_db", None)
                        bw_db_file = annotation_bigwig_config.get("db_file", None)

                        # Try Close bw db
                        try:
                            if bw_db is not None:
                                log.debug(
                                    f"Annotations 'bigwig' file '{bw_db_file}' closing..."
                                )
                                bw_db.close()
                                log.debug(
                                    f"Annotations 'bigwig' file '{bw_db_file}' closed"
                                )
                            else:
                                log.debug(
                                    f"Annotations 'bigwig' file '{bw_db_file}' is already closed or not open"
                                )
                        except RuntimeError as e:
                            log.error(
                                f"RuntimeError while closing 'bigwig' file '{bw_db_file}': {e}"
                            )
                        except Exception as e:
                            log.error(
                                f"Unexpected error while closing 'bigwig' file '{bw_db_file}': {e}"
                            )

                    # Log
                    log.debug(f"Annotation done.")

                    # Close and write file
                    log.info(f"Annotations 'bigwig' write...")
                    output_vcf.close()
                    log.debug(f"Write done.")

                    # Update variants
                    log.info(f"Annotations 'bigwig' update...")
                    self.update_from_vcf(output_vcf_file)
                    remove_if_exists([output_vcf_file])
                    log.debug(f"Update done.")

        return True

    def annotation_snpsift(self, threads: int = None) -> None:
        """
        This function annotate with bcftools

        :param threads: Number of threads to use
        :return: the value of the variable "return_value".
        """

        # DEBUG
        log.debug("Start annotation with bcftools databases")

        # Threads
        if not threads:
            threads = self.get_threads()
        log.debug("Threads: " + str(threads))

        # Config
        config = self.get_config()
        log.debug("Config: " + str(config))

        # Config - snpSift
        snpsift_bin_command = get_bin_command(
            bin="SnpSift.jar",
            tool="snpsift",
            bin_type="jar",
            config=config,
            default_folder=f"{DEFAULT_TOOLS_FOLDER}/snpeff",
        )
        if not snpsift_bin_command:
            msg_err = f"Annotation failed: no snpsift bin '{snpsift_bin_command}'"
            log.error(msg_err)
            raise ValueError(msg_err)

        # Config - bcftools
        bcftools_bin_command = get_bin_command(
            bin="bcftools",
            tool="bcftools",
            bin_type="bin",
            config=config,
            default_folder=f"{DEFAULT_TOOLS_FOLDER}/bcftools",
        )
        if not bcftools_bin_command:
            msg_err = f"Annotation failed: no bcftools bin '{bcftools_bin_command}'"
            log.error(msg_err)
            raise ValueError(msg_err)

        # Config - BCFTools databases folders
        databases_folders = set(
            self.get_config()
            .get("folders", {})
            .get("databases", {})
            .get("annotations", [DEFAULT_ANNOTATIONS_FOLDER])
            + self.get_config()
            .get("folders", {})
            .get("databases", {})
            .get("bcftools", [DEFAULT_BCFTOOLS_FOLDER])
        )
        log.debug("Databases annotations: " + str(databases_folders))

        # Param
        annotations = (
            self.get_param()
            .get("annotation", {})
            .get("snpsift", {})
            .get("annotations", None)
        )
        log.debug("Annotations: " + str(annotations))

        # Assembly
        assembly = self.get_param().get(
            "assembly", self.get_config().get("assembly", DEFAULT_ASSEMBLY)
        )

        # Data
        table_variants = self.get_table_variants()

        # Check if not empty
        log.debug("Check if not empty")
        sql_query_chromosomes = (
            f"""SELECT count(*) as count FROM {table_variants} as table_variants"""
        )
        sql_query_chromosomes_df = self.get_query_to_df(sql_query_chromosomes)
        if not sql_query_chromosomes_df["count"][0]:
            log.info(f"VCF empty")
            return

        # VCF header
        vcf_reader = self.get_header()
        log.debug("Initial header: " + str(vcf_reader.infos))

        # Existing annotations
        for vcf_annotation in self.get_header().infos:

            vcf_annotation_line = self.get_header().infos.get(vcf_annotation)
            log.debug(
                f"Existing annotations in VCF: {vcf_annotation} [{vcf_annotation_line}]"
            )

        if annotations:

            with TemporaryDirectory(dir=self.get_tmp_dir()) as tmp_dir:

                # Export VCF file
                tmp_vcf_name = os.path.join(tmp_dir, "input.vcf.gz")

                # Init
                commands = {}

                for annotation in annotations:
                    annotation_fields = annotations[annotation]

                    # Annotation Name
                    annotation_name = os.path.basename(annotation)

                    if not annotation_fields:
                        annotation_fields = {"INFO": None}

                    log.debug(f"Annotation '{annotation_name}'")
                    log.debug(
                        f"Annotation '{annotation_name}' - fields: {annotation_fields}"
                    )

                    # Create Database
                    database = Database(
                        database=annotation,
                        databases_folders=databases_folders,
                        assembly=assembly,
                    )

                    # Find files
                    db_file = database.get_database()
                    db_file = full_path(db_file)
                    db_hdr_file = database.get_header_file()
                    db_hdr_file = full_path(db_hdr_file)
                    db_file_type = database.get_format()
                    db_tbi_file = f"{db_file}.tbi"
                    db_file_compressed = database.is_compressed()

                    # Check if compressed
                    if not db_file_compressed:
                        log.error(
                            f"Annotation '{annotation}' - {db_file} NOT compressed file"
                        )
                        raise ValueError(
                            f"Annotation '{annotation}' - {db_file} NOT compressed file"
                        )

                    # Check if indexed
                    if not os.path.exists(db_tbi_file):
                        log.error(
                            f"Annotation '{annotation}' - {db_file} NOT indexed file"
                        )
                        raise ValueError(
                            f"Annotation '{annotation}' - {db_file} NOT indexed file"
                        )

                    # Check index - try to create if not exists
                    if not os.path.exists(db_file) or not os.path.exists(db_hdr_file):
                        log.error("Annotation failed: database not valid")
                        log.error(f"Annotation annotation file: {db_file}")
                        log.error(f"Annotation annotation header: {db_hdr_file}")
                        log.error(f"Annotation annotation index: {db_tbi_file}")
                        raise ValueError(
                            f"Annotation failed: database not valid - annotation file {db_file} / annotation header {db_hdr_file} / annotation index {db_tbi_file} / annotation compression {db_file_compressed}"
                        )
                    else:

                        log.debug(
                            f"Annotation '{annotation}' - file: "
                            + str(db_file)
                            + " and "
                            + str(db_hdr_file)
                        )

                        # Load header as VCF object
                        db_hdr_vcf = Variants(input=db_hdr_file)
                        db_hdr_vcf_header_infos = db_hdr_vcf.get_header().infos
                        log.debug(
                            "Annotation database header: "
                            + str(db_hdr_vcf_header_infos)
                        )

                        # For all fields in database
                        annotation_fields_full = False
                        if "ALL" in annotation_fields or "INFO" in annotation_fields:
                            annotation_fields = {
                                key: key for key in db_hdr_vcf_header_infos
                            }
                            log.debug(
                                "Annotation database header - All annotations added: "
                                + str(annotation_fields)
                            )
                            annotation_fields_full = True

                        # # Create file for field rename
                        # log.debug("Create file for field rename")
                        # tmp_rename = NamedTemporaryFile(
                        #     prefix=self.get_prefix(),
                        #     dir=self.get_tmp_dir(),
                        #     suffix=".rename",
                        #     delete=False,
                        # )
                        # tmp_rename_name = tmp_rename.name
                        # tmp_files.append(tmp_rename_name)

                        # Number of fields
                        nb_annotation_field = 0
                        annotation_list = []
                        annotation_infos_rename_list = []

                        for annotation_field in annotation_fields:

                            # field new name, if parametered SKIPPED !!!!!! not managed actually TODO
                            annotation_fields_new_name = annotation_fields.get(
                                annotation_field, annotation_field
                            )
                            if not annotation_fields_new_name:
                                annotation_fields_new_name = annotation_field

                            # Check if field is in DB and if field is not elready in input data
                            if (
                                annotation_field in db_hdr_vcf.get_header().infos
                                and annotation_fields_new_name
                                not in self.get_header().infos
                            ):

                                log.info(
                                    f"Annotation '{annotation_name}' - '{annotation_field}' -> '{annotation_fields_new_name}'"
                                )

                                # BCFTools annotate param to rename fields
                                if annotation_field != annotation_fields_new_name:
                                    annotation_infos_rename_list.append(
                                        f"{annotation_fields_new_name}:=INFO/{annotation_field}"
                                    )

                                # Add INFO field to header
                                db_hdr_vcf_header_infos_number = (
                                    db_hdr_vcf_header_infos[annotation_field].num or "."
                                )
                                db_hdr_vcf_header_infos_type = (
                                    db_hdr_vcf_header_infos[annotation_field].type
                                    or "String"
                                )
                                db_hdr_vcf_header_infos_description = (
                                    db_hdr_vcf_header_infos[annotation_field].desc
                                    or f"{annotation_field} description"
                                )
                                db_hdr_vcf_header_infos_source = (
                                    db_hdr_vcf_header_infos[annotation_field].source
                                    or "unknown"
                                )
                                db_hdr_vcf_header_infos_version = (
                                    db_hdr_vcf_header_infos[annotation_field].version
                                    or "unknown"
                                )

                                vcf_reader.infos[annotation_fields_new_name] = (
                                    vcf.parser._Info(
                                        annotation_fields_new_name,
                                        db_hdr_vcf_header_infos_number,
                                        db_hdr_vcf_header_infos_type,
                                        db_hdr_vcf_header_infos_description,
                                        db_hdr_vcf_header_infos_source,
                                        db_hdr_vcf_header_infos_version,
                                        self.code_type_map[
                                            db_hdr_vcf_header_infos_type
                                        ],
                                    )
                                )

                                annotation_list.append(annotation_field)

                                nb_annotation_field += 1

                            else:

                                if (
                                    annotation_field
                                    not in db_hdr_vcf.get_header().infos
                                ):
                                    log.warning(
                                        f"Annotation '{annotation_name}' - '{annotation_field}' - not available in vcf/bed file"
                                    )
                                if (
                                    annotation_fields_new_name
                                    in self.get_header().infos
                                ):
                                    log.warning(
                                        f"Annotation '{annotation_name}' - '{annotation_fields_new_name}' - already exists (skipped)"
                                    )

                        log.info(
                            f"Annotation '{annotation_name}' - {nb_annotation_field} annotations available in vcf/bed file"
                        )

                        annotation_infos = ",".join(annotation_list)

                        if annotation_infos != "":

                            # Annotated VCF (and error file)
                            tmp_annotation_vcf_name = os.path.join(
                                tmp_dir, os.path.basename(annotation) + ".vcf.gz"
                            )
                            tmp_annotation_vcf_name_err = (
                                tmp_annotation_vcf_name + ".err"
                            )

                            # Add fields to annotate
                            if not annotation_fields_full:
                                annotation_infos_option = f"-info {annotation_infos}"
                            else:
                                annotation_infos_option = ""

                            # Info fields rename
                            if annotation_infos_rename_list:
                                annotation_infos_rename = " -c " + ",".join(
                                    annotation_infos_rename_list
                                )
                            else:
                                annotation_infos_rename = ""

                            # Annotate command
                            command_annotate = f"{snpsift_bin_command} annotate {annotation_infos_option} {db_file} {tmp_vcf_name} 2>>{tmp_annotation_vcf_name_err} | {bcftools_bin_command} annotate --threads={threads} {annotation_infos_rename} -Oz1 -o {tmp_annotation_vcf_name} 2>>{tmp_annotation_vcf_name_err} "

                            # Add command
                            commands[command_annotate] = tmp_annotation_vcf_name

                if commands:

                    # Export VCF file
                    self.export_variant_vcf(
                        vcf_file=tmp_vcf_name,
                        remove_info=True,
                        add_samples=False,
                        index=True,
                    )

                    # Num command
                    nb_command = 0

                    # Annotate
                    for command_annotate in commands:
                        nb_command += 1
                        log.info(
                            f"Annotation - Annotate [{nb_command}/{len(commands)}]..."
                        )
                        log.debug(f"command_annotate={command_annotate}")
                        run_parallel_commands([command_annotate], threads)

                        # Update variants
                        log.info(
                            f"Annotation - Updating [{nb_command}/{len(commands)}]..."
                        )
                        self.update_from_vcf(commands[command_annotate])
                        remove_if_exists(
                            [
                                commands[command_annotate],
                                commands[command_annotate] + ".tbi",
                            ]
                        )

    def annotation_bcftools(self, threads: int = None) -> None:
        """
        This function annotate with bcftools

        :param threads: Number of threads to use
        :return: the value of the variable "return_value".
        """

        # DEBUG
        log.debug("Start annotation with bcftools databases")

        # Threads
        if not threads:
            threads = self.get_threads()
        log.debug("Threads: " + str(threads))

        # Config
        config = self.get_config()
        log.debug("Config: " + str(config))

        # DEBUG
        delete_tmp = True
        if self.get_config().get("verbosity", "warning") in ["debug"]:
            delete_tmp = False
            log.debug("Delete tmp files/folders: " + str(delete_tmp))

        # Config - BCFTools bin command
        bcftools_bin_command = get_bin_command(
            bin="bcftools",
            tool="bcftools",
            bin_type="bin",
            config=config,
            default_folder=f"{DEFAULT_TOOLS_FOLDER}/bcftools",
        )
        if not bcftools_bin_command:
            msg_err = f"Annotation failed: no bcftools bin '{bcftools_bin_command}'"
            log.error(msg_err)
            raise ValueError(msg_err)

        # Config - BCFTools databases folders
        databases_folders = set(
            self.get_config()
            .get("folders", {})
            .get("databases", {})
            .get("annotations", [DEFAULT_ANNOTATIONS_FOLDER])
            + self.get_config()
            .get("folders", {})
            .get("databases", {})
            .get("bcftools", [DEFAULT_BCFTOOLS_FOLDER])
        )
        log.debug("Databases annotations: " + str(databases_folders))

        # Param
        annotations = (
            self.get_param()
            .get("annotation", {})
            .get("bcftools", {})
            .get("annotations", None)
        )
        log.debug("Annotations: " + str(annotations))

        # Assembly
        assembly = self.get_param().get(
            "assembly", self.get_config().get("assembly", DEFAULT_ASSEMBLY)
        )

        # Data
        table_variants = self.get_table_variants()

        # Check if not empty
        log.debug("Check if not empty")
        sql_query_chromosomes = (
            f"""SELECT count(*) as count FROM {table_variants} as table_variants"""
        )
        sql_query_chromosomes_df = self.get_query_to_df(sql_query_chromosomes)
        if not sql_query_chromosomes_df["count"][0]:
            log.info(f"VCF empty")
            return

        # Export in VCF
        log.debug("Create initial file to annotate")
        tmp_vcf = NamedTemporaryFile(
            prefix=self.get_prefix(),
            dir=self.get_tmp_dir(),
            suffix=".vcf.gz",
            delete=False,
        )
        tmp_vcf_name = tmp_vcf.name

        # VCF header
        vcf_reader = self.get_header()
        log.debug("Initial header: " + str(vcf_reader.infos))

        # Existing annotations
        for vcf_annotation in self.get_header().infos:

            vcf_annotation_line = self.get_header().infos.get(vcf_annotation)
            log.debug(
                f"Existing annotations in VCF: {vcf_annotation} [{vcf_annotation_line}]"
            )

        if annotations:

            tmp_ann_vcf_list = []
            commands = []
            tmp_files = []
            err_files = []

            for annotation in annotations:
                annotation_fields = annotations[annotation]

                # Annotation Name
                annotation_name = os.path.basename(annotation)

                if not annotation_fields:
                    annotation_fields = {"INFO": None}

                log.debug(f"Annotation '{annotation_name}'")
                log.debug(
                    f"Annotation '{annotation_name}' - fields: {annotation_fields}"
                )

                # Create Database
                database = Database(
                    database=annotation,
                    databases_folders=databases_folders,
                    assembly=assembly,
                )

                # Find files
                db_file = database.get_database()
                db_file = full_path(db_file)
                db_hdr_file = database.get_header_file()
                db_hdr_file = full_path(db_hdr_file)
                db_file_type = database.get_format()
                db_tbi_file = f"{db_file}.tbi"
                db_file_compressed = database.is_compressed()

                # Check if compressed
                if not db_file_compressed:
                    log.error(
                        f"Annotation '{annotation}' - {db_file} NOT compressed file"
                    )
                    raise ValueError(
                        f"Annotation '{annotation}' - {db_file} NOT compressed file"
                    )

                # Check if indexed
                if not os.path.exists(db_tbi_file):
                    log.error(f"Annotation '{annotation}' - {db_file} NOT indexed file")
                    raise ValueError(
                        f"Annotation '{annotation}' - {db_file} NOT indexed file"
                    )

                # Check index - try to create if not exists
                if not os.path.exists(db_file) or not os.path.exists(db_hdr_file):
                    log.error("Annotation failed: database not valid")
                    log.error(f"Annotation annotation file: {db_file}")
                    log.error(f"Annotation annotation header: {db_hdr_file}")
                    log.error(f"Annotation annotation index: {db_tbi_file}")
                    raise ValueError(
                        f"Annotation failed: database not valid - annotation file {db_file} / annotation header {db_hdr_file} / annotation index {db_tbi_file} / annotation compression {db_file_compressed}"
                    )
                else:

                    log.debug(
                        f"Annotation '{annotation}' - file: "
                        + str(db_file)
                        + " and "
                        + str(db_hdr_file)
                    )

                    # Load header as VCF object
                    db_hdr_vcf = Variants(input=db_hdr_file)
                    db_hdr_vcf_header_infos = db_hdr_vcf.get_header().infos
                    log.debug(
                        "Annotation database header: " + str(db_hdr_vcf_header_infos)
                    )

                    # For all fields in database
                    if "ALL" in annotation_fields or "INFO" in annotation_fields:
                        annotation_fields = {
                            key: key for key in db_hdr_vcf_header_infos
                        }
                        log.debug(
                            "Annotation database header - All annotations added: "
                            + str(annotation_fields)
                        )

                    # Number of fields
                    nb_annotation_field = 0
                    annotation_list = []

                    for annotation_field in annotation_fields:

                        # field new name, if parametered SKIPPED !!!!!! not managed actually TODO
                        annotation_fields_new_name = annotation_fields.get(
                            annotation_field, annotation_field
                        )
                        if not annotation_fields_new_name:
                            annotation_fields_new_name = annotation_field

                        # Check if field is in DB and if field is not elready in input data
                        if (
                            annotation_field in db_hdr_vcf.get_header().infos
                            and annotation_fields_new_name
                            not in self.get_header().infos
                        ):

                            log.info(
                                f"Annotation '{annotation_name}' - '{annotation_field}' -> '{annotation_fields_new_name}'"
                            )

                            # Add INFO field to header
                            db_hdr_vcf_header_infos_number = (
                                db_hdr_vcf_header_infos[annotation_field].num or "."
                            )
                            db_hdr_vcf_header_infos_type = (
                                db_hdr_vcf_header_infos[annotation_field].type
                                or "String"
                            )
                            db_hdr_vcf_header_infos_description = (
                                db_hdr_vcf_header_infos[annotation_field].desc
                                or f"{annotation_field} description"
                            )
                            db_hdr_vcf_header_infos_source = (
                                db_hdr_vcf_header_infos[annotation_field].source
                                or "unknown"
                            )
                            db_hdr_vcf_header_infos_version = (
                                db_hdr_vcf_header_infos[annotation_field].version
                                or "unknown"
                            )

                            vcf_reader.infos[annotation_fields_new_name] = (
                                vcf.parser._Info(
                                    annotation_fields_new_name,
                                    db_hdr_vcf_header_infos_number,
                                    db_hdr_vcf_header_infos_type,
                                    db_hdr_vcf_header_infos_description,
                                    db_hdr_vcf_header_infos_source,
                                    db_hdr_vcf_header_infos_version,
                                    self.code_type_map[db_hdr_vcf_header_infos_type],
                                )
                            )

                            # annotation_list.append(annotation_field)
                            if annotation_field != annotation_fields_new_name:
                                annotation_list.append(
                                    f"{annotation_fields_new_name}:=INFO/{annotation_field}"
                                )
                            else:
                                annotation_list.append(annotation_field)

                            nb_annotation_field += 1

                        else:

                            if annotation_field not in db_hdr_vcf.get_header().infos:
                                log.warning(
                                    f"Annotation '{annotation}' - '{annotation_field}' - not available in vcf/bed file"
                                )
                            if annotation_fields_new_name in self.get_header().infos:
                                log.warning(
                                    f"Annotation '{annotation}' - '{annotation_fields_new_name}' - already exists (skipped)"
                                )

                    log.info(
                        f"Annotation '{annotation_name}' - {nb_annotation_field} annotations available in vcf/bed file"
                    )

                    annotation_infos = ",".join(annotation_list)

                    if annotation_infos != "":

                        # Protect header for bcftools (remove "#CHROM" and variants line)
                        log.debug("Protect Header file - remove #CHROM line if exists")
                        tmp_header_vcf = NamedTemporaryFile(
                            prefix=self.get_prefix(),
                            dir=self.get_tmp_dir(),
                            suffix=".hdr",
                            delete=False,
                        )
                        tmp_header_vcf_name = tmp_header_vcf.name
                        tmp_files.append(tmp_header_vcf_name)
                        # Command
                        if db_hdr_file.endswith(".gz"):
                            command_extract_header = f"zcat < {db_hdr_file} | grep '^##' > {tmp_header_vcf_name}"
                        else:
                            command_extract_header = f"cat < {db_hdr_file} | grep '^##' > {tmp_header_vcf_name}"
                        # Run
                        run_parallel_commands([command_extract_header], 1)

                        # Find chomosomes
                        log.debug("Find chromosomes ")
                        sql_query_chromosomes = f"""SELECT table_variants.\"#CHROM\" as CHROM FROM {table_variants} as table_variants GROUP BY table_variants.\"#CHROM\""""
                        sql_query_chromosomes_df = self.get_query_to_df(
                            sql_query_chromosomes
                        )
                        chomosomes_list = list(sql_query_chromosomes_df["CHROM"])

                        log.debug("Chromosomes found: " + str(list(chomosomes_list)))

                        # BED columns in the annotation file
                        if db_file_type in ["bed"]:
                            annotation_infos = "CHROM,POS,POS," + annotation_infos

                        for chrom in chomosomes_list:

                            # Create BED on initial VCF
                            log.debug("Create BED on initial VCF: " + str(tmp_vcf_name))
                            tmp_bed = NamedTemporaryFile(
                                prefix=self.get_prefix(),
                                dir=self.get_tmp_dir(),
                                suffix=".bed",
                                delete=False,
                            )
                            tmp_bed_name = tmp_bed.name
                            tmp_files.append(tmp_bed_name)

                            # Detecte regions
                            log.debug(
                                f"Annotation '{annotation}' - Chromosome '{chrom}' - Start detecting regions..."
                            )
                            window = 1000000
                            sql_query_intervals_for_bed = f"""
                                SELECT  \"#CHROM\",
                                        CASE WHEN \"POS\"-{window}-1 < 0 THEN 0 ELSE \"POS\"-{window}-1 END,
                                        \"POS\"+{window}
                                FROM {table_variants} as table_variants
                                WHERE table_variants.\"#CHROM\" = '{chrom}'
                            """
                            regions = self.conn.execute(
                                sql_query_intervals_for_bed
                            ).fetchall()
                            merged_regions = merge_regions(regions)
                            log.debug(
                                f"Annotation '{annotation}' - Chromosome '{chrom}' - Stop detecting regions..."
                            )

                            header = ["#CHROM", "START", "END"]
                            with open(tmp_bed_name, "w") as f:
                                # Write the header with tab delimiter
                                f.write("\t".join(header) + "\n")
                                for d in merged_regions:
                                    # Write each data row with tab delimiter
                                    f.write("\t".join(map(str, d)) + "\n")

                            # Tmp files
                            tmp_annotation_vcf = NamedTemporaryFile(
                                prefix=self.get_prefix(),
                                dir=self.get_tmp_dir(),
                                suffix=".vcf.gz",
                                delete=False,
                            )
                            tmp_annotation_vcf_name = tmp_annotation_vcf.name
                            tmp_files.append(tmp_annotation_vcf_name)
                            tmp_ann_vcf_list.append(f"{tmp_annotation_vcf_name}")
                            tmp_annotation_vcf_name_err = (
                                tmp_annotation_vcf_name + ".err"
                            )
                            err_files.append(tmp_annotation_vcf_name_err)

                            # Annotate Command
                            log.debug(
                                f"Annotation '{annotation}' - add bcftools command"
                            )

                            # Command
                            command_annotate = f"{bcftools_bin_command} annotate --pair-logic exact --regions-file={tmp_bed_name} -a {db_file} -h {tmp_header_vcf_name} -c {annotation_infos} {tmp_vcf_name} -o {tmp_annotation_vcf_name} -Oz1 2>>{tmp_annotation_vcf_name_err} && tabix {tmp_annotation_vcf_name} 2>>{tmp_annotation_vcf_name_err} "

                            # Add command
                            commands.append(command_annotate)

            # if some commands
            if commands:

                # Export VCF file
                self.export_variant_vcf(
                    vcf_file=tmp_vcf_name,
                    remove_info=True,
                    add_samples=False,
                    index=True,
                )

                # Threads
                # calculate threads for annotated commands
                if commands:
                    threads_bcftools_annotate = round(threads / len(commands))
                else:
                    threads_bcftools_annotate = 1

                if not threads_bcftools_annotate:
                    threads_bcftools_annotate = 1

                # Add threads option to bcftools commands
                if threads_bcftools_annotate > 1:
                    commands_threaded = []
                    for command in commands:
                        commands_threaded.append(
                            command.replace(
                                f"{bcftools_bin_command} annotate ",
                                f"{bcftools_bin_command} annotate --threads={threads_bcftools_annotate} ",
                            )
                        )
                    commands = commands_threaded

                # Command annotation multithreading
                log.debug(f"Annotation - Annotation commands: " + str(commands))
                log.info(
                    f"Annotation - Annotation multithreaded in "
                    + str(len(commands))
                    + " commands"
                )

                run_parallel_commands(commands, threads)

                # Merge
                tmp_ann_vcf_list_cmd = " ".join(tmp_ann_vcf_list)

                if tmp_ann_vcf_list_cmd:

                    # Tmp file
                    tmp_annotate_vcf = NamedTemporaryFile(
                        prefix=self.get_prefix(),
                        dir=self.get_tmp_dir(),
                        suffix=".vcf.gz",
                        delete=True,
                    )
                    tmp_annotate_vcf_name = tmp_annotate_vcf.name
                    tmp_annotate_vcf_name_err = tmp_annotate_vcf_name + ".err"
                    err_files.append(tmp_annotate_vcf_name_err)

                    # Tmp file remove command
                    tmp_files_remove_command = ""
                    if tmp_files:
                        tmp_files_remove_command = " && rm -f " + " ".join(tmp_files)

                    # Command merge
                    merge_command = f"{bcftools_bin_command} merge --force-samples --threads={threads} {tmp_vcf_name} {tmp_ann_vcf_list_cmd} -o {tmp_annotate_vcf_name} -Oz 2>>{tmp_annotate_vcf_name_err} {tmp_files_remove_command}"
                    log.info(
                        "Annotation - Annotation merging "
                        + str(len(commands))
                        + " annotated files"
                    )
                    log.debug(f"Annotation - merge command: {merge_command}")
                    run_parallel_commands([merge_command], 1)

                    # Error messages
                    error_message_command_all = []
                    error_message_command_warning = []
                    error_message_command_err = []
                    for err_file in err_files:
                        with open(err_file, "r") as f:
                            for line in f:
                                message = line.strip()
                                error_message_command_all.append(message)
                                if line.startswith("[W::"):
                                    error_message_command_warning.append(message)
                                if line.startswith("[E::"):
                                    error_message_command_err.append(
                                        f"{err_file}: " + message
                                    )

                    if len(error_message_command_err):
                        log.error(f"Error messages:")
                        for message in list(set(error_message_command_err)):
                            log.error(f"   {message}")
                    elif len(error_message_command_warning):
                        log.warning(f"Warning messages:")
                        for message in list(set(error_message_command_warning)):
                            log.warning(f"   {message}")
                    # debug info
                    log.debug(f"Warning/Error messages:")
                    for message in list(set(error_message_command_all)):
                        log.debug(f"   {message}")
                    # failed
                    if len(error_message_command_err):
                        log.error("Annotation failed: Error in commands")
                        raise ValueError("Annotation failed: Error in commands")

                    # Update variants
                    log.info("Annotation - Updating...")
                    self.update_from_vcf(tmp_annotate_vcf_name, remove_vcf_file=False)

    def annotation_exomiser(self, threads: int = None) -> None:
        """
        This function annotate with Exomiser

        This function uses args as parameters, in section "annotation" -> "exomiser", with sections:
        - "analysis" (dict/file):
            Full analysis dictionnary parameters (see Exomiser docs).
            Either a dict, or a file in JSON or YAML format.
            These parameters may change depending on other parameters (e.g. phenotipicFeatures/HPO)
            Default : None
        - "preset" (string):
            Analysis preset (available in config folder).
            Used if no full "analysis" is provided.
            Default: "exome"
        - "phenopacket" (dict/file):
            Samples and phenotipic features parameters (see Exomiser docs).
            Either a dict, or a file in JSON or YAML format.
            Default: None
        - "subject" (dict):
            Sample parameters (see Exomiser docs).
            Example:
                "subject":
                    {
                        "id": "ISDBM322017",
                        "sex": "FEMALE"
                    }
            Default: None
        - "sample" (string):
            Sample name to construct "subject" section:
                "subject":
                    {
                        "id": "<sample>",
                        "sex": "UNKNOWN_SEX"
                    }
            Default: None
        - "phenotypicFeatures" (dict)
            Phenotypic features to construct "subject" section.
            Example:
                "phenotypicFeatures":
                    [
                        { "type": { "id": "HP:0001159", "label": "Syndactyly" } },
                        { "type": { "id": "HP:0000486", "label": "Strabismus" } }
                    ]
        - "hpo" (list)
            List of HPO ids as phenotypic features.
            Example:
                "hpo": ['0001156', '0001363', '0011304', '0010055']
            Default: []
        - "outputOptions" (dict):
            Output options (see Exomiser docs).
            Default:
                "output_options" =
                    {
                        "outputContributingVariantsOnly": False,
                        "numGenes": 0,
                        "outputFormats": ["TSV_VARIANT", "VCF"]
                    }
        - "transcript_source" (string):
            Transcript source (either "refseq", "ucsc", "ensembl")
            Default: "refseq"
        - "exomiser_to_info" (boolean):
            Add exomiser TSV file columns as INFO fields in VCF.
            Default: False
        - "release" (string):
            Exomise database release.
            If not exists, database release will be downloaded (take a while).
            Default: None (provided by application.properties configuration file)
        - "exomiser_application_properties" (file):
            Exomiser configuration file (see Exomiser docs).
            Useful to automatically download databases (especially for specific genome databases).

        Notes:
        - If no sample in parameters, first sample in VCF will be chosen
        - If no HPO found, "hiPhivePrioritiser" analysis step will be switch off

        :param threads: The number of threads to use
        :return: None.
        """

        # DEBUG
        log.debug("Start annotation with Exomiser databases")

        # Threads
        if not threads:
            threads = self.get_threads()
        log.debug("Threads: " + str(threads))

        # Config
        config = self.get_config()
        log.debug("Config: " + str(config))

        # Config - Folders - Databases
        databases_folders = (
            config.get("folders", {})
            .get("databases", {})
            .get("exomiser", f"{DEFAULT_DATABASE_FOLDER}/exomiser/current")
        )
        databases_folders = full_path(databases_folders)
        if not os.path.exists(databases_folders):
            log.error(f"Databases annotations: {databases_folders} NOT found")
        log.debug("Databases annotations: " + str(databases_folders))

        # Config - Exomiser
        exomiser_bin_command = get_bin_command(
            bin="exomiser-cli*.jar",
            tool="exomiser",
            bin_type="jar",
            config=config,
            default_folder=f"{DEFAULT_TOOLS_FOLDER}/exomiser",
        )
        log.debug("Exomiser bin command: " + str(exomiser_bin_command))
        if not exomiser_bin_command:
            msg_err = f"Annotation failed: no exomiser bin '{exomiser_bin_command}'"
            log.error(msg_err)
            raise ValueError(msg_err)

        # Param
        param = self.get_param()
        log.debug("Param: " + str(param))

        # Param - Exomiser
        param_exomiser = param.get("annotation", {}).get("exomiser", {})
        log.debug(f"Param Exomiser: {param_exomiser}")

        # Param - Assembly
        assembly = param.get("assembly", config.get("assembly", DEFAULT_ASSEMBLY))
        log.debug("Assembly: " + str(assembly))

        # Data
        table_variants = self.get_table_variants()

        # Check if not empty
        log.debug("Check if not empty")
        sql_query_chromosomes = (
            f"""SELECT count(*) as count FROM {table_variants} as table_variants"""
        )
        if not self.get_query_to_df(f"{sql_query_chromosomes}")["count"][0]:
            log.info(f"VCF empty")
            return False

        # VCF header
        vcf_reader = self.get_header()
        log.debug("Initial header: " + str(vcf_reader.infos))

        # Samples
        samples = self.get_header_sample_list()
        if not samples:
            log.error("No Samples in VCF")
            return False
        log.debug(f"Samples: {samples}")

        # Memory limit
        memory_limit = self.get_memory("8G")
        log.debug(f"memory_limit: {memory_limit}")

        # Exomiser java options
        exomiser_java_options = (
            f" -Xmx{memory_limit} -XX:+UseParallelGC -XX:ParallelGCThreads={threads} "
        )
        log.debug(f"Exomiser java options: {exomiser_java_options}")

        # Download Exomiser (if not exists)
        exomiser_release = param_exomiser.get("release", None)
        exomiser_application_properties = param_exomiser.get(
            "exomiser_application_properties", None
        )
        databases_download_exomiser(
            assemblies=[assembly],
            exomiser_folder=databases_folders,
            exomiser_release=exomiser_release,
            exomiser_phenotype_release=exomiser_release,
            exomiser_application_properties=exomiser_application_properties,
        )

        # Force annotation
        force_update_annotation = True

        if "Exomiser" not in self.get_header().infos or force_update_annotation:
            log.debug("Start annotation Exomiser")

            with TemporaryDirectory(dir=self.get_tmp_dir()) as tmp_dir:

                ### ANALYSIS ###
                ################

                # Create analysis.json through analysis dict
                # either analysis in param or by default
                # depending on preset exome/genome)

                # Init analysis dict
                param_exomiser_analysis_dict = {}

                # analysis from param
                param_exomiser_analysis = param_exomiser.get("analysis", {})
                param_exomiser_analysis = full_path(param_exomiser_analysis)

                # If analysis in param -> load anlaysis json
                if param_exomiser_analysis:

                    # If param analysis is a file and exists
                    if isinstance(param_exomiser_analysis, str) and os.path.exists(
                        param_exomiser_analysis
                    ):
                        # Load analysis file into analysis dict (either yaml or json)
                        with open(param_exomiser_analysis) as json_file:
                            param_exomiser_analysis_dict = yaml.safe_load(json_file)

                    # If param analysis is a dict
                    elif isinstance(param_exomiser_analysis, dict):
                        # Load analysis dict into analysis dict (either yaml or json)
                        param_exomiser_analysis_dict = param_exomiser_analysis

                    # Error analysis type
                    else:
                        log.error(f"Analysis type unknown. Check param file.")
                        raise ValueError(f"Analysis type unknown. Check param file.")

                # Case no input analysis config file/dict
                # Use preset (exome/genome) to open default config file
                if not param_exomiser_analysis_dict:

                    # default preset
                    default_preset = "exome"

                    # Get param preset or default preset
                    param_exomiser_preset = param_exomiser.get("preset", default_preset)

                    # Try to find if preset is a file
                    if os.path.exists(param_exomiser_preset):
                        # Preset file is provided in full path
                        param_exomiser_analysis_default_config_file = (
                            param_exomiser_preset
                        )
                    # elif os.path.exists(full_path(param_exomiser_preset)):
                    #     # Preset file is provided in full path
                    #     param_exomiser_analysis_default_config_file = full_path(param_exomiser_preset)
                    elif os.path.exists(
                        os.path.join(folder_config, param_exomiser_preset)
                    ):
                        # Preset file is provided a basename in config folder (can be a path with subfolders)
                        param_exomiser_analysis_default_config_file = os.path.join(
                            folder_config, param_exomiser_preset
                        )
                    else:
                        # Construct preset file
                        param_exomiser_analysis_default_config_file = os.path.join(
                            folder_config,
                            f"preset-{param_exomiser_preset}-analysis.json",
                        )

                    # If preset file exists
                    param_exomiser_analysis_default_config_file = full_path(
                        param_exomiser_analysis_default_config_file
                    )
                    if os.path.exists(param_exomiser_analysis_default_config_file):
                        # Load prest file into analysis dict (either yaml or json)
                        with open(
                            param_exomiser_analysis_default_config_file
                        ) as json_file:
                            param_exomiser_analysis_dict["analysis"] = yaml.safe_load(
                                json_file
                            )

                    # Error preset file
                    else:
                        log.error(
                            f"No analysis preset config file ({param_exomiser_analysis_default_config_file})"
                        )
                        raise ValueError(
                            f"No analysis preset config file ({param_exomiser_analysis_default_config_file})"
                        )

                # If no analysis dict created
                if not param_exomiser_analysis_dict:
                    log.error(f"No analysis config")
                    raise ValueError(f"No analysis config")

                # Log
                log.debug(f"Pre analysis dict: {param_exomiser_analysis_dict}")

                ### PHENOPACKET ###
                ###################

                # If no PhenoPacket in analysis dict -> check in param
                if "phenopacket" not in param_exomiser_analysis_dict:

                    # If PhenoPacket in param -> load anlaysis json
                    if param_exomiser.get("phenopacket", None):

                        param_exomiser_phenopacket = param_exomiser.get("phenopacket")
                        param_exomiser_phenopacket = full_path(
                            param_exomiser_phenopacket
                        )

                        # If param phenopacket is a file and exists
                        if isinstance(
                            param_exomiser_phenopacket, str
                        ) and os.path.exists(param_exomiser_phenopacket):
                            # Load phenopacket file into analysis dict (either yaml or json)
                            with open(param_exomiser_phenopacket) as json_file:
                                param_exomiser_analysis_dict["phenopacket"] = (
                                    yaml.safe_load(json_file)
                                )

                        # If param phenopacket is a dict
                        elif isinstance(param_exomiser_phenopacket, dict):
                            # Load phenopacket dict into analysis dict (either yaml or json)
                            param_exomiser_analysis_dict["phenopacket"] = (
                                param_exomiser_phenopacket
                            )

                        # Error phenopacket type
                        else:
                            log.error(f"Phenopacket type unknown. Check param file.")
                            raise ValueError(
                                f"Phenopacket type unknown. Check param file."
                            )

                # If no PhenoPacket in analysis dict -> construct from sample and HPO in param
                if "phenopacket" not in param_exomiser_analysis_dict:

                    # Init PhenoPacket
                    param_exomiser_analysis_dict["phenopacket"] = {
                        "id": "analysis",
                        "proband": {},
                    }

                    ### Add subject ###

                    # If subject exists
                    param_exomiser_subject = param_exomiser.get("subject", {})

                    # If subject not exists -> found sample ID
                    if not param_exomiser_subject:

                        # Found sample ID in param
                        sample = param_exomiser.get("sample", None)

                        # Find sample ID (first sample)
                        if not sample:
                            sample_list = self.get_header_sample_list()
                            if len(sample_list) > 0:
                                sample = sample_list[0]
                            else:
                                log.error(f"No sample found")
                                raise ValueError(f"No sample found")

                        # Create subject
                        param_exomiser_subject = {"id": sample, "sex": "UNKNOWN_SEX"}

                    # Add to dict
                    param_exomiser_analysis_dict["phenopacket"][
                        "subject"
                    ] = param_exomiser_subject

                    ### Add "phenotypicFeatures" ###

                    # If phenotypicFeatures exists
                    param_exomiser_phenotypicfeatures = param_exomiser.get(
                        "phenotypicFeatures", []
                    )

                    # If phenotypicFeatures not exists -> Try to infer from hpo list
                    if not param_exomiser_phenotypicfeatures:

                        # Found HPO in param
                        param_exomiser_hpo = param_exomiser.get("hpo", [])

                        # Split HPO if list in string format separated by comma
                        if isinstance(param_exomiser_hpo, str):
                            param_exomiser_hpo = param_exomiser_hpo.split(",")

                        # Create HPO list
                        for hpo in param_exomiser_hpo:
                            hpo_clean = re.sub("[^0-9]", "", hpo)
                            param_exomiser_phenotypicfeatures.append(
                                {
                                    "type": {
                                        "id": f"HP:{hpo_clean}",
                                        "label": f"HP:{hpo_clean}",
                                    }
                                }
                            )

                    # Add to dict
                    param_exomiser_analysis_dict["phenopacket"][
                        "phenotypicFeatures"
                    ] = param_exomiser_phenotypicfeatures

                    # If phenotypicFeatures not exists -> Remove hiPhivePrioritiser step
                    if not param_exomiser_phenotypicfeatures:
                        for step in param_exomiser_analysis_dict.get(
                            "analysis", {}
                        ).get("steps", []):
                            if "hiPhivePrioritiser" in step:
                                param_exomiser_analysis_dict.get("analysis", {}).get(
                                    "steps", []
                                ).remove(step)

                ### Add Input File ###

                # Initial file name and htsFiles
                tmp_vcf_name = os.path.join(tmp_dir, "initial.vcf.gz")
                param_exomiser_analysis_dict["phenopacket"]["htsFiles"] = [
                    {
                        "uri": tmp_vcf_name,
                        "htsFormat": "VCF",
                        "genomeAssembly": assembly,
                    }
                ]

                ### Add metaData ###

                # If metaData not in analysis dict
                if "metaData" not in param_exomiser_analysis_dict:
                    param_exomiser_analysis_dict["phenopacket"]["metaData"] = {
                        "created": f"{datetime.datetime.now()}".replace(" ", "T") + "Z",
                        "createdBy": "howard",
                        "phenopacketSchemaVersion": 1,
                    }

                ### OutputOptions ###

                # Init output result folder
                output_results = os.path.join(tmp_dir, "results")

                # If no outputOptions in analysis dict
                if "outputOptions" not in param_exomiser_analysis_dict:

                    # default output formats
                    defaut_output_formats = ["TSV_VARIANT", "VCF"]

                    # Get outputOptions in param
                    output_options = param_exomiser.get("outputOptions", None)

                    # If no output_options in param -> check
                    if not output_options:
                        output_options = {
                            "outputContributingVariantsOnly": False,
                            "numGenes": 0,
                            "outputFormats": defaut_output_formats,
                        }

                    # Replace outputDirectory in output options
                    output_options["outputDirectory"] = output_results
                    output_options["outputFileName"] = "howard"

                    # Add outputOptions in analysis dict
                    param_exomiser_analysis_dict["outputOptions"] = output_options

                else:

                    # Replace output_results and output format (if exists in param)
                    param_exomiser_analysis_dict["outputOptions"][
                        "outputDirectory"
                    ] = output_results
                    param_exomiser_analysis_dict["outputOptions"]["outputFormats"] = (
                        list(
                            set(
                                param_exomiser_analysis_dict.get(
                                    "outputOptions", {}
                                ).get("outputFormats", [])
                                + ["TSV_VARIANT", "VCF"]
                            )
                        )
                    )

                # log
                log.debug(f"Pre analysis dict: {param_exomiser_analysis_dict}")

                ### ANALYSIS FILE ###
                #####################

                ### Full JSON analysis config file ###

                exomiser_analysis = os.path.join(tmp_dir, "analysis.json")
                with open(exomiser_analysis, "w") as fp:
                    json.dump(param_exomiser_analysis_dict, fp, indent=4)

                ### SPLIT analysis and sample config files

                # Splitted analysis dict
                param_exomiser_analysis_dict_for_split = (
                    param_exomiser_analysis_dict.copy()
                )

                # Phenopacket JSON file
                exomiser_analysis_phenopacket = os.path.join(
                    tmp_dir, "analysis_phenopacket.json"
                )
                with open(exomiser_analysis_phenopacket, "w") as fp:
                    json.dump(
                        param_exomiser_analysis_dict_for_split.get("phenopacket"),
                        fp,
                        indent=4,
                    )

                # Analysis JSON file without Phenopacket parameters
                param_exomiser_analysis_dict_for_split.pop("phenopacket")
                exomiser_analysis_analysis = os.path.join(
                    tmp_dir, "analysis_analysis.json"
                )
                with open(exomiser_analysis_analysis, "w") as fp:
                    json.dump(param_exomiser_analysis_dict_for_split, fp, indent=4)

                ### INITAL VCF file ###
                #######################

                ### Create list of samples to use and include inti initial VCF file ####

                # Subject (main sample)
                # Get sample ID in analysis dict
                sample_subject = (
                    param_exomiser_analysis_dict.get("phenopacket", {})
                    .get("subject", {})
                    .get("id", None)
                )
                sample_proband = (
                    param_exomiser_analysis_dict.get("phenopacket", {})
                    .get("proband", {})
                    .get("subject", {})
                    .get("id", None)
                )
                sample = []
                if sample_subject:
                    sample.append(sample_subject)
                if sample_proband:
                    sample.append(sample_proband)

                # Get sample ID within Pedigree
                pedigree_persons_list = (
                    param_exomiser_analysis_dict.get("phenopacket", {})
                    .get("pedigree", {})
                    .get("persons", {})
                )

                # Create list with all sample ID in pedigree (if exists)
                pedigree_persons = []
                for person in pedigree_persons_list:
                    pedigree_persons.append(person.get("individualId"))

                # Concat subject sample ID and samples ID in pedigreesamples
                samples = list(set(sample + pedigree_persons))

                # Check if sample list is not empty
                if not samples:
                    log.error(f"No samples found")
                    raise ValueError(f"No samples found")

                # Create VCF with sample (either sample in param or first one by default)
                # Export VCF file
                self.export_variant_vcf(
                    vcf_file=tmp_vcf_name,
                    remove_info=True,
                    add_samples=True,
                    list_samples=samples,
                    index=False,
                )

                ### Execute Exomiser ###
                ########################

                # Init command
                exomiser_command = ""

                # Command exomiser options
                exomiser_options = f" --spring.config.location={databases_folders}/{assembly}/application.properties --exomiser.data-directory={databases_folders}/{assembly} "

                # Release
                exomiser_release = param_exomiser.get("release", None)
                if exomiser_release:
                    # phenotype data version
                    exomiser_options += (
                        f" --exomiser.phenotype.data-version={exomiser_release} "
                    )
                    # data version
                    exomiser_options += (
                        f" --exomiser.{assembly}.data-version={exomiser_release} "
                    )
                    # variant white list
                    variant_white_list_file = (
                        f"{exomiser_release}_{assembly}_clinvar_whitelist.tsv.gz"
                    )
                    if os.path.exists(
                        os.path.join(
                            databases_folders, assembly, variant_white_list_file
                        )
                    ):
                        exomiser_options += f" --exomiser.{assembly}.variant-white-list-path={variant_white_list_file} "

                # transcript_source
                transcript_source = param_exomiser.get(
                    "transcript_source", None
                )  # ucsc, refseq, ensembl
                if transcript_source:
                    exomiser_options += (
                        f" --exomiser.{assembly}.transcript-source={transcript_source} "
                    )

                # If analysis contain proband param
                if param_exomiser_analysis_dict.get("phenopacket", {}).get(
                    "proband", {}
                ):
                    exomiser_command_analysis = f" {exomiser_bin_command} --analysis={exomiser_analysis_analysis} --sample={exomiser_analysis_phenopacket} {exomiser_options} "

                # If no proband (usually uniq sample)
                else:
                    exomiser_command_analysis = f" {exomiser_bin_command} --analysis={exomiser_analysis} {exomiser_options}"

                # Log
                log.debug(f"exomiser_command_analysis={exomiser_command_analysis}")

                # Run command
                result = subprocess.call(
                    exomiser_command_analysis.split(), stdout=subprocess.PIPE
                )
                if result:
                    log.error("Exomiser command failed")
                    raise ValueError("Exomiser command failed")

                ### RESULTS ###
                ###############

                ### Annotate with TSV fields ###

                # Init result tsv file
                exomiser_to_info = param_exomiser.get("exomiser_to_info", False)

                # Init result tsv file
                output_results_tsv = os.path.join(output_results, "howard.variants.tsv")

                # Parse TSV file and explode columns in INFO field
                if exomiser_to_info and os.path.exists(output_results_tsv):

                    # Log
                    log.debug("Exomiser columns to VCF INFO field")

                    # Retrieve columns and types
                    query = f""" SELECT * FROM read_csv('{output_results_tsv}', auto_detect=True, delim='\t', sample_size=-1) LIMIT 0 """
                    output_results_tsv_df = self.get_query_to_df(query)
                    output_results_tsv_columns = output_results_tsv_df.columns.tolist()

                    # Init concat fields for update
                    sql_query_update_concat_fields = []

                    # Fields to avoid
                    fields_to_avoid = [
                        "CONTIG",
                        "START",
                        "END",
                        "REF",
                        "ALT",
                        "QUAL",
                        "FILTER",
                        "GENOTYPE",
                    ]

                    # List all columns to add into header
                    for header_column in output_results_tsv_columns:

                        # If header column is enable
                        if header_column not in fields_to_avoid:

                            # Header info type
                            header_info_type = "String"
                            header_column_df = output_results_tsv_df[header_column]
                            header_column_df_dtype = header_column_df.dtype
                            if header_column_df_dtype == object:
                                if (
                                    pd.to_numeric(header_column_df, errors="coerce")
                                    .notnull()
                                    .all()
                                ):
                                    header_info_type = "Float"
                            else:
                                header_info_type = "Integer"

                            # Header info
                            characters_to_validate = ["-"]
                            pattern = "[" + "".join(characters_to_validate) + "]"
                            header_info_name = re.sub(
                                pattern,
                                "_",
                                f"Exomiser_{header_column}".replace("#", ""),
                            )
                            header_info_number = "."
                            header_info_description = (
                                f"Exomiser {header_column} annotation"
                            )
                            header_info_source = "Exomiser"
                            header_info_version = "unknown"
                            header_info_code = CODE_TYPE_MAP[header_info_type]
                            vcf_reader.infos[header_info_name] = vcf.parser._Info(
                                header_info_name,
                                header_info_number,
                                header_info_type,
                                header_info_description,
                                header_info_source,
                                header_info_version,
                                header_info_code,
                            )

                            # Add field to add for update to concat fields
                            sql_query_update_concat_fields.append(
                                f"""
                                CASE
                                    WHEN table_parquet."{header_column}" NOT IN ('','.')
                                    THEN concat(
                                        '{header_info_name}=',
                                        table_parquet."{header_column}",
                                        ';'
                                        )

                                    ELSE ''
                                END
                            """
                            )

                    # Update query
                    sql_query_update = f"""
                        UPDATE {table_variants} as table_variants
                            SET INFO = concat(
                                            CASE
                                                WHEN INFO NOT IN ('', '.')
                                                THEN INFO
                                                ELSE ''
                                            END,
                                            CASE
                                                WHEN table_variants.INFO NOT IN ('','.')
                                                THEN ';'
                                                ELSE ''
                                            END,
                                            (
                                            SELECT 
                                                concat(
                                                    {",".join(sql_query_update_concat_fields)}
                                                )
                                            FROM read_csv('{output_results_tsv}', auto_detect=True, delim='\t', sample_size=-1) as table_parquet
                                                    WHERE concat('chr', CAST(table_parquet.\"CONTIG\" AS STRING)) = table_variants.\"#CHROM\"
                                                    AND table_parquet.\"START\" = table_variants.\"POS\"
                                                    AND table_parquet.\"ALT\" = table_variants.\"ALT\"
                                                    AND table_parquet.\"REF\" = table_variants.\"REF\"
                                            )
                                        )
                            ;
                        """

                    # Update
                    self.conn.execute(sql_query_update)

                ### Annotate with VCF INFO field ###

                # Init result VCF file
                output_results_vcf = os.path.join(output_results, "howard.vcf.gz")

                # If VCF exists
                if os.path.exists(output_results_vcf):

                    # Log
                    log.debug("Exomiser result VCF update variants")

                    # Find Exomiser INFO field annotation in header
                    with gzip.open(output_results_vcf, "rt") as f:
                        header_list = self.read_vcf_header(f)
                    exomiser_vcf_header = vcf.Reader(
                        io.StringIO("\n".join(header_list))
                    )

                    # Add annotation INFO field to header
                    vcf_reader.infos["Exomiser"] = exomiser_vcf_header.infos["Exomiser"]

                    # Update variants with VCF
                    self.update_from_vcf(output_results_vcf)

        return True

    def annotation_snpeff(self, threads: int = None) -> None:
        """
        This function annotate with snpEff

        :param threads: The number of threads to use
        :return: the value of the variable "return_value".
        """

        # DEBUG
        log.debug("Start annotation with snpeff databases")

        # Threads
        if not threads:
            threads = self.get_threads()
        log.debug("Threads: " + str(threads))

        # Delete tmp
        delete_tmp = True
        if self.get_config().get("verbosity", "warning") in ["debug"]:
            delete_tmp = False
            log.debug("Delete tmp files/folders: " + str(delete_tmp))

        # Config
        config = self.get_config()
        log.debug("Config: " + str(config))

        # Config - Folders - Databases
        databases_folders = (
            config.get("folders", {}).get("databases", {}).get("snpeff", ["."])
        )
        log.debug("Databases annotations: " + str(databases_folders))

        # Config - snpEff bin command
        snpeff_bin_command = get_bin_command(
            bin="snpEff.jar",
            tool="snpeff",
            bin_type="jar",
            config=config,
            default_folder=f"{DEFAULT_TOOLS_FOLDER}/snpeff",
        )
        if not snpeff_bin_command:
            msg_err = f"Annotation failed: no snpeff bin '{snpeff_bin_command}'"
            log.error(msg_err)
            raise ValueError(msg_err)

        # Config - snpEff databases
        snpeff_databases = (
            config.get("folders", {})
            .get("databases", {})
            .get("snpeff", DEFAULT_SNPEFF_FOLDER)
        )
        snpeff_databases = full_path(snpeff_databases)
        if snpeff_databases is not None and snpeff_databases != "":
            log.debug(f"Create snpEff databases folder")
            if not os.path.exists(snpeff_databases):
                os.makedirs(snpeff_databases)

        # Param
        param = self.get_param()
        log.debug("Param: " + str(param))

        # Param
        options = param.get("annotation", {}).get("snpeff", {}).get("options", None)
        log.debug("Options: " + str(options))

        # Param - Assembly
        assembly = param.get("assembly", config.get("assembly", DEFAULT_ASSEMBLY))

        # Param - Options
        snpeff_options = (
            param.get("annotation", {}).get("snpeff", {}).get("options", "")
        )
        snpeff_stats = param.get("annotation", {}).get("snpeff", {}).get("stats", None)
        snpeff_csvstats = (
            param.get("annotation", {}).get("snpeff", {}).get("csvStats", None)
        )
        if snpeff_stats:
            snpeff_stats = snpeff_stats.replace("OUTPUT", self.get_output())
            snpeff_stats = full_path(snpeff_stats)
            snpeff_options += f" -stats {snpeff_stats}"
        if snpeff_csvstats:
            snpeff_csvstats = snpeff_csvstats.replace("OUTPUT", self.get_output())
            snpeff_csvstats = full_path(snpeff_csvstats)
            snpeff_options += f" -csvStats {snpeff_csvstats}"

        # Data
        table_variants = self.get_table_variants()

        # Check if not empty
        log.debug("Check if not empty")
        sql_query_chromosomes = (
            f"""SELECT count(*) as count FROM {table_variants} as table_variants"""
        )
        # if not self.conn.execute(f"{sql_query_chromosomes}").df()["count"][0]:
        if not self.get_query_to_df(f"{sql_query_chromosomes}")["count"][0]:
            log.info(f"VCF empty")
            return

        # Export in VCF
        log.debug("Create initial file to annotate")
        tmp_vcf = NamedTemporaryFile(
            prefix=self.get_prefix(),
            dir=self.get_tmp_dir(),
            suffix=".vcf.gz",
            delete=True,
        )
        tmp_vcf_name = tmp_vcf.name

        # VCF header
        vcf_reader = self.get_header()
        log.debug("Initial header: " + str(vcf_reader.infos))

        # Existing annotations
        for vcf_annotation in self.get_header().infos:

            vcf_annotation_line = self.get_header().infos.get(vcf_annotation)
            log.debug(
                f"Existing annotations in VCF: {vcf_annotation} [{vcf_annotation_line}]"
            )

        # Memory limit
        # if config.get("memory", None):
        #     memory_limit = config.get("memory", "8G")
        # else:
        #     memory_limit = "8G"
        memory_limit = self.get_memory("8G")
        log.debug(f"memory_limit: {memory_limit}")

        # snpEff java options
        snpeff_java_options = (
            f" -Xmx{memory_limit} -XX:+UseParallelGC -XX:ParallelGCThreads={threads} "
        )
        log.debug(f"Exomiser java options: {snpeff_java_options}")

        force_update_annotation = True

        if "ANN" not in self.get_header().infos or force_update_annotation:

            # Check snpEff database
            log.debug(f"Check snpEff databases {[assembly]}")
            databases_download_snpeff(
                folder=snpeff_databases, assemblies=[assembly], config=config
            )

            # Export VCF file
            self.export_variant_vcf(
                vcf_file=tmp_vcf_name,
                remove_info=True,
                add_samples=False,
                index=True,
            )

            # Tmp file
            err_files = []
            tmp_annotate_vcf = NamedTemporaryFile(
                prefix=self.get_prefix(),
                dir=self.get_tmp_dir(),
                suffix=".vcf",
                delete=False,
            )
            tmp_annotate_vcf_name = tmp_annotate_vcf.name
            tmp_annotate_vcf_name_err = tmp_annotate_vcf_name + ".err"
            err_files.append(tmp_annotate_vcf_name_err)

            # Command
            snpeff_command = f"{snpeff_bin_command} {assembly} -dataDir {snpeff_databases} {snpeff_options} {tmp_vcf_name} 1>{tmp_annotate_vcf_name} 2>>{tmp_annotate_vcf_name_err}"
            log.debug(f"Annotation - snpEff command: {snpeff_command}")
            run_parallel_commands([snpeff_command], 1)

            # Error messages
            error_message_command_all = []
            error_message_command_warning = []
            error_message_command_err = []
            for err_file in err_files:
                with open(err_file, "r") as f:
                    for line in f:
                        message = line.strip()
                        error_message_command_all.append(message)
                        if line.startswith("[W::"):
                            error_message_command_warning.append(message)
                        if line.startswith("[E::"):
                            error_message_command_err.append(f"{err_file}: " + message)

            # Warning/Error messages
            if len(error_message_command_err):
                log.error("Error messages:")
                for message in set(error_message_command_err):
                    log.error(f"   {message}")
            elif len(error_message_command_warning):
                log.warning("Warning messages:")
                for message in set(error_message_command_warning):
                    log.warning(f"   {message}")
            # debug info
            log.debug("Warning/Error messages:")
            for message in set(error_message_command_all):
                log.debug(f"   {message}")
            # failed
            if len(error_message_command_err):
                log.error("Annotation failed: Error in commands")
                raise ValueError("Annotation failed: Error in commands")

            # Find annotation in header
            with open(tmp_annotate_vcf_name, "rt") as f:
                header_list = self.read_vcf_header(f)
            annovar_vcf_header = vcf.Reader(io.StringIO("\n".join(header_list)))

            for ann in annovar_vcf_header.infos:
                if ann not in self.get_header().infos:
                    vcf_reader.infos[ann] = annovar_vcf_header.infos.get(ann)

            # Update variants
            log.info(f"Annotation - Updating...")
            self.update_from_vcf(tmp_annotate_vcf_name)
            list_to_remove = [
                tmp_annotate_vcf_name,
                tmp_annotate_vcf_name_err,
                f"{tmp_annotate_vcf_name}.tbi",
                f"{tmp_vcf_name}.tbi",
            ]
            log.debug(f"tmp_annotate_vcf_name: {list_to_remove}")
            remove_if_exists(list_to_remove)

        else:
            if "ANN" in self.get_header().infos:
                log.debug(f"Existing snpEff annotations in VCF")
            if force_update_annotation:
                log.debug(f"Existing snpEff annotations in VCF - annotation forced")

    def annotation_annovar(self, threads: int = None) -> None:
        """
        It takes a VCF file, annotates it with Annovar, and then updates the database with the new
        annotations

        :param threads: number of threads to use
        :return: the value of the variable "return_value".
        """

        # DEBUG
        log.debug("Start annotation with Annovar databases")

        # Threads
        if not threads:
            threads = self.get_threads()
        log.debug("Threads: " + str(threads))

        # Tmp en Err files
        tmp_files = []
        err_files = []

        # DEBUG
        delete_tmp = True
        if self.get_config().get("verbosity", "warning") in ["debug"]:
            delete_tmp = False
            log.debug("Delete tmp files/folders: " + str(delete_tmp))

        # Config
        config = self.get_config()
        log.debug("Config: " + str(config))

        # Config - Folders - Databases
        databases_folders = (
            config.get("folders", {}).get("databases", {}).get("annovar", ["."])
        )
        log.debug("Databases annotations: " + str(databases_folders))

        # Config - annovar bin command
        annovar_bin_command = get_bin_command(
            bin="table_annovar.pl",
            tool="annovar",
            bin_type="perl",
            config=config,
            default_folder=f"{DEFAULT_TOOLS_FOLDER}/annovar",
        )
        if not annovar_bin_command:
            msg_err = f"Annotation failed: no annovar bin '{annovar_bin_command}'"
            log.error(msg_err)
            raise ValueError(msg_err)

        # Config - BCFTools bin command
        bcftools_bin_command = get_bin_command(
            bin="bcftools",
            tool="bcftools",
            bin_type="bin",
            config=config,
            default_folder=f"{DEFAULT_TOOLS_FOLDER}/bcftools",
        )
        if not bcftools_bin_command:
            msg_err = f"Annotation failed: no bcftools bin '{bcftools_bin_command}'"
            log.error(msg_err)
            raise ValueError(msg_err)

        # Config - annovar databases
        annovar_databases = (
            config.get("folders", {})
            .get("databases", {})
            .get("annovar", DEFAULT_ANNOVAR_FOLDER)
        )
        if annovar_databases is not None:
            if isinstance(annovar_databases, list):
                annovar_databases = full_path(annovar_databases[0])
                log.warning(f"Annovar databases folder '{annovar_databases}' selected")
            annovar_databases = full_path(annovar_databases)
            if not os.path.exists(annovar_databases):
                log.info(f"Annovar databases folder '{annovar_databases}' created")
                Path(annovar_databases).mkdir(parents=True, exist_ok=True)
        else:
            msg_err = f"Annovar databases configuration failed"
            log.error(msg_err)
            raise ValueError(msg_err)

        # Param
        param = self.get_param()
        log.debug("Param: " + str(param))

        # Param - options
        options = param.get("annotation", {}).get("annovar", {}).get("options", {})
        log.debug("Options: " + str(options))

        # Param - annotations
        annotations = (
            param.get("annotation", {}).get("annovar", {}).get("annotations", {})
        )
        log.debug("Annotations: " + str(annotations))

        # Param - Assembly
        assembly = param.get("assembly", config.get("assembly", DEFAULT_ASSEMBLY))

        # Annovar database assembly
        annovar_databases_assembly = f"{annovar_databases}/{assembly}"
        if annovar_databases_assembly != "" and not os.path.exists(
            annovar_databases_assembly
        ):
            os.makedirs(annovar_databases_assembly)

        # Data
        table_variants = self.get_table_variants()

        # Check if not empty
        log.debug("Check if not empty")
        sql_query_chromosomes = (
            f"""SELECT count(*) as count FROM {table_variants} as table_variants"""
        )
        sql_query_chromosomes_df = self.get_query_to_df(sql_query_chromosomes)
        if not sql_query_chromosomes_df["count"][0]:
            log.info(f"VCF empty")
            return

        # VCF header
        vcf_reader = self.get_header()
        log.debug("Initial header: " + str(vcf_reader.infos))

        # Existing annotations
        for vcf_annotation in self.get_header().infos:

            vcf_annotation_line = self.get_header().infos.get(vcf_annotation)
            log.debug(
                f"Existing annotations in VCF: {vcf_annotation} [{vcf_annotation_line}]"
            )

        force_update_annotation = True

        if annotations:

            commands = []
            tmp_annotates_vcf_name_list = []

            # Export in VCF
            log.debug("Create initial file to annotate")
            tmp_vcf = NamedTemporaryFile(
                prefix=self.get_prefix(),
                dir=self.get_tmp_dir(),
                suffix=".vcf.gz",
                delete=False,
            )
            tmp_vcf_name = tmp_vcf.name
            tmp_files.append(tmp_vcf_name)
            tmp_files.append(tmp_vcf_name + ".tbi")

            # Export VCF file
            self.export_variant_vcf(
                vcf_file=tmp_vcf_name,
                remove_info=".",
                add_samples=False,
                index=True,
            )

            # Create file for field rename
            log.debug("Create file for field rename")
            tmp_rename = NamedTemporaryFile(
                prefix=self.get_prefix(),
                dir=self.get_tmp_dir(),
                suffix=".rename",
                delete=False,
            )
            tmp_rename_name = tmp_rename.name
            tmp_files.append(tmp_rename_name)

            # Check Annovar database
            log.debug(
                f"Check Annovar databases {[assembly]}: {list(annotations.keys())}"
            )
            databases_download_annovar(
                folder=annovar_databases,
                files=list(annotations.keys()),
                assemblies=[assembly],
                force_check_dblist=False,
            )

            for annotation in annotations:
                annotation_fields = annotations[annotation]

                if not annotation_fields:
                    annotation_fields = {"INFO": None}

                log.info(f"Annotations Annovar - database '{annotation}'")
                log.debug(f"Annotation '{annotation}' - fields: {annotation_fields}")

                # Tmp file for annovar
                err_files = []
                tmp_annotate_vcf_directory = TemporaryDirectory(
                    prefix=self.get_prefix(), dir=self.get_tmp_dir(), suffix=".annovar"
                )
                tmp_annotate_vcf_prefix = tmp_annotate_vcf_directory.name + "/annovar"
                tmp_annotate_vcf_name_annovar = (
                    tmp_annotate_vcf_prefix + "." + assembly + "_multianno.vcf"
                )
                tmp_annotate_vcf_name_err = tmp_annotate_vcf_directory.name + "/.err"
                err_files.append(tmp_annotate_vcf_name_err)
                tmp_files.append(tmp_annotate_vcf_name_err)

                # Tmp file final vcf annotated by annovar
                tmp_annotate_vcf = NamedTemporaryFile(
                    prefix=self.get_prefix(),
                    dir=self.get_tmp_dir(),
                    suffix=".vcf.gz",
                    delete=False,
                )
                tmp_annotate_vcf_name = tmp_annotate_vcf.name
                tmp_annotates_vcf_name_list.append(tmp_annotate_vcf_name)
                tmp_files.append(tmp_annotate_vcf_name)
                tmp_files.append(tmp_annotate_vcf_name + ".tbi")

                # Number of fields
                annotation_list = []
                annotation_renamed_list = []

                for annotation_field in annotation_fields:

                    # field new name, if parametered SKIPPED !!!!!! not managed actually TODO
                    annotation_fields_new_name = annotation_fields.get(
                        annotation_field, annotation_field
                    )
                    if not annotation_fields_new_name:
                        annotation_fields_new_name = annotation_field

                    if (
                        force_update_annotation
                        or annotation_fields_new_name not in self.get_header().infos
                    ):
                        annotation_list.append(annotation_field)
                        annotation_renamed_list.append(annotation_fields_new_name)
                    else:  # annotation_fields_new_name in self.get_header().infos and not force_update_annotation:
                        log.warning(
                            f"Annotation '{annotation}' - '{annotation_fields_new_name}' - already exists (skipped)"
                        )

                    # Add rename info
                    run_parallel_commands(
                        [
                            f"echo 'INFO/{annotation_field} {annotation_fields_new_name}' >> {tmp_rename_name}"
                        ],
                        1,
                    )

                # log.debug("fields_to_removed: " + str(fields_to_removed))
                log.debug("annotation_list: " + str(annotation_list))

                # protocol
                protocol = annotation

                # argument
                argument = ""

                # operation
                operation = "f"
                if annotation in ["refGene", "refGeneWithVer"] or annotation.startswith(
                    "ensGene"
                ):
                    operation = "g"
                    if options.get("genebase", None):
                        argument = f"""'{options.get("genebase","")}'"""
                elif annotation in ["cytoBand"]:
                    operation = "r"

                # argument option
                argument_option = ""
                if argument != "":
                    argument_option = " --argument " + argument

                # command options
                command_options = f""" --nastring . --vcfinput --polish --dot2underline --thread {threads} """  # --intronhgvs 10
                for option in options:
                    if option not in ["genebase"]:
                        command_options += f""" --{option}={options[option]}"""

                # Command

                # Command - Annovar
                command_annovar = f"""{annovar_bin_command} {tmp_vcf_name} {annovar_databases_assembly} --buildver {assembly} --outfile {tmp_annotate_vcf_prefix} --remove --protocol {protocol} --operation {operation} {argument_option} {command_options} 2>>{tmp_annotate_vcf_name_err} && mv {tmp_annotate_vcf_name_annovar} {tmp_annotate_vcf_name}.tmp.vcf """
                tmp_files.append(f"{tmp_annotate_vcf_name}.tmp.vcf")

                # Command - start pipe
                command_annovar += f""" && {bcftools_bin_command} view --threads={threads} {tmp_annotate_vcf_name}.tmp.vcf 2>>{tmp_annotate_vcf_name_err} """

                # Command - Clean INFO/ANNOVAR_DATE (due to Annovar issue with multiple TAGS!)
                command_annovar += """ | sed "s/ANNOVAR_DATE=[^;\t]*;//gi" """

                # Command - Special characters (refGene annotation)
                command_annovar += """ | sed "s/\\\\\\x3b/,/gi" """

                # Command - Clean empty fields (with value ".")
                command_annovar += """ | awk -F'\\t' -v OFS='\\t' '{if ($0 ~ /^#/) print; else {split($8,a,";");for(i=1;i<=length(a);i++) {split(a[i],b,"=");if(b[2]!=".") {c[b[1]]=b[2]}}; split($8,d,";");for(i=1;i<=length(d);i++) {split(d[i],e,"=");if(c[e[1]]!="") {if(info!="") {info=info";"}; info=info""e[1]"="c[e[1]]}}; if(info!="") {$8=info} else {$8=""}; delete c; info=""; print}}' """

                # Command - Extract only needed fields, and remove ANNOVAR fields, and compress and index final file
                annovar_fields_to_keep = ["INFO/ANNOVAR_DATE", "INFO/ALLELE_END"]
                if "ALL" not in annotation_list and "INFO" not in annotation_list:
                    # for ann in annotation_renamed_list:
                    for ann in annotation_list:
                        annovar_fields_to_keep.append(f"^INFO/{ann}")

                command_annovar += f""" | {bcftools_bin_command} annotate --pair-logic exact --threads={threads} -x {",".join(annovar_fields_to_keep)} --rename-annots={tmp_rename_name} -o {tmp_annotate_vcf_name} -Oz 2>>{tmp_annotate_vcf_name_err} """

                # Command - indexing
                command_annovar += f"""  && tabix {tmp_annotate_vcf_name} """

                log.debug(f"Annotation - Annovar command: {command_annovar}")
                run_parallel_commands([command_annovar], 1)

                # Error messages
                error_message_command_all = []
                error_message_command_warning = []
                error_message_command_err = []
                for err_file in err_files:
                    with open(err_file, "r") as f:
                        for line in f:
                            message = line.strip()
                            error_message_command_all.append(message)
                            if line.startswith("[W::"):
                                error_message_command_warning.append(message)
                            if line.startswith("[E::"):
                                error_message_command_err.append(
                                    f"{err_file}: " + message
                                )

                # Error/Warning messages
                if len(error_message_command_err):
                    log.error(f"Error messages:")
                    for message in list(set(error_message_command_err)):
                        log.error(f"   {message}")
                elif len(error_message_command_warning):
                    log.warning(f"Warning messages:")
                    for message in list(set(error_message_command_warning)):
                        log.warning(f"   {message}")
                # debug info
                log.debug(f"Warning/Error messages:")
                for message in list(set(error_message_command_all)):
                    log.debug(f"   {message}")
                # failed
                if len(error_message_command_err):
                    log.error("Annotation failed: Error in commands")
                    raise ValueError("Annotation failed: Error in commands")

            if tmp_annotates_vcf_name_list:

                # List of annotated files
                tmp_annotates_vcf_name_to_merge = " ".join(tmp_annotates_vcf_name_list)

                # Tmp file
                tmp_annotate_vcf = NamedTemporaryFile(
                    prefix=self.get_prefix(),
                    dir=self.get_tmp_dir(),
                    suffix=".vcf.gz",
                    delete=False,
                )
                tmp_annotate_vcf_name = tmp_annotate_vcf.name
                tmp_files.append(tmp_annotate_vcf_name)
                tmp_annotate_vcf_name_err = tmp_annotate_vcf_name + ".err"
                err_files.append(tmp_annotate_vcf_name_err)
                tmp_files.append(tmp_annotate_vcf_name_err)

                # Command merge
                merge_command = f"{bcftools_bin_command} merge --force-samples --threads={threads} {tmp_vcf_name} {tmp_annotates_vcf_name_to_merge} -o {tmp_annotate_vcf_name} -Oz 2>>{tmp_annotate_vcf_name_err} "
                log.info(
                    f"Annotation Annovar - Annotation merging "
                    + str(len(tmp_annotates_vcf_name_list))
                    + " annotated files"
                )
                log.debug(f"Annotation - merge command: {merge_command}")
                run_parallel_commands([merge_command], 1)

                # Find annotation in header
                with bgzf.open(tmp_annotate_vcf_name, "rt") as f:
                    header_list = self.read_vcf_header(f)
                annovar_vcf_header = vcf.Reader(io.StringIO("\n".join(header_list)))

                for ann in annovar_vcf_header.infos:
                    if ann not in self.get_header().infos:
                        vcf_reader.infos[ann] = annovar_vcf_header.infos.get(ann)

                # Update variants
                log.info(f"Annotation Annovar - Updating...")
                self.update_from_vcf(tmp_annotate_vcf_name)
                remove_if_exists(
                    [
                        tmp_annotate_vcf_name,
                        tmp_annotate_vcf_name_err,
                        f"{tmp_annotate_vcf_name}.tbi",
                    ]
                )

            # Clean files
            # Tmp file remove command
            if True:
                remove_if_exists(tmp_files)
                # tmp_files_remove_command = ""
                # if tmp_files:
                #     tmp_files_remove_command = " ".join(tmp_files)
                # clean_command = f" rm -f {tmp_files_remove_command} "
                # log.debug(f"Annotation Annovar - Annotation cleaning ")
                # log.debug(f"Annotation - cleaning command: {clean_command}")
                # run_parallel_commands([clean_command], 1)

    # Parquet
    def annotation_parquet(self, threads: int = None) -> None:
        """
        It takes a VCF file, and annotates it with a parquet file

        :param threads: number of threads to use for the annotation
        :return: the value of the variable "result".
        """

        # DEBUG
        log.debug("Start annotation with parquet databases")

        # Threads
        if not threads:
            threads = self.get_threads()
        log.debug("Threads: " + str(threads))

        # Chunk size
        chunk_size = self.get_config().get("chunk_size", DEFAULT_CHUNK_SIZE)

        # DEBUG
        delete_tmp = True
        if self.get_config().get("verbosity", "warning") in ["debug"]:
            delete_tmp = False
            log.debug("Delete tmp files/folders: " + str(delete_tmp))

        # Config
        databases_folders = set(
            self.get_config()
            .get("folders", {})
            .get("databases", {})
            .get("annotations", [DEFAULT_ANNOTATIONS_FOLDER])
            + self.get_config()
            .get("folders", {})
            .get("databases", {})
            .get("parquet", [DEFAULT_PARQUET_FOLDER])
        )
        log.debug("Databases annotations: " + str(databases_folders))

        # Param
        annotations = (
            self.get_param()
            .get("annotation", {})
            .get("parquet", {})
            .get("annotations", None)
        )
        log.debug("Annotations: " + str(annotations))

        # Assembly
        assembly = self.get_param().get(
            "assembly", self.get_config().get("assembly", DEFAULT_ASSEMBLY)
        )

        # Force Update Annotation
        force_update_annotation = (
            self.get_param()
            .get("annotation", {})
            .get("options", {})
            .get("annotations_update", False)
        )
        log.debug(f"force_update_annotation={force_update_annotation}")
        force_append_annotation = (
            self.get_param()
            .get("annotation", {})
            .get("options", {})
            .get("annotations_append", False)
        )
        log.debug(f"force_append_annotation={force_append_annotation}")

        # Data
        table_variants = self.get_table_variants()

        # Check if not empty
        log.debug("Check if not empty")
        sql_query_chromosomes_df = self.get_query_to_df(
            f"""SELECT count(*) as count FROM {table_variants} as table_variants LIMIT 1"""
        )
        if not sql_query_chromosomes_df["count"][0]:
            log.info("VCF empty")
            return

        # VCF header
        vcf_reader = self.get_header()
        log.debug("Initial header: " + str(vcf_reader.infos))

        # Nb Variants POS
        log.debug("NB Variants Start")
        nb_variants = self.conn.execute(
            "SELECT count(*) AS count FROM variants"
        ).fetchdf()["count"][0]
        log.debug("NB Variants Stop")

        # Existing annotations
        for vcf_annotation in self.get_header().infos:

            vcf_annotation_line = self.get_header().infos.get(vcf_annotation)
            log.debug(
                f"Existing annotations in VCF: {vcf_annotation} [{vcf_annotation_line}]"
            )

        # Added columns
        added_columns = []

        # drop indexes
        log.debug("Drop indexes...")
        self.drop_indexes()

        # Update annotations sources
        update_annotations_sources = []

        sql_query_chromosomes_dict = None

        if annotations:

            if "ALL" in annotations:

                all_param = annotations.get("ALL", {})
                all_param_formats = all_param.get("formats", None)
                all_param_releases = all_param.get("releases", None)

                databases_infos_dict = self.scan_databases(
                    database_formats=all_param_formats,
                    database_releases=all_param_releases,
                )
                for database_infos in databases_infos_dict.keys():
                    if database_infos not in annotations:
                        annotations[database_infos] = {"INFO": None}

            # Update sources for all annotations
            update_sources = []

            for annotation in annotations:

                if annotation in ["ALL"]:
                    continue

                # Annotation Name
                annotation_name = os.path.basename(annotation)

                # Annotation fields
                annotation_fields = annotations[annotation]
                if not annotation_fields:
                    annotation_fields = {"INFO": None}

                log.debug(f"Annotation '{annotation_name}'")
                log.debug(
                    f"Annotation '{annotation_name}' - fields: {annotation_fields}"
                )

                # Find file
                annotation = annotation_file_find(
                    annotation_file=annotation,
                    databases_folders=list(databases_folders),
                    assembly=assembly,
                )

                # Create Database
                database = Database(
                    database=annotation,
                    databases_folders=databases_folders,
                    assembly=assembly,
                )

                # Find files
                parquet_file = database.get_database()
                parquet_hdr_file = database.get_header_file()
                parquet_type = database.get_type()

                # Check if files exists
                if not parquet_file or not parquet_hdr_file:
                    msg_err_list = []
                    if not parquet_file:
                        msg_err_list.append(
                            "Annotation failed: Annotation file not found"
                        )
                    if parquet_file and not parquet_hdr_file:
                        msg_err_list.append(
                            f"Annotation failed: Annotation file '{parquet_file}' header not found. Check for file '{parquet_file}.hdr'"
                        )

                    log.error(". ".join(msg_err_list))
                    raise ValueError(". ".join(msg_err_list))
                else:
                    # Get parquet connexion
                    parquet_sql_attach = database.get_sql_database_attach(
                        output="query"
                    )
                    if parquet_sql_attach:
                        self.conn.execute(parquet_sql_attach)
                    parquet_file_link = database.get_sql_database_link()
                    # Log
                    log.debug(
                        f"Annotation '{annotation_name}' - file: "
                        + str(parquet_file)
                        + " and "
                        + str(parquet_hdr_file)
                    )

                    # Database full header columns
                    parquet_hdr_vcf_header_columns = database.get_header_file_columns(
                        parquet_hdr_file
                    )
                    # Log
                    log.debug(
                        "Annotation database header columns : "
                        + str(parquet_hdr_vcf_header_columns)
                    )

                    # Load header as VCF object
                    parquet_hdr_vcf_header_infos = database.get_header().infos
                    # Log
                    # log.debug(
                    #     "Annotation database header: "
                    #     + str(parquet_hdr_vcf_header_infos)
                    # )

                    # Get extra infos
                    parquet_columns = database.get_extra_columns()
                    # Log
                    log.debug("Annotation database Columns: " + str(parquet_columns))

                    # Add extra columns if "ALL" in annotation_fields
                    # if "ALL" in annotation_fields:
                    #     allow_add_extra_column = True
                    if "ALL" in annotation_fields and database.get_extra_columns():
                        for extra_column in database.get_extra_columns():
                            if (
                                extra_column not in annotation_fields
                                and extra_column.replace("INFO/", "")
                                not in parquet_hdr_vcf_header_infos
                            ):
                                parquet_hdr_vcf_header_infos[extra_column] = (
                                    vcf.parser._Info(
                                        extra_column,
                                        ".",
                                        "String",
                                        f"{extra_column} description",
                                        "unknown",
                                        "unknown",
                                        self.code_type_map["String"],
                                    )
                                )

                    # For all fields in database
                    annotation_fields_all = False
                    if "ALL" in annotation_fields or "INFO" in annotation_fields:
                        annotation_fields_all = True
                        annotation_fields = {
                            key: key for key in parquet_hdr_vcf_header_infos
                        }

                        log.debug(
                            "Annotation database header - All annotations added: "
                            + str(annotation_fields)
                        )

                    # Init

                    # List of annotation fields to use
                    sql_query_annotation_update_info_sets = []

                    # List of annotation to agregate
                    sql_query_annotation_to_agregate = []

                    # Number of fields
                    nb_annotation_field = 0

                    # Annotation fields processed
                    annotation_fields_processed = []

                    # Columns mapping
                    map_columns = database.map_columns(
                        columns=annotation_fields, prefixes=["INFO/"]
                    )

                    # Query dict for fields to remove (update option)
                    query_dict_remove = {}

                    # Fetch Anotation fields
                    for annotation_field in annotation_fields:

                        # annotation_field_column
                        annotation_field_column = map_columns.get(
                            annotation_field, "INFO"
                        )

                        # field new name, if parametered
                        annotation_fields_new_name = annotation_fields.get(
                            annotation_field, annotation_field
                        )
                        if not annotation_fields_new_name:
                            annotation_fields_new_name = annotation_field

                        # To annotate
                        # force_update_annotation = True
                        # force_append_annotation = True
                        # if annotation_field in parquet_hdr_vcf_header_infos and (force_update_annotation or (annotation_fields_new_name not in self.get_header().infos)):
                        if annotation_field in parquet_hdr_vcf_header_infos and (
                            force_update_annotation
                            or force_append_annotation
                            or (
                                annotation_fields_new_name
                                not in self.get_header().infos
                            )
                        ):

                            # Add field to annotation to process list
                            annotation_fields_processed.append(
                                annotation_fields_new_name
                            )

                            # explode infos for the field
                            annotation_fields_new_name_info_msg = ""
                            if (
                                force_update_annotation
                                and annotation_fields_new_name
                                in self.get_header().infos
                            ):
                                # Remove field from INFO
                                query = f"""
                                    UPDATE {table_variants} as table_variants
                                    SET INFO = REGEXP_REPLACE(
                                                concat(table_variants.INFO,''),
                                                ';*{annotation_fields_new_name}=[^;]*',
                                                ''
                                                )
                                    WHERE concat(';',table_variants.INFO) LIKE '%;{annotation_fields_new_name}=%'
                                """
                                annotation_fields_new_name_info_msg = " [update]"
                                query_dict_remove[
                                    f"remove 'INFO/{annotation_fields_new_name}'"
                                ] = query

                            # Sep between fields in INFO
                            nb_annotation_field += 1
                            if nb_annotation_field > 1:
                                annotation_field_sep = ";"
                            else:
                                annotation_field_sep = ""

                            log.info(
                                f"Annotation '{annotation_name}' - '{annotation_field}' -> '{annotation_fields_new_name}'{annotation_fields_new_name_info_msg}"
                            )

                            # Add INFO field to header

                            # If regions, force values as list, due to overlap/aggregation
                            if parquet_type in ["regions"]:
                                parquet_hdr_vcf_header_infos_number = "."
                            else:
                                parquet_hdr_vcf_header_infos_number = (
                                    parquet_hdr_vcf_header_infos[annotation_field].num
                                    or "."
                                )
                            parquet_hdr_vcf_header_infos_type = (
                                parquet_hdr_vcf_header_infos[annotation_field].type
                                or "String"
                            )
                            parquet_hdr_vcf_header_infos_description = (
                                parquet_hdr_vcf_header_infos[annotation_field].desc
                                or f"{annotation_field} description"
                            )
                            parquet_hdr_vcf_header_infos_source = (
                                parquet_hdr_vcf_header_infos[annotation_field].source
                                or "unknown"
                            )
                            parquet_hdr_vcf_header_infos_version = (
                                parquet_hdr_vcf_header_infos[annotation_field].version
                                or "unknown"
                            )

                            vcf_reader.infos[annotation_fields_new_name] = (
                                vcf.parser._Info(
                                    annotation_fields_new_name,
                                    parquet_hdr_vcf_header_infos_number,
                                    parquet_hdr_vcf_header_infos_type,
                                    parquet_hdr_vcf_header_infos_description,
                                    parquet_hdr_vcf_header_infos_source,
                                    parquet_hdr_vcf_header_infos_version,
                                    self.code_type_map[
                                        parquet_hdr_vcf_header_infos_type
                                    ],
                                )
                            )

                            # Append
                            if force_append_annotation:
                                query_case_when_append = f""" AND REGEXP_EXTRACT(concat(';', table_variants.INFO), ';{annotation_fields_new_name}=([^;]*)',1) IN ('','.') """
                            else:
                                query_case_when_append = ""

                            # Annotation/Update query fields
                            # Found in INFO column
                            if (
                                annotation_field_column == "INFO"
                                and "INFO" in parquet_hdr_vcf_header_columns
                            ):
                                sql_query_annotation_update_info_sets.append(
                                    f"""
                                CASE WHEN REGEXP_EXTRACT(concat(';', table_parquet.INFO), ';{annotation_field}=([^;]*)',1) NOT IN ('','.') {query_case_when_append}
                                        THEN concat('{annotation_field_sep}', '{annotation_fields_new_name}=', REGEXP_EXTRACT(concat(';', table_parquet.INFO), ';{annotation_field}=([^;]*)',1))
                                        ELSE ''
                                    END
                                """
                                )
                            # Found in a specific column
                            else:
                                sql_query_annotation_update_info_sets.append(
                                    f"""
                                CASE WHEN CAST(table_parquet."{annotation_field_column}" AS VARCHAR) NOT IN ('','.') {query_case_when_append}
                                        THEN concat('{annotation_field_sep}', '{annotation_fields_new_name}=', replace(CAST(table_parquet."{annotation_field_column}" AS VARCHAR), ';', ','))
                                        ELSE ''
                                    END
                                """
                                )
                                sql_query_annotation_to_agregate.append(
                                    f""" array_to_string(array_sort(array_distinct(string_split(string_agg(DISTINCT table_parquet_from."{annotation_field_column}", ','), ','))), ',') AS "{annotation_field_column}" """
                                )

                        # Not to annotate
                        else:

                            if force_update_annotation:
                                annotation_message = "forced"
                            else:
                                annotation_message = "skipped"

                            if annotation_field not in parquet_hdr_vcf_header_infos:
                                log.warning(
                                    f"Annotation '{annotation_name}' - '{annotation_field}' [{nb_annotation_field}] - not available in parquet file"
                                )
                            if annotation_fields_new_name in self.get_header().infos:
                                log.warning(
                                    f"Annotation '{annotation_name}' - '{annotation_fields_new_name}' [{nb_annotation_field}] - already exists in header ({annotation_message})"
                                )

                    # Check if ALL fields have to be annotated. Thus concat all INFO field
                    # allow_annotation_full_info = True
                    allow_annotation_full_info = not force_append_annotation

                    if parquet_type in ["regions"]:
                        allow_annotation_full_info = False

                    if (
                        allow_annotation_full_info
                        and nb_annotation_field == len(annotation_fields)
                        and annotation_fields_all
                        and (
                            "INFO" in parquet_hdr_vcf_header_columns
                            and "INFO" in database.get_extra_columns()
                        )
                    ):
                        log.debug("Column INFO annotation enabled")
                        sql_query_annotation_update_info_sets = []
                        sql_query_annotation_update_info_sets.append(
                            " table_parquet.INFO "
                        )

                    if sql_query_annotation_update_info_sets:

                        # Annotate
                        log.info(f"Annotation '{annotation_name}' - Annotation...")

                        # Join query annotation update info sets for SQL
                        sql_query_annotation_update_info_sets_sql = ",".join(
                            sql_query_annotation_update_info_sets
                        )

                        # Check chromosomes list (and variants infos)
                        if sql_query_chromosomes_dict is None:
                            sql_query_chromosomes_dict
                            sql_query_chromosomes = f"""
                                SELECT table_variants."#CHROM" as CHROM, count(*) AS count_variants, min(POS) AS min_variants, MAX(POS) AS max_variants
                                FROM {table_variants} as table_variants
                                GROUP BY table_variants."#CHROM"
                                ORDER BY table_variants."#CHROM"
                                """
                            sql_query_chromosomes_df = self.conn.execute(
                                sql_query_chromosomes
                            ).df()
                            sql_query_chromosomes_dict = {
                                entry["CHROM"]: {
                                    "count": entry["count_variants"],
                                    "min": entry["min_variants"],
                                    "max": entry["max_variants"],
                                }
                                for index, entry in sql_query_chromosomes_df.iterrows()
                            }

                        # Count total variants to annotate
                        total_variants_to_annotate = sum(
                            [
                                sql_query_chromosomes_dict[chromosome]["count"]
                                for chromosome in sql_query_chromosomes_dict
                            ]
                        )
                        log.debug(
                            f"Annotation '{annotation_name}' - Total variants to annotate: {total_variants_to_annotate}"
                        )

                        # Init
                        nb_of_query = 0
                        nb_of_variant_annotated = 0
                        query_dict = query_dict_remove
                        update_table_global = True

                        if True:

                            # Annotation with regions database
                            if parquet_type in ["regions"]:
                                # sql_query_annotation_from_clause = f"""
                                #     FROM (
                                #         SELECT
                                #             table_variants_from.\"#CHROM\" AS \"#CHROM\",
                                #             table_variants_from.\"POS\" AS \"POS\",
                                #             {",".join(sql_query_annotation_to_agregate)}
                                #         FROM {table_variants} as table_variants_from
                                #         LEFT JOIN {parquet_file_link} as table_parquet_from ON (
                                #             table_variants_from.\"POS\" <= table_parquet_from.\"END\"
                                #             AND table_variants_from.\"POS\" + (len(table_variants_from.\"REF\")-1) >= (table_parquet_from.\"START\"+1)
                                #         )
                                #         GROUP BY table_variants_from.\"#CHROM\", table_variants_from.\"POS\"
                                #         )
                                #         as table_parquet
                                # """

                                # sql_query_annotation_where_clause = """
                                #     table_parquet.\"#CHROM\" = table_variants.\"#CHROM\"
                                #     AND table_parquet.\"POS\" = table_variants.\"POS\"
                                # """

                                # DEVEL
                                sql_query_annotation_from_clause = f"""
                                    (
                                        SELECT 
                                            table_variants_from.\"#CHROM\" AS \"#CHROM\",
                                            table_variants_from.\"POS\" AS \"POS\",
                                            {",".join(sql_query_annotation_to_agregate)}
                                        FROM {table_variants} as table_variants_from
                                        LEFT JOIN {parquet_file_link} as table_parquet_from ON (
                                            table_variants_from.\"POS\" <= table_parquet_from.\"END\"
                                            AND table_variants_from.\"POS\" + (len(table_variants_from.\"REF\")-1) >= (table_parquet_from.\"START\"+1)
                                        )
                                        GROUP BY table_variants_from.\"#CHROM\", table_variants_from.\"POS\"
                                        )
                                        as table_parquet
                                """

                                sql_query_annotation_where_clause = """
                                    "#CHROM", "POS"
                                """

                            # Annotation with variants database
                            else:
                                # sql_query_annotation_from_clause = f"""
                                #     FROM {parquet_file_link} as table_parquet
                                # """
                                # sql_query_annotation_where_clause = """
                                #     table_parquet.\"#CHROM\" = table_variants.\"#CHROM\"
                                #     AND table_parquet.\"POS\" = table_variants.\"POS\"
                                #     AND table_parquet.\"ALT\" = table_variants.\"ALT\"
                                #     AND table_parquet.\"REF\" = table_variants.\"REF\"
                                # """

                                # DEVEL
                                sql_query_annotation_from_clause = f"""
                                    {parquet_file_link} as table_parquet
                                """
                                sql_query_annotation_where_clause = """
                                    "#CHROM", "POS", "REF", "ALT"
                                """

                            # Create table/view for the annotation
                            annotation_view_name = "annotation_view_" + get_random(10)
                            # sql_annotation_view = f"""
                            #     CREATE TABLE {annotation_view_name} AS
                            #         SELECT
                            #             table_variants."#CHROM",
                            #             table_variants."POS",
                            #             table_variants."REF",
                            #             table_variants."ALT",
                            #             concat({sql_query_annotation_update_info_sets_sql}) AS INFO
                            #         {sql_query_annotation_from_clause}
                            #         LEFT JOIN {table_variants} AS table_variants
                            #         USING ({sql_query_annotation_where_clause})
                            #         ;
                            #     """
                            sql_annotation_view = f"""
                                CREATE VIEW {annotation_view_name} AS
                                    SELECT
                                        table_variants."#CHROM",
                                        table_variants."POS",
                                        table_variants."REF",
                                        table_variants."ALT",
                                        concat({sql_query_annotation_update_info_sets_sql}) AS INFO
                                    FROM {table_variants} AS table_variants 
                                    LEFT JOIN {sql_query_annotation_from_clause}
                                    USING ({sql_query_annotation_where_clause})
                                    ;
                                """
                            log.debug(f"sql_annotation_view={sql_annotation_view}")
                            self.conn.execute(sql_annotation_view)

                            # query_devel = f"""
                            #     SELECT * FROM {annotation_view_name}
                            # """
                            # result_devel = self.get_query_to_df(query_devel)
                            # log.debug(f"result_devel0=\n{result_devel}")

                            source = {
                                "table": annotation_view_name,
                                "join_keys": ["#CHROM", "POS", "REF", "ALT"],
                                "columns": {
                                    "INFO": {
                                        "columns": ["INFO"],
                                        "mode": "append",
                                        "separator": ";",
                                    }
                                },
                            }
                            # update_sources.append(source)

                            # update_annotations_sources.append(source)

                            strategy = self.update_table(
                                dest_table=table_variants,
                                sources=[source],
                                samples=10000,
                                force_strategy=None,
                                chromosomes=sql_query_chromosomes_dict.keys(),
                                only_strategy=True,
                            )

                            strategy = "update"

                            log.debug(
                                f"Annotation '{annotation_name}' - strategy: {strategy}"
                            )

                        if strategy != "update":

                            if update_table_global:
                                update_sources.append(source)
                            else:
                                # Update with CTAS
                                strategy = self.update_table(
                                    dest_table=table_variants,
                                    sources=[source],
                                    samples=10000,
                                    force_strategy="ctas",
                                    chromosomes=None,  # sql_query_chromosomes_dict.keys(),
                                    only_strategy=False,
                                )

                        else:

                            # update_sources = []
                            # Heuristic: if more than X variants, do batch using update_table, else old fashioned way
                            heuristic_nb_variants = -1  # 1000  # 1000
                            # Create annotation source mode
                            annotation_source_mode = "TABLE"  # TABLE VIEW

                            sql_query_annotation_chrom_interval_pos_union = (
                                "annotation_chrom_interval_pos_union_" + get_random(10)
                            )

                            if annotation_source_mode == "TABLE":

                                # Crerate empty table for batch update
                                sql_create_empty_table = f"""
                                    CREATE TABLE {sql_query_annotation_chrom_interval_pos_union} AS
                                        SELECT
                                            "#CHROM",
                                            "POS",
                                            "REF",
                                            "ALT",
                                            "INFO"
                                        FROM {table_variants} AS table_variants
                                        WHERE 1=0
                                    ;
                                """
                                log.debug(
                                    f"sql_create_empty_table={sql_create_empty_table}"
                                )
                                self.conn.execute(sql_create_empty_table)

                                source = {
                                    "table": sql_query_annotation_chrom_interval_pos_union,
                                    "join_keys": [
                                        "#CHROM",
                                        "POS",
                                        "REF",
                                        "ALT",
                                    ],
                                    "columns": {
                                        "INFO": {
                                            "columns": ["INFO"],
                                            "mode": "append",
                                            "separator": ";",
                                        }
                                    },
                                }
                                update_sources.append(source)

                            # For each chromosome, first bacth by chromosome
                            for chrom in sql_query_chromosomes_dict:

                                # Number of variant by chromosome
                                nb_of_variant_by_chrom = sql_query_chromosomes_dict.get(
                                    chrom, {}
                                ).get("count", 0)

                                log.debug(
                                    f"Annotation '{annotation_name}' - Chromosome '{chrom}' [{nb_of_variant_by_chrom} variants]..."
                                )

                                # Annotation with regions database
                                if parquet_type in ["regions"]:
                                    sql_query_annotation_from_clause = f"""
                                        (
                                            SELECT
                                                '{chrom}' AS \"#CHROM\",
                                                table_variants_from.\"POS\" AS \"POS\",
                                                {",".join(sql_query_annotation_to_agregate)}
                                            FROM {table_variants} as table_variants_from
                                            LEFT JOIN {parquet_file_link} as table_parquet_from ON (
                                                table_parquet_from."#CHROM" = '{chrom}'
                                                AND table_variants_from.\"POS\" <= table_parquet_from.\"END\"
                                                AND table_variants_from.\"POS\" + (len(table_variants_from.\"REF\")-1) >= (table_parquet_from.\"START\"+1)
                                            )
                                            WHERE table_variants_from.\"#CHROM\" in ('{chrom}')
                                            GROUP BY table_variants_from.\"POS\"
                                        )
                                        as table_parquet
                                    """

                                    sql_query_annotation_where_clause = f"""
                                        table_variants."#CHROM" = '{chrom}'
                                        AND table_variants."#CHROM" = table_parquet."#CHROM"
                                        AND table_variants.POS = table_parquet.POS
                                    """

                                # Annotation with variants database
                                else:
                                    sql_query_annotation_from_clause = f"""
                                        ({parquet_file_link}) as table_parquet
                                    """
                                    sql_query_annotation_where_clause = f"""
                                        table_variants."#CHROM" = '{chrom}'
                                        AND table_variants."#CHROM" = table_parquet."#CHROM"
                                        AND table_variants.POS = table_parquet.POS
                                        AND table_variants.REF = table_parquet.REF
                                        AND table_variants.ALT = table_parquet.ALT
                                    """

                                if total_variants_to_annotate >= heuristic_nb_variants:

                                    # Get min/max POS and number of variants by chrom
                                    nb_of_variant_by_chrom = (
                                        sql_query_chromosomes_dict.get(chrom, {}).get(
                                            "count", 0
                                        )
                                    )
                                    min_of_variant_by_chrom = (
                                        sql_query_chromosomes_dict.get(chrom, {}).get(
                                            "min", 0
                                        )
                                    ) - 1
                                    max_of_variant_by_chrom = (
                                        sql_query_chromosomes_dict.get(chrom, {}).get(
                                            "max", 0
                                        )
                                    )

                                    # Create batch queries by position intervals
                                    batch_index = 0
                                    nb_windows = (
                                        nb_of_variant_by_chrom // chunk_size
                                    ) + 1
                                    chunk_size_batch_update = (
                                        int(
                                            (
                                                max_of_variant_by_chrom
                                                - min_of_variant_by_chrom
                                            )
                                            / nb_windows
                                        )
                                        + 1
                                    )

                                    # DEBUG
                                    # log.debug(f"nb_windows={nb_windows}")
                                    # log.debug(
                                    #     f"chunk_size_batch_update={chunk_size_batch_update}"
                                    # )

                                    # Create queries by position intervals
                                    for start in range(
                                        min_of_variant_by_chrom,
                                        max_of_variant_by_chrom,
                                        chunk_size_batch_update,
                                    ):
                                        end = start + chunk_size_batch_update
                                        batch_index += 1

                                        # where clause in any cases, because it speedup the query
                                        where_clause_batch_split = f" AND table_variants.POS >= {start} AND table_variants.POS < {end} "

                                        # where_clause_batch_in = f"""
                                        #         AND concat("#CHROM", "POS", "REF", "ALT") IN (
                                        #             SELECT concat("#CHROM", "POS", "REF", "ALT")
                                        #             FROM {table_variants}
                                        #             WHERE table_variants."#CHROM" = '{chrom}'
                                        #                 AND table_variants.POS >= {start} AND table_variants.POS < {end}
                                        #         )
                                        #     """

                                        # where_clause_batch_in = f"""
                                        #         AND table_variants."#CHROM" = table_parquet."#CHROM"
                                        #         AND table_variants.POS = table_parquet.POS
                                        #         AND table_variants.REF = table_parquet.REF
                                        #         AND table_variants.ALT = table_parquet.ALT

                                        #     """

                                        # # Create update query
                                        # sql_query_annotation_chrom_interval_pos_OLD = f"""
                                        #     UPDATE {table_variants} as table_variants
                                        #         SET INFO =
                                        #             concat(
                                        #                 CASE WHEN table_variants.INFO NOT IN ('','.')
                                        #                     THEN table_variants.INFO
                                        #                     ELSE ''
                                        #                 END
                                        #                 ,
                                        #                 CASE WHEN table_variants.INFO NOT IN ('','.')
                                        #                             AND (
                                        #                             concat({sql_query_annotation_update_info_sets_sql})
                                        #                             )
                                        #                             NOT IN ('','.')
                                        #                         THEN ';'
                                        #                         ELSE ''
                                        #                 END
                                        #                 ,
                                        #                 {sql_query_annotation_update_info_sets_sql}
                                        #                 )
                                        #         FROM {sql_query_annotation_from_clause}
                                        #         WHERE {sql_query_annotation_where_clause}
                                        #             {where_clause_batch_split}
                                        #         ;
                                        #     """

                                        if annotation_source_mode == "TABLE":

                                            # Insert into annotation_chrom_interval_pos_union
                                            sql_query_annotation_chrom_interval_pos = f"""
                                            INSERT INTO {sql_query_annotation_chrom_interval_pos_union}
                                                SELECT
                                                    table_variants."#CHROM",
                                                    table_variants."POS",
                                                    table_variants."REF",
                                                    table_variants."ALT",
                                                    concat(
                                                            {sql_query_annotation_update_info_sets_sql}
                                                            ) AS INFO
                                                FROM {table_variants} AS table_variants
                                                LEFT JOIN {sql_query_annotation_from_clause}
                                                USING ("#CHROM", "POS", "REF", "ALT")
                                                WHERE {sql_query_annotation_where_clause}
                                                    {where_clause_batch_split}
                                                    ;
                                                    """

                                        else:

                                            annotation_query_update_name = (
                                                "annotation_query_update_"
                                                + get_random(10)
                                            )
                                            sql_query_annotation_chrom_interval_pos = f"""
                                                CREATE VIEW {annotation_query_update_name} AS
                                                SELECT
                                                    table_variants."#CHROM",
                                                    table_variants."POS",
                                                    table_variants."REF",
                                                    table_variants."ALT",
                                                    concat(   
                                                            {sql_query_annotation_update_info_sets_sql}
                                                            ) AS INFO
                                                FROM {table_variants} AS table_variants
                                                LEFT JOIN {sql_query_annotation_from_clause}
                                                USING ("#CHROM", "POS", "REF", "ALT")
                                                WHERE {sql_query_annotation_where_clause}
                                                    {where_clause_batch_split}
                                                    
                                            """

                                            source = {
                                                "table": annotation_query_update_name,
                                                "join_keys": [
                                                    "#CHROM",
                                                    "POS",
                                                    "REF",
                                                    "ALT",
                                                ],
                                                "columns": {
                                                    "INFO": {
                                                        "columns": ["INFO"],
                                                        "mode": "append",
                                                        "separator": ";",
                                                    }
                                                },
                                            }
                                            update_sources.append(source)

                                            # if update_table_global:
                                            #     update_sources.append(source)
                                            # else:
                                            #     log.debug(f"START UPDATE...")
                                            #     strategy = self.update_table(
                                            #         dest_table=table_variants,
                                            #         sources=[source],
                                            #         samples=10000,
                                            #         force_strategy=None,
                                            #         chromosomes=[chrom],
                                            #         only_strategy=False,
                                            #     )
                                            #     log.debug(f"STOP UPDATE...")

                                        # {where_clause_batch_split}
                                        #  {where_clause_batch_split}

                                        # DEBUG
                                        log.debug(
                                            f"sql_query_annotation_chrom_interval_pos={sql_query_annotation_chrom_interval_pos}"
                                        )

                                        self.get_connexion().execute(
                                            sql_query_annotation_chrom_interval_pos
                                        )

                                        # DEVEL
                                        # query_devel = f"""
                                        #     SELECT count(*) FROM {annotation_query_update_name} LIMIT 10
                                        # """

                                        # Add update query to dict
                                        # query_dict[
                                        #     f"{chrom} [{nb_of_variant_by_chrom} variants] - batch [{batch_index}/{nb_windows}][{batch_index/nb_windows:.2%}%]"
                                        # ] = sql_query_annotation_chrom_interval_pos

                                else:  # OK, old version

                                    # Get min/max POS and number of variants by chrom
                                    nb_of_variant_by_chrom = (
                                        sql_query_chromosomes_dict.get(chrom, {}).get(
                                            "count", 0
                                        )
                                    )
                                    min_of_variant_by_chrom = (
                                        sql_query_chromosomes_dict.get(chrom, {}).get(
                                            "min", 0
                                        )
                                    ) - 1
                                    max_of_variant_by_chrom = (
                                        sql_query_chromosomes_dict.get(chrom, {}).get(
                                            "max", 0
                                        )
                                    )

                                    # Create batch queries by position intervals
                                    batch_index = 0
                                    nb_windows = (
                                        nb_of_variant_by_chrom // chunk_size
                                    ) + 1
                                    chunk_size_batch_update = (
                                        int(
                                            (
                                                max_of_variant_by_chrom
                                                - min_of_variant_by_chrom
                                            )
                                            / nb_windows
                                        )
                                        + 1
                                    )

                                    # DEBUG
                                    # log.debug(f"nb_windows={nb_windows}")
                                    # log.debug(
                                    #     f"chunk_size_batch_update={chunk_size_batch_update}"
                                    # )

                                    # Create queries by position intervals
                                    for start in range(
                                        min_of_variant_by_chrom,
                                        max_of_variant_by_chrom,
                                        chunk_size_batch_update,
                                    ):
                                        end = start + chunk_size_batch_update
                                        batch_index += 1

                                        # where clause in any cases, because it speedup the query
                                        where_clause_batch_split = f" AND table_variants.POS >= {start} AND table_variants.POS < {end} "

                                        # Create update query
                                        sql_query_annotation_chrom_interval_pos = f"""
                                            UPDATE {table_variants} as table_variants
                                                SET INFO = 
                                                    concat(
                                                        CASE WHEN table_variants.INFO NOT IN ('','.')
                                                            THEN table_variants.INFO
                                                            ELSE ''
                                                        END
                                                        ,
                                                        CASE WHEN table_variants.INFO NOT IN ('','.')
                                                                    AND (
                                                                    concat({sql_query_annotation_update_info_sets_sql})
                                                                    )
                                                                    NOT IN ('','.') 
                                                                THEN ';'
                                                                ELSE ''
                                                        END
                                                        ,
                                                        {sql_query_annotation_update_info_sets_sql}
                                                        )
                                                FROM {sql_query_annotation_from_clause}
                                                WHERE {sql_query_annotation_where_clause}
                                                    {where_clause_batch_split}
                                                ;
                                            """

                                        # DEBUG
                                        log.debug(
                                            f"sql_query_annotation_chrom_interval_pos={sql_query_annotation_chrom_interval_pos}"
                                        )

                                        # Add update query to dict
                                        query_dict[
                                            f"{chrom} [{nb_of_variant_by_chrom} variants] - batch [{batch_index}/{nb_windows}][{batch_index/nb_windows:.2%}%]"
                                        ] = sql_query_annotation_chrom_interval_pos

                        # # Update sources
                        # log.debug(f"update_sources={update_sources}")
                        # # sql_query_chromosomes_dict
                        # log.debug(f"START UPDATE...")
                        # strategy = self.update_table(
                        #     dest_table=table_variants,
                        #     sources=update_sources,
                        #     samples=10000,
                        #     force_strategy="update",
                        #     chromosomes=None,  # sql_query_chromosomes_dict.keys(),
                        #     only_strategy=False,
                        # )
                        # log.debug(f"STOP UPDATE...")

                        nb_of_query = len(query_dict)
                        num_query = 0

                        # SET max_expression_depth TO x
                        self.conn.execute("SET max_expression_depth TO 10000")

                        for query_name in query_dict:
                            query = query_dict[query_name]
                            num_query += 1
                            log.info(
                                f"Annotation '{annotation_name}' - Annotation - Query [{num_query}/{nb_of_query}][{num_query/nb_of_query:.2%}] {query_name}..."
                            )
                            result = self.conn.execute(query)
                            nb_of_variant_annotated_by_query = result.df()["Count"][0]
                            nb_of_variant_annotated += nb_of_variant_annotated_by_query
                            log.info(
                                f"Annotation '{annotation_name}' - Annotation - Query [{num_query}/{nb_of_query}][{num_query/nb_of_query:.2%}] {query_name} - {nb_of_variant_annotated_by_query} variants annotated"
                            )

                        log.info(
                            f"Annotation '{annotation_name}' - Annotation of {nb_of_variant_annotated} variants out of {nb_variants} (with {nb_of_query} queries)"
                        )

                    else:

                        log.info(
                            f"Annotation '{annotation_name}' - No Annotations available"
                        )

            # Update sources
            log.debug(f"update_sources={update_sources}")
            # sql_query_chromosomes_dict
            if len(update_sources) > 0:
                log.debug(f"START UPDATE ???...")
                self.update_table(
                    dest_table=table_variants,
                    sources=update_sources,
                    samples=10000,
                    force_strategy=None,
                    chromosomes=None,  # [],  # sql_query_chromosomes_dict.keys(),  # None,
                    only_strategy=False,
                )
                log.debug(f"STOP UPDATE...")

            # # Finalize update
            # log.debug(f"Update annotations sources: {update_annotations_sources}")
            # self.update_table(
            #     dest_table=table_variants,
            #     sources=update_annotations_sources,
            #     samples=10000,
            #     force_strategy=None,
            # )

        # Remove added columns
        for added_column in added_columns:
            self.drop_column(column=added_column)

    def annotation_splice(self, threads: int = None) -> None:
        """
        This function annotate with snpEff

        :param threads: The number of threads to use
        :return: the value of the variable "return_value".
        """

        # DEBUG
        log.debug("Start annotation with splice tools")

        # Threads
        if not threads:
            threads = self.get_threads()
        log.debug("Threads: " + str(threads))

        # DEBUG
        delete_tmp = True
        if self.get_config().get("verbosity", "warning") in ["debug"]:
            delete_tmp = False
            log.debug("Delete tmp files/folders: " + str(delete_tmp))

        # Config
        config = self.get_config()
        log.debug("Config: " + str(config))
        splice_config = config.get("tools", {}).get("splice", {})
        if not splice_config:
            splice_config = DEFAULT_TOOLS_BIN.get("splice", {})
            msg_err = "No Splice tool config"
            raise ValueError(msg_err)
        log.debug(f"splice_config: {splice_config}")

        # Config - Folders - Databases
        databases_folders = (
            config.get("folders", {}).get("databases", {}).get("splice", ["."])
        )
        log.debug("Databases annotations: " + str(databases_folders))

        # Splice docker image
        splice_docker_image = splice_config.get("docker").get("image")

        # Pull splice image if it's not already there
        if not check_docker_image_exists(splice_docker_image):
            log.warning(
                f"Annotation: splice docker image {splice_docker_image} not found locally, trying to pull from dockerhub"
            )
            try:
                command(f"docker pull {splice_config.get('docker').get('image')}")
            except subprocess.CalledProcessError:
                msg_err = f"Unable to find docker {splice_docker_image} on dockerhub"
                log.error(msg_err)
                raise ValueError(msg_err)

        # Config - splice databases
        splice_databases = (
            config.get("folders", {})
            .get("databases", {})
            .get("splice", DEFAULT_SPLICE_FOLDER)
        )
        splice_databases = full_path(splice_databases)

        # Param
        param = self.get_param()
        log.debug("Param: " + str(param))

        # Param
        options = param.get("annotation", {}).get("splice", {}).get("options", {})
        log.debug("Options: " + str(options))

        # Data
        table_variants = self.get_table_variants()

        # Check if not empty
        log.debug("Check if not empty")
        sql_query_chromosomes = (
            f"""SELECT count(*) as count FROM {table_variants} as table_variants"""
        )
        if not self.get_query_to_df(f"{sql_query_chromosomes}")["count"][0]:
            log.info("VCF empty")
            return None

        # Export in VCF
        log.debug("Create initial file to annotate")

        # Create output folder / work folder
        if options.get("output_folder", ""):
            output_folder = options.get("output_folder", "")
            if not os.path.exists(output_folder):
                Path(output_folder).mkdir(parents=True, exist_ok=True)
        else:
            output_folder = os.path.join(self.get_tmp_dir(), f"splice-{get_random()}")
            if not os.path.exists(output_folder):
                Path(output_folder).mkdir(parents=True, exist_ok=True)

        if options.get("workdir", ""):
            workdir = options.get("workdir", "")
        else:
            workdir = "/work"

        # Create tmp VCF file
        tmp_vcf = NamedTemporaryFile(
            prefix=self.get_prefix(),
            dir=output_folder,
            suffix=".vcf",
            delete=False,
        )
        tmp_vcf_name = tmp_vcf.name

        # VCF header
        header = self.get_header()

        # Existing annotations
        for vcf_annotation in self.get_header().infos:

            vcf_annotation_line = self.get_header().infos.get(vcf_annotation)
            log.debug(
                f"Existing annotations in VCF: {vcf_annotation} [{vcf_annotation_line}]"
            )

        # Memory limit
        if config.get("memory", None):
            memory_limit = config.get("memory", "8G").upper()
            # upper()
        else:
            memory_limit = "8G"
        log.debug(f"memory_limit: {memory_limit}")

        # Check number of variants to annotate
        where_clause_regex_spliceai = r"SpliceAI_\w+"
        where_clause_regex_spip = r"SPiP_\w+"
        where_clause = f""" WHERE NOT regexp_matches("INFO", '{where_clause_regex_spliceai}') AND NOT regexp_matches("INFO", '{where_clause_regex_spip}')"""
        df_list_of_variants_to_annotate = self.get_query_to_df(
            query=f""" SELECT * FROM variants {where_clause} """
        )
        if len(df_list_of_variants_to_annotate) == 0:
            log.warning(
                f"No variants to annotate with splice. Variants probably already annotated with splice"
            )
            return None
        else:
            log.info(f"Annotation: {len(df_list_of_variants_to_annotate)} variants")

        # Export VCF file
        self.export_variant_vcf(
            vcf_file=tmp_vcf_name,
            remove_info=True,
            add_samples=True,
            index=False,
            where_clause=where_clause,
        )
        mount = [f" -v {path}:{path}:rw" for path in [output_folder]]
        if any(value for value in splice_config.values() if value is None):
            log.warning("At least one splice config parameter is empty")
            # exit annotation_splice
            return None

        # Params in splice nf
        def check_values(dico: dict):
            """
            Ensure parameters for NF splice pipeline
            """
            for key, val in dico.items():
                if key == "genome":
                    if any(
                        assemb in options.get("genome", {})
                        for assemb in ["hg19", "GRCh37", "grch37", "GRCH37"]
                    ):
                        yield f"--{key} hg19"
                    elif any(
                        assemb in options.get("genome", {})
                        for assemb in ["hg38", "GRCh38", "grch38", "GRCH38"]
                    ):
                        yield f"--{key} hg38"
                elif (
                    (isinstance(val, str) and val)
                    or isinstance(val, int)
                    or isinstance(val, bool)
                ):
                    yield f"--{key} {val}"

        # Genome
        genome = options.get("genome", config.get("assembly", DEFAULT_ASSEMBLY))
        options["genome"] = genome
        # NF params
        nf_params = []
        # Add options
        if options:
            log.debug(options)
            nf_params = list(check_values(options))
            log.debug(f"Splice NF params: {' '.join(nf_params)}")
        else:
            log.debug("No NF params provided")
        # Add threads
        if "threads" not in options.keys():
            nf_params.append(f"--threads {threads}")
        # Genome path
        genome_path = find_genome(
            config.get("folders", {})
            .get("databases", {})
            .get("genomes", DEFAULT_GENOME_FOLDER),
            file=f"{genome}.fa",
        )
        # Add genome path
        if not genome_path:
            raise ValueError(
                f"Can't find genome assembly {genome}.fa in {config.get('folders', {}).get('databases', {}).get('genomes', DEFAULT_GENOME_FOLDER)}"
            )
        else:
            log.debug(f"Genome: {genome_path}")
            nf_params.append(f"--genome_path {genome_path}")

        def splice_annotations(options: dict = {}, config: dict = {}) -> list:
            """
            Setting up updated databases for SPiP and SpliceAI
            """

            try:

                # SpliceAI assembly transcriptome
                spliceai_assembly = os.path.join(
                    config.get("folders", {}).get("databases", {}).get("spliceai", {}),
                    options.get("genome"),
                    "transcriptome",
                )
                spip_assembly = options.get("genome")

                spip = find(
                    f"transcriptome_{spip_assembly}.RData",
                    config.get("folders", {}).get("databases", {}).get("spip", {}),
                )
                spliceai = find("spliceai.refseq.txt", spliceai_assembly)
                log.debug(f"SPiP annotations: {spip}")
                log.debug(f"SpliceAI annotations: {spliceai}")
                if spip and spliceai:
                    return [
                        f"--spip_transcriptome {spip}",
                        f"--spliceai_transcriptome {spliceai}",
                    ]
                else:
                    log.warning(
                        "Can't find splice databases in configuration, use annotations file from image"
                    )
            except TypeError:
                log.warning(
                    "Can't find splice databases in configuration, use annotations file from image"
                )
                return []

        # Add options, check if transcriptome option have already beend provided
        if (
            "spip_transcriptome" not in nf_params
            and "spliceai_transcriptome" not in nf_params
        ):
            splice_reference = splice_annotations(options, config)
            if splice_reference:
                nf_params.extend(splice_reference)
        # nf_params.append(f"--output_folder {output_folder}")
        random_uuid = f"HOWARD-SPLICE-{get_random()}"
        cmd = f"nextflow -log {os.path.join(output_folder, f'{random_uuid}.log')} -c /app/SpliceToolBox/src/splicetoolbox/nextflow/nextflow.docker.config run /app/SpliceToolBox/src/splicetoolbox/nextflow/main.nf -entry SPLICE --vcf {tmp_vcf_name} {' '.join(nf_params)} -profile standard,conda,singularity,report,timeline"
        log.debug(cmd)
        splice_config["docker"]["command"] = cmd

        # Ensure proxy is set
        proxy = [
            f"-e {var}={os.getenv(var)}"
            for var in ["https_proxy", "http_proxy", "ftp_proxy"]
            if os.getenv(var) is not None
        ]
        docker_cmd = get_bin_command(
            tool="splice",
            bin_type="docker",
            config=config,
            default_folder=f"{DEFAULT_TOOLS_FOLDER}/docker",
            add_options=f"--name {random_uuid} {' '.join(mount)} -e NXF_DISABLE_CHECK_LATEST=true {' '.join(proxy)}",
        )
        # print(docker_cmd)
        # exit()
        # Docker debug
        # if splice_config.get("rm_container"):
        #     rm_container = "--rm"
        # else:
        #     rm_container = ""
        # docker_cmd = f"docker run {rm_container} --entrypoint '/bin/bash' --name {random_uuid} {' '.join(mount)} {':'.join(splice_config.get('image'))} {cmd}"
        log.debug(docker_cmd)
        res = subprocess.run(docker_cmd, shell=True, capture_output=True, text=True)
        log.debug(res.stdout)
        if res.stderr:
            log.error(res.stderr)
        res.check_returncode()
        # Update variants
        log.info("Annotation - Updating...")
        # Test find output vcf
        log.debug(
            f"TMP splice output: {os.path.basename(tmp_vcf_name).replace('.vcf', '')}.spip.spliceai.sorted.vcf.gz"
        )
        output_vcf = []
        # Wrong folder to look in
        for files in os.listdir(os.path.dirname(tmp_vcf_name)):
            if (
                files
                == f"{os.path.basename(tmp_vcf_name).replace('.vcf', '')}.spip.spliceai.sorted.vcf.gz"
            ):
                output_vcf.append(os.path.join(os.path.dirname(tmp_vcf_name), files))
        # log.debug(os.listdir(options.get("output_folder")))
        log.debug(f"Splice annotated vcf: {output_vcf[0]}")
        if not output_vcf:
            log.debug(
                f"Splice output was not generated {os.path.basename(tmp_vcf_name)}*.spip.spliceai.sorted.vcf.gz"
            )
        else:
            # Get new header from annotated vcf
            log.debug(f"Initial header: {len(header.infos)} fields")
            # Create new header with splice infos
            new_vcf = Variants(input=output_vcf[0])
            new_vcf_header = new_vcf.get_header().infos
            for keys, infos in new_vcf_header.items():
                if keys not in header.infos.keys():
                    header.infos[keys] = infos
            log.debug(f"New header: {len(header.infos)} fields")
            log.debug(f"Splice tmp output: {output_vcf[0]}")
            self.update_from_vcf(output_vcf[0])

        # Remove file
        remove_if_exists(output_vcf)

    ###
    # Prioritization
    ###

    def get_config_default(self, name: str) -> dict:
        """
        The function `get_config_default` returns a dictionary containing default configurations for
        various calculations and prioritizations.

        :param name: The `get_config_default` function returns a dictionary containing default
        configurations for different calculations and prioritizations. The `name` parameter is used to
        specify which specific configuration to retrieve from the dictionary
        :type name: str
        :return: The function `get_config_default` returns a dictionary containing default configuration
        settings for different calculations and prioritizations. The specific configuration settings are
        retrieved based on the input `name` parameter provided to the function. If the `name` parameter
        matches a key in the `config_default` dictionary, the corresponding configuration settings are
        returned. If there is no match, an empty dictionary is returned.
        """

        config_default = {
            "calculations": {
                "variant_chr_pos_alt_ref": {
                    "type": "sql",
                    "name": "variant_chr_pos_alt_ref",
                    "description": "Create a variant ID with chromosome, position, alt and ref",
                    "available": False,
                    "output_column_name": "variant_chr_pos_alt_ref",
                    "output_column_number": 1,
                    "output_column_type": "String",
                    "output_column_description": "variant ID with chromosome, position, alt and ref",
                    "operation_query": """ concat("#CHROM", '_', "POS", '_', "REF", '_', "ALT") """,
                    "operation_info": True,
                },
                "VARTYPE": {
                    "type": "sql",
                    "name": "VARTYPE",
                    "description": "Variant type (e.g. SNV, INDEL, MNV, BND...)",
                    "available": True,
                    "table": "variants",
                    "output_column_name": "VARTYPE",
                    "output_column_number": 1,
                    "output_column_type": "String",
                    "output_column_description": "Variant type: SNV if X>Y, MOSAIC if X>Y,Z or X,Y>Z, INDEL if XY>Z or X>YZ",
                    "operation_query": """
                            CASE
                                WHEN "SVTYPE" NOT NULL THEN "SVTYPE"
                                WHEN LENGTH(REF) = 1 AND LENGTH(ALT) = 1 THEN 'SNV'
                                WHEN REF LIKE '%,%' OR ALT LIKE '%,%' THEN 'MOSAIC'
                                WHEN LENGTH(REF) == LENGTH(ALT) AND LENGTH(REF) > 1 THEN 'MNV'
                                WHEN LENGTH(REF) <> LENGTH(ALT) THEN 'INDEL'
                                ELSE 'UNDEFINED'
                            END
                            """,
                    "info_fields": ["SVTYPE"],
                    "operation_info": True,
                },
                "snpeff_extract": {
                    "type": "python",
                    "name": "snpeff_hgvs",
                    "description": "HGVS nomenclatures from snpEff annotation",
                    "available": True,
                    "function_name": "calculation_extract_snpeff",
                    "function_params": [
                        "ANN",
                        "snpeff_hgvs",
                        "snpeff_",
                        "snpeff_json",
                        False,
                    ],
                },
                "snpeff_hgvs": {
                    "type": "python",
                    "name": "snpeff_hgvs",
                    "description": "HGVS nomenclatures from snpEff annotation",
                    "available": True,
                    "function_name": "calculation_extract_snpeff",
                    "function_params": ["ANN", "snpeff_hgvs", None, None, False],
                },
                "snpeff_ann_explode": {
                    "type": "python",
                    "name": "snpeff_ann_explode",
                    "description": "Explode snpEff annotations with uniquify values",
                    "available": True,
                    "function_name": "calculation_extract_snpeff",
                    "function_params": ["ANN", None, "snpeff_", None, False],
                },
                "snpeff_ann_explode_uniquify": {
                    "type": "python",
                    "name": "snpeff_ann_explode_uniquify",
                    "description": "Explode snpEff annotations",
                    "available": True,
                    "function_name": "calculation_extract_snpeff",
                    "function_params": ["ANN", None, "snpeff_uniquify_", None, True],
                },
                "snpeff_ann_explode_json": {
                    "type": "python",
                    "name": "snpeff_ann_explode_json",
                    "description": "Explode snpEff annotations in JSON format",
                    "available": True,
                    "function_name": "calculation_extract_snpeff",
                    "function_params": ["ANN", None, None, "snpeff_json", True],
                },
                "NOMEN": {
                    "type": "python",
                    "name": "NOMEN",
                    "description": "NOMEN information (e.g. NOMEN, CNOMEN, PNOMEN...) from HGVS nomenclature field (see parameters help)",
                    "available": True,
                    "function_name": "calculation_extract_nomen",
                    "function_params": [],
                },
                "NOMEN_SNPEFF": {
                    "type": "python",
                    "name": "NOMEN",
                    "description": "NOMEN information (e.g. NOMEN, CNOMEN, PNOMEN...) from HGVS nomenclature field (see parameters help)",
                    "available": True,
                    "function_name": "calculation_extract_nomen",
                    "function_params": ["snpeff_hgvs"],
                },
                "RECREATE_INFO_FIELDS": {
                    "type": "python",
                    "name": "RENAME_INFO_FIELDS",
                    "description": "Recreate INFO_tags, rename or remove tags",
                    "available": True,
                    "function_name": "calculation_recreate_info_fields",
                    "function_params": [],
                },
                "RENAME_INFO_FIELDS": {
                    "type": "python",
                    "name": "RENAME_INFO_FIELDS",
                    "description": "Rename or remove INFO/tags",
                    "available": True,
                    "function_name": "calculation_rename_info_fields",
                    "function_params": [],
                },
                "FINDBYPIPELINE": {
                    "type": "python",
                    "name": "FINDBYPIPELINE",
                    "description": "Number of pipeline that identify the variant (for multi pipeline VCF)",
                    "available": True,
                    "function_name": "calculation_find_by_pipeline",
                    "function_params": ["findbypipeline"],
                },
                "FINDBYSAMPLE": {
                    "type": "python",
                    "name": "FINDBYSAMPLE",
                    "description": "Number of sample that have a genotype for the variant (for multi sample VCF)",
                    "available": True,
                    "function_name": "calculation_find_by_pipeline",
                    "function_params": ["findbysample"],
                },
                "GENOTYPECONCORDANCE": {
                    "type": "python",
                    "name": "GENOTYPECONCORDANCE",
                    "description": "Concordance of genotype for multi caller VCF",
                    "available": True,
                    "function_name": "calculation_genotype_concordance",
                    "function_params": [],
                },
                "BARCODE": {
                    "type": "python",
                    "name": "BARCODE",
                    "description": "BARCODE as VaRank tool",
                    "available": True,
                    "function_name": "calculation_barcode",
                    "function_params": [],
                },
                "BARCODEFAMILY": {
                    "type": "python",
                    "name": "BARCODEFAMILY",
                    "description": "BARCODEFAMILY as VaRank tool",
                    "available": True,
                    "function_name": "calculation_barcode_family",
                    "function_params": ["BCF"],
                },
                "TRIO": {
                    "type": "python",
                    "name": "TRIO",
                    "description": "Inheritance for a trio family",
                    "available": True,
                    "function_name": "calculation_trio",
                    "function_params": [],
                },
                "VAF": {
                    "type": "python",
                    "name": "VAF",
                    "description": "Variant Allele Frequency (VAF) harmonization",
                    "available": True,
                    "function_name": "calculation_vaf_normalization",
                    "function_params": [],
                },
                "VAF_stats": {
                    "type": "python",
                    "name": "VAF_stats",
                    "description": "Variant Allele Frequency (VAF) statistics",
                    "available": True,
                    "function_name": "calculation_genotype_stats",
                    "function_params": ["VAF"],
                },
                "DP_stats": {
                    "type": "python",
                    "name": "DP_stats",
                    "description": "Depth (DP) statistics",
                    "available": True,
                    "function_name": "calculation_genotype_stats",
                    "function_params": ["DP"],
                },
                "variant_id": {
                    "type": "python",
                    "name": "variant_id",
                    "description": "Variant ID generated from variant position and type",
                    "available": True,
                    "function_name": "calculation_variant_id",
                    "function_params": [],
                },
                "transcripts_json": {
                    "type": "python",
                    "name": "transcripts_json",
                    "description": "Add transcripts annotations in JSON format (field 'transcripts_json')",
                    "available": True,
                    "function_name": "calculation_transcripts_annotation",
                    "function_params": ["transcripts_json", None],
                },
                "transcripts_ann": {
                    "type": "python",
                    "name": "transcripts_ann",
                    "description": "Add transcripts annotations in structured format (field 'transcripts_ann')",
                    "available": True,
                    "function_name": "calculation_transcripts_annotation",
                    "function_params": [None, "transcripts_ann"],
                },
                "transcripts_annotations": {
                    "type": "python",
                    "name": "transcripts_annotations",
                    "description": "Add transcripts annotations in JSON and/or structured format (see param JSON file)",
                    "available": True,
                    "function_name": "calculation_transcripts_annotation",
                    "function_params": [None, None],
                },
                "transcripts_prioritization": {
                    "type": "python",
                    "name": "transcripts_prioritization",
                    "description": "Prioritize transcripts with a prioritization profile (using param.json)",
                    "available": True,
                    "function_name": "calculation_transcripts_prioritization",
                    "function_params": [False],
                },
                "transcripts_prioritization_strict": {
                    "type": "python",
                    "name": "transcripts_prioritization",
                    "description": "Prioritize transcripts with a prioritization profile (using param.json)",
                    "available": True,
                    "function_name": "calculation_transcripts_prioritization",
                    "function_params": [True],
                },
                "transcripts_export": {
                    "type": "python",
                    "name": "transcripts_export",
                    "description": "Export transcripts table/view as a file (using param.json)",
                    "available": True,
                    "function_name": "calculation_transcripts_export",
                    "function_params": [],
                },
            },
            "prioritizations": {
                "default": {
                    "ANN2": [
                        {
                            "type": "contains",
                            "value": "HIGH",
                            "score": 5,
                            "flag": "PASS",
                            "comment": [
                                "The variant is assumed to have high (disruptive) impact in the protein, probably causing protein truncation, loss of function or triggering nonsense mediated decay"
                            ],
                        },
                        {
                            "type": "contains",
                            "value": "MODERATE",
                            "score": 3,
                            "flag": "PASS",
                            "comment": [
                                "A non-disruptive variant that might change protein effectiveness"
                            ],
                        },
                        {
                            "type": "contains",
                            "value": "LOW",
                            "score": 0,
                            "flag": "FILTERED",
                            "comment": [
                                "Assumed to be mostly harmless or unlikely to change protein behavior"
                            ],
                        },
                        {
                            "type": "contains",
                            "value": "MODIFIER",
                            "score": 0,
                            "flag": "FILTERED",
                            "comment": [
                                "Usually non-coding variants or variants affecting non-coding genes, where predictions are difficult or there is no evidence of impact"
                            ],
                        },
                    ],
                }
            },
        }

        return config_default.get(name, None)

    def get_config_json(
        self, name: str, config_dict: dict = {}, config_file: str = None
    ) -> dict:
        """
        The function `get_config_json` retrieves a configuration JSON object with prioritizations from
        default values, a dictionary, and a file.

        :param name: The `name` parameter in the `get_config_json` function is a string that represents
        the name of the configuration. It is used to identify and retrieve the configuration settings
        for a specific component or module
        :type name: str
        :param config_dict: The `config_dict` parameter in the `get_config_json` function is a
        dictionary that allows you to provide additional configuration settings or overrides. When you
        call the `get_config_json` function, you can pass a dictionary containing key-value pairs where
        the key is the configuration setting you want to override or
        :type config_dict: dict
        :param config_file: The `config_file` parameter in the `get_config_json` function is used to
        specify the path to a configuration file that contains additional settings. If provided, the
        function will read the contents of this file and update the configuration dictionary with the
        values found in the file, overriding any existing values with the
        :type config_file: str
        :return: The function `get_config_json` returns a dictionary containing the configuration
        settings.
        """

        # Create with default prioritizations
        config_default = self.get_config_default(name=name)
        configuration = config_default
        # log.debug(f"configuration={configuration}")

        # Replace prioritizations from dict
        for config in config_dict:
            configuration[config] = config_dict[config]

        # Replace prioritizations from file
        config_file = full_path(config_file)
        if config_file:
            if os.path.exists(config_file):
                with open(config_file) as config_file_content:
                    config_file_dict = yaml.safe_load(config_file_content)
                for config in config_file_dict:
                    configuration[config] = config_file_dict[config]
            else:
                msg_error = f"Config '{name}' file '{config_file}' does NOT exist"
                log.error(msg_error)
                raise ValueError(msg_error)

        return configuration

    def prioritization(
        self,
        table: str = None,
        pz_prefix: str = None,
        pz_param: dict = None,
        pz_keys: list = None,
        strict: bool = False,
    ) -> bool:
        """
        Processes VCF files, adds new INFO fields, and prioritizes variants based on configured profiles and criteria.

        Args:
            table (str, optional): The name of the table (presumably a VCF file) on which the prioritization operation will be performed.
                If not provided, the default variants table will be used.
            pz_prefix (str, optional): A prefix to be added to certain INFO fields in the VCF file during the prioritization process.
                Defaults to "PZ" if not provided.
            pz_param (dict, optional): Additional parameters specific to the prioritization process. These parameters can include settings
                related to prioritization profiles, fields, scoring modes, flags, comments, and other configurations needed for the prioritization
                of variants.
            pz_keys (list, optional): The keys used to join the prioritization table with the variant table. Defaults to ["#CHROM", "POS", "REF", "ALT"]
                if not provided.
            strict (bool, optional): Whether to enforce strict prioritization criteria availability in view (need to be in header and in column). Defaults to False.

        Returns:
            bool: True if the prioritization operation is successful, False otherwise.
        """

        # Config
        config = self.get_config()

        # Param
        param = self.get_param()

        # Prioritization param
        if pz_param is not None:
            prioritization_param = pz_param
        else:
            prioritization_param = param.get("prioritization", {})

        # Configuration profiles
        prioritization_config_file = prioritization_param.get(
            "prioritization_config", None
        )
        prioritization_config_file = full_path(prioritization_config_file)
        prioritizations_config = self.get_config_json(
            name="prioritizations", config_file=prioritization_config_file
        )

        # Prioritization prefix
        pz_prefix_default = "PZ"
        if pz_prefix is None:
            pz_prefix = prioritization_param.get("pzprefix", pz_prefix_default)

        # Prioritization options
        profiles = prioritization_param.get("profiles", [])
        if isinstance(profiles, str):
            profiles = profiles.split(",")
        pzfields = prioritization_param.get(
            "pzfields", [f"{pz_prefix}Flag", f"{pz_prefix}Score"]
        )
        if isinstance(pzfields, str):
            pzfields = pzfields.split(",")
        default_profile = prioritization_param.get("default_profile", None)
        pzfields_sep = prioritization_param.get("pzfields_sep", "_")
        prioritization_score_mode = prioritization_param.get(
            "prioritization_score_mode", "HOWARD"
        )

        # Quick Prioritizations
        prioritizations = param.get("prioritizations", None)
        if prioritizations:
            log.info("Quick Prioritization:")
            for profile in prioritizations.split(","):
                if profile not in profiles:
                    profiles.append(profile)
                    log.info(f"   {profile}")

        # Keys for prioritization join
        if pz_keys is None:
            pz_keys = ["#CHROM", "POS", "REF", "ALT"]

        # If profile "ALL" provided, all profiles in the config profiles
        if "ALL" in profiles:
            profiles = list(prioritizations_config.keys())

        for profile in profiles:
            if prioritizations_config.get(profile, None):
                log.debug(f"Profile '{profile}' configured")
            else:
                msg_error = f"Profile '{profile}' NOT configured"
                log.error(msg_error)
                raise ValueError(msg_error)

        if profiles:
            log.info(f"Prioritization... ")
        else:
            log.debug(f"No profile defined")
            return False

        if not default_profile and len(profiles):
            default_profile = profiles[0]

        log.debug("Profiles availables: " + str(list(prioritizations_config.keys())))
        log.debug("Profiles to check: " + str(list(profiles)))

        # Variables
        if table is not None:
            table_variants = table
        else:
            table_variants = self.get_table_variants(clause="update")
        log.debug(f"Table to prioritize: {table_variants}")

        # Added columns
        added_columns = []

        # Create list of PZfields
        # List of PZFields
        list_of_pzfields_original = pzfields + [
            pzfield + pzfields_sep + profile
            for pzfield in pzfields
            for profile in profiles
        ]
        list_of_pzfields = []
        log.debug(f"{list_of_pzfields_original}")

        # Remove existing PZfields to use if exists
        for pzfield in list_of_pzfields_original:
            if self.get_header().infos.get(pzfield, None) is None:
                list_of_pzfields.append(pzfield)
                log.debug(f"VCF Input - Header - PZfield '{pzfield}' not in VCF")
            else:
                log.debug(f"VCF Input - Header - PZfield '{pzfield}' already in VCF")

        if list_of_pzfields:

            # PZfields tags description
            PZfields_INFOS = {
                f"{pz_prefix}Tags": {
                    "ID": f"{pz_prefix}Tags",
                    "Number": ".",
                    "Type": "String",
                    "Description": "Variant tags based on annotation criteria",
                },
                f"{pz_prefix}Score": {
                    "ID": f"{pz_prefix}Score",
                    "Number": 1,
                    "Type": "Integer",
                    "Description": "Variant score based on annotation criteria",
                },
                f"{pz_prefix}Flag": {
                    "ID": f"{pz_prefix}Flag",
                    "Number": 1,
                    "Type": "String",
                    "Description": "Variant flag based on annotation criteria",
                },
                f"{pz_prefix}Comment": {
                    "ID": f"{pz_prefix}Comment",
                    "Number": ".",
                    "Type": "String",
                    "Description": "Variant comment based on annotation criteria",
                },
                f"{pz_prefix}Infos": {
                    "ID": f"{pz_prefix}Infos",
                    "Number": ".",
                    "Type": "String",
                    "Description": "Variant infos based on annotation criteria",
                },
                f"{pz_prefix}Class": {
                    "ID": f"{pz_prefix}Class",
                    "Number": ".",
                    "Type": "String",
                    "Description": "Variant class based on annotation criteria",
                },
            }

            # Create INFO fields if not exist
            for field in PZfields_INFOS:
                field_ID = PZfields_INFOS[field]["ID"]
                field_description = PZfields_INFOS[field]["Description"]
                if field_ID not in self.get_header().infos and field_ID in pzfields:
                    field_description = (
                        PZfields_INFOS[field]["Description"]
                        + f", profile {default_profile}"
                    )
                    self.get_header().infos[field_ID] = vcf.parser._Info(
                        field_ID,
                        PZfields_INFOS[field]["Number"],
                        PZfields_INFOS[field]["Type"],
                        field_description,
                        "unknown",
                        "unknown",
                        code_type_map[PZfields_INFOS[field]["Type"]],
                    )

            # Create INFO fields if not exist for each profile
            for profile in prioritizations_config:
                if profile in profiles or profiles == []:
                    for field in PZfields_INFOS:
                        field_ID = PZfields_INFOS[field]["ID"] + pzfields_sep + profile
                        field_description = (
                            PZfields_INFOS[field]["Description"]
                            + f", profile {profile}"
                        )
                        if (
                            field_ID not in self.get_header().infos
                            and field in pzfields
                        ):
                            self.get_header().infos[field_ID] = vcf.parser._Info(
                                field_ID,
                                PZfields_INFOS[field]["Number"],
                                PZfields_INFOS[field]["Type"],
                                field_description,
                                "unknown",
                                "unknown",
                                code_type_map[PZfields_INFOS[field]["Type"]],
                            )

            # Header
            for pzfield in list_of_pzfields:
                if re.match(f"{pz_prefix}Score.*", pzfield):
                    added_column = self.add_column(
                        table_name=table_variants,
                        column_name=pzfield,
                        column_type="INTEGER",
                        default_value="0",
                    )
                elif re.match(f"{pz_prefix}Flag.*", pzfield):
                    added_column = self.add_column(
                        table_name=table_variants,
                        column_name=pzfield,
                        column_type="BOOLEAN",
                        default_value="1",
                    )
                elif re.match(f"{pz_prefix}Class.*", pzfield):
                    added_column = self.add_column(
                        table_name=table_variants,
                        column_name=pzfield,
                        column_type="VARCHAR[]",
                        default_value="null",
                    )
                else:
                    added_column = self.add_column(
                        table_name=table_variants,
                        column_name=pzfield,
                        column_type="STRING",
                        default_value="''",
                    )
                added_columns.append(added_column)

            # Profiles
            if profiles:

                # foreach profile in configuration file
                for profile in prioritizations_config:

                    # If profile is asked in param, or ALL are asked (empty profile [])
                    if profile in profiles or profiles == []:
                        log.info(f"Profile '{profile}'")

                        sql_set_info_option = ""

                        sql_set_info = []

                        # PZ fields set

                        # PZScore
                        if (
                            f"{pz_prefix}Score{pzfields_sep}{profile}"
                            in list_of_pzfields
                        ):
                            sql_set_info.append(
                                f"""
                                    concat(
                                        '{pz_prefix}Score{pzfields_sep}{profile}=',
                                        {pz_prefix}Score{pzfields_sep}{profile}
                                    ) 
                                """
                            )
                            if (
                                profile == default_profile
                                and f"{pz_prefix}Score" in list_of_pzfields
                            ):
                                sql_set_info.append(
                                    f"""
                                        concat(
                                            '{pz_prefix}Score=',
                                            {pz_prefix}Score{pzfields_sep}{profile}
                                        )
                                    """
                                )

                        # PZFlag
                        if (
                            f"{pz_prefix}Flag{pzfields_sep}{profile}"
                            in list_of_pzfields
                        ):
                            sql_set_info.append(
                                f"""
                                    concat(
                                        '{pz_prefix}Flag{pzfields_sep}{profile}=',
                                        CASE 
                                            WHEN {pz_prefix}Flag{pzfields_sep}{profile}==1
                                            THEN 'PASS'
                                            WHEN {pz_prefix}Flag{pzfields_sep}{profile}==0
                                            THEN 'FILTERED'
                                        END
                                    ) 
                                """
                            )
                            if (
                                profile == default_profile
                                and f"{pz_prefix}Flag" in list_of_pzfields
                            ):
                                sql_set_info.append(
                                    f"""
                                        concat(
                                            '{pz_prefix}Flag=',
                                            CASE 
                                                WHEN {pz_prefix}Flag{pzfields_sep}{profile}==1
                                                THEN 'PASS'
                                                WHEN {pz_prefix}Flag{pzfields_sep}{profile}==0
                                                THEN 'FILTERED'
                                            END
                                        )
                                    """
                                )

                        # PZClass
                        if (
                            f"{pz_prefix}Class{pzfields_sep}{profile}"
                            in list_of_pzfields
                        ):
                            sql_set_info.append(
                                f"""
                                    concat(
                                        '{pz_prefix}Class{pzfields_sep}{profile}=',
                                        CASE
                                            WHEN len({pz_prefix}Class{pzfields_sep}{profile}) > 0
                                            THEN list_aggregate(list_distinct({pz_prefix}Class{pzfields_sep}{profile}), 'string_agg', ',')
                                            ELSE '.'
                                        END 
                                    )
                                    
                                """
                            )
                            if (
                                profile == default_profile
                                and f"{pz_prefix}Class" in list_of_pzfields
                            ):
                                sql_set_info.append(
                                    f"""
                                        concat(
                                            '{pz_prefix}Class=',
                                            CASE
                                                WHEN len({pz_prefix}Class{pzfields_sep}{profile}) > 0
                                                THEN list_aggregate(list_distinct({pz_prefix}Class{pzfields_sep}{profile}), 'string_agg', ',')
                                                ELSE '.'
                                            END 
                                        )
                                    """
                                )

                        # PZComment
                        if (
                            f"{pz_prefix}Comment{pzfields_sep}{profile}"
                            in list_of_pzfields
                        ):
                            sql_set_info.append(
                                f"""
                                    CASE
                                        WHEN {pz_prefix}Comment{pzfields_sep}{profile} NOT IN ('')
                                        THEN concat('{pz_prefix}Comment{pzfields_sep}{profile}=', {pz_prefix}Comment{pzfields_sep}{profile})
                                        ELSE ''
                                    END
                                """
                            )
                            if (
                                profile == default_profile
                                and f"{pz_prefix}Comment" in list_of_pzfields
                            ):
                                sql_set_info.append(
                                    f"""
                                        CASE
                                            WHEN {pz_prefix}Comment{pzfields_sep}{profile} NOT IN ('')
                                            THEN concat('{pz_prefix}Comment=', {pz_prefix}Comment{pzfields_sep}{profile})
                                            ELSE ''
                                        END
                                    """
                                )

                        # PZInfos
                        if (
                            f"{pz_prefix}Infos{pzfields_sep}{profile}"
                            in list_of_pzfields
                        ):
                            sql_set_info.append(
                                f"""
                                    CASE
                                        WHEN {pz_prefix}Infos{pzfields_sep}{profile} NOT IN ('')
                                        THEN concat('{pz_prefix}Infos{pzfields_sep}{profile}=', {pz_prefix}Infos{pzfields_sep}{profile})
                                        ELSE ''
                                    END
                                """
                            )
                            if (
                                profile == default_profile
                                and f"{pz_prefix}Infos" in list_of_pzfields
                            ):
                                sql_set_info.append(
                                    f"""
                                        CASE
                                            WHEN {pz_prefix}Infos{pzfields_sep}{profile} NOT IN ('')
                                            THEN concat('{pz_prefix}Infos=', {pz_prefix}Infos{pzfields_sep}{profile})
                                            ELSE ''
                                        END
                                    """
                                )

                        # Merge PZfields
                        sql_set_info_option = ""
                        sql_set_sep = ""
                        for sql_set in sql_set_info:
                            if sql_set_sep:
                                sql_set_info_option += f"""
                                    , concat('{sql_set_sep}', {sql_set})
                                """
                            else:
                                sql_set_info_option += f"""
                                    , {sql_set}
                                """
                            sql_set_sep = ";"

                        sql_queries = []
                        criterion_fields_profile = []
                        annotation_view_name = (
                            "annotation_view_for_prioritization_"
                            + str(random.randrange(1000000))
                        )
                        annotations_view_prefix = ""
                        annotations_view_struct = "INFOS"
                        for annotation in prioritizations_config[profile]:

                            # skip special sections
                            if annotation.startswith("_"):
                                continue

                            # Log
                            log.info(f"Profile '{profile}' - Filter '{annotation}'")

                            # For each criterions
                            for criterion in prioritizations_config[profile][
                                annotation
                            ]:

                                # Criterion mode
                                criterion_mode = None
                                if np.any(
                                    np.isin(list(criterion.keys()), ["type", "value"])
                                ):
                                    criterion_mode = "operation"
                                elif np.any(
                                    np.isin(list(criterion.keys()), ["sql", "fields"])
                                ):
                                    criterion_mode = "sql"
                                log.debug(f"Criterion Mode: {criterion_mode}")

                                if criterion_mode in ["operation"]:
                                    log.warning(
                                        f"Prioritization criterion mode '{criterion_mode}' is deprecated. Please use 'sql' mode instead."
                                    )
                                    log.debug(f"Criterion: {criterion}")

                                # Criterion parameters
                                criterion_type = criterion.get("type", None)
                                criterion_value = criterion.get("value", None)
                                criterion_sql = criterion.get("sql", None)
                                criterion_fields = criterion.get("fields", None)
                                criterion_score = criterion.get("score", 0)
                                criterion_flag = criterion.get("flag", "PASS")
                                criterion_class = criterion.get("class", None)
                                criterion_flag_bool = criterion_flag == "PASS"
                                criterion_comment = (
                                    ", ".join(criterion.get("comment", []))
                                    .replace("'", "''")
                                    .replace(";", ",")
                                    .replace("\t", " ")
                                )
                                criterion_infos = (
                                    str(criterion)
                                    .replace("'", "''")
                                    .replace(";", ",")
                                    .replace("\t", " ")
                                )

                                # SQL
                                if criterion_sql is not None and isinstance(
                                    criterion_sql, list
                                ):
                                    criterion_sql = " ".join(criterion_sql)

                                # Fields and explode
                                if criterion_fields is None:
                                    criterion_fields = [annotation]
                                if not isinstance(criterion_fields, list):
                                    criterion_fields = str(criterion_fields).split(",")

                                # Class
                                if criterion_class is not None and not isinstance(
                                    criterion_class, list
                                ):
                                    criterion_class = str(criterion_class).split(",")

                                # Add criterion fields to the list of profile's criteria
                                criterion_fields_profile = list(
                                    set(criterion_fields_profile + criterion_fields)
                                )

                                # Create annotations view for prioritization
                                log.debug(
                                    f"""Profile '{profile}' - Prioritization - Create '{annotation_view_name}' view with '{criterion_fields_profile}'... """
                                )
                                annotation_view_name = self.create_annotations_view(
                                    view=annotation_view_name,
                                    table=table_variants,
                                    view_type="view",
                                    view_mode="explore",
                                    info_prefix_column=annotations_view_prefix,
                                    info_struct_column=annotations_view_struct,
                                    fields=criterion_fields_profile + pz_keys,
                                    fields_not_exists=(not strict),
                                    only_in_columns=strict,
                                    strict=strict,
                                    drop_view=True,
                                    detect_type_list=True,
                                )

                                # Describe annotation view and dict
                                annotation_view_describe = self.get_query_to_df(
                                    f"DESCRIBE {annotation_view_name}"
                                )
                                annotation_view_describe_dict = (
                                    annotation_view_describe.set_index("column_name")[
                                        "column_type"
                                    ].to_dict()
                                )

                                # Keys for join
                                clause_join = []
                                for key in pz_keys:
                                    if key in annotation_view_describe_dict:
                                        clause_join.append(
                                            f""" "{table_variants}"."{key}" == "{annotation_view_name}"."{key}" """
                                        )

                                sql_set = []
                                sql_set_info = []

                                # PZ fields set

                                # PZScore
                                if (
                                    f"{pz_prefix}Score{pzfields_sep}{profile}"
                                    in list_of_pzfields
                                ):
                                    # VaRank prioritization score mode
                                    if prioritization_score_mode.upper().strip() in [
                                        "VARANK",
                                        "MAX",
                                        "MAXIMUM",
                                        "TOP",
                                    ]:
                                        sql_set.append(
                                            f"{pz_prefix}Score{pzfields_sep}{profile} = CASE WHEN {criterion_score}>{pz_prefix}Score{pzfields_sep}{profile} THEN {criterion_score} ELSE {pz_prefix}Score{pzfields_sep}{profile} END "
                                        )
                                    # default HOWARD prioritization score mode
                                    else:
                                        sql_set.append(
                                            f"{pz_prefix}Score{pzfields_sep}{profile} = {pz_prefix}Score{pzfields_sep}{profile} + {criterion_score}"
                                        )

                                # PZFlag
                                if (
                                    f"{pz_prefix}Flag{pzfields_sep}{profile}"
                                    in list_of_pzfields
                                ):
                                    sql_set.append(
                                        f"{pz_prefix}Flag{pzfields_sep}{profile} = {pz_prefix}Flag{pzfields_sep}{profile} AND {criterion_flag_bool}"
                                    )

                                # PZClass
                                if (
                                    f"{pz_prefix}Class{pzfields_sep}{profile}"
                                    in list_of_pzfields
                                    and criterion_class is not None
                                ):
                                    sql_set.append(
                                        f" {pz_prefix}Class{pzfields_sep}{profile} = list_concat(list_distinct({pz_prefix}Class{pzfields_sep}{profile}), {criterion_class}) "
                                    )

                                # PZComment
                                if (
                                    f"{pz_prefix}Comment{pzfields_sep}{profile}"
                                    in list_of_pzfields
                                ):
                                    sql_set.append(
                                        f"""
                                            {pz_prefix}Comment{pzfields_sep}{profile} = 
                                                concat(
                                                    {pz_prefix}Comment{pzfields_sep}{profile},
                                                    CASE 
                                                        WHEN {pz_prefix}Comment{pzfields_sep}{profile}!=''
                                                        THEN ', '
                                                        ELSE ''
                                                    END,
                                                    '{criterion_comment}'
                                                )
                                        """
                                    )

                                # PZInfos
                                if (
                                    f"{pz_prefix}Infos{pzfields_sep}{profile}"
                                    in list_of_pzfields
                                ):
                                    sql_set.append(
                                        f"""
                                            {pz_prefix}Infos{pzfields_sep}{profile} = 
                                                concat(
                                                    {pz_prefix}Infos{pzfields_sep}{profile},
                                                    '{criterion_infos}'
                                                )
                                        """
                                    )
                                sql_set_option = ",".join(sql_set)

                                # Criterion and comparison
                                if sql_set_option:

                                    # Operation mode
                                    if criterion_mode in ["operation"]:

                                        # Check if value is a float
                                        try:

                                            # Test if criterion is a float
                                            float(criterion_value)

                                            # Query test cast as float
                                            query_test_cast = f"""
                                                SELECT "{annotation_view_name}"."{annotations_view_prefix}{annotation}"
                                                    FROM "{annotation_view_name}"
                                                    WHERE CAST("{annotation_view_name}"."{annotations_view_prefix}{annotation}" AS FLOAT) > 0
                                                LIMIT 1
                                            """
                                            self.execute_query(query_test_cast)

                                            sql_update = f"""
                                                UPDATE "{table_variants}"
                                                SET {sql_set_option}
                                                FROM (
                                                    SELECT *
                                                    FROM "{annotation_view_name}"
                                                    WHERE (
                                                        CAST("{annotation_view_name}"."{annotations_view_prefix}{annotation}" AS VARCHAR) NOT IN ('','.')
                                                        AND   CAST("{annotation_view_name}"."{annotations_view_prefix}{annotation}" AS FLOAT){comparison_map[criterion_type]}{criterion_value}
                                                        )
                                                    ) AS "{annotation_view_name}"
                                                WHERE ({" AND ".join(clause_join)})
                                                
                                            """
                                        # If not a float
                                        except:
                                            contains_option = ""
                                            if criterion_type == "contains":
                                                contains_option = ".*"
                                            sql_update = f"""
                                                UPDATE "{table_variants}"
                                                SET {sql_set_option}
                                                FROM (
                                                    SELECT *
                                                    FROM "{annotation_view_name}"
                                                    WHERE (
                                                    CAST("{annotation_view_name}"."{annotations_view_prefix}{annotation}" AS STRING) SIMILAR TO '{contains_option}{criterion_value}{contains_option}'
                                                        )
                                                    ) AS "{annotation_view_name}"
                                                WHERE ({" AND ".join(clause_join)})
                                                  
                                            """
                                        sql_queries.append(sql_update)

                                    # SQL mode
                                    elif criterion_mode in ["sql"]:

                                        sql_update = f"""
                                            UPDATE {table_variants}
                                            SET {sql_set_option}
                                            FROM (
                                                SELECT *
                                                FROM "{annotation_view_name}"
                                                WHERE ({criterion_sql})
                                                ) AS "{annotation_view_name}"
                                            WHERE ({" AND ".join(clause_join)})
                                        """
                                        sql_queries.append(sql_update)

                                    else:
                                        msg_err = f"Prioritization criterion mode failed (either 'operation' or 'sql')"
                                        log.error(msg_err)
                                        raise ValueError(msg_err)

                                else:
                                    log.warning(
                                        f"NO SQL SET option for '{annotation}' - '{criterion}'"
                                    )

                        # PZTags
                        if (
                            f"{pz_prefix}Tags{pzfields_sep}{profile}"
                            in list_of_pzfields
                        ):

                            # Create PZFalgs value
                            pztags_value = ""
                            pztags_sep_default = ","
                            pztags_sep = ""
                            for pzfield in pzfields:
                                if pzfield not in [f"{pz_prefix}Tags"]:
                                    if (
                                        f"{pzfield}{pzfields_sep}{profile}"
                                        in list_of_pzfields
                                    ):
                                        if pzfield in [f"{pz_prefix}Flag"]:
                                            pztags_value += f"""{pztags_sep}{pzfield}#', 
                                                CASE WHEN {pz_prefix}Flag{pzfields_sep}{profile}
                                                    THEN 'PASS'
                                                    ELSE 'FILTERED'
                                                END, '"""
                                        elif pzfield in [f"{pz_prefix}Class"]:
                                            pztags_value += f"""{pztags_sep}{pzfield}#', 
                                                CASE WHEN len({pz_prefix}Class{pzfields_sep}{profile}) > 0
                                                    THEN list_aggregate(list_distinct({pz_prefix}Class{pzfields_sep}{profile}), 'string_agg', ',')
                                                    ELSE '.'
                                                END, '"""
                                        else:
                                            pztags_value += f"{pztags_sep}{pzfield}#', {pzfield}{pzfields_sep}{profile}, '"
                                        pztags_sep = pztags_sep_default

                            # Add Query update for PZFlags
                            sql_update_pztags = f"""
                                UPDATE {table_variants}
                                SET INFO = concat(
                                        INFO,
                                        CASE WHEN INFO NOT in ('','.')
                                                THEN ';'
                                                ELSE ''
                                        END,
                                        '{pz_prefix}Tags{pzfields_sep}{profile}={pztags_value}'
                                    )
                                WHERE 1=1
                                """
                            sql_queries.append(sql_update_pztags)

                            # Add Query update for PZFlags for default
                            if profile == default_profile:
                                sql_update_pztags_default = f"""
                                UPDATE {table_variants}
                                SET INFO = concat(
                                        INFO,
                                        ';',
                                        '{pz_prefix}Tags={pztags_value}'
                                    )
                                    WHERE 1=1
                                """
                                sql_queries.append(sql_update_pztags_default)

                        log.info(f"""Profile '{profile}' - Prioritization... """)

                        # Chromosomes list
                        sql_uniq_chrom = f"""
                            SELECT DISTINCT "#CHROM"
                            FROM {table_variants}
                        """
                        chroms = self.get_query_to_df(sql_uniq_chrom)["#CHROM"].tolist()

                        for chrom in chroms:

                            log.debug(
                                f"""Profile '{profile}' - Prioritization query - Chromosome '{chrom}'... """
                            )

                            if sql_queries:

                                # Query num
                                num_query = 0

                                # For each query
                                for sql_query in sql_queries:

                                    # Query num
                                    num_query += 1

                                    sql_query_chrom = f"""
                                        {sql_query}
                                        AND {table_variants}."#CHROM" LIKE '{chrom}' 
                                    """
                                    log.debug(
                                        f"""Profile '{profile}' - Prioritization query - Chromosome '{chrom}' [{num_query}/{len(sql_queries)}]"""
                                    )
                                    # log.debug(
                                    #     f"""sql_query_chrom:\n{sql_query_chrom}"""
                                    # )
                                    self.execute_query(query=sql_query_chrom)

                        # Update INFO field
                        log.info(f"""Profile '{profile}' - Update... """)
                        sql_query_update = f"""
                            UPDATE {table_variants}
                            SET INFO =  
                                concat(
                                    CASE
                                        WHEN INFO NOT IN ('','.')
                                        THEN concat(INFO, ';')
                                        ELSE ''
                                    END
                                    {sql_set_info_option}
                                )
                        """
                        # log.debug(f"sql_query_update={sql_query_update}")
                        self.execute_query(query=sql_query_update)

                        # Remove annotations view for prioritization
                        self.remove_tables_or_views(tables=[annotation_view_name])

        else:

            log.warning(f"No profiles in parameters")

        # Remove added columns
        for added_column in added_columns:
            self.drop_column(column=added_column)

        return True

    ###
    # HGVS
    ###

    def annotation_hgvs(self, threads: int = None) -> None:
        """
        The `annotation_hgvs` function performs HGVS annotation on a set of variants using genomic
        coordinates and alleles.

        :param threads: The `threads` parameter is an optional integer that specifies the number of
        threads to use for parallel processing. If no value is provided, it will default to the number
        of threads obtained from the `get_threads()` method
        :type threads: int
        """

        import dask.dataframe as dd  # type: ignore

        # Function for each partition of the Dask Dataframe
        def partition_function(partition):
            """
            The function `partition_function` applies the `annotation_hgvs_partition` function to
            each row of a DataFrame called `partition`.

            :param partition: The parameter "partition" is a pandas DataFrame that contains the data
            to be processed
            :return: the result of applying the "annotation_hgvs_partition" function to each row of
            the "partition" dataframe along the axis 1.
            """
            return partition.apply(annotation_hgvs_partition, axis=1)

        def annotation_hgvs_partition(row) -> str:
            """
            The function `annotation_hgvs_partition` takes in a row of data and returns a string
            containing a list of HGVS names associated with the given genomic coordinates and alleles.

            :param row: A dictionary-like object that contains the values for the following keys:
            :return: a string that contains the HGVS names associated with the given row of data.
            """

            chr = row["CHROM"]
            pos = row["POS"]
            ref = row["REF"]
            alt = row["ALT"]

            # Find list of associated transcripts
            transcripts_list = list(
                polars_conn.execute(
                    f"""
                SELECT transcript
                FROM refseq_df
                WHERE CHROM='{chr}'
                AND POS={pos}
            """
                )["transcript"]
            )

            # Full HGVS annotation in list
            hgvs_full_list = []

            for transcript_name in transcripts_list:

                # Transcript
                transcript = get_transcript(
                    transcripts=transcripts, transcript_name=transcript_name
                )
                # Exon
                if use_exon:
                    exon = transcript.find_exon_number(pos)
                else:
                    exon = None
                # Protein
                transcript_protein = None
                if use_protein or add_protein or full_format:
                    transcripts_protein = list(
                        polars_conn.execute(
                            f"""
                        SELECT protein
                        FROM refseqlink_df
                        WHERE transcript='{transcript_name}'
                        LIMIT 1
                    """
                        )["protein"]
                    )
                    if len(transcripts_protein):
                        transcript_protein = transcripts_protein[0]

                # HGVS name
                hgvs_name = format_hgvs_name(
                    chr,
                    pos,
                    ref,
                    alt,
                    genome=genome,
                    transcript=transcript,
                    transcript_protein=transcript_protein,
                    exon=exon,
                    use_gene=use_gene,
                    use_protein=use_protein,
                    full_format=full_format,
                    use_version=use_version,
                    codon_type=codon_type,
                )
                hgvs_full_list.append(hgvs_name)
                if add_protein and not use_protein and not full_format:
                    hgvs_name = format_hgvs_name(
                        chr,
                        pos,
                        ref,
                        alt,
                        genome=genome,
                        transcript=transcript,
                        transcript_protein=transcript_protein,
                        exon=exon,
                        use_gene=use_gene,
                        use_protein=True,
                        full_format=False,
                        use_version=use_version,
                        codon_type=codon_type,
                    )
                    hgvs_full_list.append(hgvs_name)

            # Create liste of HGVS annotations
            hgvs_full = ",".join(hgvs_full_list)

            return hgvs_full

        # Polars connexion
        polars_conn = pl.SQLContext(register_globals=True, eager=True)

        # Config
        config = self.get_config()

        # Databases
        # Genome
        databases_genomes_folders = (
            config.get("folders", {})
            .get("databases", {})
            .get("genomes", DEFAULT_GENOME_FOLDER)
        )
        databases_genome = (
            config.get("folders", {}).get("databases", {}).get("genomes", "")
        )
        # refseq database folder
        databases_refseq_folders = (
            config.get("folders", {})
            .get("databases", {})
            .get("refseq", DEFAULT_REFSEQ_FOLDER)
        )
        # refseq
        databases_refseq = config.get("databases", {}).get("refSeq", None)
        # refSeqLink
        databases_refseqlink = config.get("databases", {}).get("refSeqLink", None)

        # Param
        param = self.get_param()

        # Quick HGVS
        if "hgvs_options" in param and param.get("hgvs_options", ""):
            log.info(f"Quick HGVS Annotation:")
            if not param.get("hgvs", None):
                param["hgvs"] = {}
            for option in param.get("hgvs_options", "").split(","):
                option_var_val = option.split("=")
                option_var = option_var_val[0]
                if len(option_var_val) > 1:
                    option_val = option_var_val[1]
                else:
                    option_val = "True"
                if option_val.upper() in ["TRUE"]:
                    option_val = True
                elif option_val.upper() in ["FALSE"]:
                    option_val = False
                log.info(f"   {option_var}={option_val}")
                param["hgvs"][option_var] = option_val

        # Check if HGVS annotation enabled
        if "hgvs" in param:
            log.info(f"HGVS Annotation... ")
            for hgvs_option in param.get("hgvs", {}):
                log.info(f"{hgvs_option}: {param.get('hgvs',{}).get(hgvs_option)}")
        else:
            return

        # HGVS Param
        param_hgvs = param.get("hgvs", {})
        use_exon = param_hgvs.get("use_exon", False)
        use_gene = param_hgvs.get("use_gene", False)
        use_protein = param_hgvs.get("use_protein", False)
        add_protein = param_hgvs.get("add_protein", False)
        full_format = param_hgvs.get("full_format", False)
        use_version = param_hgvs.get("use_version", False)
        codon_type = param_hgvs.get("codon_type", "3")

        # refSseq refSeqLink
        databases_refseq = param_hgvs.get("refseq", databases_refseq)
        databases_refseqlink = param_hgvs.get("refseqlink", databases_refseqlink)

        # Assembly
        assembly = param.get("assembly", config.get("assembly", DEFAULT_ASSEMBLY))

        # Genome
        genome_file = None
        if find_genome(databases_genome):
            genome_file = find_genome(databases_genome)
        else:
            genome_file = find_genome(
                genome_path=databases_genomes_folders, assembly=assembly
            )
        log.debug("Genome: " + str(genome_file))

        # refSseq
        refseq_file = find_file_prefix(
            input_file=databases_refseq,
            prefix="ncbiRefSeq",
            folder=databases_refseq_folders,
            assembly=assembly,
        )
        log.debug("refSeq: " + str(refseq_file))

        # refSeqLink
        refseqlink_file = find_file_prefix(
            input_file=databases_refseqlink,
            prefix="ncbiRefSeqLink",
            folder=databases_refseq_folders,
            assembly=assembly,
        )
        log.debug("refSeqLink: " + str(refseqlink_file))

        # Threads
        if not threads:
            threads = self.get_threads()
        log.debug("Threads: " + str(threads))

        # Variables
        table_variants = self.get_table_variants(clause="update")

        # Get variants SNV and InDel only
        query_variants = f"""
            SELECT "#CHROM" AS CHROM, POS, REF, ALT
            FROM {table_variants}
            WHERE REF ~ '^[A-Za-z]+$' AND ALT ~ '^[A-Za-z]+$'
            """
        df_variants = self.get_query_to_df(query_variants)

        if len(df_variants) == 0:
            log.debug("No variants found for HGVS annotation")
            return

        # Added columns
        added_columns = []

        # Add hgvs column in variants table
        hgvs_column_name = "hgvs_" + str(random.randrange(1000000))
        added_column = self.add_column(
            table_variants, hgvs_column_name, "STRING", default_value=None
        )
        added_columns.append(added_column)

        log.debug(f"refSeq loading...")
        # refSeq in duckDB
        refseq_table = get_refseq_table(
            conn=self.conn, refseq_table="refseq", refseq_file=refseq_file
        )
        # Loading all refSeq in Dataframe
        refseq_query = f"""
            SELECT df_variants.CHROM, df_variants.POS, {refseq_table}.name AS transcript
            FROM {refseq_table}
            JOIN df_variants ON (
                {refseq_table}.chrom = df_variants.CHROM
                AND {refseq_table}.txStart<=df_variants.POS
                AND {refseq_table}.txEnd>=df_variants.POS
            )
        """
        refseq_df = self.conn.query(refseq_query).pl()

        if refseqlink_file:
            log.debug(f"refSeqLink loading...")
            # refSeqLink in duckDB
            refseqlink_table = get_refseq_table(
                conn=self.conn, refseq_table="refseqlink", refseq_file=refseqlink_file
            )
            # Loading all refSeqLink in Dataframe
            protacc_column = "protAcc_with_ver"
            mrnaacc_column = "mrnaAcc_with_ver"
            refseqlink_query = f"""
                SELECT {refseq_table}.chrom, {protacc_column} AS protein, {mrnaacc_column} AS transcript
                FROM {refseqlink_table} 
                JOIN {refseq_table} ON ({refseq_table}.name = {refseqlink_table}.mrnaAcc_with_ver)
                WHERE protAcc_without_ver IS NOT NULL
            """
            # Polars Dataframe
            refseqlink_df = self.conn.query(f"{refseqlink_query}").pl()

        # Read RefSeq transcripts into a python dict/model.
        log.debug(f"Transcripts loading...")
        with tempfile.TemporaryDirectory() as tmpdir:
            transcripts_query = f"""
                COPY (
                    SELECT {refseq_table}.*
                    FROM {refseq_table}
                    JOIN df_variants ON (
                        {refseq_table}.chrom=df_variants.CHROM
                        AND {refseq_table}.txStart<=df_variants.POS
                        AND {refseq_table}.txEnd>=df_variants.POS
                    )
                )
                TO '{tmpdir}/transcript.tsv' (DELIMITER '\t');
            """
            self.conn.query(transcripts_query)
            with open(f"{tmpdir}/transcript.tsv") as infile:
                transcripts = read_transcripts(infile)

        # Polars connexion
        polars_conn = pl.SQLContext(register_globals=True, eager=True)

        log.debug("Genome loading...")
        # Read genome sequence using pyfaidx.
        genome = Fasta(genome_file)

        log.debug("Start annotation HGVS...")

        # Create
        # a Dask Dataframe from Pandas dataframe with partition as number of threads
        ddf = dd.from_pandas(df_variants, npartitions=threads)

        # Use dask.dataframe.apply() to apply function on each partition
        ddf[hgvs_column_name] = ddf.map_partitions(partition_function)

        # Convert Dask DataFrame to Pandas Dataframe
        df = ddf.compute()

        # Convert Pandas dataframe to parquet (due to error in cast VARCHAR -> NULL ???)
        with tempfile.TemporaryDirectory() as tmpdir:
            df_parquet = os.path.join(tmpdir, "df.parquet")
            df.to_parquet(df_parquet)

            # Update hgvs column
            update_variant_query = f"""
                UPDATE {table_variants}
                SET "{hgvs_column_name}"=df."{hgvs_column_name}"
                FROM read_parquet('{df_parquet}') as df
                WHERE variants."#CHROM" = df.CHROM
                AND variants.POS = df.POS
                AND variants.REF = df.REF
                AND variants.ALT = df.ALT
                AND df."{hgvs_column_name}" NOT IN ('') AND df."{hgvs_column_name}" NOT NULL
                """
            self.execute_query(update_variant_query)

        # Update INFO column
        sql_query_update = f"""
            UPDATE {table_variants}
            SET INFO = 
                concat(
                    CASE 
                        WHEN INFO NOT IN ('','.')
                        THEN concat(INFO, ';')
                        ELSE ''
                    END,
                    'hgvs=',
                    {hgvs_column_name}
                )
            WHERE "{hgvs_column_name}" NOT IN ('') AND "{hgvs_column_name}" NOT NULL
            """
        self.execute_query(sql_query_update)

        # Add header
        HGVS_INFOS = {
            "hgvs": {
                "ID": "hgvs",
                "Number": ".",
                "Type": "String",
                "Description": f"HGVS annotatation with HOWARD",
            }
        }

        for field in HGVS_INFOS:
            field_ID = HGVS_INFOS[field]["ID"]
            field_description = HGVS_INFOS[field]["Description"]
            self.get_header().infos[field_ID] = vcf.parser._Info(
                field_ID,
                HGVS_INFOS[field]["Number"],
                HGVS_INFOS[field]["Type"],
                field_description,
                "unknown",
                "unknown",
                code_type_map[HGVS_INFOS[field]["Type"]],
            )

        # Remove added columns
        for added_column in added_columns:
            self.drop_column(column=added_column)

    ###
    # Calculation
    ###

    def get_operations_help(
        self, operations_config_dict: dict = {}, operations_config_file: str = None
    ) -> list:

        # Init
        operations_help = []

        # operations
        operations = self.get_config_json(
            name="calculations",
            config_dict=operations_config_dict,
            config_file=operations_config_file,
        )
        for op in operations:
            op_name = operations[op].get("name", op).upper()
            op_description = operations[op].get("description", op_name)
            op_available = operations[op].get("available", False)
            if op_available:
                operations_help.append(f"   {op_name}: {op_description}")

        # Sort operations
        operations_help.sort()

        # insert header
        operations_help.insert(0, "Available calculation operations:")

        # Return
        return operations_help

    def calculation(
        self,
        operations: dict = {},
        operations_config_dict: dict = {},
        operations_config_file: str = None,
    ) -> None:
        """
        It takes a list of operations, and for each operation, it checks if it's a python or sql
        operation, and then calls the appropriate function

        param json example:
            "calculation": {
                "NOMEN": {
                    "options": {
                        "hgvs_field": "hgvs"
                    },
                "middle" : null
            }
        """

        # Param
        param = self.get_param()

        # CHeck operations config file
        if operations_config_file is None:
            operations_config_file = param.get("calculation", {}).get(
                "calculation_config", None
            )

        # operations config
        operations_config = self.get_config_json(
            name="calculations",
            config_dict=operations_config_dict,
            config_file=operations_config_file,
        )

        # Upper keys
        operations_config = {k.upper(): v for k, v in operations_config.items()}

        # Calculations

        # Operations from param
        operations = param.get("calculation", {}).get("calculations", operations)

        # Quick calculation - add
        if param.get("calculations", None):

            # List of operations
            calculations_list = [
                value.strip() for value in param.get("calculations", "").split(",")
            ]

            # Log
            log.info(f"Quick Calculations:")
            for calculation_key in calculations_list:
                log.info(f"   {calculation_key}")

            # Create tmp operations (to keep operation order)
            operations_tmp = {}
            for calculation_operation in calculations_list:
                if calculation_operation.upper() not in operations_tmp:
                    # log.debug(
                    #     f"{calculation_operation}.upper() not in {operations_tmp}"
                    # )
                    operations_tmp[calculation_operation.upper()] = {}
                    add_value_into_dict(
                        dict_tree=operations_tmp,
                        sections=[
                            calculation_operation.upper(),
                        ],
                        value=operations.get(calculation_operation.upper(), {}),
                    )
            # Add operations already in param
            for calculation_operation in operations:
                if calculation_operation not in operations_tmp:
                    operations_tmp[calculation_operation] = operations.get(
                        calculation_operation, {}
                    )

            # Update operations in param
            operations = operations_tmp

        # Operations for calculation
        if not operations:
            operations = param.get("calculation", {}).get("calculations", {})

        if operations:
            log.info(f"Calculations...")

        # Count number of variants
        nb_variants = self.get_query_to_df(
            f"SELECT count(1) AS count FROM (SELECT 1 FROM {self.get_table_variants()} LIMIT 1)"
        )["count"].tolist()[0]

        # Init
        # To store operation params for each dest table for update merge
        operation_params = {}
        perform_update_all_calculations = False  # Disabled due to operation dependances

        # For each operations
        for operation_name in operations:

            operation_name = operation_name.upper()

            if operation_name not in [""]:

                if operation_name in operations_config:

                    # Log
                    log.info(f"Calculation '{operation_name}'...")

                    # Get operation config
                    operation = operations_config[operation_name]
                    operation_type = operation.get("type", "sql")
                    operation_allow_empty = operation.get("allow_empty", False)

                    if operation_allow_empty or nb_variants > 0:

                        # Python process
                        if operation_type == "python":
                            self.calculation_process_function(
                                operation=operation, operation_name=operation_name
                            )

                        # SQL process
                        elif operation_type == "sql":

                            # Retrive parrams for operation
                            operation_dest_table, operation_param = (
                                self.calculation_process_sql(
                                    operation=operation, operation_name=operation_name
                                )
                            )

                            if perform_update_all_calculations:

                                # Create list of params for each dest table
                                if operation_dest_table not in operation_params:
                                    operation_params[operation_dest_table] = []
                                # Append param
                                operation_params[operation_dest_table].append(
                                    operation_param.copy()
                                )

                            else:

                                # Perform update for all calculations
                                log.debug(
                                    f"Process calculations for '{operation_name}'..."
                                )
                                self.update_table(
                                    dest_table=operation_dest_table,
                                    sources=[operation_param],
                                    physical_order=True,
                                    # chunk_size=10000000,
                                    force_strategy=None,
                                    chromosomes=None,
                                )

                                # Clean temp operation view
                                self.remove_tables_or_views(
                                    tables=[operation_param.get("table")]
                                )

                        # Fail process
                        else:
                            log.error(
                                f"Operations config: Type '{operation_type}' NOT available"
                            )
                            raise ValueError(
                                f"Operations config: Type '{operation_type}' NOT available"
                            )

                    else:
                        log.info(
                            f"Calculation '{operation_name}': aborded - no variants"
                        )

                else:
                    log.error(
                        f"Operations config: Calculation '{operation_name}' NOT available"
                    )
                    raise ValueError(
                        f"Operations config: Calculation '{operation_name}' NOT available"
                    )

        # Perform update for all calculations
        if perform_update_all_calculations:
            log.info("Process calculations...")
            for operation_table_dest in operation_params:
                self.update_table(
                    dest_table=operation_table_dest,
                    sources=operation_params[operation_table_dest],
                    physical_order=True,
                    # chunk_size=10000000,
                    force_strategy=None,
                )

    def calculation_process_sql(
        self, operation: dict, operation_name: str = "unknown"
    ) -> tuple:
        """
        The `calculation_process_sql` function takes in a mathematical operation as a string and
        performs the operation, updating the specified table with the result.

        :param operation: The `operation` parameter is a dictionary that contains information about the
        mathematical operation to be performed. It includes the following keys:
        :type operation: dict
        :param operation_name: The `operation_name` parameter is a string that represents the name of
        the mathematical operation being performed. It is used for logging and error handling purposes,
        defaults to unknown
        :type operation_name: str (optional)
        """

        # Operation infos
        operation_name = operation.get("name", "unknown")
        log.debug(f"process SQL {operation_name}")
        output_column_name = operation.get("output_column_name", operation_name)
        output_column_number = operation.get("output_column_number", ".")
        output_column_type = operation.get("output_column_type", "String")
        output_column_description = operation.get(
            "output_column_description", f"{operation_name} operation"
        )
        operation_query = operation.get("operation_query", None)
        if isinstance(operation_query, list):
            operation_query = " ".join(operation_query)
        operation_info_fields = operation.get("info_fields", [])
        operation_info_fields_check = operation.get("info_fields_check", False)
        operation_table = operation.get(
            "table", self.get_table_variants(clause="alter")
        )
        operation_table_source = operation.get("table_source", operation_table)
        operation_table_dest = operation.get("table_dest", operation_table)
        operation_table_key = operation.get(
            "table_key", ["#CHROM", "POS", "REF", "ALT"]
        )

        if operation_query:

            # Info fields check
            operation_info_fields_check_result = True
            if operation_info_fields_check:
                header_infos = self.get_header().infos
                for info_field in operation_info_fields:
                    operation_info_fields_check_result = (
                        operation_info_fields_check_result
                        and info_field in header_infos
                    )

            # If info fields available
            if operation_info_fields_check_result:

                # Create VCF header field
                vcf_reader = self.get_header()
                vcf_reader.infos[output_column_name] = vcf.parser._Info(
                    output_column_name,
                    output_column_number,
                    output_column_type,
                    output_column_description,
                    "howard calculation",
                    "0",
                    self.code_type_map.get(output_column_type),
                )

                # Create view
                table_view_name = "calculation_view_" + str(random.randrange(1000000))
                table_view_name = self.create_annotations_view(
                    table=operation_table_source,
                    view=table_view_name,
                    view_type="view",
                    view_mode="explore",
                    fields=operation_info_fields + ["INFO"],
                    fields_needed=operation_table_key,
                    info_prefix_column="",
                    info_struct_column="INFOS",
                    drop_view=True,
                )

                # Table key construct
                clause_key = []
                for key in operation_table_key:
                    clause_key.append(
                        f""" {operation_table_dest}."{key}" = table_view."{key}" """
                    )

                # Create table with calculation
                calculation_view_name = "calculation_view_" + str(
                    random.randrange(1000000)
                )

                try:
                    query_create_view = f"""
                        CREATE TABLE {calculation_view_name} AS
                        SELECT * FROM (
                            SELECT {", ".join([f'"{k}"' for k in operation_table_key])},
                                CASE
                                    WHEN TRY_CAST(({operation_query}) AS VARCHAR) IS NOT NULL
                                    THEN
                                        concat(
                                                '{output_column_name}=',
                                                TRY_CAST(({operation_query}) AS VARCHAR)
                                            )
                                    ELSE NULL 
                                    END AS INFO
                            FROM {table_view_name}
                            )
                        WHERE INFO IS NOT NULL
                    """
                    # log.debug(f"query_create_view={query_create_view}")
                    log.debug("Create calculation view...")
                    self.get_connexion().execute(query_create_view)

                    # Clean temp annotation view
                    self.remove_tables_or_views(tables=[table_view_name])

                except Exception as e:
                    log.error(f"Error creating calculation view: {e}")
                    msg_err = f"Operations config: Calculation '{operation_name}' query failed"
                    log.error(msg_err)
                    raise ValueError(msg_err)

                # update table
                return operation_table_dest, {
                    "table": calculation_view_name,
                    "join_keys": operation_table_key,
                    "columns": {
                        "INFO": {
                            "columns": ["INFO"],
                            "mode": "append",
                            "separator": ";",
                        }
                    },
                }

            else:
                msg_err = f"Operations config: Calculation '{operation_name}' DOES NOT contain all mandatory fields {operation_info_fields}"
                log.error(msg_err)
                raise ValueError(msg_err)

        else:
            msg_err = (
                f"Operations config: Calculation '{operation_name}' query NOT defined"
            )
            log.error(msg_err)
            raise ValueError(msg_err)

    def get_columns_type(self, table: str) -> dict:
        """
        Get columns type of a table.
        :param table: The `table` parameter is a string that represents the name of the table
        for which you want to retrieve the column types.
        :type table: str
        :return: The function `get_columns_type` returns a dictionary where the keys are the
        column names of the specified table and the values are the corresponding data types of
        those columns.
        """

        # Get columns info
        query = f"""
            PRAGMA table_info('{table}')
        """
        df_columns = self.get_query_to_df(query)

        # Construct columns type dict
        columns_type = {}
        for _, row in df_columns.iterrows():
            columns_type[row["name"]] = row["type"]

        return columns_type

    def update_table_strategy(
        self,
        dest_table: str,
        sources: list,
        mode: str = "append",
        separator: str = ";",
        physical_order: bool = True,
        cleanup: bool = False,
        strategy: str = "ctas",
        chunk_size: int = None,
        upper_case: bool = False,
        chromosomes: list = None,
    ) -> None:
        """
        Update dest_table using multiple sources via CTAS.

        :param dest_table: The `dest_table` parameter is a string that represents the name of the
        destination table that you want to update.
        :type dest_table: str
        :param sources: The `sources` parameter is a list of dictionaries, where each dictionary
        represents a source table and the columns to be updated in the destination table. Each
        dictionary should have the following keys:
            - "table": The name of the source table.
            - "join_keys": A list of column names that will be used to join the source table with the
              destination table.
            - "columns": A dictionary where the keys are the destination column names and the values
              are dictionaries with the following keys:
                - "columns": A list of column names from the source table that will be used to
                  update the destination column.
                - "mode": The mode of updating the column, either "append" or "replace".
                - "separator": The separator to use when appending values (only applicable for "append"
                  mode).
        sources example:
            sources = [
                {   # Default source to append INFO column with a table source as a view
                    "table": "calculation_view_name",
                    "join_keys": ["#CHROM", "POS", "REF", "ALT"],
                    "columns": {
                        "INFO": {"columns": ["INFO"], "mode": "append", "separator": ";"}
                    },
                },
                {   # Calculation view source to update INFO, and create AF and ALT columns
                    "table": "calculation_view_name",
                    "join_keys": ["#CHROM", "POS", "REF", "ALT"],
                    "columns": {
                        "INFO": {"columns": ["INFO"], "mode": "append", "separator": ";"},
                        "AF": {"columns": ["REF"], "mode": "replace"},
                        "ALT": {"columns": ["REF"], "mode": "replace"},
                    },
                },
                {   # Clinvar source to add in a new column CLINVAR with concatenation of 2 columns CLNSIG and CLNID
                    "table": "clinvar_table",
                    "join_keys": ["#CHROM", "POS", "REF", "ALT"],
                    "columns": {
                        "CLINVAR": {"columns": ["CLNSIG", "CLNID"], "mode": "append", "separator": "|"},
                    },
                },
            ]
        :type sources: list
        :param mode: The `mode` parameter determines how the columns in the destination table will
        be updated. It can take two values: "append" or "replace". The default value is "append".
            - If `mode` is set to "append", the values from the source tables will be concatenated to the existing
              values in the destination table, separated by the specified `separator`.
            - If `mode` is set to "replace", the values from the source tables will replace the existing values in the
              destination table.
        :type mode: str
        :param separator: The `separator` parameter is a string that specifies the character or
        sequence of characters used to separate values when appending them together. The default value is
        a semicolon (";"). This parameter is only applicable when the `mode` parameter is set to "append".
        :type separator: str
        :param physical_order: The `physical_order` parameter is a boolean value that determines
        whether the resulting table should have a physical order based on the row number. If set to
        `True`, a `_rowid` column will be added to the resulting table, which will contain the row number for each row. If
        set to `False`, the `_rowid` column will not be included. The default value is `True`.
        :type physical_order: bool
        :param cleanup: The `cleanup` parameter is a boolean value that determines whether to drop
        the temporary table created during the update process. If set to `True`, the temporary table
        will be dropped after the update is complete. If set to `False`, the temporary table will be
        retained. The default value is `False`.
        :type cleanup: bool
        :param strategy: The `strategy` parameter determines the method used to update the
        destination table. It can take two values: "ctas" or "merge". The default value is "ctas".
            - If `strategy` is set to "ctas", the update will be performed using a "Create Table As Select" (CTAS) approach. This
              involves creating a new temporary table with the updated data and then replacing the original table with the
              temporary table.
            - If `strategy` is set to "update", the update will be performed using an "UPDATE" statement. This involves directly modifying the existing
              rows in the destination table based on the data from the source tables.
        :type strategy: str
        :param chunk_size: The `chunk_size` parameter is an optional integer that specifies the
        size of the chunks to be processed during the update operation. If not provided, it will
        default to a value from the configuration file or a predefined constant `DEFAULT_CHUNK_SIZE`.
        :type chunk_size: int or None
        :param upper_case: The `upper_case` parameter is a boolean value that determines whether to
        convert the values of the join keys to uppercase before performing the join operation. If set
        to `True`, the join keys will be converted to uppercase using the `upper` function. If set to
        `False`, the join keys will be used as they are. The default value is `False`.
        :type upper_case: bool

        :return: The function `update_table` does not return anything. It performs an update
        operation on a destination table using multiple source tables and their specified columns.

        """

        conn = self.get_connexion()

        # Chunk size
        if chunk_size is None:
            chunk_size = self.get_config().get("chunk_size", DEFAULT_CHUNK_SIZE)

        # chunking desactivated because of some specific tables (e.g., operations on multiple lines) ???
        # chunking = False
        # Due to creation of entire table before update, chunking should be ok
        chunking = True

        # Upper case function for ALT and REF
        if upper_case:
            upper_func = "upper"
        else:
            upper_func = ""

        # Default configuration
        # source columns
        default_source_columns = {"INFO": {"columns": ["INFO"]}}
        # key columns
        default_join_keys = ["#CHROM", "POS", "REF", "ALT"]

        # --- Existing dest columns ---
        dest_cols = self.get_columns(dest_table)

        # --- Compute required output columns ---
        required_dest_cols = list(dest_cols)
        for src in sources:
            for dest_col in src.get("columns", default_source_columns):
                if dest_col not in required_dest_cols:
                    required_dest_cols.append(dest_col)

        column_exprs = []
        join_clauses = []
        where_clauses = {}
        where_column_exprs = []
        where_column_set = []
        list_of_sources_table = {}

        # Build JOINs
        for src in sources:

            join_keys = src.get("join_keys", default_join_keys)

            # Find type of each columns in join keys in des_table table
            join_keys_type = {}
            dest_table_columns_type = self.get_columns_type(dest_table)
            for join_key in join_keys:
                join_keys_type[join_key] = dest_table_columns_type.get(
                    join_key, "VARCHAR"
                )

            # Table source
            src_table = src.get("table", None)

            if src_table:

                # List of souce table
                list_of_sources_table[src_table] = True

                # Join clause
                join_clause = f"""
                    LEFT JOIN {src.get("table")}
                    ON {' AND '.join([
                        f'd."{k}" = {src_table}."{k}"' if join_keys_type.get(k, "").upper() not in ["VARCHAR", "TEXT"]
                        else f'{upper_func}(d."{k}") = {upper_func}({src_table}."{k}")'
                        for k in join_keys
                        ])}
                """
                # join_clause = f"""
                #     LEFT JOIN {src.get("table")}
                #     USING ({', '.join([
                #         f'"{k}"'
                #         for k in join_keys
                #         ])})
                # """

                # -- CTAS strategy --

                join_clauses.append(join_clause)

                # -- Update strategy --

                # where clause
                where_clause = (
                    f""" {' AND '.join([f'd."{k}" = n."{k}"' for k in join_keys])} """
                )

                # Store join clauses and keys for each where clause

                # if where clause not exist, create it
                if where_clause not in where_clauses:
                    where_clauses[where_clause] = {
                        "join_clause": [],
                        "join_keys": [],
                    }

                # Append join clause and keys
                where_clauses[where_clause]["join_clause"].append(join_clause)
                where_clauses[where_clause]["join_keys"] += join_keys

        # Rowid for deterministic physical ordering and chunking
        rowid_expr = ", row_number() OVER () AS _rowid"

        # Helper to normalize empty values
        def normalize(val):
            return f"NULLIF(NULLIF({val}, ''), '.')"

        # Build column expressions
        for col in required_dest_cols:

            # --- Sources contributing to this column ---
            update_sources = [
                src
                for src in sources
                if col in src.get("columns", default_source_columns)
            ]

            # --- No update source ---
            if not update_sources:
                if col in dest_cols:
                    column_exprs.append(f'd."{col}"')
                else:
                    column_exprs.append(f'NULL AS "{col}"')
                continue

            # --- Update column ---
            source_values = [
                (
                    normalize(f'{src.get("table")}."{col_name}"')
                    if src.get("table", None)
                    else None
                )
                for src in update_sources
                for col_name in src.get("columns", default_source_columns)[col][
                    "columns"
                ]
            ]

            # --- Remove None values (when no table defined) ---
            source_values = [v for v in source_values if v is not None]

            # --- Determine mode for this column ---
            columns_mode = {
                col: src.get("columns", {}).get(col, {}).get("mode", mode)
                for src in update_sources
            }

            # --- Determine separator for this column ---
            columns_separator = {
                col: src.get("columns", {}).get(col, {}).get("separator", separator)
                for src in update_sources
            }

            # ---------------------------
            # MODE REPLACE
            # ---------------------------

            if columns_mode[col] == "replace":

                if col in dest_cols:
                    replace_candidates = source_values + [f'd."{col}"']
                else:
                    replace_candidates = source_values

                column_exprs.append(
                    f"COALESCE({', '.join(replace_candidates)}) AS \"{col}\""
                )

                continue

            # ---------------------------
            # MODE APPEND (concat + clean)
            # ---------------------------

            pieces = []

            # existing column first (if exists)
            if col in dest_cols:
                pieces.append(normalize(f'd."{col}"'))

            # then all source values
            pieces.extend(source_values)

            # raw concat
            raw_concat = f"""
                concat_ws('{columns_separator[col]}',
                    {", ".join(pieces)}
                )
            """

            # cleanup of semicolons
            if cleanup:
                clean = f"""
                    TRIM(
                        REGEXP_REPLACE(
                            {raw_concat},
                            '{columns_separator[col]}{{2,}}',
                            '{columns_separator[col]}'
                        ),
                        '{columns_separator[col]}'
                    ) AS "{col}"
                """
            else:
                clean = f"""
                            {raw_concat} AS "{col}"
                """

            column_exprs.append(clean)

            where_column_exprs.append(clean)
            where_column_set.append(col)

        # ---------------------------
        # UPDATE
        # ---------------------------

        if strategy == "update":

            # For each where clause, create temp table and update dest_table
            for where_clause in where_clauses.keys():

                # Build join clause
                where_clause_join_clauses = where_clauses.get(where_clause, {}).get(
                    "join_clause", ""
                )

                # Build join keys
                where_clause_join_keys = [
                    f'd."{k}"'
                    for k in set(
                        where_clauses.get(where_clause, {}).get("join_keys", "")
                    )
                ]

                # Create temp table with required columns
                new_dest_table = f"tmp_new_{dest_table}_{random.randrange(1000000)}"
                # sql = f"""
                #     CREATE TABLE {new_dest_table} AS
                #     WITH d AS (
                #         SELECT *
                #             {rowid_expr} -- for update chunking and physical_order
                #         FROM {dest_table}
                #     )
                #     SELECT
                #         {", ".join(where_clause_join_keys)},
                #         {", ".join(where_column_exprs)}
                #         , _rowid -- keep rowid for update with chunking
                #     FROM d
                #     {" ".join(where_clause_join_clauses)}
                #     -- {"ORDER BY d._rowid" if physical_order else ""} -- only if physical order
                # """

                # sql = f"""
                #     CREATE TABLE  {new_dest_table} AS
                #         WITH joined AS (
                #             SELECT
                #                 {", ".join(where_clause_join_keys)},
                #                 {", ".join(where_column_exprs)},
                #                 row_number() OVER () AS _rowid
                #             FROM variants d
                #             {" ".join(where_clause_join_clauses)}
                #         )
                #         SELECT *
                #         FROM joined;
                # """

                sql = f"""
                    CREATE VIEW {new_dest_table} AS
                        (
                            SELECT
                                {", ".join(where_clause_join_keys)},
                                {", ".join(where_column_exprs)},
                                row_number() OVER () AS _rowid
                            FROM variants d
                            {" ".join(where_clause_join_clauses)}
                        );
                """

                # new_dest_table = f"""
                #     (
                #         SELECT
                #             {", ".join(where_clause_join_keys)},
                #             {", ".join(where_column_exprs)},
                #             row_number() OVER () AS _rowid
                #         FROM variants d
                #         {" ".join(where_clause_join_clauses)}
                #     )
                # """

                log.debug(f"SQL: {sql}")

                # Execute Update Creation View
                log.debug("Execute Update Creation View for update_table...")
                conn.execute(sql)

                # # DEVEL - show content and count of new_dest_table for debug
                # log.debug(f"Show content of {new_dest_table} for debug...")
                # df_debug = conn.execute(f"SELECT * FROM {new_dest_table} LIMIT 5").df()
                # log.debug(f"df_debug:\n{df_debug}")
                # count_debug = conn.execute(
                #     f"SELECT COUNT(*) AS count FROM {new_dest_table}"
                # ).df()
                # log.debug(f"count_debug:\n{count_debug}")

                # Update dest_table with new_dest_table
                log.debug("Update dest_table with new_dest_table...")

                if chunking:

                    # Update table {dest_table} with new table
                    # split update by chunk (chunk_size) on _rowid column to avoid transaction too large
                    # max_rowid = conn.execute(
                    #     f"SELECT MAX(_rowid) AS max_rowid FROM {new_dest_table}"
                    # ).df()["max_rowid"][0]
                    max_rowid = conn.execute(
                        f"SELECT count(1) AS max_rowid FROM {dest_table}"
                    ).df()["max_rowid"][0]
                    # Handle NaN case or None
                    if max_rowid is None or math.isnan(max_rowid):
                        max_rowid = 0

                    # Process chunks
                    if max_rowid >= 0:
                        for chunk_start in range(1, int(max_rowid) + 1, chunk_size):
                            chunk_end = min(
                                chunk_start + chunk_size - 1, int(max_rowid)
                            )
                            log.debug(
                                f"  Updating rows with _rowid between {chunk_start} and {chunk_end}..."
                            )

                            if chromosomes is None:
                                update_query = f"""
                                    UPDATE {dest_table} AS d
                                    SET
                                        {", ".join([f'"{col}" = n."{col}"' for col in where_column_set])}
                                        FROM {new_dest_table} AS n
                                    WHERE {" ".join(set(where_clauses.keys()))}
                                    -- AND n."#CHROM" = 'chr1'
                                    AND n._rowid BETWEEN {chunk_start} AND {chunk_end}
                                """
                                log.debug(f"update_query:\n{update_query}")
                                conn.execute(update_query)

                            else:

                                if len(chromosomes) == 0:
                                    # List of chromosomes in dest_table with query
                                    query_chromosomes = f"""
                                        SELECT DISTINCT "#CHROM" AS chrom FROM {dest_table}
                                        ORDER BY TRY_CAST(regexp_extract("#CHROM", '(\d+)') AS INTEGER) NULLS LAST, "#CHROM"
                                    """
                                    df_chromosomes = conn.execute(
                                        query_chromosomes
                                    ).df()
                                    chromosomes = df_chromosomes["chrom"].tolist()

                                for chrom in chromosomes:
                                    # chrom_min = chromosomes[chrom].get("min", None)
                                    # chrom_max = chromosomes[chrom].get("max", None)
                                    log.debug(f"    Chromosome: {chrom}")

                                    update_query = f"""
                                        UPDATE {dest_table} AS d
                                        SET
                                            {", ".join([f'"{col}" = n."{col}"' for col in where_column_set])}
                                            FROM {new_dest_table} AS n
                                        WHERE {" ".join(set(where_clauses.keys()))}
                                        AND n."#CHROM" = '{chrom}'
                                        AND n._rowid BETWEEN {chunk_start} AND {chunk_end}
                                    """
                                    log.debug(f"update_query:\n{update_query}")
                                    conn.execute(update_query)

                else:

                    # Update table {dest_table} with new table
                    update_query = f"""
                        UPDATE {dest_table} AS d
                        SET
                            {", ".join([f'"{col}" = n."{col}"' for col in where_column_set])}
                            FROM {new_dest_table} AS n
                            WHERE {" ".join(set(where_clauses.keys()))}
                        """
                    conn.execute(update_query)

                # Cleanup
                log.debug("Cleanup temporary view...")
                self.remove_tables_or_views(tables=[new_dest_table])

        # ---------------------------
        # CTAS
        # ---------------------------

        elif strategy == "ctas":

            # with TemporaryDirectory() as tmp_dir:

            #     tmp_dir = self.get_tmp_dir()

            #     # Create parquet file for variant table with required columns
            #     variant_parquet = os.path.join(
            #         tmp_dir,
            #         f"tmp_variants_parquet_{dest_table}_{get_random(10)}.parquet",
            #     )
            #     sql = f"""
            #         COPY (
            #         SELECT * {rowid_expr}
            #         FROM {dest_table}
            #         )
            #         TO '{variant_parquet}'
            #     """
            #     log.debug(f"CTAS SQL for variant parquet:\n{sql}")
            #     conn.execute(sql)

            #     # Create parquet file
            #     parquet_name = os.path.join(
            #         tmp_dir, f"tmp_parquet_{dest_table}_{get_random(10)}.parquet"
            #     )
            #     sql = f"""
            #         COPY (
            #         SELECT
            #             {", ".join(column_exprs)}
            #         FROM read_parquet('{variant_parquet}') AS d
            #         {" ".join(join_clauses)}
            #         {"ORDER BY d._rowid" if physical_order else ""}
            #         )
            #         TO '{parquet_name}'
            #     """
            #     log.debug(f"CTAS SQL for update_table:\n{sql}")

            #     # Execute CTAS
            #     log.debug(f"Create Parquet '{parquet_name}' for update_table...")
            #     conn.execute(sql)

            #     # Load parquet into new dest table
            #     log.debug("Load Parquet into new dest table for update_table...")
            #     conn.execute(
            #         f"CREATE OR REPLACE TABLE {dest_table} AS SELECT * FROM read_parquet('{parquet_name}')"
            #     )

            # with TemporaryDirectory() as tmp_dir:

            #     tmp_dir = self.get_tmp_dir()

            #     # # Create parquet file for variant table with required columns
            #     # variant_parquet = os.path.join(
            #     #     tmp_dir,
            #     #     f"tmp_variants_parquet_{dest_table}_{get_random(10)}.parquet",
            #     # )
            #     # sql = f"""
            #     #     COPY (
            #     #     SELECT * {rowid_expr}
            #     #     FROM {dest_table}
            #     #     )
            #     #     TO '{variant_parquet}'
            #     # """
            #     # log.debug(f"CTAS SQL for variant parquet:\n{sql}")
            #     # conn.execute(sql)

            #     # Create parquet file
            #     parquet_name = os.path.join(
            #         tmp_dir, f"tmp_parquet_{dest_table}_{get_random(10)}.parquet"
            #     )
            #     sql = f"""
            #         COPY (
            #         WITH d AS (
            #             SELECT * {rowid_expr}
            #             FROM {dest_table}
            #         )
            #         SELECT
            #             {", ".join(column_exprs)}
            #         FROM d
            #         {" ".join(join_clauses)}
            #         {"ORDER BY d._rowid" if physical_order else ""}
            #         )
            #         TO '{parquet_name}'
            #     """
            #     log.debug(f"CTAS SQL for update_table:\n{sql}")

            #     # Execute CTAS
            #     log.debug(f"Create Parquet '{parquet_name}' for update_table...")
            #     conn.execute(sql)

            #     # Load parquet into new dest table
            #     log.debug("Load Parquet into new dest table for update_table...")
            #     conn.execute(
            #         f"CREATE OR REPLACE TABLE {dest_table} AS SELECT * FROM read_parquet('{parquet_name}')"
            #     )

            if chunking:
                # Create new dest table with required columns OK
                new_dest_table = f"tmp_new_{dest_table}_{get_random(10)}"
                new_dest_table_union = f"tmp_new_{dest_table}_{get_random(10)}"

                schema_sql = f"""
                    CREATE TABLE {new_dest_table} AS
                    SELECT * FROM {dest_table} WHERE 1=0
                """
                log.debug(f"Schema SQL for update_table:\n{schema_sql}")
                conn.execute(schema_sql)

                max_rowid = conn.execute(
                    f"SELECT count(1) AS max_rowid FROM {dest_table}"
                ).df()["max_rowid"][0]
                # Handle NaN case or None
                if max_rowid is None or math.isnan(max_rowid):
                    max_rowid = 0

                # SQL template
                sql = """
                    INSERT INTO {new_dest_table}
                    WITH d AS (
                        SELECT * {rowid_expr}
                        FROM {dest_table}
                        {dest_where_chrom}
                        QUALIFY _rowid BETWEEN {start} AND {end}
                    ),
                    joined AS (
                        SELECT
                            {join_colunls_exprs}, _rowid
                        FROM d
                        {join_clauses}
                        {join_where_chrom}
                    ),
                    dedup AS (
                        SELECT *
                        FROM joined
                        QUALIFY row_number() OVER (
                            PARTITION BY _rowid
                            ORDER BY "INFO" DESC NULLS LAST
                        ) = 1
                    )
                    SELECT
                        {dest_cols}
                    FROM dedup
                    
                    {order_by}
                """

                # DEVEL UNION
                sql_union = """
                    SELECT *
                    FROM (
                        WITH d AS (
                            SELECT * {rowid_expr}
                            FROM {dest_table}
                            {dest_where_chrom}
                            QUALIFY _rowid BETWEEN {start} AND {end}
                        ),
                        joined AS (
                            SELECT
                                {join_colunls_exprs}, _rowid
                            FROM d
                            {join_clauses}
                            {join_where_chrom}
                        ),
                        dedup AS (
                            SELECT *
                            FROM joined
                            QUALIFY row_number() OVER (
                                PARTITION BY _rowid
                                ORDER BY "INFO" DESC NULLS LAST
                            ) = 1
                        )
                        SELECT
                            {dest_cols}, _rowid
                        FROM dedup
                    )
                    -- {order_by}
                """

                sql_union_query_list = []

                for start in range(1, max_rowid + 1, chunk_size):
                    end = start + chunk_size - 1

                    log.debug("Execute CTAS for update_table...")

                    if chromosomes is None:
                        sql_query = sql.format(
                            new_dest_table=new_dest_table,
                            rowid_expr=rowid_expr,
                            dest_table=dest_table,
                            start=start,
                            end=end,
                            join_colunls_exprs=", ".join(column_exprs),
                            join_clauses=" ".join(join_clauses),
                            dest_cols=", ".join(f'"{col}"' for col in dest_cols),
                            order_by="ORDER BY _rowid" if physical_order else "",
                            dest_where_chrom="",
                            join_where_chrom="",
                        )
                        log.debug(f"CTAS SQL for update_table:\n{sql_query}")

                        # Execute CTAS
                        # conn.execute(sql_query)

                        # DEVEL UNION
                        sql_union_query = sql_union.format(
                            new_dest_table=new_dest_table,
                            rowid_expr=rowid_expr,
                            dest_table=new_dest_table_union,
                            start=start,
                            end=end,
                            join_colunls_exprs=", ".join(column_exprs),
                            join_clauses=" ".join(join_clauses),
                            dest_cols=", ".join(f'"{col}"' for col in dest_cols),
                            order_by="",
                            dest_where_chrom="",
                            join_where_chrom="",
                        )
                        # log.debug(
                        #     f"CTAS SQL for update_table UNION:\n{sql_union_query}"
                        # )
                        sql_union_query_list.append(sql_union_query)

                    else:

                        if len(chromosomes) == 0:
                            # List of chromosomes in dest_table with query
                            query_chromosomes = f"""
                                SELECT DISTINCT "#CHROM" AS chrom FROM {dest_table}
                                ORDER BY TRY_CAST(regexp_extract("#CHROM", '(\d+)') AS INTEGER) NULLS LAST, "#CHROM"
                            """
                            df_chromosomes = conn.execute(query_chromosomes).df()
                            chromosomes = df_chromosomes["chrom"].tolist()

                        for chrom in chromosomes:
                            sql_query = sql.format(
                                new_dest_table=new_dest_table,
                                rowid_expr=rowid_expr,
                                dest_table=dest_table,
                                start=start,
                                end=end,
                                join_colunls_exprs=", ".join(column_exprs),
                                join_clauses=" ".join(join_clauses),
                                dest_cols=", ".join(f'"{col}"' for col in dest_cols),
                                order_by="ORDER BY _rowid" if physical_order else "",
                                dest_where_chrom=f"""WHERE "#CHROM" LIKE '{chrom}'""",
                                join_where_chrom=f"""WHERE d."#CHROM" LIKE '{chrom}'""",
                            )

                            log.debug(f"CTAS SQL for update_table:\n{sql_query}")

                            # Execute CTAS
                            log.debug(
                                f"Execute CTAS for update_table - chromosome {chrom}..."
                            )
                            conn.execute(sql_query)

                # DEVEL UNION
                sql_union_query_i = 0
                sql_union_final = ""
                for sql_union_query in sql_union_query_list:
                    if sql_union_query_i > 0:
                        sql_union_final += """
                            UNION ALL
                        """
                    sql_union_final += f"""
                        {sql_union_query}
                    """
                    sql_union_query_i += 1
                sql_union_final = f"""
                    CREATE OR REPLACE VIEW {dest_table} AS
                    WITH all_chunks AS (
                        {sql_union_final}
                    )
                    SELECT
                        {", ".join(f'"{col}"' for col in dest_cols)}
                    FROM all_chunks
                    {"ORDER BY _rowid" if physical_order else ""}
                """
                conn.execute(
                    f"ALTER TABLE {dest_table} RENAME TO {new_dest_table_union}"
                )
                log.debug(f"CTAS SQL for update_table UNION ALL:\n{sql_union_final}")
                # Execute CTAS UNION ALL into new_dest_table
                df = conn.execute(
                    f"""
                    
                    {sql_union_final}
                """
                ).df()
                log.debug(f"df:\n{df.head()}")

                # self.export_output(
                #     query=sql_union_final,
                #     output_file="/tmp/test.ctas.union.vcf",
                # )

                # Replace dest_table with new_dest_table
                # conn.execute(f"DROP TABLE {dest_table}")
                # conn.execute(f"ALTER TABLE {new_dest_table} RENAME TO {dest_table}")

            else:

                # Create new dest table with required columns OK
                new_dest_table = f"tmp_new_{dest_table}_{get_random(10)}"
                sql = f"""
                    CREATE TABLE {new_dest_table} AS
                    WITH d AS (
                        SELECT * {rowid_expr}
                        FROM {dest_table}
                    )
                    SELECT
                        {", ".join(column_exprs)}
                    FROM d
                    {" ".join(join_clauses)}
                    {"ORDER BY d._rowid" if physical_order else ""}
                """
                log.debug(f"CTAS SQL for update_table:\n{sql}")

                # Execute CTAS
                log.debug("Execute CTAS for update_table...")
                conn.execute(sql)

                # Replace dest_table with new_dest_table
                conn.execute(f"DROP TABLE {dest_table}")
                conn.execute(f"ALTER TABLE {new_dest_table} RENAME TO {dest_table}")

        else:
            log.error(f"Strategy '{strategy}' NOT available")
            raise ValueError(f"Strategy '{strategy}' NOT available")

        # Remove source tables if cleanup
        log.debug("Cleanup source tables/views...")
        # self.remove_tables_or_views(tables=list(list_of_sources_table.keys()))

        # Optimize table
        self.optimize_table(dest_table)

    def update_table(
        self,
        dest_table: str,
        sources: list,
        mode: str = "append",
        separator: str = ";",
        physical_order: bool = True,
        cleanup: bool = False,
        force_strategy: str = None,
        chunk_size: int = None,
        upper_case: bool = False,
        samples: int = 100000,
        chromosomes: list = None,
        only_strategy: bool = False,
    ) -> None:
        """
        Update dest_table using multiple sources via CTAS or hybrid UPDATE.
        Heuristic chooses between UPDATE and CTAS based on number of columns to update.

        :param dest_table: destination table to update
        :type dest_table: str
        :param sources: list of source dicts
            sources example:
            sources = [
                {   # Default source to append INFO column with a table source as a view
                    "table": "calculation_view_name",
                    "join_keys": ["#CHROM", "POS", "REF", "ALT"],
                    "columns": {
                        "INFO": {"columns": ["INFO"], "mode": "append", "separator": ";"}
                    },
                },
                {   # Calculation view source to update INFO, and create AF and ALT columns
                    "table": "calculation_view_name",
                    "join_keys": ["#CHROM", "POS", "REF", "ALT"],
                    "columns": {
                        "INFO": {"columns": ["INFO"], "mode": "append", "separator": ";"},
                        "AF": {"columns": ["REF"], "mode": "replace"},
                        "ALT": {"columns": ["REF"], "mode": "replace"},
                    },
                },
                {   # Clinvar source to add in a new column CLINVAR with concatenation of 2 columns CLNSIG and CLNID
                    "table": "clinvar_table",
                    "join_keys": ["#CHROM", "POS", "REF", "ALT"],
                    "columns": {
                        "CLINVAR": {"columns": ["CLNSIG", "CLNID"], "mode": "append", "separator": "|"},
                    },
                },
            ]
        :type sources: list
        :param mode: "append" or "replace"
        :type mode: str
        :param mode: mode of update ("append" or "replace")
        :type mode: str
        :param separator: separator for concatenation
        :type separator: str
        :param physical_order: keep physical order of dest_table
        :type physical_order: bool
        :param cleanup: clean concatenated values (remove duplicates, start/end separators)
        :type cleanup: bool
        :param force_use_ctas: force use of CTAS (True), UPDATE (False) or heuristic (None)
        :type force_use_ctas: bool or None
        :param ctas_threshold: threshold to use CTAS based on number of columns to update / total columns ratio
        :type ctas_threshold: float
        :param chunk_size: threshold to use CTAS based on dest table size (number of rows)
        :type chunk_size: int

        :return: None
        :rtype: None
        """

        conn = self.get_connexion()

        # Chunk size
        if chunk_size is None:
            chunk_size = self.get_config().get("chunk_size", DEFAULT_CHUNK_SIZE)

        # Default configuration
        default_source_columns = {"INFO": {"columns": ["INFO"]}}
        default_join_keys = ["#CHROM", "POS", "REF", "ALT"]

        # --- Existing dest columns and update columns ---
        update_cols = set()
        for src in sources:
            update_cols.update(src.get("columns", default_source_columns).keys())

        # --- Available memory and threads ---
        memory = int(self.get_memory("1G", available=True).replace("G", ""))
        threads = self.get_threads("1")

        # Heuristics to choose between CTAS or UPDATE
        log.debug(f"force_strategy={force_strategy}")
        if force_strategy is None:

            # --- Choose safe strategy -----------
            strategy, reasoning = choose_update_strategy_safe(
                dest_total_rows=None,
                dest_total_cols=None,
                avg_row_size_bytes=None,
                update_cols_count=len(update_cols),
                update_row_ratio=None,  # unknown yet
                ram_available_gb=memory,
                chunk_size=chunk_size,
                conn=conn,
                dest_table=dest_table,
                sources=sources,
                default_join_keys=default_join_keys,
                samples=samples,
                threads=threads,
            )

        else:
            strategy = force_strategy
            reasoning = [f"Strategy forced to '{strategy}'"]

        # Log chosen strategy
        log.debug(f"Chosen strategy: {strategy}")
        for reason in reasoning:
            log.debug(reason)

        # Return strategy if only_strategy
        if only_strategy:
            log.info(f"Chosen strategy: {strategy}")
            return strategy

        # Perform update with chosen strategy
        self.update_table_strategy(
            dest_table=dest_table,
            sources=sources,
            mode=mode,
            separator=separator,
            physical_order=physical_order,
            cleanup=cleanup,
            strategy=strategy.lower(),
            chunk_size=chunk_size,
            upper_case=upper_case,
            chromosomes=chromosomes,
        )

        return None

    def calculation_process_function(
        self, operation: dict, operation_name: str = "unknown"
    ) -> None:
        """
        The `calculation_process_function` takes in an operation dictionary and performs the specified
        function with the given parameters.

        :param operation: The `operation` parameter is a dictionary that contains information about the
        operation to be performed. It has the following keys:
        :type operation: dict
        :param operation_name: The `operation_name` parameter is a string that represents the name of
        the operation being performed. It is used for logging purposes, defaults to unknown
        :type operation_name: str (optional)
        """

        operation_name = operation["name"]
        log.debug(f"process Python {operation_name}")
        function_name = operation["function_name"]
        function_params = operation["function_params"]
        getattr(self, function_name)(*function_params)

    def calculation_variant_id(self) -> None:
        """
        The function `calculation_variant_id` adds a variant ID annotation to a VCF file header and
        updates the INFO field of a variants table with the variant ID.
        """

        # variant_id annotation field
        variant_id_tag = self.get_variant_id_column()
        added_columns = [variant_id_tag]

        # variant_id hgvs tags"
        vcf_infos_tags = {
            variant_id_tag: "howard variant ID annotation",
        }

        # Variants table
        table_variants = self.get_table_variants()

        # Header
        vcf_reader = self.get_header()

        # Add variant_id to header
        vcf_reader.infos[variant_id_tag] = vcf.parser._Info(
            variant_id_tag,
            ".",
            "String",
            vcf_infos_tags.get(variant_id_tag, "howard variant ID annotation"),
            "howard calculation",
            "0",
            self.code_type_map.get("String"),
        )

        # Update
        sql_update = f"""
            UPDATE {table_variants}
            SET "INFO" = 
                concat(
                    CASE
                        WHEN "INFO" IS NULL OR "INFO" IN ('','.')
                        THEN ''
                        ELSE concat("INFO", ';')
                    END,
                    '{variant_id_tag}=',
                    "{variant_id_tag}"
                )
        """
        self.conn.execute(sql_update)

        # Remove added columns
        for added_column in added_columns:
            self.drop_column(column=added_column)

    def calculation_extract_snpeff(
        self,
        snpeff_field: str = "ANN",
        snpeff_hgvs: str = "snpeff_hgvs",
        snpeff_explode: bool = "snpeff_",
        snpeff_json: bool = "snpeff_json",
        uniquify: bool = True,
    ) -> None:
        """
        This function extracts SnpEff annotations from the specified field in the VCF file and processes them according to the provided parameters. The annotations can be exploded into separate rows, converted into JSON format, and/or ensured to be unique. The processed annotations are then added to the VCF file with the specified prefixes.

        Args:
            snpeff_field (str): The annotation field in the VCF file to extract SnpEff annotations from. Default is "ANN".
            snpeff_hgvs (str): The prefix for the HGVS annotations extracted from SnpEff. Default is "snpeff_hgvs".
            snpeff_explode (bool): Whether to explode the annotations into separate rows. Default is "snpeff_".
            snpeff_json (bool): Whether to convert the annotations into JSON format. Default is "snpeff_json".
            uniquify (bool): Whether to ensure unique annotations. Default is True.

        Returns:
            None

        """

        # Variants table
        table_variants = self.get_table_variants()

        # Header
        vcf_reader = self.get_header()

        # Log
        log.info(f"Extract snpEff annotations")

        # If snpeff_field exists
        if snpeff_field in vcf_reader.infos:

            # Log
            log.info(f"Extract snpEff annotations - from INFO/Tag '{snpeff_field}'")

            # Create view
            view_name = "snpeff_hgvs_" + str(random.randint(1000, 9999))
            view_infos = self.annotation_format_to_table(
                annotation_field=snpeff_field,
                annotation_id="Feature_ID",
                view_name=view_name,
                column_rename={},
                column_clean=False,
                column_case=None,
            )
            view_name = view_infos[0]

            # Describe
            sql_describe = f"""
                SELECT *
                FROM (
                    DESCRIBE {view_name}
                )
                WHERE column_name NOT IN ('#CHROM', 'POS', 'REF', 'ALT', 'INFO')
            """
            sql_describe_result = self.get_query_to_df(sql_describe)

            # Create dict of snpEff annotations
            annotation_dict = {}
            for _, annotation in sql_describe_result.iterrows():

                # Process values for dict
                annotation_name = annotation.column_name
                annotation_clean = clean_annotation_field(name=annotation_name)
                annotation_type = annotation.column_type
                annotation_type_vcf = code_type_map_to_vcf.get(
                    annotation_type, "String"
                )
                annotation_column = f'"{annotation_name}"'
                annotation_number = 1
                if annotation_type_vcf in ["Flag"]:
                    annotation_number = 0
                elif annotation_name in ["Annotation"]:
                    annotation_number = "."
                    annotation_column = (
                        f"""replace(CAST("{annotation_name}" AS VARCHAR), '&', ',')"""
                    )
                elif annotation_name in ["Distance"]:
                    annotation_column = f"""string_split(CAST("{annotation_name}" AS VARCHAR), '.')[1]"""

                    annotation_number = 1
                annotation_desc = f"snpEff annotation '{annotation_name}'"

                # Create dict
                annotation_dict[annotation_name] = {
                    "name": annotation_name,
                    "id": annotation_clean,
                    "number": annotation_number,
                    "type": annotation_type_vcf,
                    "desc": annotation_desc,
                    "column": annotation_column,
                }

            # update clauses
            sql_clauses = []

            # Prepare sql update
            if snpeff_json is not None:

                # Log
                log.info(
                    f"Extract snpEff annotations - into INFO/tag '{snpeff_json}' in JSON format"
                )

                # Add snpeff_hgvs to header
                vcf_reader.infos[snpeff_json] = vcf.parser._Info(
                    snpeff_json,
                    1,
                    "String",
                    "snpEff annotation in JSON format",
                    "howard calculation",
                    "0",
                    self.code_type_map.get("String"),
                )

                # Prepare annotations
                sql_from_select_annotation_list = []
                for annotation in annotation_dict.values():
                    sql_from_select_annotation_list.append(
                        f""" '{annotation.get("id")}', {annotation.get("column")} """
                    )

                # Add snpeff JSON to header
                vcf_reader.infos[snpeff_hgvs] = vcf.parser._Info(
                    snpeff_hgvs,
                    ".",
                    "String",
                    "HGVS nomenclatures from snpEff annotation",
                    "howard calculation",
                    "0",
                    self.code_type_map.get("String"),
                )

                # Clause for INFO concat
                sql_info_concat = f"""
                    CASE
                        WHEN (INFO IS NULL OR INFO IN ('', '.')) OR (
                            SNPEFF_HGVS.json_data IS NULL
                        )
                        THEN INFO
                        ELSE concat(INFO, ';')
                    END,
                    CASE
                        WHEN SNPEFF_HGVS.json_data IS NOT NULL
                        THEN concat(
                            '{snpeff_json}=',
                            SNPEFF_HGVS.json_data
                        )
                    END
                    
                """

                # Clause for subquery
                sql_from_select = f"""
                    CASE
                        WHEN string_agg("Allele") IS NOT NULL
                        THEN
                            concat(
                                '[',
                                string_agg(
                                    json_object(
                                        {",".join(sql_from_select_annotation_list)}
                                    )::JSON
                                ),
                                ']'
                            )
                        ELSE NULL
                    END AS json_data
                """

                # Append clauses
                sql_clauses.append(
                    {
                        "sql_info_concat": sql_info_concat,
                        "sql_from_select": sql_from_select,
                    }
                )

            if snpeff_explode is not None:

                # Log
                log.info(
                    f"Extract snpEff annotations - into INFO/Tags separately with '{snpeff_explode}' prefix"
                )

                # Prepare annotations
                sql_info_concat_annotation_list = []
                sql_from_select_annotation_list = []
                for annotation in annotation_dict.values():

                    # Add snpeff_hgvs to header
                    annotation_id = f'{snpeff_explode}{annotation.get("id")}'
                    vcf_reader.infos[annotation_id] = vcf.parser._Info(
                        annotation_id,
                        annotation.get("number"),
                        annotation.get("type"),
                        annotation.get("desc"),
                        "howard calculation",
                        "0",
                        self.code_type_map.get(annotation.get("type")),
                    )

                    # Log
                    log.info(
                        f"Extract snpEff annotations - into INFO/Tags separately with '{snpeff_explode}' prefix - '{annotation_id}'"
                    )

                    # Clause for INFO concat for each annotation
                    sql_info_concat_annotation_list.append(
                        f""" 
                            CASE
                                WHEN SNPEFF_HGVS.{annotation.get("id")} IS NOT NULL AND CAST(SNPEFF_HGVS.{annotation.get("id")} AS STRING) NOT IN ('','.')
                                THEN concat('{snpeff_explode}{annotation.get("id")}=', CAST(SNPEFF_HGVS.{annotation.get("id")} AS STRING))
                            END
                        """
                    )

                    # Clause for subquery for each annotation
                    if uniquify:
                        sql_from_select_annotation_list.append(
                            f""" string_agg(DISTINCT CAST({annotation.get("column")} AS STRING), ',') AS '{annotation.get("id")}' """
                        )
                    else:
                        sql_from_select_annotation_list.append(
                            f""" string_agg(COALESCE(CAST({annotation.get("column")} AS STRING), '.'), ',') AS {annotation.get("id")} """
                        )

                # Clause for INFO concat
                sql_info_concat = f"""
                    CASE
                        WHEN (INFO IS NULL OR INFO IN ('', '.')) OR ("Allele" IS NULL)
                        THEN INFO
                        ELSE concat(INFO, ';')
                    END,
                    concat_ws(
                        ';',
                        {" , ".join(sql_info_concat_annotation_list)}
                    )
                """

                # Clause for subquery
                sql_from_select = " , ".join(
                    {" , ".join(sql_from_select_annotation_list)}
                )

                # Append clauses
                sql_clauses.append(
                    {
                        "sql_info_concat": sql_info_concat,
                        "sql_from_select": sql_from_select,
                    }
                )

            if snpeff_hgvs is not None:

                log.info(
                    f"Extract snpEff annotations - into INFO/Tags '{snpeff_hgvs}' with list of HGVS nomenclature"
                )

                # Add snpeff_hgvs to header
                vcf_reader.infos[snpeff_hgvs] = vcf.parser._Info(
                    snpeff_hgvs,
                    ".",
                    "String",
                    "HGVS nomenclatures from snpEff annotation",
                    "howard calculation",
                    "0",
                    self.code_type_map.get("String"),
                )

                # Clause for INFO concat
                sql_info_concat = f"""
                    CASE
                        WHEN (INFO IS NULL OR INFO IN ('', '.')) OR (SNPEFF_HGVS.hgvs IS NULL OR SNPEFF_HGVS.hgvs IN (''))
                        THEN INFO
                        ELSE concat(INFO, ';')
                    END,
                    CASE
                        WHEN SNPEFF_HGVS.hgvs IS NOT NULL AND SNPEFF_HGVS.hgvs NOT IN ('')
                        THEN concat('{snpeff_hgvs}=', SNPEFF_HGVS.hgvs)
                    END
                """

                # Clause for subquery
                sql_from_select = f"""
                    string_agg(
                        concat_ws(
                            ':',
                            "Gene_ID",
                            "Feature_ID",
                            CASE 
                                WHEN "Rank" IS NOT NULL
                                THEN concat('exon', split(CAST("Rank" AS VARCHAR), '/')[1])
                                ELSE NULL
                            END,
                            "HGVS.c",
                            "HGVS.p"
                        ),
                    ',') AS hgvs
                """

                # Append clauses
                sql_clauses.append(
                    {
                        "sql_info_concat": sql_info_concat,
                        "sql_from_select": sql_from_select,
                    }
                )

            # Update
            nb_update = 0
            for sql_clause_item in sql_clauses:

                # Nb update
                nb_update += 1

                # Query
                sql_update = f"""
                    UPDATE variants
                    SET INFO = concat(
                                    {sql_clause_item.get("sql_info_concat")}
                                    )
                    FROM (
                        SELECT "#CHROM", "POS", "REF", "ALT",
                            {sql_clause_item.get("sql_from_select")}
                        FROM {view_name}
                        GROUP BY "#CHROM", "POS", "REF", "ALT"
                        ) AS SNPEFF_HGVS
                    WHERE {table_variants}."#CHROM" = SNPEFF_HGVS."#CHROM"
                    AND {table_variants}."POS" = SNPEFF_HGVS."POS"
                    AND {table_variants}."REF" = SNPEFF_HGVS."REF"
                    AND {table_variants}."ALT" = SNPEFF_HGVS."ALT"
                """

                # Log
                log.info(
                    f"Extract snpEff annotations - Process [{nb_update}/{len(sql_clauses)}]"
                )

                # Process query
                self.conn.execute(sql_update)

            # Delete view
            sql_drop_view = f"""
                DROP VIEW {view_name}
            """
            self.conn.execute(sql_drop_view)

        else:

            log.warning(
                f"Extract snpEff annotations - No snpEff annotation '{snpeff_field}'. Please Anotate with snpEff before use this calculation option"
            )

    def calculation_extract_nomen(self, hgvs_field: str = None) -> None:
        """
        Extracts the HGVS nomenclature from the provided field and calculates the NOMEN patterns.

        This function performs the following steps:
        1. Retrieves extra information fields and constructs the NOMEN pattern.
        2. Splits the NOMEN pattern based on predefined separators.
        3. Constructs SQL queries to parse and extract various components of the NOMEN pattern.
        4. Calculates scores for each variant based on the extracted NOMEN components.
        5. Creates a temporary table to store the results of the NOMEN extraction and scoring.
        6. Updates the main variants table with the extracted NOMEN information.

        Args:
            hgvs_field (str, optional): The field containing the HGVS nomenclature to be extracted. Defaults to None.

        Returns:
            None

        Raises:
            Any exceptions raised during the execution of the SQL queries or file operations.

        Example:
            self.calculation_extract_nomen(hgvs_field="hgvs_column")
        """

        # NOMEN structure
        nomen_dict = {
            "NOMEN": "NOMEN hgvs nomenclature considered as reference hgvs",
            "CNOMEN": "CNOMEN hgvs nomenclature at DNA level related to a transcript (TNOMEN)",
            "RNOMEN": "RNOMEN hgvs nomenclature at RNA level related to a transcript (TNOMEN)",
            "NNOMEN": "NNOMEN hgvs nomenclature for non-coding variant",
            "PNOMEN": "PNOMEN hgvs nomenclature at Protein level related to a transcript (TNOMEN)",
            "UPNOMEN": "UPNOMEN hgvs nomenclature at Protein level as uncertain related to a transcript (TNOMEN)",
            "TVNOMEN": "TVNOMEN hgvs transcript with version (if any) used (e.g. for CNOMEN and PNOMEN)",
            "TNOMEN": "TNOMEN hgvs transcript used (e.g. for CNOMEN and PNOMEN)",
            "VNOMEN": "VNOMEN hgvs transcript version used (e.g. for CNOMEN and PNOMEN)",
            "TPVNOMEN": "TPVNOMEN hgvs protein transcript with version (if any) used (e.g. for CNOMEN and PNOMEN)",
            "TPNOMEN": "TNOMEN hgvs protein transcript used (e.g. for CNOMEN and PNOMEN)",
            "TPVVNOMEN": "VNOMEN hgvs protein transcript version used (e.g. for CNOMEN and PNOMEN)",
            "ENOMEN": "ENOMEN hgvs exon nomenclature related to a transcript (TNOMEN)",
            "GNOMEN": "GNOMEN hgvs gene nomenclature related to a transcript (TNOMEN)",
        }

        # Param
        param = self.get_param()

        # Prefix
        prefix = self.get_explode_infos_prefix()

        # Header
        vcf_reader = self.get_header()

        # Get HGVS field
        if hgvs_field is None:
            hgvs_field = (
                param.get("calculation", {})
                .get("calculations", {})
                .get("NOMEN", {})
                .get("options", {})
                .get("hgvs_field", "hgvs")
            )

        # Get NOMEN pattern
        nomen_pattern = (
            param.get("calculation", {})
            .get("calculations", {})
            .get("NOMEN", {})
            .get("options", {})
            .get("pattern", None)
        )
        # default NOMEN pattern
        if nomen_pattern is None:
            nomen_pattern = "GNOMEN:TNOMEN:ENOMEN:CNOMEN:RNOMEN:NNOMEN:PNOMEN"

        if isinstance(nomen_pattern, str):
            nomen_patterns = {"NOMEN": nomen_pattern}
        elif isinstance(nomen_pattern, dict):
            nomen_patterns = nomen_pattern
        else:
            msg_err = f"NOMEN pattern '{nomen_pattern}' is not well formed"
            log.error(msg_err)
            raise ValueError(msg_err)

        # Get NOMEN pattern
        nomen_fields = (
            param.get("calculation", {})
            .get("calculations", {})
            .get("NOMEN", {})
            .get("options", {})
            .get("fields", None)
        )

        # default NOMEN pattern
        if nomen_fields is None:  # or nomen_fields == []:
            nomen_fields = list(nomen_dict.keys())

        # Remove "NOMEN" as patterns as separetly processed
        for nomen_pattern in nomen_patterns.keys():
            if nomen_pattern in nomen_fields:
                nomen_fields.remove(nomen_pattern)

        # transcripts list of preference sources
        transcripts_sources = {}

        # Get transcripts
        transcripts_file = (
            param.get("calculation", {})
            .get("calculations", {})
            .get("NOMEN", {})
            .get("options", {})
            .get("transcripts", None)
        )
        transcripts_file = full_path(transcripts_file)
        if transcripts_file:
            if os.path.exists(transcripts_file):
                transcripts_dataframe = transcripts_file_to_df(transcripts_file)
                transcripts_from_file = transcripts_dataframe.iloc[:, 0].tolist()
                transcripts_sources["file"] = transcripts_from_file
            else:
                msg_err = f"Transcript file '{transcripts_file}' does NOT exist"
                log.error(msg_err)
                raise ValueError(msg_err)

        # Get transcripts table
        transcripts_table = (
            param.get("calculation", {})
            .get("calculations", {})
            .get("NOMEN", {})
            .get("options", {})
            .get("transcripts_table", self.get_table_variants())
        )
        # Get transcripts column
        transcripts_column = (
            param.get("calculation", {})
            .get("calculations", {})
            .get("NOMEN", {})
            .get("options", {})
            .get("transcripts_column", None)
        )

        # Transcripts of preference source order
        transcripts_order = (
            param.get("calculation", {})
            .get("calculations", {})
            .get("NOMEN", {})
            .get("options", {})
            .get("transcripts_order", ["column", "file"])
        )

        # Transcripts from file
        transcripts = transcripts_sources.get("file", [])

        # Log
        log.info(f"Start NOMEN calculation configuration...")

        # Create annotation view
        annotations_view = "annotations_view_for_extract_nomen_" + str(
            random.randrange(1000000)
        )
        if transcripts_column is None:
            transcripts_column = "transcript"
        annotations_view = self.create_annotations_view(
            table=transcripts_table,
            view=annotations_view,
            view_type="table",
            view_mode="explore",
            fields=[transcripts_column, hgvs_field],
            fields_forced_as_varchar=True,
            info_prefix_column="",
        )

        # extra infos
        extra_infos = self.get_extra_infos()
        extra_field = prefix + hgvs_field

        if extra_field in extra_infos or True:

            # Construct NOMEN Pattern
            separators = [",", "(", ")", "|", ":", "[", "]", "{", "}"]
            regex_pattern = "|".join(map(re.escape, separators))

            # Init
            nomen_patterns_sql = {}

            for nomen_pattern_name, nomen_pattern in nomen_patterns.items():

                # Split NOMEN pattern
                split_nomen_pattern = re.split(rf"({regex_pattern})", nomen_pattern)

                # Construct SQL NOMEN Pattern
                nomen_pattern_sql_list = []
                nomen_info_previous = ""
                inside_parentheses = False
                inside_brackets = False
                inside_braces = False

                # Parse NOMEN pattern
                for i, nomen_info in enumerate(split_nomen_pattern):
                    if nomen_info == "(":
                        inside_parentheses = True
                        nomen_info_previous += nomen_info
                    elif nomen_info == ")":
                        inside_parentheses = False
                        if nomen_info_previous:
                            nomen_pattern_sql_list.append(nomen_info)
                        nomen_info_previous = ""
                    elif nomen_info == "[":
                        inside_brackets = True
                        nomen_info_previous += nomen_info
                    elif nomen_info == "]":
                        inside_brackets = False
                        if nomen_info_previous:
                            nomen_pattern_sql_list.append(nomen_info)
                        nomen_info_previous = ""
                    elif nomen_info == "{":
                        inside_braces = True
                        nomen_info_previous += nomen_info
                    elif nomen_info == "}":
                        inside_braces = False
                        if nomen_info_previous:
                            nomen_pattern_sql_list.append(nomen_info)
                        nomen_info_previous = ""
                    elif nomen_info in separators:
                        nomen_info_previous += nomen_info
                    else:
                        if nomen_info != "":
                            next_info = (
                                split_nomen_pattern[i + 1]
                                if i + 1 < len(split_nomen_pattern)
                                else ""
                            )
                            if next_info in separators:
                                if (
                                    inside_parentheses
                                    or inside_brackets
                                    or inside_braces
                                ):
                                    nomen_pattern_sql_list.append(
                                        f"""
                                            CASE
                                                WHEN {nomen_info} IS NOT NULL
                                                THEN concat('{nomen_info_previous}', {nomen_info}, '{next_info}')
                                            END
                                        """
                                    )
                                else:
                                    nomen_pattern_sql_list.append(
                                        f"""
                                            CASE
                                                WHEN {nomen_info} IS NOT NULL
                                                THEN concat('{nomen_info_previous}', {nomen_info})
                                            END
                                        """
                                    )
                            else:
                                nomen_pattern_sql_list.append(
                                    f"""
                                        CASE
                                            WHEN {nomen_info} IS NOT NULL
                                            THEN concat('{nomen_info_previous}', {nomen_info})
                                        END
                                    """
                                )
                            nomen_info_previous = ""

                    # Construcut NOMEN pattern for SQL
                    nomen_pattern_sql = ", ".join(nomen_pattern_sql_list)

                    # Add NOMEN pattern for SQL
                    nomen_patterns_sql[nomen_pattern_name] = nomen_pattern_sql

            # Transcript source order and index window
            transcripts_order_length = len(transcripts_order)
            transcripts_order_window = 10000000
            try:
                index_transcript_selected = (
                    transcripts_order_length - transcripts_order.index("column")
                ) * transcripts_order_window
            except:
                index_transcript_selected = 0
            try:
                index_transcript_prefered = (
                    transcripts_order_length - transcripts_order.index("file")
                ) * transcripts_order_window
            except:
                index_transcript_prefered = 0

            # Transcripts rank
            if len(transcripts) >= 1:
                transcripts_pond = {
                    transcript: len(transcripts) - rank
                    for rank, transcript in enumerate(transcripts, start=0)
                }

                # Construct transcripts pond table
                transcripts_pond_table = "transcripts_pond_" + str(
                    random.randrange(1000000)
                )
                transcripts_pond_df = pd.DataFrame(
                    list(transcripts_pond.items()), columns=["transcript", "rank"]
                )
                self.execute_query(
                    f"CREATE TABLE {transcripts_pond_table} AS SELECT * FROM transcripts_pond_df"
                )

                transcripts_pond_score_sql = f"""
                    + CASE
                            WHEN TVNOMEN in (SELECT transcript FROM {transcripts_pond_table})
                                OR TNOMEN IN (SELECT transcript FROM {transcripts_pond_table})
                            THEN {index_transcript_prefered} + (
                                    SELECT {transcripts_pond_table}.rank
                                    FROM {transcripts_pond_table}
                                    WHERE {transcripts_pond_table}.transcript = TVNOMEN
                                        OR {transcripts_pond_table}.transcript = TNOMEN
                                    LIMIT 1
                                )
                            ELSE 0
                        END
                """
            else:
                transcripts_pond_score_sql = ""

            # NOMEN Patterns
            pattern_tvnomen = r".*[:]*([NX][MR]_[^:]*).*"
            pattern_tpvnomen = r".*[:]*([NX]P_[^:]*).*"
            pattern_cnomen = r".*[:]*([cgm]\.[^:]*).*"
            pattern_pnomen = r".*[:]*([p]\.[^:]*).*"
            pattern_nnomen = r".*[:]*([n]\.[^:]*).*"
            pattern_rnomen = r".*[:]*([r]\.[^:]*).*"
            pattern_enomen = r".*[:]*(exon[^:]*).*"

            # Check NOMEN fields length
            nomen_fields_select_sql = ""
            if len(nomen_fields) >= 1:
                nomen_fields_select_sql = ", ".join(nomen_fields) + ","
            else:
                nomen_fields_select_sql = ""

            # NOMEN patterns SQL select
            nomen_patterns_sql_select_list = []
            for nomen_pattern_name, nomen_pattern_sql in nomen_patterns_sql.items():
                nomen_patterns_sql_select_list.append(
                    f"""
                    concat({nomen_pattern_sql}) AS "{nomen_pattern_name}",
                """
                )
            nomen_patterns_sql_select = " ".join(nomen_patterns_sql_select_list)

            # Query find NOMEN
            query_find_nomen = f"""
                WITH
                nomen_variants AS (
                    SELECT
                        "#CHROM", "POS", "REF", "ALT",
                        "{hgvs_field}"::VARCHAR AS hgvs, {transcripts_column}::VARCHAR AS 'transcript',
                        UNNEST(STRING_SPLIT("{hgvs_field}"::VARCHAR, ',')) AS 'nomen'
                    FROM {annotations_view}
                ),
                decomposed_variants AS (
                    SELECT
                        "#CHROM", "POS", "REF", "ALT",
                        "transcript",
                        -- TVNOMEN
                        NULLIF(regexp_extract(nomen, '{pattern_tvnomen}', 1), '') AS 'TVNOMEN',
                        CASE
                            WHEN array_length(string_split(regexp_extract(nomen, '{pattern_tvnomen}', 1), '.'), 1) >= 1
                            THEN NULLIF(string_split(regexp_extract(nomen, '{pattern_tvnomen}', 1), '.')[1], '')
                            ELSE NULL
                        END AS 'TNOMEN',
                        CASE
                            WHEN array_length(string_split(regexp_extract(nomen, '{pattern_tvnomen}', 1), '.'), 1) >= 2
                            THEN NULLIF(string_split(regexp_extract(nomen, '{pattern_tvnomen}', 1), '.')[2], '')
                            ELSE NULL
                        END AS 'VNOMEN',
                        -- TPVNOMEN
                        NULLIF(regexp_extract(nomen, '{pattern_tpvnomen}', 1), '') AS 'TPVNOMEN',
                        CASE
                            WHEN array_length(string_split(regexp_extract(nomen, '{pattern_tpvnomen}', 1), '.'), 1) >= 1
                            THEN NULLIF(string_split(regexp_extract(nomen, '{pattern_tpvnomen}', 1), '.')[1], '')
                            ELSE NULL
                        END AS 'TPNOMEN',
                        CASE
                            WHEN array_length(string_split(regexp_extract(nomen, '{pattern_tpvnomen}', 1), '.'), 1) >= 2
                            THEN IFNULL(string_split(regexp_extract(nomen, '{pattern_tpvnomen}', 1), '.')[2], '')
                            ELSE NULL
                        END AS 'TPVVNOMEN',
                        -- CPNR-NOMEN
                        NULLIF(regexp_extract(nomen, '{pattern_cnomen}', 1), '') AS 'CNOMEN',
                        NULLIF(regexp_extract(nomen, '{pattern_pnomen}', 1), '') AS 'PNOMEN',
                        NULLIF(regexp_extract(nomen, '{pattern_nnomen}', 1), '') AS 'NNOMEN',
                        NULLIF(regexp_extract(nomen, '{pattern_rnomen}', 1), '') AS 'RNOMEN',
                        -- Uncertain p.
                        NULLIF(
                            CASE
                                WHEN NULLIF(regexp_extract(nomen, '{pattern_pnomen}', 1), '') IS NOT NULL
                                THEN concat(
                                    'p.',
                                    '(',
                                    string_split(regexp_extract(nomen, '{pattern_pnomen}', 1), '.')[2],
                                    ')'
                                )
                            END
                        , '') AS 'UPNOMEN',
                        -- exon
                        NULLIF(regexp_extract(nomen, '{pattern_enomen}', 1), '') AS 'ENOMEN',
                        -- gene
                        CASE
                            WHEN NULLIF(regexp_extract(string_split(nomen, ':')[1], '{pattern_tvnomen}', 1), '') IS NOT NULL
                            OR NULLIF(regexp_extract(string_split(nomen, ':')[1], '{pattern_tpvnomen}', 1), '') IS NOT NULL
                            OR NULLIF(regexp_extract(string_split(nomen, ':')[1], '{pattern_cnomen}', 1), '') IS NOT NULL
                            OR NULLIF(regexp_extract(string_split(nomen, ':')[1], '{pattern_pnomen}', 1), '') IS NOT NULL
                            OR NULLIF(regexp_extract(string_split(nomen, ':')[1], '{pattern_nnomen}', 1), '') IS NOT NULL
                            OR NULLIF(regexp_extract(string_split(nomen, ':')[1], '{pattern_rnomen}', 1), '') IS NOT NULL
                            OR NULLIF(regexp_extract(string_split(nomen, ':')[1], '{pattern_enomen}', 1), '') IS NOT NULL
                            THEN NULL
                            ELSE NULLIF(string_split(nomen, ':')[1], '')
                        END AS 'GNOMEN'
                    FROM nomen_variants
                ),
                scored_variants AS (
                    SELECT
                        "#CHROM", "POS", "REF", "ALT",
                        "TNOMEN", "TVNOMEN", "VNOMEN",
                        "TPVNOMEN", "TPNOMEN", "TPVVNOMEN",
                        "CNOMEN", "PNOMEN", "NNOMEN", "RNOMEN", "UPNOMEN",
                        "ENOMEN", "GNOMEN",
                        -- Score calculation
                        0
                        + CASE WHEN CNOMEN IS NOT NULL THEN 1 ELSE 0 END
                        + CASE WHEN NNOMEN IS NOT NULL THEN 1 ELSE 0 END
                        + CASE WHEN RNOMEN IS NOT NULL THEN 1 ELSE 0 END
                        + CASE WHEN ENOMEN IS NOT NULL THEN 1 ELSE 0 END
                        + CASE WHEN PNOMEN IS NOT NULL THEN 1 ELSE 0 END
                        + CASE WHEN TPVNOMEN IS NOT NULL THEN 1 ELSE 0 END
                        + CASE WHEN regexp_matches(TVNOMEN, '^NM_.*') THEN 2 ELSE 0 END
                        + CASE WHEN regexp_matches(TVNOMEN, '^NR_.*') THEN 1 ELSE 0 END
                        -- Selected transcript
                        + CASE WHEN transcript IS NOT NULL AND (TVNOMEN == transcript OR TNOMEN == transcript) THEN {index_transcript_selected} ELSE 0 END
                        -- Preferend transcripts
                        {transcripts_pond_score_sql}
                        AS 'SCORE'
                    FROM decomposed_variants
                )
                    SELECT
                        "#CHROM", "POS", "REF", "ALT",
                        {nomen_fields_select_sql}
                        {nomen_patterns_sql_select}
                    FROM (
                        SELECT *,
                            ROW_NUMBER() OVER (PARTITION BY "#CHROM", "POS", "REF", "ALT" ORDER BY SCORE DESC) AS rn
                        FROM scored_variants
                    )
                    WHERE rn = 1
            """
            nomen_annotations_view = "annotations_vies_for_extract_nomen_" + str(
                random.randrange(1000000)
            )
            query_find_nomen_create = f"""
            CREATE TABLE {nomen_annotations_view} AS (
                {query_find_nomen}
            )
            """
            # log.debug(f"query_devel={query_find_nomen}")
            log.info(f"Start NOMEN calculation...")
            self.execute_query(query=query_find_nomen_create)
            log.debug(f"Stop NOMEN calculation")

            # Explode NOMEN Structure and create SQL set for update
            sql_nomen_fields = []
            for nomen_field in list(nomen_patterns.keys()) + nomen_fields:

                # Description
                nomen_field_desc = nomen_dict.get(
                    nomen_field, "howard calculation NOMEN"
                )
                if nomen_field in list(nomen_patterns.keys()):
                    nomen_field_desc = (
                        nomen_dict.get("NOMEN", "howard calculation NOMEN")
                        + f""". Format '{nomen_patterns.get(nomen_field)}'"""
                    )

                # Create VCF header field
                vcf_reader.infos[nomen_field] = vcf.parser._Info(
                    nomen_field,
                    1,
                    "String",
                    nomen_field_desc,
                    "howard calculation",
                    "0",
                    self.code_type_map.get("String"),
                )

                # Add field to SQL query update
                sql_nomen_fields.append(
                    f"""
                        CASE 
                            WHEN {nomen_annotations_view}."{nomen_field}" NOT NULL AND {nomen_annotations_view}."{nomen_field}" NOT IN ('')
                            THEN concat(
                                    ';{nomen_field}=',
                                    {nomen_annotations_view}."{nomen_field}"
                                )
                            ELSE ''
                        END
                    """
                )

            # SQL set for update
            sql_nomen_fields_set = ", ".join(sql_nomen_fields)

            # Update
            sql_update = f"""
                UPDATE {transcripts_table}
                SET "INFO" = 
                    concat(
                        CASE
                            WHEN "INFO" IS NULL
                            THEN ''
                            ELSE concat("INFO", ';')
                        END,
                        regexp_replace(
                            concat(
                                {sql_nomen_fields_set}
                            )
                            ,'^;', ''
                        )
                    )
                FROM {nomen_annotations_view}
                WHERE {transcripts_table}."#CHROM" = {nomen_annotations_view}."#CHROM"
                    AND {transcripts_table}."POS" = {nomen_annotations_view}."POS" 
                    AND {transcripts_table}."REF" = {nomen_annotations_view}."REF"
                    AND {transcripts_table}."ALT" = {nomen_annotations_view}."ALT"
            """
            log.debug(f"Start NOMEN update...")
            self.conn.execute(sql_update)
            log.debug(f"Stop NOMEN update...")

            # Remove tables and view
            self.remove_tables_or_views(
                tables=[annotations_view, nomen_annotations_view]
            )

    def calculation_find_by_pipeline(self, tag: str = "findbypipeline") -> None:
        """
        The function `calculation_find_by_pipeline` performs a calculation to find the number of
        pipeline/sample for a variant and updates the variant information in a VCF file.

        :param tag: The `tag` parameter is a string that represents the annotation field for the
        "findbypipeline" information in the VCF file. It is used to create the annotation field in the
        VCF header and to update the corresponding field in the variants table, defaults to
        findbypipeline
        :type tag: str (optional)
        """

        # if FORMAT and samples
        if (
            "FORMAT" in self.get_header_columns_as_list()
            and self.get_header_sample_list()
        ):

            # findbypipeline annotation field
            findbypipeline_tag = tag

            # VCF infos tags
            vcf_infos_tags = {
                findbypipeline_tag: f"Number of pipeline/sample for a variant ({findbypipeline_tag})",
            }

            # Prefix
            prefix = self.get_explode_infos_prefix()

            # Field
            findbypipeline_infos = prefix + findbypipeline_tag

            # Variants table
            table_variants = self.get_table_variants()

            # Header
            vcf_reader = self.get_header()

            # Create variant id
            variant_id_column = self.get_variant_id_column()
            added_columns = [variant_id_column]

            # variant_id, FORMAT and samples
            samples_fields = f" {variant_id_column}, FORMAT , " + " , ".join(
                [f""" "{sample}" """ for sample in self.get_header_sample_list()]
            )

            # Create dataframe
            dataframe_findbypipeline = self.get_query_to_df(
                f""" SELECT {samples_fields} FROM {table_variants} """
            )

            # Create findbypipeline column
            dataframe_findbypipeline[findbypipeline_infos] = (
                dataframe_findbypipeline.apply(
                    lambda row: findbypipeline(
                        row, samples=self.get_header_sample_list()
                    ),
                    axis=1,
                )
            )

            # Add snpeff_hgvs to header
            vcf_reader.infos[findbypipeline_tag] = vcf.parser._Info(
                findbypipeline_tag,
                ".",
                "String",
                vcf_infos_tags.get(findbypipeline_tag, "Find in pipeline/sample"),
                "howard calculation",
                "0",
                self.code_type_map.get("String"),
            )

            # Update
            sql_update = f"""
                UPDATE variants
                SET "INFO" = 
                    concat(
                        CASE
                            WHEN "INFO" IS NULL OR "INFO" IN ('','.')
                            THEN ''
                            ELSE concat("INFO", ';')
                        END,
                        CASE 
                            WHEN dataframe_findbypipeline."{findbypipeline_infos}" NOT IN ('','.')
                                AND dataframe_findbypipeline."{findbypipeline_infos}" NOT NULL
                            THEN concat(
                                    '{findbypipeline_tag}=',
                                    dataframe_findbypipeline."{findbypipeline_infos}"
                                )
                            ELSE ''
                        END
                    )
                FROM dataframe_findbypipeline
                WHERE variants."{variant_id_column}" = dataframe_findbypipeline."{variant_id_column}"
            """
            self.conn.execute(sql_update)

            # Remove added columns
            for added_column in added_columns:
                self.drop_column(column=added_column)

            # Delete dataframe
            del dataframe_findbypipeline
            gc.collect()

    def calculation_genotype_concordance(self) -> None:
        """
        The function `calculation_genotype_concordance` calculates the genotype concordance for
        multi-caller VCF files and updates the variant information in the database.
        """

        # if FORMAT and samples
        if (
            "FORMAT" in self.get_header_columns_as_list()
            and self.get_header_sample_list()
        ):

            # genotypeconcordance annotation field
            genotypeconcordance_tag = "genotypeconcordance"

            # VCF infos tags
            vcf_infos_tags = {
                genotypeconcordance_tag: "Concordance of genotype for multi caller VCF",
            }

            # Prefix
            prefix = self.get_explode_infos_prefix()

            # Field
            genotypeconcordance_infos = prefix + genotypeconcordance_tag

            # Variants table
            table_variants = self.get_table_variants()

            # Header
            vcf_reader = self.get_header()

            # Create variant id
            variant_id_column = self.get_variant_id_column()
            added_columns = [variant_id_column]

            # variant_id, FORMAT and samples
            samples_fields = f" {variant_id_column}, FORMAT , " + " , ".join(
                [f""" "{sample}" """ for sample in self.get_header_sample_list()]
            )

            # Create dataframe
            dataframe_genotypeconcordance = self.get_query_to_df(
                f""" SELECT {samples_fields} FROM {table_variants} """
            )

            # Create genotypeconcordance column
            dataframe_genotypeconcordance[genotypeconcordance_infos] = (
                dataframe_genotypeconcordance.apply(
                    lambda row: genotypeconcordance(
                        row, samples=self.get_header_sample_list()
                    ),
                    axis=1,
                )
            )

            # Add genotypeconcordance to header
            vcf_reader.infos[genotypeconcordance_tag] = vcf.parser._Info(
                genotypeconcordance_tag,
                ".",
                "String",
                vcf_infos_tags.get(genotypeconcordance_tag, "snpEff hgvs annotations"),
                "howard calculation",
                "0",
                self.code_type_map.get("String"),
            )

            # Update
            sql_update = f"""
                UPDATE variants
                SET "INFO" = 
                    concat(
                        CASE
                            WHEN "INFO" IS NULL OR "INFO" IN ('','.')
                            THEN ''
                            ELSE concat("INFO", ';')
                        END,
                        CASE
                            WHEN dataframe_genotypeconcordance."{genotypeconcordance_infos}" NOT IN ('','.')
                                AND dataframe_genotypeconcordance."{genotypeconcordance_infos}" NOT NULL
                            THEN concat(
                                    '{genotypeconcordance_tag}=',
                                    dataframe_genotypeconcordance."{genotypeconcordance_infos}"
                                )
                            ELSE ''
                        END
                    )
                FROM dataframe_genotypeconcordance
                WHERE variants."{variant_id_column}" = dataframe_genotypeconcordance."{variant_id_column}"
            """
            self.conn.execute(sql_update)

            # Remove added columns
            for added_column in added_columns:
                self.drop_column(column=added_column)

            # Delete dataframe
            del dataframe_genotypeconcordance
            gc.collect()

    def calculation_barcode(self, tag: str = "barcode") -> None:
        """
        The `calculation_barcode` function calculates barcode values for variants in a VCF file and
        updates the INFO field in the file with the calculated barcode values.

        :param tag: The `tag` parameter in the `calculation_barcode` function is used to specify the tag
        name that will be used for the barcode calculation in the VCF file. If no tag name is provided,
        the default tag name is set to "barcode", defaults to barcode
        :type tag: str (optional)
        """

        # if FORMAT and samples
        if (
            "FORMAT" in self.get_header_columns_as_list()
            and self.get_header_sample_list()
        ):

            # barcode annotation field
            if not tag:
                tag = "barcode"

            # VCF infos tags
            vcf_infos_tags = {
                tag: "barcode calculation (VaRank)",
            }

            # Prefix
            prefix = self.get_explode_infos_prefix()

            # Field
            barcode_infos = prefix + tag

            # Variants table
            table_variants = self.get_table_variants()

            # Header
            vcf_reader = self.get_header()

            # Create variant id
            variant_id_column = self.get_variant_id_column()
            added_columns = [variant_id_column]

            # variant_id, FORMAT and samples
            samples_fields = f" {variant_id_column}, FORMAT , " + " , ".join(
                [f""" "{sample}" """ for sample in self.get_header_sample_list()]
            )

            # Create dataframe
            dataframe_barcode = self.get_query_to_df(
                f""" SELECT {samples_fields} FROM {table_variants} """
            )

            # Create barcode column
            dataframe_barcode[barcode_infos] = dataframe_barcode.apply(
                lambda row: barcode(row, samples=self.get_header_sample_list()), axis=1
            )

            # Add barcode to header
            vcf_reader.infos[tag] = vcf.parser._Info(
                tag,
                ".",
                "String",
                vcf_infos_tags.get(tag, vcf_infos_tags.get(tag)),
                "howard calculation",
                "0",
                self.code_type_map.get("String"),
            )

            # Update
            sql_update = f"""
                UPDATE {table_variants}
                SET "INFO" = 
                    concat(
                        CASE
                            WHEN "INFO" IS NULL OR "INFO" IN ('','.')
                            THEN ''
                            ELSE concat("INFO", ';')
                        END,
                        CASE
                            WHEN dataframe_barcode."{barcode_infos}" NOT IN ('','.')
                            AND dataframe_barcode."{barcode_infos}" NOT NULL
                            THEN concat(
                                    '{tag}=',
                                    dataframe_barcode."{barcode_infos}"
                                )
                            ELSE ''
                        END
                    )
                FROM dataframe_barcode
                WHERE {table_variants}."{variant_id_column}" = dataframe_barcode."{variant_id_column}"
            """
            self.conn.execute(sql_update)

            # Remove added columns
            for added_column in added_columns:
                self.drop_column(column=added_column)

            # Delete dataframe
            del dataframe_barcode
            gc.collect()

    def calculation_barcode_family(
        self, tag: str = None, tag_samples: str = None
    ) -> None:
        """
        The `calculation_barcode_family` function calculates barcode values for variants in a VCF file
        and updates the INFO field in the file with the calculated barcode values.

        :param tag: The `tag` parameter in the `calculation_barcode_family` function is used to specify
        the barcode tag that will be added to the VCF file during the calculation process. If no value
        is provided for the `tag` parameter, the default value used is "BCF", defaults to BCF
        :type tag: str (optional)
        :param tag_samples: The `tag_samples` parameter in the `calculation_barcode_family` function is
        used to specify the barcode tag that will be added to the VCF file for samples during the
        calculation process. If no value is provided for the `tag_samples` parameter, the default value
        used is "BCFS", defaults to BCFS

        """

        # if FORMAT and samples
        if (
            "FORMAT" in self.get_header_columns_as_list()
            and self.get_header_sample_list()
        ):

            # barcode annotation field
            if not tag:
                tag = "BCF"

            # barcode annotation field for samples
            if not tag_samples:
                tag_samples = f"{tag}S"

            # VCF infos tags
            vcf_infos_tags = {
                "tag": "barcode family calculation",
                "tag_samples": "barcode family samples",
            }

            # Param
            param = self.get_param()
            log.debug(f"param={param}")

            # Prefix
            prefix = self.get_explode_infos_prefix()

            # PED param
            ped = (
                param.get("calculation", {})
                .get("calculations", {})
                .get("BARCODEFAMILY", {})
                .get("family_pedigree", None)
            )
            log.debug(f"ped={ped}")

            # Load PED
            if ped:

                # Pedigree is a file
                if isinstance(ped, str) and os.path.exists(full_path(ped)):
                    log.debug("Pedigree is file")
                    with open(full_path(ped)) as ped:
                        ped = yaml.safe_load(ped)

                # Pedigree is a string
                elif isinstance(ped, str):
                    log.debug("Pedigree is str")
                    try:
                        ped = json.loads(ped)
                        log.debug("Pedigree is json str")
                    except ValueError as e:
                        ped_samples = ped.split(",")
                        ped = {}
                        for ped_sample in ped_samples:
                            ped[ped_sample] = ped_sample

                # Pedigree is a dict
                elif isinstance(ped, dict):
                    log.debug("Pedigree is dict")

                # Pedigree is not well formatted
                else:
                    msg_error = "Pedigree not well formatted"
                    log.error(msg_error)
                    raise ValueError(msg_error)

                # Construct list
                ped_samples = list(ped.values())

            else:
                log.debug("Pedigree not defined. Take all samples")
                ped_samples = self.get_header_sample_list()
                ped = {}
                for ped_sample in ped_samples:
                    ped[ped_sample] = ped_sample

            # Check pedigree
            if not ped or len(ped) == 0:
                msg_error = f"Error in pedigree: samples {ped_samples}"
                log.error(msg_error)
                raise ValueError(msg_error)

            # Log
            log.info(
                "Calculation 'BARCODEFAMILY' - Samples: "
                + ", ".join([f"{member}='{ped[member]}'" for member in ped])
            )
            log.debug(f"ped_samples={ped_samples}")

            # Header
            vcf_reader = self.get_header()

            # Check for other tag names starting with 'tag'
            log.debug(f"tag={tag}")
            log.debug(f"tag_samples={tag_samples}")
            if tag in vcf_reader.formats:
                # Create a new tag name with a suffix based on the number of tags match with the same 'tag' pattern '{tag}_<integer>'
                tag_new = f"{tag}_" + str(
                    len(
                        [
                            t
                            for t in vcf_reader.formats
                            if (t == tag or re.match(rf"^{tag}_\d+$", t))
                        ]
                    )
                )

                tag = tag_new
            if tag_samples in vcf_reader.formats:
                # Create a new tag name with a suffix based on the number of tags match with the same 'tag' pattern '{tag}_<integer>'
                tag_samples_new = f"{tag_samples}_" + str(
                    len(
                        [
                            t
                            for t in vcf_reader.formats
                            if (
                                t == tag_samples or re.match(rf"^{tag_samples}_\d+$", t)
                            )
                        ]
                    )
                )

                tag_samples = tag_samples_new

            # Create vcf_infos_tags for the tags
            vcf_infos_tags[tag] = vcf_infos_tags.get(
                "tag", "barcode family calculation"
            )
            vcf_infos_tags[tag_samples] = vcf_infos_tags.get(
                "tag_samples", "barcode family samples"
            )
            log.debug(f"vcf_infos_tags={vcf_infos_tags}")

            # Field
            barcode_infos = prefix + tag

            # Variants table
            table_variants = self.get_table_variants()

            # Create variant id
            variant_id_column = self.get_variant_id_column()
            added_columns = [variant_id_column]

            # variant_id, FORMAT and samples
            samples_fields = f" {variant_id_column}, FORMAT , " + " , ".join(
                [f""" "{sample}" """ for sample in ped_samples]
            )

            # Create dataframe
            dataframe_barcode = self.get_query_to_df(
                f""" SELECT {samples_fields} FROM {table_variants} """
            )

            # Create barcode column
            dataframe_barcode[barcode_infos] = dataframe_barcode.apply(
                lambda row: barcode(row, samples=ped_samples), axis=1
            )

            # Add barcode family to header
            # Add vaf_normalization to header
            vcf_reader.formats[tag] = vcf.parser._Format(
                id=tag,
                num=1,
                type="String",
                desc=vcf_infos_tags.get(
                    tag, f"barcode family calculation for {ped_samples}"
                )
                + f" for {ped_samples}",
                type_code=self.code_type_map.get("String"),
            )
            vcf_reader.formats[f"{tag}S"] = vcf.parser._Format(
                id=tag_samples,
                num=str(len(ped_samples)),
                type="String",
                desc=vcf_infos_tags.get(
                    tag_samples, f"barcode family samples for {ped_samples}"
                )
                + f" for {ped_samples}",
                type_code=self.code_type_map.get("String"),
            )

            # Update
            # for sample in ped_samples:
            sql_update_set = []
            for sample in self.get_header_sample_list() + ["FORMAT"]:
                if sample in ped_samples:
                    value = f'dataframe_barcode."{barcode_infos}"'
                    value_samples = (
                        "'"
                        + ",".join([f"""{sample}""" for sample in ped_samples])
                        + "'"
                    )
                    ped_samples
                elif sample == "FORMAT":
                    value = f"'{tag}'"
                    value_samples = f"'{tag_samples}'"
                else:
                    value = "'.'"
                    value_samples = "'.'"

                # Format regex
                format_regex = r"[a-zA-Z0-9\s]"

                # Update query
                sql_update_set.append(
                    f"""
                        "{sample}" = 
                        concat(
                            CASE
                                WHEN {table_variants}."{sample}" = './.'
                                THEN concat('./.',regexp_replace(regexp_replace({table_variants}.FORMAT, '{format_regex}', '', 'g'), ':', ':.', 'g'))
                                ELSE {table_variants}."{sample}"
                            END,
                            ':',
                            {value},
                            ':',
                            {value_samples}
                        )
                    """
                )

            sql_update_set_join = ", ".join(sql_update_set)
            sql_update = f"""
                UPDATE {table_variants}
                SET {sql_update_set_join}
                FROM dataframe_barcode
                WHERE {table_variants}."{variant_id_column}" = dataframe_barcode."{variant_id_column}"
            """
            self.conn.execute(sql_update)

            # Remove added columns
            for added_column in added_columns:
                self.drop_column(column=added_column)

            # Delete dataframe
            del dataframe_barcode
            gc.collect()

    def calculation_trio(self) -> None:
        """
        The `calculation_trio` function performs trio calculations on a VCF file by adding trio
        information to the INFO field of each variant.
        """

        # if FORMAT and samples
        if (
            "FORMAT" in self.get_header_columns_as_list()
            and self.get_header_sample_list()
        ):

            # trio annotation field
            trio_tag = "trio"

            # VCF infos tags
            vcf_infos_tags = {
                "trio": "trio calculation",
            }

            # Param
            param = self.get_param()

            # Prefix
            prefix = self.get_explode_infos_prefix()

            # Trio param
            trio_ped = (
                param.get("calculation", {})
                .get("calculations", {})
                .get("TRIO", {})
                .get("trio_pedigree", None)
            )

            # Load trio
            if trio_ped:

                # Trio pedigree is a file
                if isinstance(trio_ped, str) and os.path.exists(full_path(trio_ped)):
                    log.debug("TRIO pedigree is file")
                    with open(full_path(trio_ped)) as trio_ped:
                        trio_ped = yaml.safe_load(trio_ped)

                # Trio pedigree is a string
                elif isinstance(trio_ped, str):
                    log.debug("TRIO pedigree is str")
                    try:
                        trio_ped = json.loads(trio_ped)
                        log.debug("TRIO pedigree is json str")
                    except ValueError as e:
                        trio_samples = trio_ped.split(",")
                        if len(trio_samples) == 3:
                            trio_ped = {
                                "father": trio_samples[0],
                                "mother": trio_samples[1],
                                "child": trio_samples[2],
                            }
                            log.debug("TRIO pedigree is list str")
                        else:
                            msg_error = "TRIO pedigree not well formatted"
                            log.error(msg_error)
                            raise ValueError(msg_error)

                # Trio pedigree is a dict
                elif isinstance(trio_ped, dict):
                    log.debug("TRIO pedigree is dict")

                # Trio pedigree is not well formatted
                else:
                    msg_error = "TRIO pedigree not well formatted"
                    log.error(msg_error)
                    raise ValueError(msg_error)

                # Construct trio list
                trio_samples = [
                    trio_ped.get("father", ""),
                    trio_ped.get("mother", ""),
                    trio_ped.get("child", ""),
                ]

            else:
                log.debug("TRIO pedigree not defined. Take the first 3 samples")
                samples_list = self.get_header_sample_list()
                if len(samples_list) >= 3:
                    trio_samples = self.get_header_sample_list()[0:3]
                    trio_ped = {
                        "father": trio_samples[0],
                        "mother": trio_samples[1],
                        "child": trio_samples[2],
                    }
                else:
                    msg_error = f"Error in TRIO pedigree: only {len(samples_list)} samples {samples_list}"
                    log.error(msg_error)
                    raise ValueError(msg_error)

            # Check trio pedigree
            if not trio_ped or len(trio_ped) != 3:
                msg_error = f"Error in TRIO pedigree: {trio_ped}"
                log.error(msg_error)
                raise ValueError(msg_error)

            # Log
            log.info(
                f"Calculation 'TRIO' - Samples: "
                + ", ".join([f"{member}='{trio_ped[member]}'" for member in trio_ped])
            )

            # Field
            trio_infos = prefix + trio_tag

            # Variants table
            table_variants = self.get_table_variants()

            # Header
            vcf_reader = self.get_header()

            # Create variant id
            variant_id_column = self.get_variant_id_column()
            added_columns = [variant_id_column]

            # variant_id, FORMAT and samples
            samples_fields = f" {variant_id_column}, FORMAT , " + " , ".join(
                [f""" "{sample}" """ for sample in self.get_header_sample_list()]
            )

            # Create dataframe
            dataframe_trio = self.get_query_to_df(
                f""" SELECT {samples_fields} FROM {table_variants} """
            )

            # Create trio column
            dataframe_trio[trio_infos] = dataframe_trio.apply(
                lambda row: trio(row, samples=trio_samples), axis=1
            )

            # Add trio to header
            vcf_reader.infos[trio_tag] = vcf.parser._Info(
                trio_tag,
                ".",
                "String",
                vcf_infos_tags.get(trio_tag, "snpEff hgvs annotations"),
                "howard calculation",
                "0",
                self.code_type_map.get("String"),
            )

            # Update
            sql_update = f"""
                UPDATE {table_variants}
                SET "INFO" = 
                    concat(
                        CASE
                            WHEN "INFO" IS NULL OR "INFO" IN ('','.')
                            THEN ''
                            ELSE concat("INFO", ';')
                        END,
                        CASE
                            WHEN dataframe_trio."{trio_infos}" NOT IN ('','.')
                             AND dataframe_trio."{trio_infos}" NOT NULL
                            THEN concat(
                                    '{trio_tag}=',
                                    dataframe_trio."{trio_infos}"
                                )
                            ELSE ''
                        END
                    )
                FROM dataframe_trio
                WHERE {table_variants}."{variant_id_column}" = dataframe_trio."{variant_id_column}"
            """
            self.conn.execute(sql_update)

            # Remove added columns
            for added_column in added_columns:
                self.drop_column(column=added_column)

            # Delete dataframe
            del dataframe_trio
            gc.collect()

    def calculation_vaf_normalization(self) -> None:
        """
        The `calculation_vaf_normalization` function calculates the VAF (Variant Allele Frequency)
        normalization for each sample in a VCF file and updates the FORMAT and INFO fields accordingly.
        :return: The function does not return anything.
        """

        # if FORMAT and samples
        if (
            "FORMAT" in self.get_header_columns_as_list()
            and self.get_header_sample_list()
        ):

            # vaf_normalization annotation field
            vaf_normalization_tag = "VAF"

            # VCF infos tags
            vcf_infos_tags = {
                "VAF": "VAF Variant Frequency",
            }

            # Prefix
            prefix = self.get_explode_infos_prefix()

            # Variants table
            table_variants = self.get_table_variants()

            # Header
            vcf_reader = self.get_header()

            # Do not calculate if VAF already exists
            if "VAF" in vcf_reader.formats:
                log.debug("VAF already on genotypes")
                return

            # Create variant id
            variant_id_column = self.get_variant_id_column()
            added_columns = [variant_id_column]

            # variant_id, FORMAT and samples
            samples_fields = f" {variant_id_column}, FORMAT , " + " , ".join(
                f""" "{sample}" """ for sample in self.get_header_sample_list()
            )

            # Create dataframe
            query = f""" SELECT {variant_id_column}, FORMAT, {samples_fields} FROM {table_variants} """
            log.debug(f"query={query}")
            dataframe_vaf_normalization = self.get_query_to_df(query=query)

            vaf_normalization_set = []

            # for each sample vaf_normalization
            for sample in self.get_header_sample_list():
                dataframe_vaf_normalization[sample] = dataframe_vaf_normalization.apply(
                    lambda row: vaf_normalization(row, sample=sample), axis=1
                )
                vaf_normalization_set.append(
                    f""" "{sample}" = dataframe_vaf_normalization."{sample}" """
                )

            # Add VAF to FORMAT
            dataframe_vaf_normalization["FORMAT"] = dataframe_vaf_normalization[
                "FORMAT"
            ].apply(lambda x: str(x) + ":VAF")
            vaf_normalization_set.append(
                f""" "FORMAT" = dataframe_vaf_normalization."FORMAT" """
            )

            # Add vaf_normalization to header
            vcf_reader.formats[vaf_normalization_tag] = vcf.parser._Format(
                id=vaf_normalization_tag,
                num=1,
                type="Float",
                desc=vcf_infos_tags.get(vaf_normalization_tag, "VAF Variant Frequency"),
                type_code=self.code_type_map.get("Float"),
            )

            # Create fields to add in INFO
            sql_vaf_normalization_set = " , ".join(vaf_normalization_set)

            # Update
            sql_update = f"""
                UPDATE {table_variants}
                SET {sql_vaf_normalization_set}
                FROM dataframe_vaf_normalization
                WHERE variants."{variant_id_column}" = dataframe_vaf_normalization."{variant_id_column}"

            """
            self.conn.execute(sql_update)

            # Remove added columns
            for added_column in added_columns:
                self.drop_column(column=added_column)

            # Delete dataframe
            del dataframe_vaf_normalization
            gc.collect()

    def calculation_genotype_stats(self, info: str = "VAF") -> None:
        """
        The `calculation_genotype_stats` function calculates genotype statistics for a given information
        field in a VCF file and updates the INFO column of the variants table with the calculated
        statistics.

        :param info: The `info` parameter is a string that represents the type of information for which
        genotype statistics are calculated. It is used to generate various VCF info tags for the
        statistics, such as the number of occurrences, the list of values, the minimum value, the
        maximum value, the mean, the median, defaults to VAF
        :type info: str (optional)
        """

        # if FORMAT and samples
        if (
            "FORMAT" in self.get_header_columns_as_list()
            and self.get_header_sample_list()
        ):

            # vaf_stats annotation field
            vaf_stats_tag = info + "_stats"

            # VCF infos tags
            vcf_infos_tags = {
                info + "_stats_nb": f"genotype {info} Statistics - number of {info}",
                info + "_stats_list": f"genotype {info} Statistics - list of {info}",
                info + "_stats_min": f"genotype {info} Statistics - min {info}",
                info + "_stats_max": f"genotype {info} Statistics - max {info}",
                info + "_stats_mean": f"genotype {info} Statistics - mean {info}",
                info + "_stats_mediane": f"genotype {info} Statistics - mediane {info}",
                info
                + "_stats_stdev": f"genotype {info} Statistics - standard deviation {info}",
            }

            # Prefix
            prefix = self.get_explode_infos_prefix()

            # Field
            vaf_stats_infos = prefix + vaf_stats_tag

            # Variants table
            table_variants = self.get_table_variants()

            # Header
            vcf_reader = self.get_header()

            # Create variant id
            variant_id_column = self.get_variant_id_column()
            added_columns = [variant_id_column]

            # variant_id, FORMAT and samples
            samples_fields = f" {variant_id_column}, FORMAT , " + " , ".join(
                [f""" "{sample}" """ for sample in self.get_header_sample_list()]
            )

            # Create dataframe
            dataframe_vaf_stats = self.get_query_to_df(
                f""" SELECT {samples_fields} FROM {table_variants} """
            )

            # Create vaf_stats column
            dataframe_vaf_stats[vaf_stats_infos] = dataframe_vaf_stats.apply(
                lambda row: genotype_stats(
                    row, samples=self.get_header_sample_list(), info=info
                ),
                axis=1,
            )

            # List of vcf tags
            sql_vaf_stats_fields = []

            # Check all VAF stats infos
            for stat in vcf_infos_tags:

                # Extract stats
                dataframe_vaf_stats[stat] = dataframe_vaf_stats[vaf_stats_infos].apply(
                    lambda x: dict(x).get(stat, "")
                )

                # Add snpeff_hgvs to header
                vcf_reader.infos[stat] = vcf.parser._Info(
                    stat,
                    ".",
                    "String",
                    vcf_infos_tags.get(stat, "genotype statistics"),
                    "howard calculation",
                    "0",
                    self.code_type_map.get("String"),
                )

                if len(sql_vaf_stats_fields):
                    sep = ";"
                else:
                    sep = ""

                # Create fields to add in INFO
                sql_vaf_stats_fields.append(
                    f"""
                        CASE
                            WHEN dataframe_vaf_stats."{stat}" NOT NULL
                            THEN concat(
                                    '{sep}{stat}=',
                                    dataframe_vaf_stats."{stat}"
                                )
                            ELSE ''
                        END
                    """
                )

            # SQL set for update
            sql_vaf_stats_fields_set = ",  ".join(sql_vaf_stats_fields)

            # Update
            sql_update = f"""
                UPDATE {table_variants}
                SET "INFO" = 
                    concat(
                        CASE
                            WHEN "INFO" IS NULL OR "INFO" IN ('','.')
                            THEN ''
                            ELSE concat("INFO", ';')
                        END,
                        {sql_vaf_stats_fields_set}
                    )
                FROM dataframe_vaf_stats
                WHERE {table_variants}."{variant_id_column}" = dataframe_vaf_stats."{variant_id_column}"

            """
            self.conn.execute(sql_update)

            # Remove added columns
            for added_column in added_columns:
                self.drop_column(column=added_column)

            # Delete dataframe
            del dataframe_vaf_stats
            gc.collect()

    def calculation_transcripts_annotation(
        self, info_json: str = None, info_format: str = None
    ) -> None:
        """
        The `calculation_transcripts_annotation` function creates a transcripts table and adds an info
        field to it if transcripts are available.

        :param info_json: The `info_json` parameter in the `calculation_transcripts_annotation` method
        is a string parameter that represents the information field to be used in the transcripts JSON.
        It is used to specify the JSON format for the transcripts information. If no value is provided
        when calling the method, it defaults to "
        :type info_json: str
        :param info_format: The `info_format` parameter in the `calculation_transcripts_annotation`
        method is a string parameter that specifies the format of the information field to be used in
        the transcripts JSON. It is used to define the format of the information field
        :type info_format: str
        """

        # Create transcripts table
        transcripts_table = self.create_transcript_view()

        # Add info field
        if transcripts_table:
            self.transcript_view_to_variants(
                transcripts_table=transcripts_table,
                transcripts_info_field_json=info_json,
                transcripts_info_field_format=info_format,
            )
        else:
            log.info("No Transcripts to process. Check param.json file configuration")

    def calculation_transcripts_prioritization(self, strict: bool = False) -> None:
        """
        The function `calculation_transcripts_prioritization` creates a transcripts table and
        prioritizes transcripts based on certain criteria.
        """

        # Create transcripts table
        transcripts_table = self.create_transcript_view()

        # Add info field
        if transcripts_table:
            self.transcripts_prioritization(
                transcripts_table=transcripts_table, strict=strict
            )
        else:
            log.info("No Transcripts to process. Check param.json file configuration")

    def calculation_transcripts_export(self) -> None:
        """ """

        # Create transcripts table
        transcripts_table = self.create_transcript_view()

        # Add info field
        if transcripts_table:
            self.transcripts_export(transcripts_table=transcripts_table)
        else:
            log.info("No Transcripts to process. Check param.json file configuration")

    ###############
    # Transcripts #
    ###############

    # Transcripts view creation

    def create_transcript_view(
        self,
        transcripts_table: str = None,
        transcripts_table_drop: bool = False,
        param: dict = {},
    ) -> str:
        """
        Generates a transcript view by processing data from a specified table based on provided parameters and structural information.

        Args:
            transcripts_table (str, optional): The name of the table that will store the final transcript view data.
                If not provided, the function will create a new table to store the transcript view data. Defaults to "transcripts".
            transcripts_table_drop (bool, optional): Determines whether to drop the existing transcripts table before creating a new one.
                If set to True, the function will drop the existing transcripts table if it exists. Defaults to False.
            param (dict, optional): A dictionary that contains information needed to create a transcript view.
                It includes details such as the structure of the transcripts, columns mapping, column formats, and other necessary information
                for generating the view. This parameter allows for flexibility and customization.

        Returns:
            str: The name of the transcripts table that was created or modified during the execution of the function.
        """

        log.info("Transcripts view creation")

        # Default
        transcripts_table_default = "transcripts"

        # Param
        if not param:
            param = self.get_param()

        # Struct
        struct = param.get("transcripts", {}).get("struct", None)

        # Transcript veresion
        transcript_id_remove_version = param.get("transcripts", {}).get(
            "transcript_id_remove_version", False
        )

        # Transcripts mapping
        transcript_id_mapping_file = param.get("transcripts", {}).get(
            "transcript_id_mapping_file", None
        )

        # Transcripts mapping
        transcript_id_mapping_force = param.get("transcripts", {}).get(
            "transcript_id_mapping_force", None
        )

        # Transcripts table
        if transcripts_table is None:
            transcripts_table = param.get("transcripts", {}).get(
                "table", transcripts_table_default
            )

        # Check transcripts table exists
        if transcripts_table:

            # Query to check if transcripts table exists
            query_check_table = f"""
                SELECT * 
                FROM information_schema.tables 
                WHERE table_name = '{transcripts_table}'
            """
            df_check_table = self.get_query_to_df(query=query_check_table)

            # Check if transcripts table exists
            if len(df_check_table) > 0 and not transcripts_table_drop:
                log.debug(f"Table {transcripts_table} exists and not drop option")
                log.info("Transcripts view creation - already exists")
                return transcripts_table

        # Variants table
        variants_table = self.get_table_variants()

        if struct:

            # added_columns
            added_columns = []

            # Temporary tables
            temporary_tables = []
            temporary_intermediate_tables = []

            # Annotation fields
            annotation_fields = []

            # Annotation fields
            annotation_fields_type = {}

            # from columns map
            # temporary_tables and annotation_fields are appended within the function
            log.info("Transcripts view creation - Annotations mapping...")
            columns_maps = struct.get("from_columns_map", [])
            (
                added_columns_tmp,
                temporary_tables_tmp,
                temporary_intermediate_tables_tmp,
                annotation_fields_tmp,
                annotation_fields_type_tmp,
            ) = self.create_transcript_view_from_columns_map(
                transcripts_table=transcripts_table,
                columns_maps=columns_maps,
                added_columns=added_columns,
                temporary_tables=temporary_tables,
                annotation_fields=annotation_fields,
            )

            # Append temporary tables infos
            added_columns += added_columns_tmp
            temporary_intermediate_tables += temporary_intermediate_tables_tmp
            for field in annotation_fields_type_tmp:
                field_type = annotation_fields_type_tmp.get(field, "VARCHAR")
                annotation_fields_type[field] = field_type

            # from column format
            # temporary_tables and annotation_fields are appended within the function
            log.info("Transcripts view creation - Annotations in format field...")
            column_formats = struct.get("from_column_format", [])
            (
                added_columns,
                temporary_tables_tmp,
                annotation_fields_tmp,
                added_columns_type_list,
            ) = self.create_transcript_view_from_column_format(
                transcripts_table=transcripts_table,
                column_formats=column_formats,
                temporary_tables=temporary_tables,
                view_type="table",
                annotation_fields=annotation_fields,
            )

            # Append temporary tables infos
            added_columns += added_columns_tmp
            for field in added_columns_type_list:
                annotation_fields_type[field] = added_columns_type_list.get(
                    field, "VARCHAR"
                )

            # Remove some specific fields/column
            annotation_fields = list(set(annotation_fields))
            for field in ["#CHROM", "POS", "REF", "ALT", "INFO", "transcript"]:
                if field in annotation_fields:
                    annotation_fields.remove(field)

            # Merge temporary tables query
            query_merge = ""
            for temporary_table in list(set(temporary_tables)):

                # First temporary table
                if not query_merge:
                    query_merge = f"""
                        SELECT * FROM {temporary_table}
                    """
                # other temporary table (using UNION)
                else:
                    query_merge += f"""
                        UNION BY NAME SELECT * FROM {temporary_table}
                    """

            # Create final merge query with transcript handling
            # Field transcript as None or '' to 'UNKNOWN' prevent issues with group by and association of variants with all avaialble transcripts
            # A field 'transcript_1' will be created within this table
            query_merge = f"""
                SELECT CASE WHEN "transcript" IS NULL THEN 'UNKNOWN' ELSE "transcript" END AS "transcript", *
                FROM ({query_merge}) AS transcripts_merged
            """

            # transcript table tmp
            transcript_table_tmp = "transcripts_tmp"
            transcript_table_tmp2 = "transcripts_tmp2"
            transcript_table_tmp3 = "transcripts_tmp3"

            # Merge on transcript
            query_merge_on_transcripts_annotation_fields = []

            # Add transcript list
            query_merge_on_transcripts_annotation_fields.append(
                f""" list_aggregate(list_distinct(array_agg({transcript_table_tmp}.transcript)), 'string_agg', ',') AS transcript_list """
            )

            # Aggregate all annotations fields
            for annotation_field in set(annotation_fields):

                # Annotation field type
                annotation_field_type = "VARCHAR"

                # Aggregate field
                query_merge_on_transcripts_annotation_fields.append(
                    f""" list_aggregate(list_distinct(array_agg({transcript_table_tmp}.{annotation_field})), 'string_agg', ',')::{annotation_field_type} AS {annotation_field} """
                )

            # Transcripts mapping
            if transcript_id_mapping_file:

                # Transcript dataframe
                transcript_id_mapping_dataframe_name = "transcript_id_mapping_dataframe"
                transcript_id_mapping_dataframe = transcripts_file_to_df(
                    transcript_id_mapping_file, column_names=["transcript", "alias"]
                )

                # Transcript version remove
                if transcript_id_remove_version:
                    query_transcript_column_select = f"split_part({transcript_table_tmp}.transcript, '.', 1) AS transcript_original, split_part({transcript_id_mapping_dataframe_name}.transcript, '.', 1) AS transcript_mapped"
                    query_transcript_column_group_by = f"split_part({transcript_table_tmp}.transcript, '.', 1), split_part({transcript_id_mapping_dataframe_name}.transcript, '.', 1)"
                    query_left_join = f"""
                        LEFT JOIN {transcript_id_mapping_dataframe_name} ON (split_part({transcript_id_mapping_dataframe_name}.alias, '.', 1)=split_part({transcript_table_tmp}.transcript, '.', 1))
                    """
                else:
                    query_transcript_column_select = f"{transcript_table_tmp}.transcript AS transcript_original, {transcript_id_mapping_dataframe_name}.transcript AS transcript_mapped"
                    query_transcript_column_group_by = f"{transcript_table_tmp}.transcript, {transcript_id_mapping_dataframe_name}.transcript"
                    query_left_join = f"""
                        LEFT JOIN {transcript_id_mapping_dataframe_name} ON (split_part({transcript_id_mapping_dataframe_name}.alias, '.', 1)=split_part({transcript_table_tmp}.transcript, '.', 1))
                    """

                # Transcript column for group by merge
                query_transcript_merge_group_by = """
                        CASE
                            WHEN transcript_mapped NOT IN ('')
                            THEN split_part(transcript_mapped, '.', 1)
                            ELSE split_part(transcript_original, '.', 1)
                        END
                    """

                # Merge query
                transcripts_tmp2_query = f"""
                    SELECT "#CHROM", POS, REF, ALT, {query_transcript_column_select}, {", ".join(query_merge_on_transcripts_annotation_fields)}
                    FROM ({query_merge}) AS {transcript_table_tmp}
                    {query_left_join}
                    GROUP BY "#CHROM", POS, REF, ALT, {query_transcript_column_group_by}
                """

                # Retrive columns after mege
                transcripts_tmp2_describe_query = f"""
                    DESCRIBE {transcripts_tmp2_query}
                """
                transcripts_tmp2_describe_list = list(
                    self.get_query_to_df(query=transcripts_tmp2_describe_query)[
                        "column_name"
                    ]
                )

                # Create list of columns for select clause
                transcripts_tmp2_describe_select_clause = []
                for field in transcripts_tmp2_describe_list:
                    if field not in [
                        "#CHROM",
                        "POS",
                        "REF",
                        "ALT",
                        "INFO",
                        "transcript_mapped",
                    ]:
                        as_field = field
                        if field in ["transcript_original"]:
                            as_field = "transcripts_mapped"
                        transcripts_tmp2_describe_select_clause.append(
                            f""" list_aggregate(list_distinct(array_agg({transcript_table_tmp2}.{field})), 'string_agg', ',') AS {as_field} """
                        )

                # Merge with mapping
                query_merge_on_transcripts = f"""
                    SELECT "#CHROM", POS, REF, ALT, '' AS INFO,
                        CASE
                            WHEN ANY_VALUE(transcript_mapped) NOT IN ('')
                            THEN ANY_VALUE(transcript_mapped)
                            ELSE ANY_VALUE(transcript_original)
                        END AS transcript,
                        {", ".join(transcripts_tmp2_describe_select_clause)}
                    FROM ({transcripts_tmp2_query}) AS {transcript_table_tmp2}
                    GROUP BY "#CHROM", POS, REF, ALT,
                        {query_transcript_merge_group_by}
                """

                # Add transcript filter from mapping file
                if transcript_id_mapping_force:
                    query_merge_on_transcripts = f"""
                        SELECT *
                        FROM ({query_merge_on_transcripts}) AS {transcript_table_tmp3}
                        WHERE split_part({transcript_table_tmp3}.transcript, '.', 1) in (SELECT split_part(transcript, '.', 1) FROM transcript_id_mapping_dataframe)
                    """

            # No transcript mapping
            else:

                # Remove transcript version
                if transcript_id_remove_version:
                    query_transcript_column = f"""
                        split_part({transcript_table_tmp}.transcript, '.', 1)
                    """
                else:
                    query_transcript_column = """
                        transcript
                    """

                # Query sections
                query_transcript_column_select = (
                    f"{query_transcript_column} AS transcript"
                )
                query_transcript_column_group_by = query_transcript_column

                # Query for transcripts view
                query_merge_on_transcripts = f"""
                    SELECT "#CHROM", POS, REF, ALT, '' AS INFO, {query_transcript_column} AS transcript, NULL AS transcript_mapped, {", ".join(query_merge_on_transcripts_annotation_fields)}
                    FROM ({query_merge}) AS {transcript_table_tmp}
                    GROUP BY "#CHROM", POS, REF, ALT, {query_transcript_column}
                """

            # Drop transcript view is necessary
            if transcripts_table_drop:
                query_drop = f"""
                    DROP TABLE IF EXISTS {transcripts_table};
                """
                self.execute_query(query=query_drop)

            # Log
            log.info(f"Transcripts view creation - Create view...")

            # Create table with structure but without data, if not exists
            query_create_table = f"""
                CREATE TABLE IF NOT EXISTS {transcripts_table} AS
                SELECT * FROM ({query_merge_on_transcripts}) LIMIT 0
            """
            self.execute_query(query=query_create_table)

            # Evaluate block size
            batch_split = self.get_batch_split()

            # Insert by batch
            for batch_index in range(batch_split):
                # where clause
                if batch_split > 1:
                    where_clause = f" WHERE (POS % {batch_split}) = {batch_index} "
                else:
                    where_clause = ""
                # Insert data
                query_insert_chunk = f"""
                    INSERT INTO {transcripts_table}
                    SELECT * FROM ({query_merge_on_transcripts})
                    {where_clause}
                """
                # Log
                log.debug(
                    f"Transcripts view creation - Insert batch [{batch_index+1}/{batch_split}]..."
                )
                # Execute
                self.execute_query(query=query_insert_chunk)

            # Extract annotations from variants

            # Columns from variants parameters
            columns_from_variants = struct.get("from_variants", {})
            columns_from_variants_prefix = columns_from_variants.get("prefix", "")
            columns_from_variants_fields = columns_from_variants.get("fields", [])
            columns_from_variants_info = columns_from_variants.get("INFO", False)

            # Columns from variants processing
            if len(columns_from_variants):
                log.info(
                    "Transcripts view creation - Extract annotations from variants"
                )

                # Add INFO column from variants table
                if columns_from_variants_info:
                    query_update_info_column = f"""
                        UPDATE {transcripts_table}
                        SET "INFO" = {variants_table}."INFO"
                        FROM {variants_table}
                        WHERE {transcripts_table}."#CHROM" = {variants_table}."#CHROM"
                        AND {transcripts_table}."POS" = {variants_table}."POS"
                        AND {transcripts_table}."REF" = {variants_table}."REF"
                        AND {transcripts_table}."ALT" = {variants_table}."ALT"
                    """
                    # log.debug(f"query_update_info_column={query_update_info_column}")
                    log.info(
                        "Transcripts view creation - Extract annotations from variants - All INFO column..."
                    )
                    self.execute_query(query=query_update_info_column)

                # Add columns from variants table as exploded from a list of fields
                if len(columns_from_variants_fields) > 0:
                    log.info(
                        f"Transcripts view creation - Extract annotations from variants - Extract {len(columns_from_variants_fields)} fields..."
                    )
                    fields_exploded = self.explode_infos(
                        fields=columns_from_variants_fields,
                        prefix=columns_from_variants_prefix,
                        table_source=variants_table,
                        table_dest=transcripts_table,
                        table_key=["#CHROM", "POS", "REF", "ALT"],
                        proccess_all_fields_together=True,
                        fields_not_exists=False,
                        fields_forced_as_varchar=False,
                    )
                    log.debug(
                        f"Transcripts view creation - Extract annotations from variants - Extract {len(columns_from_variants_fields)} fields: {fields_exploded}"
                    )

            # Remove temporary tables
            self.remove_tables_or_views(
                tables=temporary_tables + temporary_intermediate_tables
            )

            # Remove added columns
            for added_column in added_columns:
                self.drop_column(column=added_column)

        else:

            transcripts_table = None

        return transcripts_table

    def create_transcript_view_from_columns_map(
        self,
        transcripts_table: str = "transcripts",
        columns_maps: dict = {},
        added_columns: list = [],
        temporary_tables: list = None,
        annotation_fields: list = None,
        column_rename: dict = {},
        column_clean: bool = False,
        column_case: str = None,
    ) -> tuple[list, list, list]:
        """
        Generates a temporary table view based on specified columns mapping for transcripts data.

        Args:
            transcripts_table (str, optional): The name of the table where the transcripts data is stored or will be stored in the database.
                This table typically contains information about transcripts such as Ensembl transcript IDs, gene names, scores, predictions, etc.
                Defaults to "transcripts".
            columns_maps (dict): A dictionary that contains information about how to map columns from a transcripts table to create a view.
                Each entry in the dictionary represents a mapping configuration for a specific set of columns.
            added_columns (list): A list that stores the additional columns that will be added to the view being created based on the columns map provided.
                These columns are generated by exploding the transcript information columns along with the main transcript column.
            temporary_tables (list, optional): A list that stores the names of temporary tables created during the process of creating a transcript view from a columns map.
                These temporary tables are used to store intermediate results or transformations before the final view is generated.
            annotation_fields (list, optional): A list that stores the fields that are used for annotation in the query view creation process.
                These fields are extracted from the `transcripts_column` and `transcripts_infos_columns` specified in the `columns_maps`.
            column_rename (dict, optional): A dictionary that allows you to specify custom renaming for columns during the creation of the temporary table view.
                This parameter provides a mapping of original column names to the desired renamed column names.
            column_clean (bool, optional): A boolean flag that determines whether the column values should be cleaned or not.
                If set to `True`, the column values will be cleaned by removing any non-alphanumeric characters from them. Defaults to False.
            column_case (str, optional): Specifies the case transformation to be applied to the columns during the view creation process.
                It allows you to control whether the column values should be converted to lowercase, uppercase, or remain unchanged.

        Returns:
            tuple[list, list, list]: The function returns a tuple containing three lists: `added_columns`, `temporary_tables`, and `annotation_fields`.
        """

        log.debug("Start transcrpts view creation from columns map...")

        # "from_columns_map": [
        #     {
        #         "transcripts_column": "Ensembl_transcriptid",
        #         "transcripts_infos_columns": [
        #             "genename",
        #             "Ensembl_geneid",
        #             "LIST_S2_score",
        #             "LIST_S2_pred",
        #         ],
        #     },
        #     {
        #         "transcripts_column": "Ensembl_transcriptid",
        #         "transcripts_infos_columns": [
        #             "genename",
        #             "VARITY_R_score",
        #             "Aloft_pred",
        #         ],
        #     },
        # ],

        # Init
        if temporary_tables is None:
            temporary_tables = []
        if annotation_fields is None:
            annotation_fields = []

        # Init
        annotation_fields_type = {}
        temporary_intermediate_tables = []

        # Variants table
        table_variants = self.get_table_variants()

        for columns_map in columns_maps:

            # Log
            log.debug(f"columns_map={columns_map}")

            # Transcript column
            transcripts_column = columns_map.get("transcripts_column", None)

            # Transcripts infos columns
            transcripts_infos_columns = columns_map.get("transcripts_infos_columns", [])

            # Transcripts infos columns rename
            column_rename = columns_map.get("column_rename", column_rename)

            # Transcripts infos columns clean
            column_clean = columns_map.get("column_clean", column_clean)

            # Transcripts infos columns case
            column_case = columns_map.get("column_case", column_case)

            if transcripts_column is not None:

                table_for_view = table_variants

                annotation_view_name_for_type = None

                if True:

                    # Create annotations view
                    annotation_view_name = (
                        table_variants
                        + "_view_"
                        + "".join(
                            random.choices(string.ascii_uppercase + string.digits, k=10)
                        )
                    )
                    annotation_view_fields = [
                        transcripts_column
                    ] + transcripts_infos_columns
                    annotation_view_name = self.create_annotations_view(
                        table=table_variants,
                        view=annotation_view_name,
                        view_type="table",
                        view_mode="full",
                        info_prefix_column="",
                        detect_type_list=False,
                        fields=annotation_view_fields,
                        fields_not_exists=True,
                        fields_forced_as_varchar=True,
                        fields_needed_all=False,
                    )
                    temporary_intermediate_tables.append(annotation_view_name)
                    table_for_view = annotation_view_name

                    # Create annotation view for field type
                    annotation_view_name_for_type = self.create_annotations_view(
                        table=table_variants,
                        view=annotation_view_name + "for_type",
                        view_type="view",
                        view_mode="full",
                        info_prefix_column="",
                        detect_type_list=True,
                        fields=annotation_view_fields,
                        fields_not_exists=True,
                        fields_needed_all=False,
                    )
                    temporary_intermediate_tables.append(annotation_view_name_for_type)

                # View clauses
                clause_select_variants = []
                clause_select_tanscripts = []
                for field in [transcripts_column] + transcripts_infos_columns:

                    # AS field
                    as_field = field

                    # Rename
                    if column_rename:
                        as_field = column_rename.get(as_field, as_field)

                    # Clean
                    if column_clean:
                        as_field = clean_annotation_field(as_field)

                    # Case
                    if column_case:
                        if column_case.lower() in ["lower"]:
                            as_field = as_field.lower()
                        elif column_case.lower() in ["upper"]:
                            as_field = as_field.upper()

                    # Field Type
                    if annotation_view_name_for_type:
                        field_type = self.get_query_to_df(
                            f""" 
                                    SELECT column_type
                                    FROM (
                                        DESCRIBE {annotation_view_name_for_type}
                                    )
                                    WHERE column_name == '{field}'
                                """
                        )["column_type"][0].replace("[]", "")

                        # If field type is "NULL" due to no data
                        if field_type == '"NULL"':
                            field_type = "VARCHAR"
                    else:
                        field_type = "VARCHAR"

                    # Clause select Variants
                    clause_select_variants.append(
                        f""" TRY_CAST(regexp_split_to_table(CAST("{field}" AS VARCHAR), ',') AS VARCHAR) AS '{field}' """
                    )

                    # Clause select Transcripts
                    if field in [transcripts_column]:
                        clause_select_tanscripts.append(
                            f""" TRY_CAST(regexp_split_to_table("{field}", ',') AS {field_type}) AS '{field}' """
                        )
                    else:
                        clause_select_tanscripts.append(
                            f""" TRY_CAST(regexp_split_to_table("{field}", ',') AS {field_type}) AS '{as_field}' """
                        )
                        annotation_fields.append(as_field)
                        annotation_fields_type[as_field] = field_type

                # Query View
                query = f""" 
                    SELECT
                        "#CHROM", POS, REF, ALT,
                        "{transcripts_column}" AS 'transcript',
                        {", ".join(clause_select_tanscripts)}
                    FROM (
                        SELECT 
                            "#CHROM", POS, REF, ALT,
                            {", ".join(clause_select_variants)}
                        FROM {table_for_view}
                        )
                    WHERE "{transcripts_column}" IS NOT NULL
                """

                # Create temporary table
                temporary_table = transcripts_table + "".join(
                    random.choices(string.ascii_uppercase + string.digits, k=10)
                )

                # Temporary view
                temporary_tables.append(temporary_table)
                query_view = f"""
                    CREATE view {temporary_table}
                    AS ({query})
                """
                # log.debug(f"Create view:{query_view}")
                self.execute_query(query=query_view)

        return (
            added_columns,
            temporary_tables,
            temporary_intermediate_tables,
            annotation_fields,
            annotation_fields_type,
        )

    def create_transcript_view_from_column_format(
        self,
        transcripts_table: str = "transcripts",
        column_formats: dict = {},
        temporary_tables: list = None,
        annotation_fields: list = None,
        column_rename: dict = {},
        view_type: str = "view",
        column_clean: bool = False,
        column_case: str = None,
    ) -> tuple[list, list, list]:
        """
        Generates a transcript view based on specified column formats, adds additional columns and annotation fields,
        and returns the list of temporary tables and annotation fields.

        Args:
            transcripts_table (str, optional): The name of the table containing the transcripts data. This table will be used
                as the base table for creating the transcript view. Defaults to "transcripts".
            column_formats (dict): A dictionary that contains information about the columns to be used for creating the transcript view.
                Each entry in the dictionary specifies the mapping between a transcripts column and a transcripts infos column.
            temporary_tables (list, optional): A list that stores the names of temporary views created during the process of creating
                a transcript view from a column format. These temporary views are used to manipulate and extract data before generating
                the final transcript view.
            annotation_fields (list, optional): A list that stores the annotation fields that are extracted from the temporary views
                created during the process. These annotation fields are obtained by querying the temporary views and extracting the column
                names excluding specific columns.
            column_rename (dict, optional): A dictionary that allows you to specify custom renaming of columns in the transcripts infos table.
                By providing a mapping of original column names to new column names in this dictionary, you can rename specific columns during
                the process.
            view_type (str, optional): The type of the view to be created. Defaults to "view".
            column_clean (bool, optional): A flag that determines whether the transcripts infos columns should undergo a cleaning process.
                If set to True, the columns will be cleaned during the creation of the transcript view based on the specified column format.
                Defaults to False.
            column_case (str, optional): Specifies the case transformation to be applied to the columns in the transcript view.
                It can be set to either "upper" or "lower" to convert the column names to uppercase or lowercase, respectively.

        Returns:
            tuple[list, list, list]: The function returns two lists: `temporary_tables` and `annotation_fields`.
        """

        log.debug("Start transcrpts view creation from column format...")

        #  "from_column_format": [
        #     {
        #         "transcripts_column": "ANN",
        #         "transcripts_infos_column": "Feature_ID",
        #     }
        # ],

        # Init
        if temporary_tables is None:
            temporary_tables = []
        if annotation_fields is None:
            annotation_fields = []

        added_columns = []
        added_columns_type_list = {}

        for column_format in column_formats:

            # annotation field and transcript annotation field
            annotation_field = column_format.get("transcripts_column", "ANN")
            transcript_annotation = column_format.get(
                "transcripts_infos_column", "Feature_ID"
            )

            # Transcripts infos columns rename
            column_rename = column_format.get("column_rename", column_rename)

            # Transcripts infos columns clean
            column_clean = column_format.get("column_clean", column_clean)

            # Transcripts infos columns case
            column_case = column_format.get("column_case", column_case)

            # Temporary View name
            temporary_view_name = transcripts_table + "".join(
                random.choices(string.ascii_uppercase + string.digits, k=10)
            )

            # Create temporary view name
            temporary_view_name, added_columns, added_columns_type = (
                self.annotation_format_to_table(
                    annotation_field=annotation_field,
                    view_name=temporary_view_name,
                    view_type=view_type,
                    annotation_id=transcript_annotation,
                    column_rename=column_rename,
                    column_clean=column_clean,
                    column_case=column_case,
                )
            )

            # columns_types
            for column_type in added_columns_type:
                added_columns_type_list[column_type] = added_columns_type.get(
                    column_type, "VARCHAR"
                )

            # Annotation fields
            if temporary_view_name:
                query_annotation_fields = f"""
                    SELECT *
                    FROM (
                        DESCRIBE SELECT *
                        FROM {temporary_view_name}
                        )
                        WHERE column_name not in ('#CHROM', 'POS', 'REF', 'ALT')
                """
                df_annotation_fields = self.get_query_to_df(
                    query=query_annotation_fields
                )

                # Add temporary view and annotation fields
                temporary_tables.append(temporary_view_name)
                annotation_fields += list(set(df_annotation_fields["column_name"]))

        return (
            added_columns,
            temporary_tables,
            annotation_fields,
            added_columns_type_list,
        )

    def annotation_format_to_table(
        self,
        annotation_field: str = "ANN",
        annotation_id: str = "Feature_ID",
        view_name: str = "transcripts",
        view_type: str = "view",
        column_rename: dict = {},
        column_clean: bool = False,
        column_case: str = None,
        column_split: str = "&",
    ) -> str:
        """
        Converts annotation data from a VCF file into a structured table format, ensuring unique values
        and creating a temporary table for further processing or analysis.

        Args:
            annotation_field (str, optional): The field in the VCF file that contains the annotation information
                for each variant. Defaults to "ANN".
            annotation_id (str, optional): The identifier for the annotation feature, used as a column name
                in the resulting table or view. Defaults to "Feature_ID".
            view_name (str, optional): The name of the temporary table that will be created to store the transformed
                annotation data. Defaults to "transcripts".
            view_type (str, optional): The type of the view to be created. Defaults to "view".
            column_rename (dict, optional): A dictionary to specify custom renaming for columns. By providing key-value
                pairs in this dictionary, you can rename specific columns in the resulting table or view.
            column_clean (bool, optional): A flag to determine whether the annotation field should undergo a cleaning process.
                If set to True, the function will clean the annotation field before further processing. Defaults to False.
            column_case (str, optional): Specifies the case transformation to be applied to the column names extracted from
                the annotation data. It allows you to set the case of the column names to either lowercase or uppercase.
            column_split (str, optional): The separator to split field values. Defaults to "&". Set to None to disable splitting.

        Returns:
            str: The name of the view created, which is stored in the variable `view_name`.
        """

        # annotation_id original name
        annotation_id_original = annotation_id

        # Transcript annotation
        if column_rename:
            annotation_id = column_rename.get(annotation_id, annotation_id)

        if column_clean:
            annotation_id = clean_annotation_field(annotation_id)

        # Prefix
        prefix = self.get_explode_infos_prefix()
        if prefix:
            prefix = "INFO/"

        # Variants table
        table_variants = self.get_table_variants()

        # Header
        vcf_reader = self.get_header()

        # Add columns
        added_columns = []
        added_columns_type = {}

        # If annotation_field exists
        if annotation_field in vcf_reader.infos:

            # Extract ANN header
            ann_description = vcf_reader.infos[annotation_field].desc
            pattern = r"'(.+?)'"
            match = re.search(pattern, ann_description)
            if match:
                ann_header_match = match.group(1).split(" | ")
                ann_header = []
                ann_header_desc = {}
                for i in range(len(ann_header_match)):
                    ann_header_info = "".join(
                        char for char in ann_header_match[i] if char.isalnum()
                    )
                    ann_header.append(ann_header_info)
                    ann_header_desc[ann_header_info] = ann_header_match[i]
                if not ann_header_desc:
                    raise ValueError("Invalid header description format")
            else:
                raise ValueError("Invalid header description format")

            # annotation field pattern
            annotation_field_pattern = rf"(^|;)({annotation_field})=([^;]*)?"

            annotation_fields_for_format = []
            for i, header in enumerate(ann_header_desc.values()):
                if header in [annotation_id_original]:
                    annotation_fields_for_format.append(
                        f"SPLIT_PART(annotation, '|', {i+1}) AS '{header}'"
                    )
                else:
                    annotation_fields_for_format.append(
                        f"string_agg(SPLIT_PART(annotation, '|', {i+1}), ',') AS '{header}'"
                    )

            query = f""" 
                WITH exploded_annotations AS (
                    SELECT
                        "#CHROM", POS, REF, ALT,
                        UNNEST(
                            STRING_SPLIT(
                                regexp_extract("INFO", '{annotation_field_pattern}', 3),
                                ','
                            )
                        ) AS annotation
                    FROM {table_variants}
                ),
                split_annotations AS (
                    SELECT
                        "#CHROM", POS, REF, ALT,
                        {", ".join(annotation_fields_for_format)}
                    FROM exploded_annotations
                    GROUP BY "#CHROM", POS, REF, ALT, "{annotation_id_original}"
                )
                SELECT * FROM split_annotations
                LIMIT 10000
                """
            dataframe_annotation_format = self.get_query_to_df(query=query)

            # Init
            query_list_keys = []
            key_i = 0

            for key in dataframe_annotation_format.keys():

                if key in ann_header_desc.values():

                    # Key
                    key_i += 1
                    key_clean = key

                    # key rename
                    if column_rename:
                        key_clean = column_rename.get(key_clean, key_clean)

                    # key clean
                    if column_clean:
                        key_clean = clean_annotation_field(key_clean)

                    # Key case
                    if column_case:
                        if column_case.lower() in ["lower"]:
                            key_clean = key_clean.lower()
                        elif column_case.lower() in ["upper"]:
                            key_clean = key_clean.upper()

                    # Detect column type
                    column_type = detect_column_type(dataframe_annotation_format[key])
                    added_columns_type[key] = column_type
                    log.debug(f"Field '{key}' type detected: {column_type}")

                    # Append key to list
                    if column_split is not None:
                        query_list_keys.append(
                            f""" TRY_CAST(replace(NULLIF(SPLIT_PART(annotation, '|', {key_i}), ''), '{column_split}', ',') AS {column_type}) AS '{prefix}{key_clean}' """
                        )
                    else:
                        query_list_keys.append(
                            f""" TRY_CAST(NULLIF(SPLIT_PART(annotation, '|', {key_i}), '') AS {column_type}) AS '{prefix}{key_clean}' """
                        )

            # Create temporary table
            query_create_view = f"""
                CREATE {view_type} {view_name} AS (
                    WITH exploded_annotations AS (
                        SELECT
                            "#CHROM", POS, REF, ALT,
                            UNNEST(
                                STRING_SPLIT(
                                    regexp_extract("INFO", '{annotation_field_pattern}', 3),
                                    ','
                                )
                            ) AS annotation
                        FROM {table_variants}
                    ),
                    split_annotations AS (
                        SELECT
                            "#CHROM", POS, REF, ALT,
                            {", ".join(query_list_keys)},
                        FROM exploded_annotations
                    )
                    SELECT *, {annotation_id} AS 'transcript' FROM split_annotations
                )
            """
            # log.debug(f"Create view FORMAT:{query_create_view}")
            self.execute_query(query=query_create_view)

        else:

            # Return None
            view_name = None

        return view_name, added_columns, added_columns_type

    # Transcripts operations
    #######################

    def transcripts_export(
        self,
        transcripts_table: str = None,
        param_export: dict = {},
        param_explode: dict = {},
    ) -> bool:
        """
        Exports transcript data from a table to a specified file, with options for formatting and additional information.

        :param transcripts_table: The name of the transcripts table to export data from. If None, it defaults to "transcripts".
        :type transcripts_table: str, optional
        :param param_export: A dictionary of parameters to customize the export process, such as output file path, header options, etc.
        :type param_export: dict, optional
        :param param_explode: A dictionary of parameters for exploding fields in the transcripts table, such as prefix and fields to explode.
        :type param_explode: dict, optional
        :return: Returns True if the export is successful, False otherwise.
        :rtype: bool
        """

        log.debug("Start transcripts export...")

        # Param
        param = self.get_param()

        # Transcripts table
        if transcripts_table is None:
            transcripts_table = param.get("transcripts", {}).get("table", "transcripts")

        # Param export
        if not param_export:
            param_export = self.get_param().get("transcripts", {}).get("export", {})
        transcripts_export_output = param_export.get("output", None)
        transcripts_export_header = param_export.get("export_header", False)
        transcripts_export_header_in_output = param_export.get(
            "header_in_output", False
        )
        transcripts_export_add_info = param_export.get("add_info", False)

        if not param_export or not transcripts_export_output:
            log.warning(f"No transcriipts export parameters defined!")
            return False

        # Param explode
        if not param_explode:
            param_explode = self.get_param().get("transcripts", {}).get("explode", {})

        # Explode fields
        if param_explode.get("explode_infos_fields", None) and param_explode.get(
            "explode_infos", True
        ):
            self.explode_infos(
                table=transcripts_table,
                prefix=param_explode.get("explode_infos_prefix", None),
                fields=param_explode.get("explode_infos_fields", None),
                force=False,
                fields_forced_as_varchar=False,
            )

        # Create transcripts table description
        query_describe = f"""
            SELECT *
            FROM (
                    DESCRIBE SELECT * FROM {transcripts_table}
                )
            WHERE column_name NOT IN ('#CHROM', 'POS', 'REF', 'ALT', 'INFO')
        """
        result_describe = self.execute_query(query=query_describe)
        description_dict = {
            row[0]: {"type": row[1]} for row in result_describe.fetchall()
        }
        transcripts_annotations_list = list(description_dict.keys())

        transcripts_annotations_list_columns = [
            f'"{field}"' for field in transcripts_annotations_list
        ]

        # Output file format
        transcripts_export_output_format = get_file_format(
            filename=transcripts_export_output
        )

        # Format VCF - construct INFO
        if transcripts_export_output_format in ["vcf"]:

            # Construct query update INFO and header
            query_update_info = []
            for field in transcripts_annotations_list:

                # If field not in header
                if field not in self.get_header_infos_list():

                    # Find previous desc
                    if self.get_header().infos.get(field, None) is not None:
                        field_description = self.get_header().infos.get(field).desc
                        field_number = self.get_header().infos.get(field).num
                        field_type = self.get_header().infos.get(field).type
                    else:
                        field_description = "Unknown annotation"
                        field_number = "."
                        field_type = "String"

                    # Add description about transription prioritization
                    field_description += f". Annotation '{field}' from transcript view"

                    # Add PZ Transcript in header
                    self.get_header().infos[field] = vcf.parser._Info(
                        field,
                        field_number,
                        field_type,
                        field_description,
                        "unknown",
                        "unknown",
                        code_type_map.get(field_type, 0),
                    )

                # Add field as INFO/tag
                column_type = description_dict.get(field, {}).get("type", "VARCHAR")
                if column_type.endswith("[]"):
                    column_type = "VARCHAR"
                    field_value = f""" list_aggregate("{field}", 'string_agg', ',') """
                else:
                    field_value = f""" "{field}" """

                # Add INFO field to query
                query_update_info.append(
                    f"""
                        CASE
                            WHEN "{field}" IS NOT NULL
                            THEN concat(
                                '{field}=',
                                {field_value},
                                ';'
                            )    
                            ELSE ''     
                        END
                        """
                )

            # Query param
            query_update_info_value = f""" regexp_replace(concat('',  {", ".join(query_update_info)}), ';$', '') """
            query_export_columns = f""" "#CHROM", "POS", '.' AS 'ID', "REF", "ALT", '.' AS 'QUAL', '.' AS 'FILTER', "INFO" """

        else:

            # Query param

            if transcripts_export_add_info:
                query_update_info_value = f""" INFO """
                query_export_columns = f""" "#CHROM", "POS", "REF", "ALT", "INFO", {', '.join(transcripts_annotations_list)} """
            else:
                query_update_info_value = f""" NULL """
                query_export_columns = f""" "#CHROM", "POS", "REF", "ALT", {', '.join(transcripts_annotations_list)} """

        # Query export
        query_export = f"""
                SELECT
                {query_export_columns}
                FROM (
                    SELECT "#CHROM", "POS", "REF", "ALT",
                    {query_update_info_value} 
                    AS 'INFO',
                    {', '.join(transcripts_annotations_list_columns)}
                    FROM {transcripts_table}
                    ORDER BY "#CHROM", "POS", "REF", "ALT"
                )
            """

        # Export
        self.export_output(
            output_file=transcripts_export_output,
            query=query_export,
            export_header=transcripts_export_header,
            header_in_output=transcripts_export_header_in_output,
        )

    def transcripts_prioritization(
        self, transcripts_table: str = None, param: dict = {}, strict: bool = False
    ) -> bool:
        """
        Prioritizes transcripts based on specified parameters and updates the variants table with the prioritized information.

        Args:
            transcripts_table (str, optional): The name of the table containing transcripts data. If not provided, it defaults to "transcripts".
                This parameter is used to identify the table where the transcripts data is stored for the prioritization process.
            param (dict, optional): A dictionary containing various configuration settings for the prioritization process of transcripts.
                It is used to customize the behavior of the prioritization algorithm and includes settings such as the prefix for prioritization fields,
                default profiles, and other relevant configurations.
            strict (bool, optional): A flag indicating whether to enforce strict prioritization criteria. Defaults to False.

        Returns:
            bool: True if the transcripts prioritization process is successfully completed, and False if there are any issues or if no profile is defined
                for transcripts prioritization.
        """

        log.debug("Start transcripts prioritization...")

        # Param
        if not param:
            param = self.get_param()

        # Variants table
        table_variants = self.get_table_variants()

        # Transcripts table
        if transcripts_table is None:
            transcripts_table = self.create_transcript_view(
                transcripts_table="transcripts", param=param
            )
        if transcripts_table is None:
            msg_err = "No Transcripts table availalble"
            log.error(msg_err)
            raise ValueError(msg_err)

        # Get transcripts columns
        columns_as_list_query = f"""
            DESCRIBE {transcripts_table}
        """
        columns_as_list = list(
            self.get_query_to_df(columns_as_list_query)["column_name"]
        )

        # Create INFO if not exists
        if "INFO" not in columns_as_list:
            query_add_info = f"""
                ALTER TABLE {transcripts_table} ADD COLUMN INFO STRING DEFAULT '';
            """
            self.execute_query(query_add_info)

        # Prioritization param and Force only PZ Score and Flag
        pz_param = param.get("transcripts", {}).get("prioritization", {})

        # PZ profile by default
        pz_profile_default = (
            param.get("transcripts", {}).get("prioritization", {}).get("profiles", None)
        )

        # Exit if no profile
        if pz_profile_default is None:
            log.warning("No profile defined for transcripts prioritization")
            return False

        # PZ fields
        pz_param_pzfields = {}

        # Order by
        pz_orders = (
            param.get("transcripts", {})
            .get("prioritization", {})
            .get("prioritization_transcripts_order", {})
        )
        if not pz_orders:
            pz_orders = {
                pz_param.get("pzprefix", "PTZ") + "Flag": "DESC",
                pz_param.get("pzprefix", "PTZ") + "Score": "DESC",
            }

        # PZ field transcripts
        pz_fields_transcripts = pz_param.get("pzprefix", "PTZ") + "Transcript"

        # Add description about transription prioritization
        pz_field_transcripts_description = f"Transcript selected from prioritization process, profile {pz_profile_default}"

        # Add PZ Transcript in header
        self.get_header().infos[pz_fields_transcripts] = vcf.parser._Info(
            pz_fields_transcripts,
            1,
            "String",
            pz_field_transcripts_description,
            "HOWARD transcript prioritization",
            "unknown",
            code_type_map.get("String", 0),
        )

        # Mandatory fields if asked in param
        pz_mandatory_fields_list = [
            "Score",
            "Flag",
            "Tags",
            "Comment",
            "Infos",
            "Class",
        ]
        pz_mandatory_fields = []
        for pz_mandatory_field in pz_mandatory_fields_list:
            pz_mandatory_fields.append(
                pz_param.get("pzprefix", "PTZ") + pz_mandatory_field
            )

        # PZ fields in param
        pz_param_mandatory_fields = []
        for pz_field in pz_param.get("pzfields", []):
            if pz_field in pz_mandatory_fields_list:
                pz_param_pzfields[pz_param.get("pzprefix", "PTZ") + pz_field] = (
                    pz_param.get("pzprefix", "PTZ") + pz_field
                )
                pz_param_mandatory_fields.append(
                    pz_param.get("pzprefix", "PTZ") + pz_field
                )
            else:
                pz_field_new = pz_param.get("pzprefix", "PTZ") + pz_field
                pz_param_pzfields[pz_field] = pz_field_new

                # Find previous desc and type
                if self.get_header().infos.get(pz_field, None) is not None:
                    pz_field_new_description = (
                        self.get_header().infos.get(pz_field).desc
                    )
                    pz_field_new_type = self.get_header().infos.get(pz_field).type
                else:
                    pz_field_new_description = "Unknown annotation"
                    pz_field_new_type = "String"

                # Add description about transription prioritization
                pz_field_new_description += f". Annotation '{pz_field}' from transcript selected from prioritization process, profile {pz_profile_default}"

                # Add PZ Transcript in header
                self.get_header().infos[pz_field_new] = vcf.parser._Info(
                    pz_field_new,
                    1,
                    pz_field_new_type,
                    pz_field_new_description,
                    "unknown",
                    "unknown",
                    code_type_map.get(pz_field_new_type, 0),
                )
        # Add order by fields in mandatory fields
        for pz_order in pz_orders:
            if pz_order not in pz_param_mandatory_fields:
                pz_param_mandatory_fields.append(pz_order)

        # PZ fields param
        pz_mandatory_fields = pz_param_mandatory_fields
        pz_param["pzfields"] = pz_mandatory_fields

        # Prioritization
        prioritization_result = self.prioritization(
            table=transcripts_table,
            pz_param=param.get("transcripts", {}).get("prioritization", {}),
            pz_keys=["#CHROM", "POS", "REF", "ALT", "transcript"],
            strict=strict,
        )
        if not prioritization_result:
            log.warning("Transcripts prioritization not processed")
            return False

        log.info(f"Update {table_variants} table with transcripts prioritization...")

        # PZ fields sql query
        query_update_select_list = []
        query_update_concat_list = []
        query_update_order_list = []
        for pz_param_pzfield in set(
            list(pz_param_pzfields.keys()) + pz_mandatory_fields
        ):
            query_update_select_list.append(f" {pz_param_pzfield}, ")

        for pz_param_pzfield in pz_param_pzfields:
            query_update_concat_list.append(
                f"""
                    , CASE 
                        WHEN {pz_param_pzfield} IS NOT NULL
                        THEN concat(';{pz_param_pzfields.get(pz_param_pzfield)}=', {pz_param_pzfield})
                        ELSE ''
                    END
                """
            )

        for pz_order in pz_orders:
            query_update_order_list.append(
                f""" {pz_order} {pz_orders.get(pz_order, "DESC")} """
            )

        # Fields to explode
        fields_to_explode = (
            list(pz_param_pzfields.keys())
            + pz_mandatory_fields
            + list(pz_orders.keys())
        )

        # Remove transcript column as a specific transcript column
        if "transcript" in fields_to_explode:
            fields_to_explode.remove("transcript")

        # Fields intranscripts table
        query_transcripts_table = f"""
            DESCRIBE SELECT * FROM {transcripts_table}
        """
        query_transcripts_table = self.get_query_to_df(query=query_transcripts_table)

        # Check fields to explode
        for field_to_explode in fields_to_explode:
            if field_to_explode not in self.get_header_infos_list() + list(
                query_transcripts_table.column_name
            ):
                msg_err = f"INFO/{field_to_explode} NOT IN header"
                log.error(msg_err)
                raise ValueError(msg_err)

        # Create view as table
        annotation_view_name = "annotation_view_for_transcripts_prioritization_" + str(
            random.randrange(1000000)
        )
        annotation_view_name = self.create_annotations_view(
            table=transcripts_table,
            view=annotation_view_name,
            view_type="table",
            view_mode="explore",
            info_prefix_column="",
            detect_type_list=True,
            fields=fields_to_explode + ["transcript"],
            fields_not_exists=False,
            fields_forced_as_varchar=False,
            fields_needed_all=False,
        )
        transcripts_table = annotation_view_name

        # Transcript preference file
        transcripts_preference_file = (
            param.get("transcripts", {})
            .get("prioritization", {})
            .get("prioritization_transcripts", {})
        )
        transcripts_preference_file = full_path(transcripts_preference_file)

        # Transcript preference forced
        transcript_preference_force = (
            param.get("transcripts", {})
            .get("prioritization", {})
            .get("prioritization_transcripts_force", False)
        )
        # Transcript version forced
        transcript_version_force = (
            param.get("transcripts", {})
            .get("prioritization", {})
            .get("prioritization_transcripts_version_force", False)
        )

        # Transcripts Ranking
        if transcripts_preference_file:

            # Transcripts file to dataframe
            if os.path.exists(transcripts_preference_file):
                transcripts_preference_dataframe = transcripts_file_to_df(
                    transcripts_preference_file
                )
            else:
                log.error(
                    f"Transcript file '{transcripts_preference_file}' does NOT exist"
                )
                raise ValueError(
                    f"Transcript file '{transcripts_preference_file}' does NOT exist"
                )

            # Order by depending to transcript preference forcing
            if transcript_preference_force:
                order_by = f""" transcripts_preference.transcripts_preference_order ASC, {" , ".join(query_update_order_list)} """
            else:
                order_by = f""" {" , ".join(query_update_order_list)}, transcripts_preference.transcripts_preference_order ASC """

            # Transcript columns joined depend on version consideration
            if transcript_version_force:
                transcripts_version_join = f""" {transcripts_table}.transcript = transcripts_preference.transcripts_preference """
            else:
                transcripts_version_join = f""" split_part({transcripts_table}.transcript, '.', 1) = split_part(transcripts_preference.transcripts_preference, '.', 1) """

            # Query ranking for update
            query_update_ranking = f"""
                SELECT
                    "#CHROM", POS, REF, ALT, {transcripts_table}.transcript, {" ".join(query_update_select_list)}
                    ROW_NUMBER() OVER (
                        PARTITION BY "#CHROM", POS, REF, ALT
                        ORDER BY {order_by}
                    ) AS rn
                FROM {transcripts_table}
                LEFT JOIN 
                    (
                        SELECT transcript AS 'transcripts_preference', row_number() OVER () AS transcripts_preference_order
                        FROM transcripts_preference_dataframe
                    ) AS transcripts_preference
                ON {transcripts_version_join}
            """

        else:

            # Query ranking for update
            query_update_ranking = f"""
                SELECT
                    "#CHROM", POS, REF, ALT, transcript, {" ".join(query_update_select_list)}
                    ROW_NUMBER() OVER (
                        PARTITION BY "#CHROM", POS, REF, ALT
                        ORDER BY {" , ".join(query_update_order_list)}
                    ) AS rn
                FROM {transcripts_table}
            """

        # Export Transcripts prioritization infos to variants table
        query_update = f"""
            WITH RankedTranscripts AS (
                {query_update_ranking}
            )
            UPDATE {table_variants}
                SET
                INFO = CONCAT(CASE
                            WHEN "INFO" IS NULL OR "INFO" IN ('','.')
                            THEN ''
                            ELSE concat("INFO", ';')
                        END,
                        concat('{pz_fields_transcripts}=', transcript {" ".join(query_update_concat_list)})
                        )
            FROM
                RankedTranscripts
            WHERE
                rn = 1
                AND variants."#CHROM" = RankedTranscripts."#CHROM"
                AND variants."POS" = RankedTranscripts."POS"
                AND variants."REF" = RankedTranscripts."REF"
                AND variants."ALT" = RankedTranscripts."ALT"     
        """

        # Query update
        self.execute_query(query=query_update)

        # Return
        return True

    def transcript_view_to_variants(
        self,
        transcripts_table: str = None,
        transcripts_column_id: str = None,
        transcripts_info_json: str = None,
        transcripts_info_field_json: str = None,
        transcripts_info_format: str = None,
        transcripts_info_field_format: str = None,
        param: dict = {},
    ) -> bool:
        """
        The `transcript_view_to_variants` function updates a variants table with information from
        transcripts in JSON format.

        :param transcripts_table: The `transcripts_table` parameter is used to specify the name of the
        table containing the transcripts data. If this parameter is not provided, the function will
        attempt to retrieve it from the `param` dictionary or use a default value of "transcripts"
        :type transcripts_table: str
        :param transcripts_column_id: The `transcripts_column_id` parameter is used to specify the
        column in the `transcripts_table` that contains the unique identifier for each transcript. This
        identifier is used to match transcripts with variants in the database
        :type transcripts_column_id: str
        :param transcripts_info_json: The `transcripts_info_json` parameter is used to specify the name
        of the column in the variants table where the transcripts information will be stored in JSON
        format. This parameter allows you to define the column in the variants table that will hold the
        JSON-formatted information about transcripts
        :type transcripts_info_json: str
        :param transcripts_info_field_json: The `transcripts_info_field_json` parameter is used to
        specify the field in the VCF header that will contain information about transcripts in JSON
        format. This field will be added to the VCF header as an INFO field with the specified name
        :type transcripts_info_field_json: str
        :param transcripts_info_format: The `transcripts_info_format` parameter is used to specify the
        format of the information about transcripts that will be stored in the variants table. This
        format can be used to define how the transcript information will be structured or displayed
        within the variants table
        :type transcripts_info_format: str
        :param transcripts_info_field_format: The `transcripts_info_field_format` parameter is used to
        specify the field in the VCF header that will contain information about transcripts in a
        specific format. This field will be added to the VCF header as an INFO field with the specified
        name
        :type transcripts_info_field_format: str
        :param param: The `param` parameter in the `transcript_view_to_variants` method is a dictionary
        that contains various configuration settings related to transcripts. It is used to provide
        default values for certain parameters if they are not explicitly provided when calling the
        method. The `param` dictionary can be passed as an argument
        :type param: dict
        :return: The function `transcript_view_to_variants` returns a boolean value. It returns `True`
        if the operation is successful and `False` if certain conditions are not met.
        """

        msg_info_prefix = "Start transcripts view to variants annotations"

        log.debug(f"{msg_info_prefix}...")

        # Default
        transcripts_table_default = "transcripts"
        transcripts_column_id_default = "transcript"
        transcripts_info_json_default = None
        transcripts_info_format_default = None
        transcripts_info_field_json_default = None
        transcripts_info_field_format_default = None

        # Param
        if not param:
            param = self.get_param()

        # Transcripts table
        if transcripts_table is None:
            transcripts_table = param.get("transcripts", {}).get(
                "table", transcripts_table_default
            )

        # Transcripts column ID
        if transcripts_column_id is None:
            transcripts_column_id = param.get("transcripts", {}).get(
                "column_id", transcripts_column_id_default
            )

        # Transcripts info json
        if transcripts_info_json is None:
            transcripts_info_json = param.get("transcripts", {}).get(
                "transcripts_info_json", transcripts_info_json_default
            )

        # Transcripts info field JSON
        if transcripts_info_field_json is None:
            transcripts_info_field_json = param.get("transcripts", {}).get(
                "transcripts_info_field_json", transcripts_info_field_json_default
            )
        # if transcripts_info_field_json is not None and transcripts_info_json is None:
        #     transcripts_info_json = transcripts_info_field_json

        # Transcripts info format
        if transcripts_info_format is None:
            transcripts_info_format = param.get("transcripts", {}).get(
                "transcripts_info_format", transcripts_info_format_default
            )

        # Transcripts info field FORMAT
        if transcripts_info_field_format is None:
            transcripts_info_field_format = param.get("transcripts", {}).get(
                "transcripts_info_field_format", transcripts_info_field_format_default
            )
        # if (
        #     transcripts_info_field_format is not None
        #     and transcripts_info_format is None
        # ):
        #     transcripts_info_format = transcripts_info_field_format

        # Variants table
        table_variants = self.get_table_variants()

        # Check info columns param
        if (
            transcripts_info_json is None
            and transcripts_info_field_json is None
            and transcripts_info_format is None
            and transcripts_info_field_format is None
        ):
            return False

        # Transcripts infos columns
        query_transcripts_infos_columns = f"""
            SELECT *
            FROM (
                DESCRIBE SELECT * FROM {transcripts_table}
                )
            WHERE "column_name" NOT IN ('#CHROM', 'POS', 'REF', 'ALT', '{transcripts_column_id}')
        """
        transcripts_infos_columns = list(
            self.get_query_to_df(query=query_transcripts_infos_columns)["column_name"]
        )

        # View results
        clause_select = []
        clause_to_json = []
        clause_to_format = []
        for field in transcripts_infos_columns:
            # Do not consider INFO field for export into fields
            if field not in ["INFO"]:
                clause_select.append(
                    f""" regexp_split_to_table(CAST("{field}" AS STRING), ',') AS '{field}' """
                )
                clause_to_json.append(f""" '{field}': "{field}" """)
                clause_to_format.append(f""" "{field}" """)

        # Update
        update_set_json = []
        update_set_format = []

        # VCF header
        vcf_reader = self.get_header()

        # Transcripts to info column in JSON
        if transcripts_info_json:

            # Create column on variants table
            self.add_column(
                table_name=table_variants,
                column_name=transcripts_info_json,
                column_type="JSON",
                default_value=None,
                drop=False,
            )

            # Add header
            vcf_reader.infos[transcripts_info_json] = vcf.parser._Info(
                transcripts_info_json,
                ".",
                "String",
                "Transcripts in JSON format",
                "unknwon",
                "unknwon",
                self.code_type_map["String"],
            )

            # Add to update
            update_set_json.append(
                f""" {transcripts_info_json}=t.{transcripts_info_json} """
            )

        # Transcripts to info field in JSON
        if transcripts_info_field_json:

            log.debug(f"{msg_info_prefix} - Annotation in JSON format...")

            # Add to update
            update_set_json.append(
                f""" 
                    INFO = concat(
                            CASE
                                WHEN INFO NOT IN ('', '.')
                                THEN INFO
                                ELSE ''
                            END,
                            CASE
                                WHEN CAST(t.{transcripts_info_json} AS VARCHAR) NOT IN ('', '.')
                                THEN concat(
                                    ';{transcripts_info_field_json}=',
                                    t.{transcripts_info_json}
                                )
                                ELSE ''
                            END
                            )
                """
            )

            # Add header
            vcf_reader.infos[transcripts_info_field_json] = vcf.parser._Info(
                transcripts_info_field_json,
                ".",
                "String",
                "Transcripts in JSON format",
                "unknwon",
                "unknwon",
                self.code_type_map["String"],
            )

        if update_set_json:

            # Update query
            query_update = f"""
                UPDATE {table_variants}
                    SET {", ".join(update_set_json)}
                FROM
                (
                    SELECT
                        "#CHROM", POS, REF, ALT,
                            concat(
                            '{{',
                            string_agg(
                                '"' || "{transcripts_column_id}" || '":' ||
                                to_json(json_output)
                            ),
                            '}}'
                            )::JSON AS {transcripts_info_json}
                    FROM
                        (
                        SELECT
                            "#CHROM", POS, REF, ALT,
                            "{transcripts_column_id}",
                            to_json(
                                {{{",".join(clause_to_json)}}}
                            )::JSON AS json_output
                        FROM
                            (SELECT "#CHROM", POS, REF, ALT, "{transcripts_column_id}", {", ".join(clause_select)} FROM {transcripts_table})
                        WHERE "{transcripts_column_id}" IS NOT NULL
                        )
                    GROUP BY "#CHROM", POS, REF, ALT
                ) AS t
                WHERE {table_variants}."#CHROM" = t."#CHROM"
                    AND {table_variants}."POS" = t."POS"
                    AND {table_variants}."REF" = t."REF"
                    AND {table_variants}."ALT" = t."ALT"
            """

            self.execute_query(query=query_update)

        # Transcripts to info column in FORMAT
        if transcripts_info_format:

            # Create column on variants table
            self.add_column(
                table_name=table_variants,
                column_name=transcripts_info_format,
                column_type="VARCHAR",
                default_value=None,
                drop=False,
            )

            # Add header
            vcf_reader.infos[transcripts_info_format] = vcf.parser._Info(
                transcripts_info_format,
                ".",
                "String",
                f"Transcripts annotations: 'transcript | {' | '.join(transcripts_infos_columns)}'",
                "unknwon",
                "unknwon",
                self.code_type_map["String"],
            )

            # Add to update
            update_set_format.append(
                f""" {transcripts_info_format}=t.{transcripts_info_format} """
            )

        else:

            # Set variable for internal queries
            transcripts_info_format = "transcripts_info_format"

        # Transcripts to info field in JSON
        if transcripts_info_field_format:

            log.debug(f"{msg_info_prefix} - Annotation in structured format...")

            # Add to update
            update_set_format.append(
                f""" 
                    INFO = concat(
                            CASE
                                WHEN INFO NOT IN ('', '.')
                                THEN INFO
                                ELSE ''
                            END,
                            CASE
                                WHEN CAST(t.{transcripts_info_format} AS VARCHAR) NOT IN ('', '.')
                                THEN concat(
                                    ';{transcripts_info_field_format}=',
                                    t.{transcripts_info_format}
                                )
                                ELSE ''
                            END
                            )
                """
            )

            # Add header
            vcf_reader.infos[transcripts_info_field_format] = vcf.parser._Info(
                transcripts_info_field_format,
                ".",
                "String",
                f"Transcripts annotations: 'transcript | {' | '.join(transcripts_infos_columns)}'",
                "unknwon",
                "unknwon",
                self.code_type_map["String"],
            )

        if update_set_format:

            # Update query
            query_update = f"""
                UPDATE {table_variants}
                    SET {", ".join(update_set_format)}
                FROM
                (
                    SELECT
                        "#CHROM", POS, REF, ALT,
                            string_agg({transcripts_info_format}) AS {transcripts_info_format}
                    FROM 
                        (
                        SELECT
                            "#CHROM", POS, REF, ALT,
                            "{transcripts_column_id}",
                            concat(
                                "{transcripts_column_id}",
                                '|',
                                {", '|', ".join(clause_to_format)}
                            ) AS {transcripts_info_format}
                        FROM
                            (SELECT "#CHROM", POS, REF, ALT, "{transcripts_column_id}", {", ".join(clause_select)} FROM {transcripts_table})
                        )
                    GROUP BY "#CHROM", POS, REF, ALT
                ) AS t
                WHERE {table_variants}."#CHROM" = t."#CHROM"
                    AND {table_variants}."POS" = t."POS"
                    AND {table_variants}."REF" = t."REF"
                    AND {table_variants}."ALT" = t."ALT"
            """

            self.execute_query(query=query_update)

        return True

    ############################
    # Rename and remove fields #
    ############################

    def rename_info_fields(
        self, fields_to_rename: dict = None, table: str = None
    ) -> dict:
        """
        The `rename_info_fields` function renames specified fields in a VCF file header and updates
        corresponding INFO fields in the variants table.

        :param fields_to_rename: The `fields_to_rename` parameter is a dictionary that contains the
        mapping of fields to be renamed in a VCF (Variant Call Format) file. The keys in the dictionary
        represent the original field names that need to be renamed, and the corresponding values
        represent the new names to which the fields should be
        :type fields_to_rename: dict
        :param table: The `table` parameter in the `rename_info_fields` function represents the name of
        the table in which the variants data is stored. This table contains information about genetic
        variants, and the function updates the corresponding INFO fields in this table when renaming
        specified fields in the VCF file header
        :type table: str
        :return: The `rename_info_fields` function returns a dictionary `fields_processed` that contains
        the original field names as keys and their corresponding new names (or None if the field was
        removed) as values after renaming or removing specified fields in a VCF file header and updating
        corresponding INFO fields in the variants table.
        """

        # Config
        config = self.get_config()
        access = config.get("access")

        # Init
        fields_processed = {
            "renamed": {},
            "removed": {},
            "not_processed": {},
            "not_found": {},
        }

        if table is None:
            table = self.get_table_variants()

        # Clasue case
        clause_case = []

        # Init
        fields_to_process = {}

        # For each field to rename or remove, one by one
        if fields_to_rename is not None:
            for field_to_rename, field_renamed in fields_to_rename.items():

                # If no field to process
                if field_to_rename != "":

                    # rename empty is remove
                    if field_renamed == "":
                        field_renamed = None

                    # Check if already to process
                    check_field_to_rename_found = False
                    for (
                        field_to_process_to_rename,
                        field_to_process_renamed,
                    ) in fields_to_process.items():
                        if field_to_rename == field_to_process_renamed:
                            # Replace filed to process
                            fields_to_process[field_to_process_to_rename] = (
                                field_renamed
                            )
                            check_field_to_rename_found = True
                    if not check_field_to_rename_found:
                        # Add field to process
                        fields_to_process[field_to_rename] = field_renamed

        if len(fields_to_process) and access not in ["RO"]:

            # Log
            log.info("Rename or remove fields...")

            # Header
            header = self.get_header()

            # For each field
            for field_to_rename, field_renamed in fields_to_process.items():

                # If to rename or remove
                if (
                    field_to_rename != field_renamed
                    and field_to_rename not in ["", None]
                    and field_renamed not in [""]
                ):

                    # Rename
                    if field_renamed is not None:

                        # Case clause
                        clause_case.append(
                            f""" WHEN k = '{field_to_rename}' THEN '{field_renamed}'  """
                        )

                        # Fields processed
                        fields_processed["renamed"][field_to_rename] = field_renamed

                        # Log
                        log.debug(
                            f"Rename or remove fields - field '{field_to_rename}' renamed as '{field_renamed}'"
                        )

                    # Remove
                    else:

                        # Case clause
                        clause_case.append(
                            f""" WHEN k = '{field_to_rename}' THEN NULL  """
                        )

                        # Fields processed
                        fields_processed["removed"][field_to_rename] = field_renamed

                        # Log
                        log.debug(
                            f"Rename or remove fields - field '{field_to_rename}' removed"
                        )

                    # Header
                    if field_to_rename in header.infos:

                        # Rename header if to rename
                        if field_renamed is not None:
                            header.infos[field_renamed] = vcf.parser._Info(
                                field_renamed,
                                header.infos[field_to_rename].num,
                                header.infos[field_to_rename].type,
                                header.infos[field_to_rename].desc,
                                header.infos[field_to_rename].source,
                                header.infos[field_to_rename].version,
                                header.infos[field_to_rename].type_code,
                            )

                        # Remove header, if rename or remove
                        del header.infos[field_to_rename]

                    else:

                        # Log
                        log.warning(
                            f"Rename or remove fields - field '{field_to_rename}' not in header"
                        )

                        # Fields processed
                        fields_processed["not_found"][field_to_rename] = field_renamed

                else:

                    # Fields processed
                    fields_processed["not_pocessed"][field_to_rename] = field_renamed

            # Process
            if len(clause_case):

                # Update query
                query_update = f"""
                    UPDATE {table}
                    SET INFO = renamed_table.INFO                       -- update INFO
                    FROM (
                        SELECT
                            "#CHROM", POS, REF, ALT,                    -- variant id
                            IFNULL(string_agg(kv, ';'), '.') AS INFO    -- INFO
                        FROM (
                            SELECT
                                "#CHROM", POS, REF, ALT,                -- variant id
                                CASE
                                    WHEN k IS NOT NULL                  -- key not null
                                    THEN
                                        CASE 
                                            WHEN v IS NOT NULL          -- value not null
                                            THEN concat(k, '=', v)      -- key-value: either String, Integer, Float
                                            ELSE k                      -- Flag
                                        END
                                    ELSE NULL                           -- remove
                                END AS kv
                            FROM (
                                SELECT "#CHROM", POS, REF, ALT,         -- variant id
                                    CASE
                                        {" ".join(clause_case)}         -- rename or remove
                                        ELSE k                          -- no change
                                    END AS k,                           -- key
                                    v                                   -- value
                                FROM (
                                    SELECT
                                        "#CHROM", POS, REF, ALT,        -- variant id
                                        string_split(kv, '=')[1] AS k,  -- key
                                        string_split(kv, '=')[2] AS v   -- value
                                    FROM (
                                        SELECT "#CHROM", POS, REF, ALT, unnest(string_split(INFO, ';')) AS kv
                                        FROM variants
                                        )
                                    )
                                )
                            )
                        GROUP BY "#CHROM", POS, REF, ALT
                    ) AS renamed_table
                    WHERE {table}."#CHROM" = renamed_table."#CHROM"     -- join
                    AND {table}."POS" = renamed_table."POS"
                    AND {table}."REF" = renamed_table."REF"
                    AND {table}."ALT" = renamed_table."ALT"
                """
                # log.debug(f"query_update={query_update}")
                self.execute_query(query_update)

        return fields_processed

    def recreate_info_fields(
        self, fields_to_rename: dict = None, table: str = None
    ) -> dict:
        """
        The `recreate_info_fields` function renames specified fields in a VCF file header and updates
        corresponding INFO fields in the variants table.

        :param fields_to_rename: The `fields_to_rename` parameter is a dictionary that contains the
        mapping of fields to be renamed in a VCF (Variant Call Format) file. The keys in the dictionary
        represent the original field names that need to be renamed, and the corresponding values
        represent the new names to which the fields should be renamed. Default {}
        :type fields_to_rename: dict
        :param table: The `table` parameter in the `recreate_info_fields` function represents the name of
        the table in which the variants data is stored. This table contains information about genetic
        variants, and the function updates the corresponding INFO fields in this table when renaming
        specified fields in the VCF file header. Default Variants table 'variants'.
        :type table: str
        :return: The `recreate_info_fields` function returns a dictionary `fields_renamed` that contains
        the original field names as keys and their corresponding new names (or None if the field was
        removed) as values after renaming or removing specified fields in a VCF file header and updating
        corresponding INFO fields in the variants table.
        """

        # Init
        config = self.get_config()
        access = config.get("access")

        # Table
        if table is None:
            table = self.get_table_variants()

        # Fields to rename or remove
        if fields_to_rename is None:
            fields_to_rename = {}

        # Fields on header
        fields_to_process = {k: k for k in self.get_header().infos.keys()}

        # For each field to rename or remove, one by one
        for field_to_rename, field_renamed in fields_to_rename.items():
            log.debug(f"{field_to_rename}, {field_renamed}")
            if field_to_rename != "" and field_renamed != "":
                log.debug(f"{field_to_rename}, {field_renamed} OK")
                # Check if already to process
                check_field_to_rename_found = False
                for (
                    field_to_process_to_rename,
                    field_to_process_renamed,
                ) in fields_to_process.items():
                    if field_to_rename == field_to_process_renamed:
                        fields_to_process[field_to_process_to_rename] = field_renamed
                        check_field_to_rename_found = True
                if not check_field_to_rename_found:
                    fields_to_process[field_to_rename] = field_renamed

        # Init
        fields_processed = {"removed": {}, "renamed": {}, "not_found": {}}

        # if fields_to_rename is not None and access not in ["RO"]:
        if fields_to_process is not None and access not in ["RO"]:

            log.info(f"Recreate INFO with {len(fields_to_process)} fields...")

            # Create view
            annotation_view_name = "annotation_view_for_recreate_infos_" + str(
                random.randrange(1000000)
            )

            # Header
            header = self.get_header()

            # Update query select clauses
            query_view_select_clause = []

            for field_to_rename, field_renamed in fields_to_process.items():

                # Action
                action = None

                # Header

                # Field to remove
                if field_renamed is None:

                    if field_renamed in header.infos:

                        # Remove in header
                        del header.infos[field_renamed]

                    # Action
                    action = "removed"

                # Field to rename
                elif field_to_rename in header.infos:

                    # Rename header
                    if field_renamed is not None and field_renamed != field_to_rename:
                        header.infos[field_renamed] = vcf.parser._Info(
                            field_renamed,
                            header.infos[field_to_rename].num,
                            header.infos[field_to_rename].type,
                            header.infos[field_to_rename].desc,
                            header.infos[field_to_rename].source,
                            header.infos[field_to_rename].version,
                            header.infos[field_to_rename].type_code,
                        )
                        del header.infos[field_to_rename]

                    # Update query
                    if header.infos[field_renamed].type == "Flag":
                        query_view_select_clause.append(
                            f"""
                                CASE
                                    WHEN "{field_renamed}" IS NOT NULL AND TRY_CAST("{field_renamed}" AS FLOAT) != 0
                                    THEN '{field_renamed};'
                                END
                            """
                        )
                    else:
                        query_view_select_clause.append(
                            f"""
                                CASE
                                    WHEN "{field_renamed}" IS NOT NULL
                                    THEN concat('{field_renamed}=',"{field_renamed}",';')
                                END
                            """
                        )

                    # Action
                    action = "renamed"

                else:

                    log.warning(
                        f"Rename or remove fields - field '{field_to_rename}' not in header"
                    )

                    # Action
                    action = "not_found"

                # List of renamed or removed fields
                if action:
                    fields_processed[action][field_to_rename] = field_renamed

            if len(query_view_select_clause):

                # Create view
                annotation_view_name = self.create_annotations_view(
                    table=table,
                    view=annotation_view_name,
                    view_type="view",
                    view_mode="full",
                    info_prefix_column="",
                    # info_struct_column="INFOS",
                    detect_type_list=False,
                    fields=fields_processed["renamed"].keys(),
                    fields_not_exists=True,
                    fields_forced_as_varchar=True,
                    fields_needed_all=False,
                    fields_to_rename=fields_processed["renamed"],
                    drop_view=True,
                )

                # Log
                log.info(
                    f"Recreate INFO with {len(query_view_select_clause)} found fields..."
                )

                # Query
                query = f"""
                    UPDATE {table}
                    SET
                        INFO = regexp_replace(
                                concat({", ".join(query_view_select_clause)}),
                                ';$',
                                ''
                            )
                    FROM {annotation_view_name}
                    WHERE {table}."#CHROM" = {annotation_view_name}."#CHROM"
                      AND {table}."POS" = {annotation_view_name}."POS"
                      AND {table}."REF" = {annotation_view_name}."REF"
                      AND {table}."ALT" = {annotation_view_name}."ALT"
                """
                # log.debug(f"query={query}")

                # Excecute query
                self.execute_query(query=query)

        return fields_processed

    def calculation_rename_info_fields(
        self,
        fields_to_rename: dict = None,
        table: str = None,
        operation_name: str = "RENAME_INFO_FIELDS",
    ) -> None:
        """
        The `calculation_rename_info_fields` function retrieves parameters from a dictionary, updates
        fields to rename and table if provided, and then calls another function to rename the fields.

        :param fields_to_rename: `fields_to_rename` is a dictionary that contains the fields to be
        renamed in a table. Each key-value pair in the dictionary represents the original field name as
        the key and the new field name as the value
        :type fields_to_rename: dict
        :param table: The `table` parameter in the `calculation_rename_info_fields` method is used to
        specify the name of the table for which the fields are to be renamed. It is a string type
        parameter
        :type table: str
        :param operation_name: The `operation_name` parameter in the `calculation_rename_info_fields`
        method is a string that specifies the name of the operation being performed. In this context, it
        is used as a default value for the operation name if not explicitly provided when calling the
        function, defaults to RENAME_INFO_FIELDS
        :type operation_name: str (optional)
        """

        # Param
        param = self.get_param()

        # Get param fields to rename
        param_fields_to_rename = (
            param.get("calculation", {})
            .get("calculations", {})
            .get(operation_name, {})
            .get("fields_to_rename", None)
        )

        # Get param table
        param_table = (
            param.get("calculation", {})
            .get("calculations", {})
            .get(operation_name, {})
            .get("table", None)
        )

        # Init fields_to_rename
        if fields_to_rename is None:
            fields_to_rename = param_fields_to_rename

        # Init table
        if table is None:
            table = param_table

        renamed_fields = self.rename_info_fields(
            fields_to_rename=fields_to_rename, table=table
        )

        log.debug(f"renamed_fields:{renamed_fields}")

    def calculation_recreate_info_fields(
        self,
        fields_to_rename: dict = None,
        table: str = None,
        operation_name: str = "RENAME_INFO_FIELDS",
    ) -> None:
        """
        The `calculation_recreate_info_fields` function retrieves parameters from a dictionary, recreate
        INFO fields with rename and table if provided, and then calls another function to rename the fields.

        :param fields_to_rename: `fields_to_rename` is a dictionary that contains the fields to be
        renamed in a table. Each key-value pair in the dictionary represents the original field name as
        the key and the new field name as the value
        :type fields_to_rename: dict
        :param table: The `table` parameter in the `calculation_recreate_info_fields` method is used to
        specify the name of the table for which the fields are to be renamed. It is a string type
        parameter
        :type table: str
        :param operation_name: The `operation_name` parameter in the `calculation_recreate_info_fields`
        method is a string that specifies the name of the operation being performed. In this context, it
        is used as a default value for the operation name if not explicitly provided when calling the
        function, defaults to RENAME_INFO_FIELDS
        :type operation_name: str (optional)
        """

        # Param
        param = self.get_param()

        # Get param fields to rename
        param_fields_to_rename = (
            param.get("calculation", {})
            .get("calculations", {})
            .get(operation_name, {})
            .get("fields_to_rename", None)
        )

        # Get param table
        param_table = (
            param.get("calculation", {})
            .get("calculations", {})
            .get(operation_name, {})
            .get("table", None)
        )

        # Init fields_to_rename
        if fields_to_rename is None:
            fields_to_rename = param_fields_to_rename

        # Init table
        if table is None:
            table = param_table

        renamed_fields = self.recreate_info_fields(
            fields_to_rename=fields_to_rename, table=table
        )

        log.debug(f"renamed_fields:{renamed_fields}")

    ####################
    # Anontations view #
    ####################

    def create_annotations_view(
        self,
        table: str = None,
        view: str = None,
        view_type: str = None,
        view_mode: str = None,
        fields: list = None,
        fields_needed: list = None,
        fields_needed_all: bool = False,
        detect_type_list: bool = True,
        fields_not_exists: bool = True,
        only_in_columns: bool = False,
        formats: list = None,
        strict: bool = False,
        info_prefix_column: str = None,
        info_struct_column: str = None,
        sample_struct_column: str = None,
        drop_view: bool = False,
        fields_to_rename: dict = None,
        fields_forced_as_varchar: bool = False,
        limit: int = None,
    ) -> str:
        """
        Creates a SQL view from fields in a VCF INFO column, or already in a column.

        :param table: The name of the table containing variant data. If not provided, the default table is used.
        :type table: str, optional
        :param view: The name of the view that will be created based on the fields in the VCF INFO column. Defaults to None.
        :type view: str, optional
        :param view_type: The type of view to be created. It can be either a `VIEW` or a `TABLE`. Defaults to `VIEW`.
        :type view_type: str, optional
        :param view_mode: The mode of view to be created. It can be either `full` or `explore`. Defaults to `full`.
        :type view_mode: str, optional
        :param fields: A list of field names to be extracted from the INFO column in the VCF file. Defaults to None.
        :type fields: list, optional
        :param fields_needed: A list of fields that are required for the view. Defaults to None.
        :type fields_needed: list, optional
        :param fields_needed_all: A flag that determines whether to include all fields in the table in the view. Defaults to False.
        :type fields_needed_all: bool, optional
        :param detect_type_list: A flag that determines whether to detect the type of the fields extracted from the INFO column. Defaults to True.
        :type detect_type_list: bool, optional
        :param fields_not_exists: A flag that determines whether to include fields that do not exist in the table in the view. Defaults to True.
        :type fields_not_exists: bool, optional
        :param only_in_columns: A flag that determines whether to include only the fields that exist in the columns of the table. Defaults to False.
        :type only_in_columns: bool, optional
        :param formats: A list of field names to be extracted from the FORMAT column in the VCF file. Defaults to None.
        :type formats: list, optional
        :param strict: A flag that determines whether to enforce strict criteria for the fields in the view. Defaults to False.
        :type strict: bool, optional
        :param info_prefix_column: A prefix to be added to the field names in the view. Defaults to None.
        :type info_prefix_column: str, optional
        :param info_struct_column: The name of the column that will contain the extracted fields from the INFO column in the view. Defaults to None.
        :type info_struct_column: str, optional
        :param sample_struct_column: The name of the column that will contain the extracted formats from the samples columns in the view. Defaults to None.
        :type sample_struct_column: str, optional
        :param drop_view: A flag that determines whether to drop the existing view with the same name before creating a new view. Defaults to False.
        :type drop_view: bool, optional
        :param fields_to_rename: A dictionary that contains the mapping of fields to be renamed in the VCF file. Defaults to None.
        :type fields_to_rename: dict, optional
        :param fields_forced_as_varchar: A flag that forces fields to be treated as type VARCHAR. Defaults to False.
        :type fields_forced_as_varchar: bool, optional
        :param limit: The maximum number of rows to be included in the view. Defaults to None.
        :type limit: int, optional

        :return: The name of the view that is created based on the fields extracted from the INFO column in the VCF file.
        :rtype: str
        """

        # Create a sql view from fields in VCF INFO column, with each column is a field present in the VCF header (with a specific type from VCF header) and extracted from INFO column (with a regexp like in rename_info_fields), and each row is a variant.

        # Get table
        if table is None:
            table = self.get_table_variants()

        # Get view
        if view is None:
            view = f"{table}_view"

        # Get view type
        if view_type is None:
            view_type = "VIEW"

        # Get mode
        view_mode_allowed = ["full", "explore"]
        if view_mode is None:
            view_mode = "explore"

        # Mode lower
        view_mode = view_mode.lower()

        # Mode check
        if view_mode not in view_mode_allowed:
            msg_err = f"Invalid view mode: '{view_mode}' (either {view_mode_allowed})"
            log.error(msg_err)
            raise ValueError(msg_err)

        # Prefix
        if info_prefix_column is not None:
            prefix = info_prefix_column
        else:
            prefix = ""

        # Check view type value
        view_type_allowed = ["view", "table"]
        if view_type.lower() not in view_type_allowed:
            msg_err = f"Invalid view type: {view_type} (either {view_type_allowed})"
            log.error(msg_err)
            raise ValueError(msg_err)

        # Get header
        header = self.get_header()

        # Get fields
        if fields is None:
            fields = list(header.infos.keys())

        # # Get format fields
        # if formats is None:
        #     formats = list(header.formats.keys())
        #     # fields = list(header.infos.keys())

        # Get fields to rename
        if fields_to_rename is None:
            fields_to_rename = {}

        # If Samples structured columns
        if sample_struct_column:

            # # Get format
            # formats = list(header.formats.keys())
            if formats is None:
                formats = list(header.formats.keys())

            # Get samples
            samples = list(header.samples)

        else:

            # Empty format and samples
            formats = []
            samples = []

        log.debug(
            f"Create '{view}' view (as '{view_type}' mode '{view_mode}') from table '{table}' with {len(fields)} fields"
        )

        connexion_type = self.get_connexion_type()

        # Describe table
        if connexion_type in ["duckdb"]:
            table_describe_query = f"""
                DESCRIBE {table}
            """
            table_describe = self.get_query_to_df(query=table_describe_query)
        else:
            table_describe_query = f"""
                PRAGMA table_info({table})
            """
            table_describe = self.get_query_to_df(query=table_describe_query)
            table_describe["column_name"] = table_describe.get("name")

        # fields needed
        if fields_needed is None:
            if fields_needed_all:
                fields_needed = list(table_describe.get("column_name"))
            else:
                fields_needed = ["#CHROM", "POS", "REF", "ALT"]

        # Add samples in view mode 'full'
        # log.debug(f"samples={samples}")
        # log.debug(f"table_describe={table_describe}")
        if view_mode in ["full"] and sample_struct_column and len(samples):
            if "FORMAT" not in fields_needed and "FORMAT" in list(
                table_describe.get("column_name")
            ):  # in table_describe:
                fields_needed += ["FORMAT"]
            for field_needed in fields_needed:
                if field_needed not in fields_needed:
                    fields_needed += [field_needed]

        # Check needed fieds
        for field in fields_needed:
            if field not in list(table_describe.get("column_name")):
                msg_err = f"Field '{field}' is needed, but not in file"
                raise ValueError(msg_err)

        # Create fields for annotation view extracted from INFO column in table variants (with regexp_replace like in rename_info_fields), with column type from VCF header
        fields_columns = []
        fields_columns_annotations_struct = []
        samples_format_struct = []
        field_sql_type_list = False

        # Find "INFO" column
        if "INFO" in list(table_describe.get("column_name")):
            info_column = '"INFO"'
        else:
            info_column = "''"

        # Each field
        for field in set(fields):

            # Rename field
            field_to_rename = fields_to_rename.get(field, field)
            if field_to_rename is None:
                field_to_rename = field

            # Check field type

            # Field info
            field_infos = header.infos.get(field, None)

            # Field SQL type
            if field_infos is not None:

                # Field SQL type
                field_sql_type = code_type_map_to_sql.get(field_infos.type, "VARCHAR")

                # Column is a list
                if detect_type_list and str(field_infos.num) != "1":
                    field_sql_type_list = True
                else:
                    field_sql_type_list = False

            else:

                # Field SQL type
                field_sql_type = "VARCHAR"

                # Column is a list
                field_sql_type_list = False

            # fields_forced_as_varchar
            if fields_forced_as_varchar:
                field_sql_type = "VARCHAR"
                field_sql_type_list = False

            # Needed fields, not in other annotation fields (useful for DB with fields in column)
            if field in fields_needed and not field in list(
                table_describe.get("column_name")
            ):
                continue

            # Fields in table
            elif field in list(table_describe.get("column_name")):

                # Add field in needed if 'full' view mode
                if view_mode in ["full"]:
                    if field not in fields_needed:
                        fields_needed += [field]

                # Only if not needes (already in a column)
                if not field in fields_needed:
                    # log.debug(f"Filed '{field}' not in needed")
                    fields_columns.append(
                        f"""
                            "{field}" AS '{prefix}{field_to_rename}' -- field in column but not in needed
                        """
                    )

                # Flag
                if field_infos is not None and field_infos.type == "Flag":
                    fields_columns_annotations_struct.append(
                        f"""
                            "{field_to_rename}":= TRY_CAST("{field}" AS BOOLEAN)
                        """
                    )
                else:
                    if field_sql_type_list:
                        fields_columns_annotations_struct.append(
                            f"""
                                "{field_to_rename}":= CAST(list_transform(string_split(CAST("{field}" AS VARCHAR), ','), x -> CASE WHEN x = '.' OR x = '' THEN NULL ELSE x END) AS {field_sql_type}[]) -- field in column
                            """
                        )
                    else:
                        fields_columns_annotations_struct.append(
                            f"""
                                "{field_to_rename}":= COALESCE(NULLIF(regexp_replace(CAST("{field}" AS VARCHAR), '^\\.$', ''), '')::{field_sql_type}, NULL)  -- field in column
                            """
                        )

            # Fields in header
            elif (
                field in header.infos
                and not only_in_columns
                and "INFO" in list(table_describe.get("column_name"))
            ):

                # Colonne is a flag
                if field_infos.type == "Flag":

                    # Field pattern
                    field_pattern = rf"(^|;)({field})([^;]*)?"

                    if view_mode in ["explore"]:
                        fields_columns.append(
                            f"""
                                regexp_matches({info_column}, '{field_pattern}')::BOOLEAN AS '{prefix}{field_to_rename}'
                            """
                        )
                        fields_columns_annotations_struct.append(
                            f"""
                                "{field_to_rename}":= regexp_matches({info_column}, '{field_pattern}')::BOOLEAN
                            """
                        )
                    elif view_mode in ["full"]:
                        fields_columns.append(
                            f"""
                                string_agg(CASE WHEN k = '{field}' THEN true END, ',')::BOOLEAN AS '{prefix}{field_to_rename}'
                            """
                        )
                        fields_columns_annotations_struct.append(
                            f"""
                                "{field_to_rename}":= string_agg(CASE WHEN k = '{field}' THEN true END, ',')::BOOLEAN
                            """
                        )

                # Colonne with a type
                else:

                    # Field pattern
                    field_pattern = rf"(^|;)({field})=([^;]*)?"

                    if view_mode in ["explore"]:
                        field_source = (
                            f""" regexp_extract({info_column}, '{field_pattern}', 3) """
                        )
                    elif view_mode in ["full"]:
                        field_source = (
                            f""" string_agg(CASE WHEN k = '{field}' THEN v END, ',') """
                        )

                    # Field is a list
                    if field_sql_type_list:

                        fields_columns.append(
                            f"""
                                CAST(list_transform(string_split({field_source}, ','), x -> CASE WHEN x = '.' OR x = '' THEN NULL ELSE x END) AS {field_sql_type}[]) AS '{prefix}{field_to_rename}'
                            """
                        )
                        fields_columns_annotations_struct.append(
                            f"""
                                "{field_to_rename}":= CAST(list_transform(string_split({field_source}, ','), x -> CASE WHEN x = '.' OR x = '' THEN NULL ELSE x END) AS {field_sql_type}[])
                            """
                        )

                    # Field is a unique value
                    else:

                        fields_columns.append(
                            f"""
                                NULLIF(regexp_replace({field_source}, '^\\.$', ''), '')::{field_sql_type} AS '{prefix}{field_to_rename}'
                            """
                        )
                        fields_columns_annotations_struct.append(
                            f"""
                                "{field_to_rename}":= COALESCE(NULLIF(regexp_replace({field_source}, '^\\.$', ''), '')::{field_sql_type}, NULL)
                            """
                        )

            # Add field even if not exists
            elif fields_not_exists:

                fields_columns.append(
                    f"""
                            null AS '{prefix}{field_to_rename}'
                        """
                )
                fields_columns_annotations_struct.append(
                    f"""
                            "{field_to_rename}":= NULL
                        """
                )
                msg_err = f"Field '{field}' is not found (in table or header): '{field}' will be set to NULL"
                log.warning(msg=msg_err)

            else:

                # Field not found
                msg_err = f"Field '{field}' is not found (in table or header or column)"

                if strict:
                    log.error(msg=msg_err)
                    raise ValueError(msg_err)
                else:
                    log.warning(msg=msg_err)

        # If samples and struct as option
        if sample_struct_column and len(samples):

            # Format info
            format_infos = header.formats

            # For each sample
            for sample in samples:

                # Struct by format
                sample_format_struct = []

                # For each format
                for format in formats:
                    # for format in ["GT"]:

                    # Format cast and list
                    format_cast = ""
                    format_list = False
                    format_cast = code_type_map_to_sql.get(
                        format_infos.get(format).type, "VARCHAR"
                    )
                    if format_infos.get(format).num != 1:
                        format_list = True

                    # If format is a list
                    if format_list:
                        sample_format_struct.append(
                            f""" 
                                "{format}":= 
                                    list_transform(
                                        string_split(
                                            NULLIF(
                                                string_split("{sample}", ':')[list_position(string_split("FORMAT", ':'), '{format}')]
                                                , ''
                                            )
                                            , ',')
                                        , x -> CASE WHEN x = '.' OR x = '' THEN NULL ELSE x END
                                    )::{format_cast}[]
                            """
                        )
                    # If format is NOT a list
                    else:
                        sample_format_struct.append(
                            f""" 
                                "{format}":= 
                                    COALESCE(
                                        NULLIF(
                                            regexp_replace(
                                                string_split("{sample}", ':')[list_position(string_split("FORMAT", ':'), '{format}')]
                                                , '^\\.$', ''
                                            )
                                        , ''
                                        )
                                    )::{format_cast}
                            """
                        )

                # Add struct of the sample
                if len(sample_format_struct):
                    samples_format_struct.append(
                        f"""
                        "{sample}":= STRUCT_PACK({", ".join(sample_format_struct)})
                    """
                    )

        # Combine fields into columns
        if info_prefix_column is not None and len(fields_columns):
            annotations_column_annotations_columns = (
                f""", {", ".join(fields_columns)}"""
            )
        else:
            annotations_column_annotations_columns = ""

        # Combine fields into a STRUCT
        if info_struct_column and len(fields_columns_annotations_struct):
            annotations_column_annotations_struct = f""" 
                , STRUCT_PACK({", ".join(fields_columns_annotations_struct)}) AS {info_struct_column}
                """
        else:
            annotations_column_annotations_struct = ""

        # Combine samples into a STRUCT
        if sample_struct_column and len(samples_format_struct):
            samples_format_struct_clause = f""", STRUCT_PACK({", ".join(samples_format_struct)}) AS {sample_struct_column} """
        else:
            samples_format_struct_clause = ""

        # Limit
        limit_clause = ""
        if limit is not None:
            limit_clause = f" LIMIT {limit} "

        # Query select

        if view_mode in ["explore"]:
            query_select = f"""
                SELECT
                    {', '.join([f'"{field}"' for field in fields_needed])}  -- variant id
                    {annotations_column_annotations_columns}                -- annotations_column_annotations_columns
                    {annotations_column_annotations_struct}                 -- annotations_column_annotations_struct
                    {samples_format_struct_clause}                          -- samples_format_struct_clause
                FROM
                    {table}
                {limit_clause}
            """

        elif view_mode in ["full"]:
            query_select = f"""
                    SELECT
                        {', '.join([f'"{field}"' for field in fields_needed])}          -- variant id
                        {annotations_column_annotations_columns}                        -- annotations_column_annotations_columns
                        {annotations_column_annotations_struct}                         -- annotations_column_annotations_struct
                        {samples_format_struct_clause}                                  -- samples_format_struct_clause
                    FROM (
                        SELECT
                            {', '.join([f'"{field}"' for field in fields_needed])},     -- variant id
                            k,      -- key
                            v       -- value
                        FROM (
                            SELECT
                                {', '.join([f'"{field}"' for field in fields_needed])},     -- variant id
                                INFO,                                                       -- INFO
                                string_split(kv, '=')[1] AS k,  -- key
                                string_split(kv, '=')[2] AS v   -- value
                            FROM (
                                SELECT {', '.join([f'"{field}"' for field in fields_needed])},  -- variant id
                                {info_column} AS INFO,                                          -- INFO
                                -- unnest(string_split({info_column}, ';')) AS kv
                                unnest(string_split(concat({info_column}, ''), ';')) AS kv
                                FROM {table}
                                )
                            WHERE k in ('{"', '".join(fields)}') OR TRIM(INFO) in ('', '.')  OR INFO IS NULL
                            )
                        )
                    GROUP BY {', '.join([f'"{field}"' for field in fields_needed])}
                    {limit_clause}

            """

        # Drop if any
        if drop_view:
            log.debug(f"Drop view: {view}")
            try:
                query_create_view = f"""
                    DROP view IF EXISTS {view}
                """
                self.execute_query(query=query_create_view)
                log.debug(f"View dropped: {view}")
            except:
                try:
                    query_create_view = f"""
                        DROP table IF EXISTS {view}
                    """
                    self.execute_query(query=query_create_view)
                    log.debug(f"View dropped: {view}")
                except:
                    msg_err = f"View '{view}' can NOT be dropped"
                    log.error(msg_err)
                    raise ValueError(msg_err)

        # Create view
        log.debug(f"Create view: {view}")
        query_create_view = f"""
            CREATE {view_type} IF NOT EXISTS {view} AS {query_select}
        """
        # log.debug(f"Create view:{query_create_view}")
        self.execute_query(query=query_create_view)
        log.debug(f"View created: {view}")

        return view

    def remove_tables_or_views(self, tables: list = None, views: list = None) -> list:
        """
        Remove specified tables and views from the database.

        Args:
            tables (list): A list of table names to be removed. Default is None.
            views (list): A list of view names to be removed. Default is None.

        Returns:
            list: A list of tables and views that were successfully removed.

        This function attempts to remove the specified tables and views from the database.
        It first tries to drop each item as a table, and if that fails, it tries to drop it as a view.
        If an item is neither a table nor a view, an error is logged.
        """
        temporary_tables = (tables or []) + (views or [])
        removed_items = []

        # Remove temporary tables and views
        if temporary_tables:
            for temporary_table in set(temporary_tables):
                try:
                    query_drop_tmp_table = f"""
                        DROP TABLE IF EXISTS {temporary_table}
                    """
                    self.execute_query(query=query_drop_tmp_table)
                    log.debug(f"DROP TABLE '{temporary_table}' done.")
                    removed_items.append(temporary_table)
                except Exception as e:
                    log.debug(
                        f"DROP TABLE '{temporary_table}': Failed (not a table)! Try as a view."
                    )

                    try:
                        query_drop_tmp_view = f"""
                            DROP VIEW IF EXISTS {temporary_table}
                        """
                        self.execute_query(query=query_drop_tmp_view)
                        log.debug(f"DROP VIEW '{temporary_table}' done.")
                        removed_items.append(temporary_table)
                    except Exception as e:
                        log.debug(f"DROP VIEW '{temporary_table}': Failed (not a view)")
                        log.error(
                            f"DROP '{temporary_table}': Failed! Neither a table nor a view"
                        )

        return removed_items
