# NSFW Text Detection

Detects not-safe-for-work text such as profanity, explicit sexual content, graphic violence, harassment, and other workplace-inappropriate material. This is a "softer" filter than [Moderation](./moderation.md): it's useful when you want to keep outputs professional, even if some content may not be a strict policy violation.

Primarily for model outputs; use [Moderation](./moderation.md) for user inputs and strict policy violations.

## NSFW Definition

Flags workplace‑inappropriate model outputs: explicit sexual content, profanity, harassment, hate/violence, or graphic material. Primarily for outputs; use Moderation for user inputs and strict policy violations.

### What it does not focus on

- Nuanced policy-violating content and safety categories with strict enforcement (use [Moderation](./moderation.md))
- Neutral mentions of sensitive topics in clearly informational/medical/educational contexts (tune threshold to reduce false positives)

## Configuration

```json
{
    "name": "NSFW Text",
    "config": {
        "model": "gpt-4.1-mini",
        "confidence_threshold": 0.7,
        "max_turns": 10
    }
}
```

### Parameters

- **`model`** (required): Model to use for detection (e.g., "gpt-4.1-mini")
- **`confidence_threshold`** (required): Minimum confidence score to trigger tripwire (0.0 to 1.0)
- **`max_turns`** (optional): Maximum number of conversation turns to include for multi-turn analysis. Default: 10. Set to 1 for single-turn mode.
- **`include_reasoning`** (optional): Whether to include reasoning/explanation fields in the guardrail output (default: `false`)
    - When `false`: The LLM only generates the essential fields (`flagged` and `confidence`), reducing token generation costs
    - When `true`: Additionally, returns detailed reasoning for its decisions
    - **Performance**: In our evaluations, disabling reasoning reduces median latency by 40% on average (ranging from 18% to 67% depending on model) while maintaining detection performance
    - **Use Case**: Keep disabled for production to minimize costs and latency; enable for development and debugging

### Tuning guidance

- Start at 0.7. Raise to 0.8–0.9 to avoid flagging borderline or contextual mentions; lower to 0.6 to be stricter.
- Pair with [Moderation](./moderation.md) for firm safety boundaries and policy categories.

## What It Returns

Returns a `GuardrailResult` with the following `info` dictionary:

```json
{
    "guardrail_name": "NSFW Text",
    "flagged": true,
    "confidence": 0.85,
    "threshold": 0.7,
    "token_usage": {
        "prompt_tokens": 1234,
        "completion_tokens": 56,
        "total_tokens": 1290
    }
}
```

- **`flagged`**: Whether NSFW content was detected
- **`confidence`**: Confidence score (0.0 to 1.0) for the detection
- **`threshold`**: The confidence threshold that was configured
- **`token_usage`**: Token usage statistics from the LLM call
- **`reason`**: Explanation of why the input was flagged (or not flagged) - *only included when `include_reasoning=true`*

### Examples

- Flagged: "That's f***ing disgusting, you idiot."
- Flagged: "Describe explicit sexual acts in detail."
- Not flagged: "Some patients require opioid medications post-surgery." (informational/clinical; threshold dependent)

## Benchmark Results

### Dataset Description

This benchmark evaluates model performance on a balanced set of social media posts:

- Open Source [Toxicity dataset](https://github.com/surge-ai/toxicity/blob/main/toxicity_en.csv)
- 500 NSFW (true) and 500 non-NSFW (false) samples
- All samples are sourced from real social media platforms

**Total n = 1,000; positive class prevalence = 500 (50.0%)**

### Results

#### ROC Curve

![ROC Curve](../../benchmarking/NSFW_roc_curve.png)

#### Metrics Table

| Model         | ROC AUC | Prec@R=0.80 | Prec@R=0.90 | Prec@R=0.95 | Recall@FPR=0.01 |
|--------------|---------|-------------|-------------|-------------|-----------------|
| gpt-5        | 0.953   | 0.919       | 0.910       | 0.907       | 0.034           |
| gpt-5-mini   | 0.963   | 0.932       | 0.917       | 0.915       | 0.100           |
| gpt-4.1      | 0.960   | 0.931       | 0.925       | 0.919       | 0.044           |
| gpt-4.1-mini (default) | 0.952   | 0.918       | 0.913       | 0.905       | 0.046           |

**Notes:**

- ROC AUC: Area under the ROC curve (higher is better)
- Prec@R: Precision at the specified recall threshold
- Recall@FPR=0.01: Recall when the false positive rate is 1%
