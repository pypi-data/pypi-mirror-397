"""
Fast Query Processor - Claude Code Style
=========================================
Optimized 2-call approach with streaming and aggressive slicing.

Performance: 136s ‚Üí 26s (80% faster!)

Flow:
1. AI picks files from compact diagram (5-10s)
2. Aggressive slicing (0.01s)  
3. AI answers with streaming (15-20s)

Total: ~25-35s when model is warm
"""

import re
import json
import time
import requests
from pathlib import Path
from typing import Optional, Dict, List, Callable, Generator
from dataclasses import dataclass


# StackSense Identity - tells AI who it is and when to skip codebase scanning
STACKSENSE_SYSTEM_PROMPT = """You are StackSense, an AI-powered code intelligence assistant.

ABOUT YOU:
- You are StackSense v0.1.0, created by PilgrimStack
- Learn more about the creator: https://portfolio-pied-five-61.vercel.app/
- You help developers understand their codebase, debug issues, and guide development
- You analyze repository structure, dependencies, and serve as a coding mentor
- You run locally using Ollama for privacy and speed

GUIDELINES:
- When asked about yourself (who you are, what you do), answer from this identity
- When asked about your creator, mention PilgrimStack and his portfolio link
- For code questions, reference the provided code context
- Be concise and helpful, cite specific files when relevant
- If a question is general (not about the codebase), answer directly without needing code context

"""


@dataclass
class QueryResult:
    """Result from query processing"""
    answer: str
    files_used: List[str]
    context_size: int
    timings: Dict[str, float]
    total_time: float


class FastQueryProcessor:
    """
    Claude Code style query processor.
    
    Uses 2 AI calls instead of 3:
    1. AI picks files from compact diagram
    2. AI answers with sliced context
    
    Features:
    - Streaming responses
    - Aggressive slicing (9-12KB target)
    - Model kept warm between steps
    """
    
    def __init__(
        self,
        workspace_path: Path,
        diagram_path: Path,
        model_name: str = "phi3:mini",
        ollama_url: str = "http://localhost:11434",
        debug: bool = False
    ):
        self.workspace_path = Path(workspace_path)
        self.diagram_path = Path(diagram_path)
        self.model_name = model_name
        self.ollama_url = ollama_url
        self.debug = debug
        
        # Target context size
        self.TARGET_CONTEXT_BYTES = 12000  # 12KB
        
        # Callbacks for progress updates
        self.on_step_start: Optional[Callable[[str], None]] = None
        self.on_step_complete: Optional[Callable[[str, float], None]] = None
        self.on_stream_chunk: Optional[Callable[[str], None]] = None
    
    def warm_up_model(self) -> float:
        """Warm up the model to ensure fast responses"""
        start = time.time()
        self._generate("ping", max_tokens=5)
        return time.time() - start
    
    def keep_model_warm(self):
        """
        Quick ping to keep model in memory.
        IMPORTANT: Must generate at least 1 token to keep model loaded!
        Empty prompts do NOT keep the model warm.
        """
        try:
            # Generate 1 token - this keeps the model loaded
            requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    'model': self.model_name,
                    'prompt': 'hi',
                    'stream': False,
                    'keep_alive': '30m',
                    'options': {'num_predict': 1}  # Just 1 token
                },
                timeout=10
            )
        except:
            pass
    
    def _generate(
        self, 
        prompt: str, 
        max_tokens: int = 500,
        stream: bool = False
    ) -> str:
        """Generate response from Ollama"""
        try:
            if stream:
                return self._generate_stream(prompt, max_tokens)
            
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    'model': self.model_name,
                    'prompt': prompt,
                    'stream': False,
                    'keep_alive': '30m',
                    'options': {
                        'num_predict': max_tokens,
                        'temperature': 0.1
                    }
                },
                timeout=180
            )
            
            if response.status_code == 200:
                return response.json().get('response', '')
            return f"Error: {response.status_code}"
            
        except Exception as e:
            return f"Error: {e}"
    
    def _generate_stream(self, prompt: str, max_tokens: int = 1500) -> str:
        """Generate response with streaming"""
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    'model': self.model_name,
                    'prompt': prompt,
                    'stream': True,
                    'keep_alive': '30m',
                    'options': {
                        'num_predict': max_tokens,
                        'temperature': 0.3
                    }
                },
                stream=True,
                timeout=180
            )
            
            full_response = []
            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line)
                        chunk = data.get('response', '')
                        if chunk:
                            full_response.append(chunk)
                            # Callback for streaming
                            if self.on_stream_chunk:
                                self.on_stream_chunk(chunk)
                    except json.JSONDecodeError:
                        pass
            
            return ''.join(full_response)
            
        except Exception as e:
            return f"Error: {e}"
    
    def _load_compact_diagram(self) -> str:
        """Load and compress diagram for AI"""
        try:
            with open(self.diagram_path) as f:
                diagram = json.load(f)
            
            nodes = diagram.get('nodes', [])
            
            lines = [f"üìä {len(nodes)} Python files"]
            lines.append("\nüìÅ Key files:")
            
            # Group by type
            for node in nodes[:15]:  # Max 15 files shown
                node_id = node.get('id', '')
                lines.append(f"  ‚Ä¢ {node_id}")
            
            if len(nodes) > 15:
                lines.append(f"  ... +{len(nodes)-15} more")
            
            return '\n'.join(lines)
            
        except Exception as e:
            return f"Error loading diagram: {e}"
    
    def _select_files(self, query: str, compact_diagram: str) -> List[str]:
        """AI selects relevant files from diagram"""
        
        if self.on_step_start:
            self.on_step_start("Analyzing codebase structure")
        
        start = time.time()
        
        prompt = f"""CODEBASE:
{compact_diagram}

QUERY: "{query}"

Pick 3-4 most relevant files to answer this query.
List ONLY filenames, one per line:"""
        
        response = self._generate(prompt, max_tokens=100)
        
        elapsed = time.time() - start
        if self.on_step_complete:
            self.on_step_complete("Analyzed structure", elapsed)
        
        # Parse files
        files = []
        for line in response.split('\n'):
            line = line.strip().strip('-‚Ä¢*').strip()
            if line and '.py' in line:
                # Extract just filename
                match = re.search(r'[\w_]+\.py', line)
                if match:
                    files.append(match.group(0))
        
        return files[:20]  # Return up to 20 relevant files
    
    def _slice_file(self, filepath: str, query: str, budget: int = 3000) -> Dict:
        """Aggressively slice file to relevant sections"""
        
        # Try to find file
        full_path = self.workspace_path / filepath
        if not full_path.exists():
            # Try in subdirectories
            for subdir in ['agents', 'models', 'parsers']:
                alt_path = self.workspace_path / subdir / filepath
                if alt_path.exists():
                    full_path = alt_path
                    break
        
        if not full_path.exists():
            return {'content': '', 'original': 0, 'sliced': 0, 'file': filepath}
        
        try:
            content = full_path.read_text()
            original_size = len(content)
            
            # Extract key sections
            sliced = self._extract_key_sections(content, query, budget)
            
            return {
                'content': sliced,
                'original': original_size,
                'sliced': len(sliced),
                'file': filepath
            }
        except Exception as e:
            return {'content': f'# Error: {e}', 'original': 0, 'sliced': 0, 'file': filepath}
    
    def _extract_key_sections(self, content: str, query: str, budget: int) -> str:
        """Extract only relevant code sections"""
        
        lines = content.split('\n')
        sections = []
        current = []
        
        for i, line in enumerate(lines):
            # Start of class/function
            if re.match(r'^(class |def |async def )', line.strip()):
                if current:
                    sections.append('\n'.join(current))
                current = [line]
            elif current:
                current.append(line)
                # End section after reasonable length
                if len(current) > 25 or (not line.strip() and len(current) > 10):
                    sections.append('\n'.join(current))
                    current = []
        
        if current:
            sections.append('\n'.join(current))
        
        # Score by query relevance
        query_words = set(query.lower().split())
        scored = []
        for section in sections:
            score = sum(1 for w in query_words if w in section.lower())
            if 'class ' in section or 'def __init__' in section:
                score += 2
            if '"""' in section:
                score += 1
            scored.append((score, section))
        
        scored.sort(reverse=True)
        
        # Take best sections up to budget
        result = []
        total = 0
        for score, section in scored:
            if total + len(section) > budget:
                break
            result.append(section)
            total += len(section)
        
        return '\n\n'.join(result) if result else content[:budget]
    
    def _is_general_question(self, query: str) -> bool:
        """
        Detect if query is general (doesn't need codebase scanning).
        
        Returns True for questions like:
        - "who are you?"
        - "what's 2+2?"
        - "tell me a joke"
        """
        query_lower = query.lower().strip()
        
        # Code-related keywords that NEED codebase scanning
        code_keywords = [
            'function', 'class', 'file', 'code', 'implement', 
            'how does', 'where is', 'what does', 'error', 'bug', 
            'fix', 'variable', 'method', 'import', 'module',
            'api', 'endpoint', 'database', 'model', 'view',
            'config', 'setting', 'dependency', 'package',
            '.py', '.js', '.ts', '.go', '.rs', '.java',
            'folder', 'directory', 'project', 'codebase'
        ]
        
        # General question patterns that DON'T need codebase
        general_patterns = [
            'who are you', 'what are you', 'tell me about yourself',
            'who made you', 'who created you', 'your creator',
            "what's ", "what is ", 'calculate', 'compute',
            'tell me a', 'write a poem', 'joke', 'story',
            'hello', 'hi ', 'hey ', 'thanks', 'thank you',
            'help me understand', 'explain ', 'define '
        ]
        
        # Check if it matches general patterns
        for pattern in general_patterns:
            if pattern in query_lower:
                # Double check it's not code-related
                if not any(kw in query_lower for kw in code_keywords):
                    return True
        
        # Check if it has NO code keywords (likely general)
        if len(query_lower.split()) <= 10:  # Short query
            if not any(kw in query_lower for kw in code_keywords):
                return True
        
        return False
    
    def _answer_general_question(self, query: str, stream: bool = True) -> str:
        """Answer a general question without codebase context."""
        prompt = f"""{STACKSENSE_SYSTEM_PROMPT}QUERY: "{query}"

Answer this general question directly. You are StackSense, an AI assistant."""
        
        return self._generate(prompt, max_tokens=500, stream=stream)
    
    def process_query(
        self, 
        query: str,
        stream: bool = True
    ) -> QueryResult:
        """
        Process query using optimized Claude Code style.
        
        Args:
            query: User's question
            stream: Whether to stream the response
            
        Returns:
            QueryResult with answer, files used, timings
        """
        total_start = time.time()
        timings = {}
        
        # SMART SKIP: Check if this is a general question
        if self._is_general_question(query):
            if self.on_step_start:
                self.on_step_start("Answering directly")
            
            start = time.time()
            answer = self._answer_general_question(query, stream)
            elapsed = time.time() - start
            
            if self.on_step_complete:
                self.on_step_complete("Answered directly", elapsed)
            
            return QueryResult(
                answer=answer,
                files_used=[],  # No files needed
                context_size=0,
                timings={'direct_answer': elapsed},
                total_time=time.time() - total_start
            )
        
        # Step 0: Warm up model first (show as "Understanding query")
        if self.on_step_start:
            self.on_step_start("Understanding query")
        
        warmup_start = time.time()
        self.warm_up_model()  # Actual warmup, not just keep_alive
        timings['warmup'] = time.time() - warmup_start
        
        if self.on_step_complete:
            self.on_step_complete("Understanding query", timings['warmup'])
        
        # Step 1: Load compact diagram
        compact_diagram = self._load_compact_diagram()
        
        # Step 2: AI picks files
        step2_start = time.time()
        selected_files = self._select_files(query, compact_diagram)
        timings['file_selection'] = time.time() - step2_start
        
        if self.debug:
            print(f"Selected files: {selected_files}")
        
        # Keep model warm between steps
        self.keep_model_warm()
        
        # Step 3: Slice files
        if self.on_step_start:
            self.on_step_start("Reading relevant code")
        
        step3_start = time.time()
        budget_per_file = self.TARGET_CONTEXT_BYTES // max(len(selected_files), 1)
        
        sliced_files = []
        for filepath in selected_files:
            result = self._slice_file(filepath, query, budget_per_file)
            if result['sliced'] > 0:
                sliced_files.append(result)
                if self.debug:
                    print(f"  ‚úÇÔ∏è {filepath}: {result['original']} ‚Üí {result['sliced']} bytes")
        
        timings['slicing'] = time.time() - step3_start
        
        if self.on_step_complete:
            self.on_step_complete(f"Read {len(sliced_files)} files", timings['slicing'])
        
        total_context = sum(f['sliced'] for f in sliced_files)
        
        # Keep model warm
        self.keep_model_warm()
        
        # Step 4: AI answers with streaming
        if self.on_step_start:
            self.on_step_start("Generating answer")
        
        step4_start = time.time()
        
        # Format context
        code_context = ""
        for f in sliced_files:
            code_context += f"\n--- {f['file']} ---\n{f['content']}\n"
        
        answer_prompt = f"""{STACKSENSE_SYSTEM_PROMPT}QUERY: "{query}"

RELEVANT CODE:
{code_context[:self.TARGET_CONTEXT_BYTES]}

Answer the query based on this code. Be specific and cite filenames."""
        
        answer = self._generate(answer_prompt, max_tokens=1500, stream=stream)
        
        timings['answer_generation'] = time.time() - step4_start
        
        if self.on_step_complete and not stream:
            self.on_step_complete("Generated answer", timings['answer_generation'])
        
        total_time = time.time() - total_start
        
        return QueryResult(
            answer=answer,
            files_used=[f['file'] for f in sliced_files],
            context_size=total_context,
            timings=timings,
            total_time=total_time
        )


class StreamingProgressUI:
    """
    Rich UI for streaming responses with progress indicators.
    """
    
    def __init__(self, console=None):
        try:
            from rich.console import Console
            from rich.live import Live
            from rich.panel import Panel
            from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
            from rich.markdown import Markdown
            
            self.console = console or Console()
            self.Live = Live
            self.Panel = Panel
            self.Progress = Progress
            self.SpinnerColumn = SpinnerColumn
            self.TextColumn = TextColumn
            self.BarColumn = BarColumn
            self.TimeElapsedColumn = TimeElapsedColumn
            self.Markdown = Markdown
            self.rich_available = True
        except ImportError:
            self.rich_available = False
            self.console = None
    
    def create_progress(self):
        """Create a progress bar for steps"""
        if not self.rich_available:
            return None
        
        return self.Progress(
            self.SpinnerColumn(),
            self.TextColumn("[bold blue]{task.description}"),
            self.BarColumn(),
            self.TimeElapsedColumn(),
            console=self.console,
            transient=True
        )
    
    def show_step(self, step_name: str, completed: bool = False, time_taken: float = None):
        """Show a step in the progress"""
        if completed:
            time_str = f"({time_taken:.1f}s)" if time_taken else ""
            print(f"   ‚úì {step_name} {time_str}")
        else:
            print(f"   ‚ñ∏ {step_name}...")
    
    def stream_response(self, model_name: str):
        """Context manager for streaming response display"""
        if self.rich_available:
            return self._rich_stream(model_name)
        else:
            return self._simple_stream(model_name)
    
    def _rich_stream(self, model_name: str):
        """Rich streaming with live update"""
        from contextlib import contextmanager
        
        @contextmanager
        def stream_context():
            self.console.print(f"\n[bold cyan]{model_name}[/bold cyan] (streaming):\n")
            buffer = []
            
            def on_chunk(chunk):
                buffer.append(chunk)
                print(chunk, end='', flush=True)
            
            yield on_chunk
            
            print("\n")
        
        return stream_context()
    
    def _simple_stream(self, model_name: str):
        """Simple streaming without Rich"""
        from contextlib import contextmanager
        
        @contextmanager
        def stream_context():
            print(f"\n{model_name}:\n")
            
            def on_chunk(chunk):
                print(chunk, end='', flush=True)
            
            yield on_chunk
            
            print("\n")
        
        return stream_context()
    
    def show_stats(self, result: QueryResult):
        """Show query statistics"""
        print()
        print("‚îÄ" * 50)
        stats = f"üìä {result.total_time:.1f}s | {len(result.files_used)} files | {result.context_size//1000}KB context"
        print(stats)
        
        if self.rich_available:
            details = " | ".join([f"{k}: {v:.1f}s" for k, v in result.timings.items()])
            self.console.print(f"[dim]{details}[/dim]")
