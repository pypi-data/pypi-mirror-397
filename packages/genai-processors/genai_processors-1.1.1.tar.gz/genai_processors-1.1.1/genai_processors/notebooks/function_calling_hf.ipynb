{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBxPnCcuuAuO"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 DeepMind Technologies Limited. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "metadata": {
        "id": "x8Qd6mWRuFHz"
      },
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-gemini/genai-processors/blob/main/notebooks/agent_gemma.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "A8yIcf81uf_s"
      },
      "cell_type": "markdown",
      "source": [
        "# Function Calling with Transformers ü§ñ\n",
        "\n",
        "The GenAI Processor library provides an easy way to call a transformer with a\n",
        "Function Calling loop. This notebook provides a step-by-step guide on how to do\n",
        "this.\n",
        "\n",
        ""
      ]
    },
    {
      "metadata": {
        "id": "rYqU_6n4v-dQ"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. üõ†Ô∏è Setup\n",
        "\n",
        "First, install the GenAI Processors library:"
      ]
    },
    {
      "metadata": {
        "id": "bDal7EHPwCDk"
      },
      "cell_type": "code",
      "source": [
        "!pip install genai-processors"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "wWoaB7Xw1I-1"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. üëÜ Define the functions to be called\n",
        "\n",
        "The tool definition is automatically derived from the function signature.\n",
        "\n",
        "A good docstring covering the arguments and returned value is therefore advised.\n",
        "Although introspection allows the model to see the function schema even without\n",
        "a docstring, its absence will still hinder the model's ability to generate a\n",
        "correct function call."
      ]
    },
    {
      "metadata": {
        "id": "GuSwrv5D1pAy"
      },
      "cell_type": "code",
      "source": [
        "def get_temperature(location: str) -> str:\n",
        "  \"\"\"Gets the temperature in Celsius at location including a weather description.\n",
        "\n",
        "  Args:\n",
        "    location: name of the city, region or place where the weather is requested.\n",
        "  \"\"\"\n",
        "  return {'temperature': 21, 'weather': 'rainy'}\n",
        "\n",
        "\n",
        "def to_fahrenheit(celsius: float) -> float:\n",
        "  \"\"\"Gets the temperature in Fahrenheit from a temperature in Celsius.\n",
        "\n",
        "  Args:\n",
        "    celsius: temperature in Celsius.\n",
        "  \"\"\"\n",
        "  fahrenheit = (celsius * 9 / 5) + 32\n",
        "  return fahrenheit"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "pklvojGQ2_A9"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. ‚ú® Create the Transformer model\n",
        "\n",
        "Set the following parameters in the form below:\n",
        "\n",
        "-   **model_name**: Set the model name to a HuggingFace transformer name. The\n",
        "    model should support tool use. Note that loading the model can take a while.\n",
        "\n",
        "-   **log_chat_template**: True if you want to log the prompt to the transformer\n",
        "    in your colab runtime logs. This is quite verbose but useful for debugging.\n",
        "\n",
        "-   **tool_response_format_dict**: True if the function declaration uses a\n",
        "    dictionary instead of a plain string to represents its output. Set to False\n",
        "    for std transformers.\n",
        "\n",
        "-   **system_instructions**: this will be prepended to the transformer chat\n",
        "    prompt."
      ]
    },
    {
      "metadata": {
        "id": "8ArjWNMB3Nyn"
      },
      "cell_type": "code",
      "source": [
        "from genai_processors.core import transformers_model\n",
        "\n",
        "model_name = ''  # @param {type: \"string\"}\n",
        "tool_response_format_dict = True  # @param {\"type\":\"boolean\"}\n",
        "log_chat_template = True  # @param {\"type\":\"boolean\"}\n",
        "system_instruction = ''  # @param {type: \"string\"}\n",
        "\n",
        "tool_response_format = 'dict' if tool_response_format_dict else 'string'\n",
        "\n",
        "\n",
        "# Initialize the GenAI model processor\n",
        "hf_model = transformers_model.TransformersModel(\n",
        "    model_name=model_name,\n",
        "    generate_content_config=transformers_model.GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "        tools=[get_temperature, to_fahrenheit],\n",
        "    ),\n",
        "    log_chat_template=log_chat_template,\n",
        "    tool_response_format=tool_response_format,  # NOTE: Set to True for std models.\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "SPwAeFQh4CXF"
      },
      "cell_type": "markdown",
      "source": [
        "Then adding function calling is done as follows:"
      ]
    },
    {
      "metadata": {
        "id": "UC3jcXXE4IMP"
      },
      "cell_type": "code",
      "source": [
        "# @title Adding function calling loop\n",
        "\n",
        "from genai_processors import debug\n",
        "from genai_processors.core import function_calling\n",
        "\n",
        "fc = function_calling.FunctionCalling(\n",
        "    debug.print_stream('MODEL') + hf_model,\n",
        "    fns=[get_temperature, to_fahrenheit],\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "btH0h1Gq47ra"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. ‚ñ∂Ô∏è Run the function calling processor\n",
        "\n",
        "The function calling processor is typically used on a stream of user prompts."
      ]
    },
    {
      "metadata": {
        "id": "TXDcjJXg5A_i"
      },
      "cell_type": "code",
      "source": [
        "from genai_processors import streams\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()  # Needed to run async loops in Colab\n",
        "\n",
        "input_stream = streams.stream_content(['What is the temperature in London?'])\n",
        "\n",
        "async for part in fc(input_stream):\n",
        "  if not part.substream_name:\n",
        "    # default substream - contains what the user would see.\n",
        "    print(f'{part.text}', flush=True, end='')\n",
        "\n",
        "  if part.substream_name:\n",
        "    # subtream_name = \"function_call\" / internal function calls.\n",
        "    if part.function_call:\n",
        "      print(\n",
        "          f'\\033[96m FC: {part.function_call.name}:'\n",
        "          f' {part.function_call.args}\\033[0m ',\n",
        "          flush=True,\n",
        "      )\n",
        "    elif part.function_response:\n",
        "      print(\n",
        "          f'\\033[96m FR: {part.function_response.response}\\033[0m ',\n",
        "          flush=True,\n",
        "      )\n",
        "    else:\n",
        "      print(f'\\033[96m {part}\\033[0m ', flush=True, end='')"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      }
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
