{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Reading PyArrow Dataset from S3\n",
                "\n",
                "This example demonstrates how to use fsspec-utils to read PyArrow datasets from \n",
                "S3-compatible storage systems including AWS S3, Cloudflare R2, and self-hosted MinIO.\n",
                "\n",
                "The example shows:\n",
                "1. Configuring storage options for different S3-compatible services\n",
                "2. Creating PyArrow datasets from these storage systems\n",
                "3. Reading data into PyArrow tables\n",
                "4. Working with partitioned datasets\n",
                "5. Using optimized dataset reading with metadata files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pyarrow as pa\n",
                "import pyarrow.dataset as pds\n",
                "\n",
                "from fsspeckit import filesystem\n",
                "from fsspeckit.storage_options import AwsStorageOptions"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## AWS S3 Configuration\n",
                "\n",
                "Configure AWS S3 storage options and create a PyArrow dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configure AWS S3 storage options\n",
                "# Replace with your actual AWS credentials and region\n",
                "s3_options = AwsStorageOptions(\n",
                "    access_key_id=\"YOUR_AWS_ACCESS_KEY_ID\",  # Replace with your AWS access key\n",
                "    secret_access_key=\"YOUR_AWS_SECRET_ACCESS_KEY\",  # Replace with your AWS secret key\n",
                "    region=\"us-east-1\",  # AWS region\n",
                ")\n",
                "\n",
                "# Create fsspec filesystem instance from storage options\n",
                "fs = filesystem(\"s3\", storage_options=s3_options)\n",
                "\n",
                "# Create PyArrow dataset from S3 bucket\n",
                "# Assumes Parquet data in s3://your-bucket/data/\n",
                "try:\n",
                "    dataset = fs.pyarrow_dataset(\"s3://your-bucket/data/\")\n",
                "\n",
                "    # Read data from the dataset into a PyArrow table\n",
                "    table = dataset.to_table()\n",
                "\n",
                "    print(f\"Dataset schema: {dataset.schema}\")\n",
                "    print(f\"Table shape: {table.shape}\")\n",
                "    print(f\"First few rows:\\n{table.slice(0, 5)}\")\n",
                "except Exception as e:\n",
                "    print(f\"Error reading from AWS S3: {e}\")\n",
                "    print(\"Make sure you have valid AWS credentials and the bucket exists.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cloudflare R2 Configuration\n",
                "\n",
                "Configure Cloudflare R2 storage options and create a PyArrow dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configure Cloudflare R2 storage options\n",
                "# R2 is S3-compatible, so we use AwsStorageOptions with a custom endpoint\n",
                "# Replace with your actual R2 credentials and account ID\n",
                "r2_options = AwsStorageOptions(\n",
                "    access_key_id=\"YOUR_R2_ACCESS_KEY_ID\",  # Replace with your R2 access key\n",
                "    secret_access_key=\"YOUR_R2_SECRET_KEY\",  # Replace with your R2 secret key\n",
                "    endpoint_url=\"https://YOUR_ACCOUNT_ID.r2.cloudflarestorage.com\",  # R2 endpoint URL\n",
                "    # Note: R2 doesn't use AWS regions in the same way\n",
                ")\n",
                "\n",
                "# Create fsspec filesystem instance for R2\n",
                "r2_fs = filesystem(\"s3\", storage_options=r2_options)\n",
                "\n",
                "# Create PyArrow dataset from R2 bucket\n",
                "try:\n",
                "    r2_dataset = r2_fs.pyarrow_dataset(\"your-bucket-name/data/\")\n",
                "\n",
                "    # Read data from the R2 dataset\n",
                "    r2_table = r2_dataset.to_table()\n",
                "\n",
                "    print(f\"R2 Dataset schema: {r2_dataset.schema}\")\n",
                "    print(f\"R2 Table shape: {r2_table.shape}\")\n",
                "except Exception as e:\n",
                "    print(f\"Error reading from Cloudflare R2: {e}\")\n",
                "    print(\"Make sure you have valid R2 credentials and the bucket exists.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## MinIO Configuration\n",
                "\n",
                "Configure MinIO storage options and create a PyArrow dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configure MinIO storage options\n",
                "# MinIO is S3-compatible, so we use AwsStorageOptions with custom endpoint and credentials\n",
                "# Replace with your actual MinIO credentials and endpoint\n",
                "minio_options = AwsStorageOptions(\n",
                "    access_key_id=\"YOUR_MINIO_ACCESS_KEY\",  # Your MinIO access key\n",
                "    secret_access_key=\"YOUR_MINIO_SECRET_KEY\",  # Your MinIO secret key\n",
                "    endpoint_url=\"http://localhost:9000\",  # MinIO server endpoint\n",
                "    allow_http=True,  # Allow HTTP (not HTTPS) for local development\n",
                "    # Note: MinIO doesn't require AWS regions\n",
                ")\n",
                "\n",
                "# Create fsspec filesystem instance for MinIO\n",
                "minio_fs = filesystem(\"s3\", storage_options=minio_options)\n",
                "\n",
                "# Create PyArrow dataset from MinIO bucket\n",
                "try:\n",
                "    minio_dataset = minio_fs.pyarrow_dataset(\"your-bucket/data/\")\n",
                "\n",
                "    # Read data from the MinIO dataset\n",
                "    minio_table = minio_dataset.to_table()\n",
                "\n",
                "    print(f\"MinIO Dataset schema: {minio_dataset.schema}\")\n",
                "    print(f\"MinIO Table shape: {minio_table.shape}\")\n",
                "except Exception as e:\n",
                "    print(f\"Error reading from MinIO: {e}\")\n",
                "    print(\"Make sure you have a MinIO server running and the bucket exists.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Working with Partitioned Datasets\n",
                "\n",
                "Example of working with partitioned datasets and using partition pruning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For partitioned Parquet data (e.g., data partitioned by date)\n",
                "try:\n",
                "    partitioned_dataset = fs.pyarrow_dataset(\n",
                "        \"s3://your-bucket/partitioned-data/\",\n",
                "        partitioning=[\"year\", \"month\", \"day\"],  # Hive-style partitioning\n",
                "    )\n",
                "\n",
                "    # Query with partition pruning - only read specific partitions\n",
                "    filtered_table = partitioned_dataset.to_table(\n",
                "        filter=(\n",
                "            (partitioned_dataset.field(\"year\") == 2024)\n",
                "            & (partitioned_dataset.field(\"month\") == 1)\n",
                "            & (partitioned_dataset.field(\"day\") > 15)\n",
                "        )\n",
                "    )\n",
                "\n",
                "    print(f\"Filtered table shape: {filtered_table.shape}\")\n",
                "except Exception as e:\n",
                "    print(f\"Error working with partitioned dataset: {e}\")\n",
                "    print(\"Make sure you have a partitioned dataset in the specified location.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Optimized Parquet Dataset Reading\n",
                "\n",
                "Using `pyarrow_parquet_dataset` for optimized reading with metadata files."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# If you have a _metadata file in your dataset directory\n",
                "try:\n",
                "    parquet_dataset = fs.pyarrow_parquet_dataset(\"s3://your-bucket/data-with-metadata/\")\n",
                "\n",
                "    # This automatically uses the _metadata file for optimized reading\n",
                "    optimized_table = parquet_dataset.to_table()\n",
                "\n",
                "    print(f\"Optimized table shape: {optimized_table.shape}\")\n",
                "    print(f\"Dataset files: {parquet_dataset.files}\")\n",
                "except Exception as e:\n",
                "    print(f\"Error reading optimized Parquet dataset: {e}\")\n",
                "    print(\n",
                "        \"Make sure you have a dataset with a _metadata file in the specified location.\"\n",
                "    )"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}