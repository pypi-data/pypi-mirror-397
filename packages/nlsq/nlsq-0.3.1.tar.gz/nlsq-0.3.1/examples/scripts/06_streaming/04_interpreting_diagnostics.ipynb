{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    return a + b * x + c * x**2\ndef print_diagnostics_structure(diagnostics):\n    \"\"\"Print the structure and contents of diagnostics dictionary\"\"\"\n    print(\"DIAGNOSTICS STRUCTURE\")\n    print(\"=\" * 70)\n    print()\n    print(\"Available diagnostic fields:\")\n    for key in sorted(diagnostics.keys()):\n        value_type = type(diagnostics[key]).__name__\n        print(f\"  - {key:30s} : {value_type}\")\n    print()\ndef analyze_success_metrics(diagnostics):\n    \"\"\"Analyze overall success metrics\"\"\"\n    print(\"SUCCESS METRICS\")\n    print(\"=\" * 70)\n    print(f\"Batch success rate: {diagnostics['batch_success_rate']:.1%}\")\n    print(f\"Total batches attempted: {diagnostics['total_batches_attempted']}\")\n    print(f\"Failed batches: {len(diagnostics['failed_batches'])}\")\n    print(f\"Total retries: {diagnostics['total_retries']}\")\n    print(f\"Convergence achieved: {diagnostics['convergence_achieved']}\")\n    print(f\"Final epoch: {diagnostics['final_epoch']}\")\n    print(f\"Elapsed time: {diagnostics['elapsed_time']:.2f}s\")\n    print()\ndef analyze_failure_patterns(diagnostics):\n    \"\"\"Analyze failure patterns and error types\"\"\"\n    print(\"FAILURE ANALYSIS\")\n    print(\"=\" * 70)\n    if not diagnostics[\"failed_batches\"]:\n        print(\"No failed batches!\")\n        print()\n        return\n    print(f\"Failed batch indices: {diagnostics['failed_batches']}\")\n    print()\n    print(\"Error Type Distribution:\")\n    error_types = diagnostics[\"error_types\"]\n    total_errors = sum(error_types.values())\n    for error_type, count in sorted(\n        error_types.items(), key=lambda x: x[1], reverse=True\n    ):\n        pct = count / total_errors * 100\n        print(f\"  {error_type:20s}: {count:3d} ({pct:5.1f}%)\")\n    print()\n    print(\"Retry Patterns:\")\n    retry_counts = diagnostics[\"retry_counts\"]\n    if retry_counts:\n        retry_values = list(retry_counts.values())\n        print(f\"  Batches with retries: {len(retry_values)}\")\n        print(f\"  Min retries: {min(retry_values)}\")\n        print(f\"  Max retries: {max(retry_values)}\")\n        print(f\"  Avg retries: {np.mean(retry_values):.2f}\")\n        print(f\"  Total retries: {sum(retry_values)}\")\n    else:\n        print(\"  No retries performed\")\n    print()\ndef analyze_aggregate_statistics(diagnostics):\n    \"\"\"Analyze aggregate statistics from batch buffer\"\"\"\n    print(\"AGGREGATE STATISTICS\")\n    print(\"=\" * 70)\n    agg = diagnostics[\"aggregate_stats\"]\n    print(f\"Mean loss:          {agg['mean_loss']:.6e}\")\n    print(f\"Std loss:           {agg['std_loss']:.6e}\")\n    print(f\"Mean gradient norm: {agg['mean_grad_norm']:.6f}\")\n    print(f\"Std gradient norm:  {agg['std_grad_norm']:.6f}\")\n    print(f\"Mean batch time:    {agg['mean_batch_time'] * 1000:.2f}ms\")\n    print(f\"Std batch time:     {agg['std_batch_time'] * 1000:.2f}ms\")\n    print()\n    print(\"Interpretation:\")\n    cv_loss = agg[\"std_loss\"] / max(agg[\"mean_loss\"], 1e-10)\n    print(f\"  - Coefficient of variation (loss): {cv_loss:.2%}\")\n    if cv_loss < 0.1:\n        print(\"    => Very stable optimization\")\n    elif cv_loss < 0.5:\n        print(\"    => Moderately stable optimization\")\n    else:\n        print(\"    => High variability in loss\")\n    print()\ndef analyze_recent_batches(diagnostics, n_recent=10):\n    \"\"\"Analyze recent batch statistics\"\"\"\n    print(f\"RECENT BATCH STATISTICS (last {n_recent} batches)\")\n    print(\"=\" * 70)\n    recent_stats = diagnostics[\"recent_batch_stats\"]\n    if not recent_stats:\n        print(\"No batch statistics available\")\n        print()\n        return\n    last_n = recent_stats[-n_recent:]\n    print(f\"Showing {len(last_n)} most recent batches:\")\n    print()\n    print(\n        f\"{'Batch':>8s} {'Status':>10s} {'Loss':>12s} {'GradNorm':>10s} {'Time':>8s} {'Retries':>8s}\"\n    )\n    print(\"-\" * 70)\n    for stats in last_n:\n        batch_idx = stats[\"batch_idx\"]\n        status = \"SUCCESS\" if stats[\"success\"] else \"FAILED\"\n        loss = stats[\"loss\"]\n        grad_norm = stats[\"grad_norm\"]\n        batch_time = stats[\"batch_time\"] * 1000  # Convert to ms\n        retry_count = stats[\"retry_count\"]\n        loss_str = f\"{loss:.4e}\" if np.isfinite(loss) else \"inf\"\n        print(\n            f\"{batch_idx:8d} {status:>10s} {loss_str:>12s} {grad_norm:10.4f} {batch_time:7.1f}ms {retry_count:8d}\"\n        )\n    print()\n    successful_recent = [s for s in last_n if s[\"success\"]]\n    if successful_recent:\n        recent_losses = [s[\"loss\"] for s in successful_recent]\n        print(\"Recent batch statistics:\")\n        print(f\"  Success rate: {len(successful_recent) / len(last_n):.1%}\")\n        print(f\"  Mean loss: {np.mean(recent_losses):.6e}\")\n        print(f\"  Min loss: {min(recent_losses):.6e}\")\n        print(f\"  Max loss: {max(recent_losses):.6e}\")\n        print()\ndef analyze_checkpoint_info(diagnostics):\n    \"\"\"Analyze checkpoint information\"\"\"\n    print(\"CHECKPOINT INFORMATION\")\n    print(\"=\" * 70)\n    cp_info = diagnostics.get(\"checkpoint_info\")\n    if not cp_info:\n        print(\"No checkpoint information available\")\n        print(\"(Checkpoints may be disabled or not saved yet)\")\n        print()\n        return\n    print(\"Latest checkpoint:\")\n    print(f\"  Path: {cp_info['path']}\")\n    print(f\"  Saved at: {cp_info['saved_at']}\")\n    print(f\"  Batch index: {cp_info['batch_idx']}\")\n    print()\n    print(\"Resume using:\")\n    print(f\"  config = StreamingConfig(resume_from_checkpoint='{cp_info['path']}')\")\n    print()\ndef export_diagnostics_json(diagnostics, filename=\"diagnostics.json\"):\n    \"\"\"Export diagnostics to JSON for further analysis\"\"\"\n    print(\"EXPORT DIAGNOSTICS\")\n    print(\"=\" * 70)\n    diagnostics_copy = diagnostics.copy()\n    with open(filename, \"w\") as f:\n        json.dump(diagnostics_copy, f, indent=2)\n    print(f\"Diagnostics exported to: {filename}\")\n    print(f\"File size: {len(json.dumps(diagnostics_copy))} bytes\")\n    print()\ndef main():\n    print(\"=\" * 70)\n    print(\"Streaming Optimizer: Interpreting Diagnostics Example\")\n    print(\"=\" * 70)\n    print()\n    np.random.seed(42)\n    n_samples = 5000\n    x_data = np.linspace(-5, 5, n_samples)\n    true_a, true_b, true_c = 1.0, 2.0, -0.5\n    y_true = polynomial_model(x_data, true_a, true_b, true_c)\n    y_data = y_true + 0.2 * np.random.randn(n_samples)\n    print(f\"Dataset: {n_samples} samples\")\n    print(f\"True parameters: a={true_a}, b={true_b}, c={true_c}\")\n    print()\n    config = StreamingConfig(\n        batch_size=100,\n        max_epochs=5,\n        learning_rate=0.001,\n        enable_fault_tolerance=True,\n        checkpoint_dir=\"checkpoints_diagnostics\",\n        checkpoint_frequency=10,\n        enable_checkpoints=True,\n        batch_stats_buffer_size=100,  # Track last 100 batches\n    )\n    optimizer = StreamingOptimizer(config)\n    p0 = np.array([0.5, 1.0, -0.2])\n    print(\"Running optimization...\")\n    result = optimizer.fit(\n        (x_data, y_data),\n        polynomial_model,\n        p0,\n        verbose=1,\n    )\n    print()\n    print(\"Optimization complete!\")\n    print()\n    diagnostics = result[\"streaming_diagnostics\"]\n    print_diagnostics_structure(diagnostics)\n    analyze_success_metrics(diagnostics)\n    analyze_failure_patterns(diagnostics)\n    analyze_aggregate_statistics(diagnostics)\n    analyze_recent_batches(diagnostics, n_recent=10)\n    analyze_checkpoint_info(diagnostics)\n    export_diagnostics_json(diagnostics, \"streaming_diagnostics_example.json\")\n    print(\"FINAL RESULTS\")\n    print(\"=\" * 70)\n    best_params = result[\"x\"]\n    print(\"Best parameters:\")\n    print(f\"  a = {best_params[0]:.6f} (true: {true_a})\")\n    print(f\"  b = {best_params[1]:.6f} (true: {true_b})\")\n    print(f\"  c = {best_params[2]:.6f} (true: {true_c})\")\n    print(f\"  Best loss = {result['best_loss']:.6e}\")\n    print()\n    print(\"=\" * 70)\n    print(\"Example complete!\")\n    print()\n    print(\"Key takeaways:\")\n    print(\"  - streaming_diagnostics contains comprehensive information\")\n    print(\"  - Aggregate statistics summarize overall performance\")\n    print(\"  - Recent batch statistics show optimization trajectory\")\n    print(\"  - Checkpoint information enables recovery\")\n    print(\"  - Error analysis helps diagnose issues\")\n    print(\"  - Diagnostics can be exported to JSON for further analysis\")\nif __name__ == \"__main__\":\n    main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
