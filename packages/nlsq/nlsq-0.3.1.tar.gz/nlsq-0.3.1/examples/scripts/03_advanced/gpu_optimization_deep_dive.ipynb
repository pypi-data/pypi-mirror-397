{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    params = jnp.array([3.0, 0.5])\n    def loss(p):\n        return jnp.sum((y_single - exponential_model(x_batch_data, *p)) ** 2)\n    for _ in range(20):\n        g = jax.grad(loss)(params)\n        params = params - 0.05 * g\n    return params\nfit_batch = jit(vmap(fit_one_dataset))\n_ = fit_batch(y_batch_data[:10])\nstart = time.time()\nresults_batch = fit_batch(y_batch_data)\nresults_batch[0].block_until_ready()\ntime_batch = time.time() - start\nprint(\n    f\"  Time for {n_datasets} datasets: {time_batch * 1000:.0f} ms ({time_batch * 1000 / n_datasets:.3f} ms/fit)\"\n)\nprint(f\"  Throughput: {n_datasets / time_batch:.0f} fits/second\")\nprint()\nestimated_sequential_time = time_sequential * n_datasets / 100\nspeedup = estimated_sequential_time / time_batch\nprint(f\"Speedup: {speedup:.0f}x faster with vmap + JIT \u2713\")\nprint()\nprint(\"Key insight: vmap parallelizes across datasets, JIT compiles once\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part 4: Memory Optimization\n",
        "\n",
        "Avoiding out-of-memory (OOM) errors with large datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Memory Optimization Strategies:\")\nprint(\"=\" * 60)\nprint()\nprint(\"1. Use float32 instead of float64:\")\nx_f64 = jnp.array([1.0, 2.0, 3.0], dtype=jnp.float64)\nx_f32 = jnp.array([1.0, 2.0, 3.0], dtype=jnp.float32)\nprint(f\"   float64 memory: {x_f64.nbytes} bytes per element\")\nprint(f\"   float32 memory: {x_f32.nbytes} bytes per element\")\nprint(f\"   Savings: {(1 - x_f32.nbytes / x_f64.nbytes) * 100:.0f}%\")\nprint(\"   \u2192 Use float32 unless high precision is critical\\n\")\nprint(\"2. Process data in chunks (streaming):\")\nprint(\"   # For very large datasets (millions of points)\")\nprint(\"   chunk_size = 100000\")\nprint(\"   for i in range(0, len(data), chunk_size):\")\nprint(\"       chunk = data[i:i+chunk_size]\")\nprint(\"       result = fit(chunk)\")\nprint(\"       results.append(result)\\n\")\nprint(\"3. Clear JAX cache if needed:\")\nprint(\"   from jax import clear_caches\")\nprint(\"   clear_caches()  # Frees compilation cache\\n\")\nprint(\"4. Monitor memory usage:\")\ndef get_array_memory_mb(arr):\n    return arr.nbytes / (1024**2)\nlarge_array = jnp.ones((10000, 1000), dtype=jnp.float32)\nprint(\n    f\"   Example: {large_array.shape} array uses {get_array_memory_mb(large_array):.1f} MB\"\n)\nprint()\nprint(\"5. Typical memory requirements:\")\nprint(\"   10K points:     ~0.1 MB (negligible)\")\nprint(\"   1M points:      ~10 MB (easy)\")\nprint(\"   100M points:    ~1 GB (manageable)\")\nprint(\"   1B points:      ~10 GB (need chunking or distributed)\")\nprint()\nprint(\"\u2192 For datasets >100M points, use chunked processing or streaming\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part 5: Performance Benchmarking\n",
        "\n",
        "Systematic performance measurement and optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def benchmark_nlsq(n_points_list, n_params=2, n_runs=5):\n    \"\"\"Benchmark NLSQ across different problem sizes.\n    Parameters\n    ----------\n    n_points_list : list\n        List of dataset sizes to test\n    n_params : int\n        Number of parameters to fit\n    n_runs : int\n        Number of runs to average\n    Returns\n    -------\n    results : dict\n        Benchmark results\n    \"\"\"\n    results = {\"n_points\": [], \"mean_time_ms\": [], \"std_time_ms\": []}\n    cf_bench = CurveFit()\n    for n_points in n_points_list:\n        x = jnp.linspace(0, 5, n_points)\n        y = 3.0 * jnp.exp(-0.5 * x) + np.random.normal(0, 0.1, n_points)\n        _ = cf_bench.curve_fit(exponential_model, x, y, p0=[2.0, 0.3], maxiter=20)\n        times = []\n        for _ in range(n_runs):\n            start = time.time()\n            popt, _ = cf_bench.curve_fit(\n                exponential_model, x, y, p0=[2.0, 0.3], maxiter=20\n            )\n            times.append((time.time() - start) * 1000)\n        results[\"n_points\"].append(n_points)\n        results[\"mean_time_ms\"].append(np.mean(times))\n        results[\"std_time_ms\"].append(np.std(times))\n    return results\nprint(\"Running comprehensive benchmark...\")\nprint(\"(This may take 30-60 seconds)\")\nprint()\nsizes = [100, 500, 1000, 5000, 10000]\nbench_results = benchmark_nlsq(sizes, n_runs=5)\nprint(\"Benchmark Results:\")\nprint(\"=\" * 60)\nprint(f\"{'N Points':<12} {'Mean Time (ms)':<20} {'Throughput (fits/s)'}\")\nprint(\"-\" * 60)\nfor i, n in enumerate(bench_results[\"n_points\"]):\n    mean_t = bench_results[\"mean_time_ms\"][i]\n    std_t = bench_results[\"std_time_ms\"][i]\n    throughput = 1000 / mean_t\n    print(f\"{n:<12} {mean_t:>8.2f} \u00b1 {std_t:<8.2f} {throughput:>12.1f}\")\nprint()\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\nax1.errorbar(\n    bench_results[\"n_points\"],\n    bench_results[\"mean_time_ms\"],\n    yerr=bench_results[\"std_time_ms\"],\n    marker=\"o\",\n    capsize=5,\n    label=\"NLSQ\",\n)\nax1.set_xlabel(\"Number of Data Points\")\nax1.set_ylabel(\"Time (ms)\")\nax1.set_title(\"Performance Scaling\")\nax1.legend()\nax1.grid(alpha=0.3)\nax2.loglog(bench_results[\"n_points\"], bench_results[\"mean_time_ms\"], \"o-\", label=\"NLSQ\")\nax2.set_xlabel(\"Number of Data Points\")\nax2.set_ylabel(\"Time (ms)\")\nax2.set_title(\"Scaling Behavior (log-log)\")\nax2.legend()\nax2.grid(alpha=0.3, which=\"both\")\nplt.tight_layout()\nfig_dir = Path(__file__).parent / \"figures\" / \"gpu_optimization_deep_dive\"\nfig_dir.mkdir(parents=True, exist_ok=True)\nplt.savefig(fig_dir / \"fig_01.png\", dpi=300, bbox_inches=\"tight\")\nplt.close()\nprint(\"Interpretation:\")\nprint(\"  - Nearly flat scaling: Well-optimized (GPU benefits)\")\nprint(\"  - Linear scaling: Expected for iterative optimization\")\nprint(\"  - Superlinear scaling: May indicate memory issues or poor caching\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Summary and Best Practices\n",
        "\n",
        "Performance Optimization Checklist\n",
        "\n",
        "**For Maximum Speed:**\n",
        "\n",
        "1. \u2705 **Use GPU** if available (5-50x speedup for large problems)\n",
        "2. \u2705 **Keep array shapes consistent** to avoid recompilation\n",
        "3. \u2705 **Use float32** unless high precision is needed (2x memory savings)\n",
        "4. \u2705 **Batch process** with `vmap` for multiple datasets (10-100x faster)\n",
        "5. \u2705 **Warm up JIT** with small dataset before benchmarking\n",
        "6. \u2705 **Use `block_until_ready()`** when timing (JAX is async)\n",
        "\n",
        "**For Large Datasets:**\n",
        "\n",
        "1. \u2705 **Chunk data** if >100M points\n",
        "2. \u2705 **Monitor memory** usage\n",
        "3. \u2705 **Consider downsampling** for smooth, oversampled data\n",
        "4. \u2705 **Use streaming** for datasets that don't fit in memory\n",
        "\n",
        "Performance Expectations\n",
        "\n",
        "| **Scenario** | **Typical Time** | **Optimization** |\n",
        "|--------------|------------------|------------------|\n",
        "| First call (cold start) | 0.5-2 seconds | Expected (JIT compilation) |\n",
        "| Subsequent calls (warm) | 1-50 ms | Cached compilation |\n",
        "| Large dataset (10K points) | 5-100 ms | Use GPU if available |\n",
        "| Batch (1000 fits) | 100-5000 ms | Use vmap for parallelization |\n",
        "| Huge dataset (1M points) | 50-500 ms | GPU + chunking |\n",
        "\n",
        "Troubleshooting Performance Issues\n",
        "\n",
        "**Problem**: First call is slow (>5 seconds)\n",
        "- **Solution**: Normal for JIT. Subsequent calls will be fast.\n",
        "\n",
        "**Problem**: All calls are slow (>1 second for small data)\n",
        "- **Solution**: Check if recompiling each time (varying shapes/dtypes)\n",
        "\n",
        "**Problem**: Out of memory errors\n",
        "- **Solution**: Use float32, chunk data, or downsample\n",
        "\n",
        "**Problem**: GPU not being used\n",
        "- **Solution**: Check `jax.devices()`, install jax[cuda] or jax[rocm]\n",
        "\n",
        "**Problem**: Batch processing not faster than sequential\n",
        "- **Solution**: Problem may be too small, try larger batches or datasets\n",
        "\n",
        "Advanced Profiling\n",
        "\n",
        "For detailed profiling:\n",
        "\n",
        "```python\n",
        "JAX profiling (requires jax[profiling])\n",
        "import jax.profiler\n",
        "\n",
        "Profile a code block\n",
        "with jax.profiler.trace(\"/tmp/jax-trace\", create_perfetto_link=True):\n",
        "Your NLSQ code here\n",
        "popt, pcov = cf.curve_fit(model, x, y, p0=...)\n",
        "\n",
        "Opens profiling UI in browser\n",
        "```\n",
        "\n",
        "Production Recommendations\n",
        "\n",
        "```python\n",
        "Example: Optimized production setup\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from nlsq import CurveFit\n",
        "\n",
        "Configure JAX for production\n",
        "jax.config.update('jax_enable_x64', False)  # Use float32\n",
        "\n",
        "Pre-warm JIT cache at startup\n",
        "cf = CurveFit()\n",
        "x_dummy = jnp.linspace(0, 1, 100)\n",
        "y_dummy = jnp.ones(100)\n",
        "_ = cf.curve_fit(model, x_dummy, y_dummy, p0=initial_guess)\n",
        "\n",
        "Now ready for fast production fitting\n",
        "```\n",
        "\n",
        "Next Steps\n",
        "\n",
        "- **Scale up**: Try batch processing 10,000+ datasets with vmap\n",
        "- **Optimize models**: Simplify model functions for faster evaluation\n",
        "- **Profile**: Use JAX profiler to identify bottlenecks\n",
        "- **Distribute**: For massive scale, consider JAX's `pmap` for multi-GPU\n",
        "\n",
        "References\n",
        "\n",
        "1. **JAX Performance**: https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html\n",
        "2. **JAX Profiling**: https://jax.readthedocs.io/en/latest/profiling.html\n",
        "3. **GPU Acceleration**: https://jax.readthedocs.io/en/latest/gpu_performance_tips.html\n",
        "4. **Related examples**:\n",
        "- `custom_algorithms_advanced.ipynb` - vmap for batch fitting\n",
        "- `troubleshooting_guide.ipynb` - Performance debugging\n",
        "\n",
        "---\n",
        "\n",
        "**Remember**: Premature optimization is the root of all evil. Profile first, optimize what matters!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
