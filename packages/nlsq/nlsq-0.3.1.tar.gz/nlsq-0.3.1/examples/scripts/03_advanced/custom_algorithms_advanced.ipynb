{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    a, b = params\n    y_pred = exponential(x, a, b)\n    return y - y_pred\ndef loss_fn(params, x, y):\n    \"\"\"Sum of squared residuals L(\u03b8) = 0.5 * ||r(\u03b8)||^2.\"\"\"\n    r = residual_fn(params, x, y)\n    return 0.5 * jnp.sum(r**2)\ngrad_fn = grad(loss_fn)\ngradient = grad_fn(popt, x_data, y_data)\nprint(f\"  Gradient at optimum: {gradient}\")\nprint(f\"  Gradient norm: {jnp.linalg.norm(gradient):.2e} (should be \u2248 0)\")\nprint(\"  \u2192 Confirms NLSQ found a critical point where \u2207L = 0 \u2713\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part 2: Custom Loss Functions\n",
        "\n",
        "Beyond standard least squares, we can implement custom loss functions for specialized needs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_robust = jnp.linspace(0, 10, 50)\ny_robust = 2.0 * x_robust + 1.0 + np.random.normal(0, 0.5, 50)\noutlier_idx = jnp.array([5, 15, 35, 42])\ny_robust = y_robust.at[outlier_idx].add(jnp.array([5.0, -6.0, 4.0, -5.0]))\ndef linear_model(x, a, b):\n    return a * x + b\ndef least_squares_loss(params, x, y):\n    a, b = params\n    residuals = y - linear_model(x, a, b)\n    return jnp.sum(residuals**2)\ndef huber_loss(params, x, y, delta=1.0):\n    \"\"\"Huber loss: quadratic for small errors, linear for large.\n    Parameters\n    ----------\n    delta : float\n        Threshold for switching from quadratic to linear\n    \"\"\"\n    a, b = params\n    residuals = y - linear_model(x, a, b)\n    abs_residuals = jnp.abs(residuals)\n    huber = jnp.where(\n        abs_residuals <= delta,\n        0.5 * residuals**2,\n        delta * (abs_residuals - 0.5 * delta),\n    )\n    return jnp.sum(huber)\nif OPTAX_AVAILABLE:\n    def optimize_custom(loss_fn, p0, x, y, n_steps=1000, lr=0.01):\n        \"\"\"Custom optimizer using Optax Adam.\"\"\"\n        params = jnp.array(p0)\n        optimizer = optax.adam(lr)\n        opt_state = optimizer.init(params)\n        @jit\n        def step(params, opt_state):\n            loss, grads = value_and_grad(loss_fn)(params, x, y)\n            updates, opt_state = optimizer.update(grads, opt_state)\n            params = optax.apply_updates(params, updates)\n            return params, opt_state, loss\n        losses = []\n        for i in range(n_steps):\n            params, opt_state, loss = step(params, opt_state)\n            if i % 100 == 0:\n                losses.append(float(loss))\n        return params, losses\n    p0 = [1.0, 0.0]\n    params_ls, losses_ls = optimize_custom(least_squares_loss, p0, x_robust, y_robust)\n    params_huber, losses_huber = optimize_custom(\n        lambda p, x, y: huber_loss(p, x, y, delta=1.5), p0, x_robust, y_robust\n    )\n    print(\"Least Squares (sensitive to outliers):\")\n    print(f\"  a={params_ls[0]:.3f}, b={params_ls[1]:.3f}\")\n    print(\"\\nHuber Loss (robust to outliers):\")\n    print(f\"  a={params_huber[0]:.3f}, b={params_huber[1]:.3f}\")\n    print(\"\\nTrue parameters: a=2.0, b=1.0\")\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n    x_plot = jnp.linspace(0, 10, 100)\n    ax1.plot(x_robust, y_robust, \"o\", alpha=0.5, label=\"Data (with outliers)\")\n    ax1.plot(\n        x_robust[outlier_idx],\n        y_robust[outlier_idx],\n        \"rx\",\n        ms=12,\n        mew=3,\n        label=\"Outliers\",\n    )\n    ax1.plot(\n        x_plot,\n        linear_model(x_plot, *params_ls),\n        \"r--\",\n        lw=2,\n        label=\"Least Squares\",\n    )\n    ax1.plot(\n        x_plot, linear_model(x_plot, *params_huber), \"g-\", lw=2, label=\"Huber Loss\"\n    )\n    ax1.plot(x_plot, 2.0 * x_plot + 1.0, \"k:\", lw=2, label=\"True\")\n    ax1.set_xlabel(\"x\")\n    ax1.set_ylabel(\"y\")\n    ax1.set_title(\"Robust Fitting with Custom Loss\")\n    ax1.legend()\n    ax1.grid(alpha=0.3)\n    ax2.semilogy(losses_ls, \"r-\", label=\"Least Squares\")\n    ax2.semilogy(losses_huber, \"g-\", label=\"Huber Loss\")\n    ax2.set_xlabel(\"Iteration (\u00d7100)\")\n    ax2.set_ylabel(\"Loss\")\n    ax2.set_title(\"Convergence\")\n    ax2.legend()\n    ax2.grid(alpha=0.3)\n    plt.tight_layout()\n    fig_dir = Path(__file__).parent / \"figures\" / \"custom_algorithms_advanced\"\n    fig_dir.mkdir(parents=True, exist_ok=True)\n    plt.savefig(fig_dir / \"fig_01.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\nelse:\n    print(\"\u26a0 Install optax to run this example: pip install optax\")\ndef asymmetric_loss(params, x, y, alpha=2.0):\n    \"\"\"Asymmetric quadratic loss.\n    Penalizes overestimation more than underestimation.\n    Useful when overestimation is more costly (e.g., drug dosing).\n    Parameters\n    ----------\n    alpha : float\n        Asymmetry parameter (alpha > 1 penalizes positive residuals more)\n    \"\"\"\n    a, b = params\n    residuals = y - linear_model(x, a, b)\n    loss = jnp.where(\n        residuals > 0,  # Overestimation (model too low)\n        alpha * residuals**2,  # Higher penalty\n        residuals**2,  # Normal penalty\n    )\n    return jnp.sum(loss)\nif OPTAX_AVAILABLE:\n    params_asym, _ = optimize_custom(\n        lambda p, x, y: asymmetric_loss(p, x, y, alpha=3.0),\n        [1.0, 0.0],\n        x_robust,\n        y_robust,\n    )\n    print(\"Asymmetric Loss (penalizes overestimation 3x):\")\n    print(f\"  a={params_asym[0]:.3f}, b={params_asym[1]:.3f}\")\n    print(\n        \"  \u2192 Fit is conservative (tends to underestimate to avoid costly overestimation)\"\n    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part 3: Custom Optimization Algorithms\n",
        "\n",
        "Implement specialized optimization algorithms for specific problem structures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_descent_momentum(\n    loss_fn, p0, x, y, lr=0.01, momentum=0.9, n_steps=1000, tol=1e-6\n):\n    \"\"\"Gradient descent with momentum optimizer.\n    Parameters\n    ----------\n    loss_fn : callable\n        Loss function: loss_fn(params, x, y) -> scalar\n    p0 : array\n        Initial parameters\n    lr : float\n        Learning rate\n    momentum : float\n        Momentum coefficient (0 = no momentum, 0.9 typical)\n    n_steps : int\n        Maximum iterations\n    tol : float\n        Convergence tolerance on gradient norm\n    Returns\n    -------\n    params : array\n        Optimized parameters\n    history : dict\n        Optimization history (params, loss, grad_norm)\n    \"\"\"\n    params = jnp.array(p0, dtype=jnp.float32)\n    velocity = jnp.zeros_like(params)\n    history = {\"params\": [], \"loss\": [], \"grad_norm\": []}\n    grad_fn = jit(grad(loss_fn))\n    loss_fn_jit = jit(loss_fn)\n    for i in range(n_steps):\n        g = grad_fn(params, x, y)\n        grad_norm = float(jnp.linalg.norm(g))\n        velocity = momentum * velocity - lr * g\n        params = params + velocity\n        if i % 50 == 0:\n            loss_val = float(loss_fn_jit(params, x, y))\n            history[\"params\"].append(params.copy())\n            history[\"loss\"].append(loss_val)\n            history[\"grad_norm\"].append(grad_norm)\n        if grad_norm < tol:\n            print(f\"  Converged at iteration {i} (grad_norm={grad_norm:.2e})\")\n            break\n    return params, history\nprint(\"Custom Gradient Descent with Momentum:\")\nparams_gd, history_gd = gradient_descent_momentum(\n    least_squares_loss, [0.0, 0.0], x_data, y_data, lr=0.01, momentum=0.9, n_steps=2000\n)\nprint(f\"  Final params: a={params_gd[0]:.3f}, b={params_gd[1]:.3f}\")\nprint(f\"  Optimization steps: {len(history_gd['loss'])}\")\npopt_nlsq, _ = cf.curve_fit(exponential, x_data, y_data, p0=[0.0, 0.0])\nprint(\"\\nNLSQ (Levenberg-Marquardt):\")\nprint(f\"  Final params: a={popt_nlsq[0]:.3f}, b={popt_nlsq[1]:.3f}\")\nprint(\"\\n\u2192 Both converge to similar solution \u2713\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part 4: Advanced JAX Patterns for Curve Fitting\n",
        "\n",
        "Leverage JAX's advanced features for efficient batch fitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_datasets = 100\nx_batch = jnp.linspace(0, 5, 30)\na_true_batch = np.random.uniform(2.0, 4.0, n_datasets)\nb_true_batch = np.random.uniform(0.3, 0.7, n_datasets)\ny_batch = jnp.array(\n    [\n        a * jnp.exp(-b * x_batch) + np.random.normal(0, 0.05, len(x_batch))\n        for a, b in zip(a_true_batch, b_true_batch, strict=True)\n    ]\n)\nprint(f\"Batch fitting: {n_datasets} datasets simultaneously\")\nprint(f\"  Data shape: {y_batch.shape} (datasets \u00d7 points)\")\nprint()\ndef fit_single_dataset(y_single):\n    \"\"\"Fit one dataset (simplified Newton's method).\"\"\"\n    params = jnp.array([3.0, 0.5])  # Initial guess\n    def loss(p):\n        return jnp.sum((y_single - exponential(x_batch, *p)) ** 2)\n    for _ in range(20):\n        g = grad(loss)(params)\n        params = params - 0.05 * g\n    return params\nfit_batch = jit(vmap(fit_single_dataset))\nimport time\nstart = time.time()\nparams_batch = fit_batch(y_batch)\nbatch_time = time.time() - start\nprint(f\"\u2713 Fitted {n_datasets} datasets in {batch_time * 1000:.1f} ms\")\nprint(\n    f\"  Average time per dataset: {batch_time / n_datasets * 1000:.2f} ms (with vmap)\"\n)\nprint()\na_fitted = params_batch[:, 0]\nb_fitted = params_batch[:, 1]\na_error = np.mean(np.abs(a_fitted - a_true_batch))\nb_error = np.mean(np.abs(b_fitted - b_true_batch))\nprint(\"Fitting accuracy:\")\nprint(f\"  Mean absolute error in a: {a_error:.4f}\")\nprint(f\"  Mean absolute error in b: {b_error:.4f}\")\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nax1.scatter(a_true_batch, a_fitted, alpha=0.5, s=20)\nax1.plot([2, 4], [2, 4], \"r--\", lw=2, label=\"Perfect fit\")\nax1.set_xlabel(\"True a\")\nax1.set_ylabel(\"Fitted a\")\nax1.set_title(\"Parameter Recovery: a\")\nax1.legend()\nax1.grid(alpha=0.3)\nax2.scatter(b_true_batch, b_fitted, alpha=0.5, s=20)\nax2.plot([0.3, 0.7], [0.3, 0.7], \"r--\", lw=2, label=\"Perfect fit\")\nax2.set_xlabel(\"True b\")\nax2.set_ylabel(\"Fitted b\")\nax2.set_title(\"Parameter Recovery: b\")\nax2.legend()\nax2.grid(alpha=0.3)\nplt.tight_layout()\nfig_dir = Path(__file__).parent / \"figures\" / \"custom_algorithms_advanced\"\nfig_dir.mkdir(parents=True, exist_ok=True)\nplt.savefig(fig_dir / \"fig_02.png\", dpi=300, bbox_inches=\"tight\")\nplt.close()\nprint(\"\\n\u2192 vmap enables efficient parallel fitting across datasets \u2713\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part 5: Research Extensions\n",
        "\n",
        "Advanced techniques for cutting-edge applications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def constrained_loss(params, x, y, lambda_penalty=10.0):\n    \"\"\"Fit with constraint: a + b = 1.0 (sum constraint).\n    Uses quadratic penalty method.\n    \"\"\"\n    a, b = params\n    residuals = y - (a * jnp.exp(-x) + b * jnp.exp(-2 * x))\n    data_loss = jnp.sum(residuals**2)\n    constraint_violation = (a + b - 1.0) ** 2\n    penalty = lambda_penalty * constraint_violation\n    return data_loss + penalty\nx_const = jnp.linspace(0, 3, 40)\na_true_const = 0.6\nb_true_const = 0.4  # a + b = 1.0\ny_const = (\n    a_true_const * jnp.exp(-x_const)\n    + b_true_const * jnp.exp(-2 * x_const)\n    + np.random.normal(0, 0.02, len(x_const))\n)\nif OPTAX_AVAILABLE:\n    params_unconstr, _ = optimize_custom(\n        lambda p, x, y: jnp.sum(\n            (y - (p[0] * jnp.exp(-x) + p[1] * jnp.exp(-2 * x))) ** 2\n        ),\n        [0.5, 0.5],\n        x_const,\n        y_const,\n        n_steps=2000,\n    )\n    params_constr, _ = optimize_custom(\n        lambda p, x, y: constrained_loss(p, x, y, lambda_penalty=100.0),\n        [0.5, 0.5],\n        x_const,\n        y_const,\n        n_steps=2000,\n    )\n    print(\"Unconstrained fit:\")\n    print(\n        f\"  a={params_unconstr[0]:.4f}, b={params_unconstr[1]:.4f}, sum={params_unconstr[0] + params_unconstr[1]:.4f}\"\n    )\n    print(\"\\nConstrained fit (a + b = 1):\")\n    print(\n        f\"  a={params_constr[0]:.4f}, b={params_constr[1]:.4f}, sum={params_constr[0] + params_constr[1]:.4f}\"\n    )\n    print(f\"\\nTrue values: a={a_true_const}, b={b_true_const}, sum=1.0\")\n    print(\n        f\"\u2192 Constraint enforced: sum = {params_constr[0] + params_constr[1]:.6f} \u2248 1.0 \u2713\"\n    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Summary and Best Practices\n",
        "\n",
        "When to Use Custom Algorithms\n",
        "\n",
        "| **Application** | **Standard NLSQ** | **Custom Algorithm** |\n",
        "|-----------------|-------------------|----------------------|\n",
        "| Standard curve fitting | \u2705 Recommended | Unnecessary |\n",
        "| Outlier-heavy data | Use sigma weights | Robust loss (Huber, Cauchy) |\n",
        "| Asymmetric costs | N/A | Asymmetric loss function |\n",
        "| Constrained parameters | Use bounds | Penalty methods, Lagrangian |\n",
        "| Batch processing (1000s of fits) | Serial fitting | vmap for parallelization |\n",
        "| Novel research problems | May not apply | Custom optimizer |\n",
        "\n",
        "Implementation Checklist\n",
        "\n",
        "When implementing custom algorithms:\n",
        "\n",
        "1. **Start simple**: Test with toy problems where you know the answer\n",
        "2. **Verify gradients**: Use `jax.grad` and compare with finite differences\n",
        "3. **Check convergence**: Monitor loss and gradient norms\n",
        "4. **Use JIT**: Compile with `@jit` for 10-100x speedups\n",
        "5. **Numerical stability**: Check for NaN/Inf, use stable formulations\n",
        "6. **Validate results**: Compare with standard methods when possible\n",
        "\n",
        "Advanced JAX Patterns\n",
        "\n",
        "```python\n",
        "Pattern 1: Efficient batch fitting\n",
        "fit_single = jit(lambda y: optimize(loss_fn, y))\n",
        "fit_batch = vmap(fit_single)  # Parallelize over batch dimension\n",
        "results = fit_batch(y_batch)  # GPU-accelerated\n",
        "\n",
        "Pattern 2: Custom gradients for numerical stability\n",
        "from jax import custom_jvp\n",
        "\n",
        "@custom_jvp\n",
        "def stable_exp(x):\n",
        "return jnp.exp(jnp.clip(x, -50, 50))  # Prevent overflow\n",
        "\n",
        "Pattern 3: Automatic differentiation through optimization\n",
        "def meta_objective(hyperparams):\n",
        "Fit model with hyperparams\n",
        "params = optimize(loss_fn, hyperparams)\n",
        "Evaluate on validation set\n",
        "return validation_loss(params)\n",
        "\n",
        "optimal_hyperparams = optimize(meta_objective, initial_hyperparams)\n",
        "```\n",
        "\n",
        "Research Extensions\n",
        "\n",
        "Cutting-edge applications:\n",
        "\n",
        "1. **Bilevel optimization**: Hyperparameter tuning via gradient descent\n",
        "2. **Meta-learning**: Learning to fit across multiple tasks\n",
        "3. **Differentiable physics**: PDE-constrained optimization\n",
        "4. **Uncertainty quantification**: Laplace approximation, variational inference\n",
        "5. **Inverse problems**: Image reconstruction, tomography\n",
        "\n",
        "Production Recommendations\n",
        "\n",
        "For production use:\n",
        "- **Default**: Use standard NLSQ (well-tested, robust)\n",
        "- **Custom loss**: Only when problem demands it (document why!)\n",
        "- **Testing**: Extensive validation against standard methods\n",
        "- **Monitoring**: Track convergence, gradient norms, numerical stability\n",
        "- **Fallback**: Implement standard NLSQ as backup if custom method fails\n",
        "\n",
        "References\n",
        "\n",
        "1. **Optimization**: Nocedal & Wright, *Numerical Optimization* (2006)\n",
        "2. **JAX**: https://jax.readthedocs.io/\n",
        "3. **Optax**: https://optax.readthedocs.io/\n",
        "4. **Robust fitting**: Huber, *Robust Statistics* (2009)\n",
        "5. **Related examples**:\n",
        "- `advanced_features_demo.ipynb` - NLSQ diagnostics\n",
        "- `ml_integration_tutorial.ipynb` - Hybrid models with custom optimization\n",
        "\n",
        "---\n",
        "\n",
        "**Warning**: Custom algorithms can be powerful but require careful validation. Always test thoroughly before using in production!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
