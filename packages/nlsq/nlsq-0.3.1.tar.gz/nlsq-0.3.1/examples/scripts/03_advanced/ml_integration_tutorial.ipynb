{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "        features: list = (16, 16, 1)\n        @nn.compact\n        def __call__(self, x):\n            for feat in self.features[:-1]:\n                x = nn.Dense(feat)(x)\n                x = nn.relu(x)\n            x = nn.Dense(self.features[-1])(x)\n            return x.squeeze()\n    model = CorrectionMLP()\n    params = model.init(key, jnp.ones((1, 1)))\n    print(\"\u2705 Correction MLP initialized\")\n    print(f\"   Parameter shapes: {jax.tree_util.tree_map(lambda x: x.shape, params)}\")\n    x_data = np.linspace(0, 5, 200)\n    true_a, true_b = 5.0, 1.2\n    y_physics = true_a * np.exp(-true_b * x_data)\n    y_correction = 0.5 * np.sin(3 * x_data) * np.exp(-0.3 * x_data)\n    y_data = y_physics + y_correction + np.random.normal(0, 0.1, len(x_data))\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 3, 1)\n    plt.plot(x_data, y_physics, \"g-\", linewidth=2, label=\"Physics (exponential)\")\n    plt.plot(x_data, y_data, \"b.\", alpha=0.5, markersize=3, label=\"Observed data\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"Data vs Physics Model\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.subplot(1, 3, 2)\n    plt.plot(x_data, y_correction, \"r-\", linewidth=2, label=\"True correction\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Correction\")\n    plt.title(\"Systematic Deviation\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.subplot(1, 3, 3)\n    residuals = y_data - y_physics\n    plt.plot(x_data, residuals, \"r.\", alpha=0.5, markersize=3, label=\"Residuals\")\n    plt.plot(x_data, y_correction, \"k--\", linewidth=2, label=\"True correction\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Residual\")\n    plt.title(\"Physics Model Residuals\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    fig_dir = Path(__file__).parent / \"figures\" / \"ml_integration_tutorial\"\n    fig_dir.mkdir(parents=True, exist_ok=True)\n    plt.savefig(fig_dir / \"fig_01.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\nprint(\"\\n\ud83d\udcca Dataset Statistics:\")\nprint(f\"   Data points: {len(x_data)}\")\nprint(f\"   Physics RMSE: {np.sqrt(np.mean((y_data - y_physics) ** 2)):.3f}\")\nprint(f\"   Correction amplitude: {np.max(np.abs(y_correction)):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Strategy 1: Two-Stage Fitting\n",
        "\n",
        "**Step 1**: Fit physics parameters with NLSQ\n",
        "**Step 2**: Train neural network on residuals\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if FLAX_AVAILABLE:\n    print(\"=\" * 70)\n    print(\"TWO-STAGE HYBRID FITTING\")\n    print(\"=\" * 70)\n    print(\"\\n\ud83d\udd27 Stage 1: Fitting physics parameters with NLSQ...\")\n    def exponential_decay(x, a, b):\n        return a * jnp.exp(-b * x)\n    cf = CurveFit()\n    start_time = time.time()\n    popt_physics, pcov_physics = cf.curve_fit(\n        exponential_decay, x_data, y_data, p0=[4.0, 1.0]\n    )\n    physics_time = time.time() - start_time\n    a_fit, b_fit = popt_physics\n    print(f\"   Fitted parameters: a={a_fit:.3f}, b={b_fit:.3f}\")\n    print(f\"   True parameters:   a={true_a:.3f}, b={true_b:.3f}\")\n    print(f\"   Fit time: {physics_time:.3f}s\")\n    y_physics_fit = np.array(exponential_decay(x_data, *popt_physics))\n    residuals = y_data - y_physics_fit\n    physics_rmse = np.sqrt(np.mean(residuals**2))\n    print(f\"   Physics RMSE: {physics_rmse:.4f}\")\n    print(\"\\n\ud83e\udde0 Stage 2: Training neural network on residuals...\")\n    x_train = x_data.reshape(-1, 1).astype(np.float32)\n    y_train = residuals.astype(np.float32)\n    def create_train_state(rng, learning_rate=1e-3):\n        model_nn = CorrectionMLP()\n        params_nn = model_nn.init(rng, jnp.ones((1, 1)))\n        tx = optax.adam(learning_rate)\n        return train_state.TrainState.create(\n            apply_fn=model_nn.apply, params=params_nn, tx=tx\n        )\n    state = create_train_state(key, learning_rate=5e-3)\n    @jit\n    def train_step(state, x_batch, y_batch):\n        def loss_fn(params):\n            pred = state.apply_fn(params, x_batch)\n            return jnp.mean((pred - y_batch) ** 2)\n        loss, grads = jax.value_and_grad(loss_fn)(state.params)\n        state = state.apply_gradients(grads=grads)\n        return state, loss\n    n_epochs = 500\n    losses = []\n    start_time = time.time()\n    for epoch in range(n_epochs):\n        state, loss = train_step(state, x_train, y_train)\n        losses.append(float(loss))\n        if (epoch + 1) % 100 == 0:\n            print(f\"   Epoch {epoch + 1}/{n_epochs}, Loss: {loss:.6f}\")\n    nn_time = time.time() - start_time\n    print(f\"   NN training time: {nn_time:.3f}s\")\n    correction_pred = model.apply(state.params, x_train).squeeze()\n    y_hybrid = y_physics_fit + np.array(correction_pred)\n    hybrid_rmse = np.sqrt(np.mean((y_data - y_hybrid) ** 2))\n    print(\"\\n\ud83d\udcca Results:\")\n    print(f\"   Physics-only RMSE: {physics_rmse:.4f}\")\n    print(f\"   Hybrid model RMSE: {hybrid_rmse:.4f}\")\n    print(f\"   Improvement: {(1 - hybrid_rmse / physics_rmse) * 100:.1f}%\")\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 3, 1)\n    plt.plot(losses)\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"MSE Loss\")\n    plt.title(\"Neural Network Training\")\n    plt.yscale(\"log\")\n    plt.grid(True, alpha=0.3)\n    plt.subplot(1, 3, 2)\n    plt.plot(x_data, y_data, \"b.\", alpha=0.5, markersize=3, label=\"Data\")\n    plt.plot(x_data, y_physics_fit, \"g--\", linewidth=2, label=\"Physics only\")\n    plt.plot(x_data, y_hybrid, \"r-\", linewidth=2, label=\"Hybrid (Physics + NN)\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"Model Comparison\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.subplot(1, 3, 3)\n    plt.plot(\n        x_data,\n        y_correction,\n        \"k--\",\n        linewidth=2,\n        label=\"True correction\",\n        alpha=0.7,\n    )\n    plt.plot(x_data, correction_pred, \"r-\", linewidth=2, label=\"NN learned correction\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Correction\")\n    plt.title(\"Learned vs True Correction\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\nfig_dir = Path(__file__).parent / \"figures\" / \"ml_integration_tutorial\"\nfig_dir.mkdir(parents=True, exist_ok=True)\nplt.savefig(fig_dir / \"fig_02.png\", dpi=300, bbox_inches=\"tight\")\nplt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Key Insights\n",
        "\n",
        "1. **Physics provides structure**: The exponential decay captures the dominant behavior\n",
        "2. **NN learns deviations**: Small neural network captures systematic errors\n",
        "3. **Data efficiency**: Physics model requires fewer parameters than pure ML\n",
        "4. **Interpretability**: Physical parameters (a, b) have clear meaning\n",
        "5. **Better extrapolation**: Physics guides behavior outside training range\n",
        "\n",
        "**When to use this approach**:\n",
        "- \u2705 Known physics with systematic deviations\n",
        "- \u2705 Limited data (physics provides inductive bias)\n",
        "- \u2705 Need interpretable parameters\n",
        "- \u2705 Extrapolation is important\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part 2: Neural ODEs with NLSQ\n",
        "\n",
        "Concept: Learning Dynamics\n",
        "\n",
        "**Neural ODEs** parameterize the derivative of a system with a neural network:\n",
        "\n",
        "$$\n",
        "\\frac{dy}{dt} = f_{\\theta}(y, t)\n",
        "$$\n",
        "\n",
        "Where $f_{\\theta}$ is a neural network. We can then integrate this ODE to get predictions.\n",
        "\n",
        "**NLSQ Integration**: Use NLSQ to fit:\n",
        "1. Initial conditions\n",
        "2. ODE parameters (if partially mechanistic)\n",
        "3. Neural network parameters (jointly or in stages)\n",
        "\n",
        "Example 2.1: Damped Oscillator with Learned Damping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def euler_integrate(f, y0, t, *args):\n    \"\"\"Simple Euler integration for demonstration.\"\"\"\n    dt = t[1] - t[0]\n    y = jnp.zeros((len(t), len(y0)))\n    y = y.at[0].set(y0)\n    for i in range(1, len(t)):\n        dydt = f(y[i - 1], t[i - 1], *args)\n        y = y.at[i].set(y[i - 1] + dt * dydt)\n    return y\ndef damped_oscillator_ode(state, t, omega, gamma):\n    \"\"\"dy/dt for damped harmonic oscillator.\n    state = [position, velocity]\n    omega = natural frequency\n    gamma = damping coefficient\n    \"\"\"\n    x, v = state\n    dxdt = v\n    dvdt = -(omega**2) * x - 2 * gamma * v\n    return jnp.array([dxdt, dvdt])\nt_ode = np.linspace(0, 10, 200)\nomega_true = 2.0  # Natural frequency\ngamma_true = 0.3  # Damping\ny0_true = jnp.array([1.0, 0.0])  # Initial [position, velocity]\ny_true = euler_integrate(damped_oscillator_ode, y0_true, t_ode, omega_true, gamma_true)\nx_true = y_true[:, 0]  # Extract position\nx_obs = x_true + np.random.normal(0, 0.05, len(t_ode))\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(t_ode, x_true, \"g-\", linewidth=2, label=\"True dynamics\")\nplt.plot(t_ode, x_obs, \"b.\", alpha=0.5, markersize=3, label=\"Noisy observations\")\nplt.xlabel(\"Time (t)\")\nplt.ylabel(\"Position (x)\")\nplt.title(\"Damped Harmonic Oscillator\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.subplot(1, 2, 2)\nplt.plot(y_true[:, 0], y_true[:, 1], \"g-\", linewidth=2, label=\"Phase space\")\nplt.xlabel(\"Position (x)\")\nplt.ylabel(\"Velocity (v)\")\nplt.title(\"Phase Space Trajectory\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nfig_dir = Path(__file__).parent / \"figures\" / \"ml_integration_tutorial\"\nfig_dir.mkdir(parents=True, exist_ok=True)\nplt.savefig(fig_dir / \"fig_03.png\", dpi=300, bbox_inches=\"tight\")\nplt.close()\nprint(f\"True parameters: \u03c9={omega_true}, \u03b3={gamma_true}\")\nprint(f\"Initial state: x\u2080={y0_true[0]}, v\u2080={y0_true[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fitting ODE Parameters with NLSQ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 70)\nprint(\"FITTING ODE PARAMETERS WITH NLSQ\")\nprint(\"=\" * 70)\ndef oscillator_model(t, omega, gamma, x0, v0):\n    \"\"\"Model function that integrates ODE for given parameters.\"\"\"\n    y0 = jnp.array([x0, v0])\n    y = euler_integrate(damped_oscillator_ode, y0, t, omega, gamma)\n    return y[:, 0]  # Return position only\nprint(\"\\n\ud83d\udd27 Fitting ODE parameters...\")\ncf_ode = CurveFit()\np0 = [1.5, 0.2, 0.8, 0.1]  # [omega, gamma, x0, v0]\nstart_time = time.time()\npopt_ode, pcov_ode = cf_ode.curve_fit(\n    oscillator_model,\n    t_ode,\n    x_obs,\n    p0=p0,\n    bounds=([0, 0, -2, -2], [5, 2, 2, 2]),  # Reasonable physical bounds\n)\node_time = time.time() - start_time\nomega_fit, gamma_fit, x0_fit, v0_fit = popt_ode\nprint(\"\\n\ud83d\udcca Results:\")\nprint(\n    f\"   Fitted: \u03c9={omega_fit:.4f}, \u03b3={gamma_fit:.4f}, x\u2080={x0_fit:.4f}, v\u2080={v0_fit:.4f}\"\n)\nprint(\n    f\"   True:   \u03c9={omega_true:.4f}, \u03b3={gamma_true:.4f}, x\u2080={y0_true[0]:.4f}, v\u2080={y0_true[1]:.4f}\"\n)\nprint(f\"   Fit time: {ode_time:.3f}s\")\nx_fit = oscillator_model(t_ode, *popt_ode)\node_rmse = np.sqrt(np.mean((x_obs - np.array(x_fit)) ** 2))\nprint(f\"   RMSE: {ode_rmse:.5f}\")\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(t_ode, x_obs, \"b.\", alpha=0.5, markersize=3, label=\"Observations\")\nplt.plot(t_ode, x_true, \"g--\", linewidth=2, alpha=0.7, label=\"True dynamics\")\nplt.plot(t_ode, x_fit, \"r-\", linewidth=2, label=\"Fitted ODE\")\nplt.xlabel(\"Time (t)\")\nplt.ylabel(\"Position (x)\")\nplt.title(\"ODE Parameter Fitting\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.subplot(1, 2, 2)\nresiduals_ode = x_obs - np.array(x_fit)\nplt.plot(t_ode, residuals_ode, \"r.\", alpha=0.5, markersize=3)\nplt.axhline(y=0, color=\"k\", linestyle=\"-\", alpha=0.3)\nplt.xlabel(\"Time (t)\")\nplt.ylabel(\"Residual\")\nplt.title(f\"Residuals (RMSE={ode_rmse:.5f})\")\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nfig_dir = Path(__file__).parent / \"figures\" / \"ml_integration_tutorial\"\nfig_dir.mkdir(parents=True, exist_ok=True)\nplt.savefig(fig_dir / \"fig_04.png\", dpi=300, bbox_inches=\"tight\")\nplt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Key Takeaways\n",
        "\n",
        "1. **NLSQ handles ODEs naturally**: Just wrap ODE integration in model function\n",
        "2. **Automatic differentiation**: JAX computes gradients through ODE solver\n",
        "3. **Joint parameter estimation**: Fit dynamics parameters + initial conditions\n",
        "4. **Physical constraints**: Use bounds to enforce physically reasonable values\n",
        "\n",
        "**Production tip**: Use `diffrax` for more robust ODE integration:\n",
        "```python\n",
        "import diffrax\n",
        "solver = diffrax.Tsit5()  # Adaptive Runge-Kutta\n",
        "solution = diffrax.diffeqsolve(...)\n",
        "```\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Part 3: Physics-Informed Loss Functions\n",
        "\n",
        "Concept: Incorporating Physical Constraints\n",
        "\n",
        "**Physics-informed fitting** adds physical constraints to the loss:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = \\mathcal{L}_{\\text{data}} + \\lambda \\mathcal{L}_{\\text{physics}}\n",
        "$$\n",
        "\n",
        "Examples:\n",
        "- **Conservation laws**: Energy, mass, momentum conservation\n",
        "- **PDE residuals**: Equations of motion, Maxwell's equations\n",
        "- **Boundary conditions**: Initial/final state constraints\n",
        "- **Symmetries**: Rotational, translational invariance\n",
        "\n",
        "Example 3.1: Energy-Conserving Pendulum\n",
        "\n",
        "For a frictionless pendulum, total energy should be conserved:\n",
        "\n",
        "$$\n",
        "E = \\frac{1}{2}mv^2 + mgh = \\text{constant}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pendulum_ode(state, t, omega):\n    \"\"\"Pendulum ODE: d\u00b2\u03b8/dt\u00b2 = -\u03c9\u00b2 sin(\u03b8)\"\"\"\n    theta, theta_dot = state\n    return jnp.array([theta_dot, -(omega**2) * jnp.sin(theta)])\nt_pend = np.linspace(0, 10, 150)\nomega_pend = 2.0\ny0_pend = jnp.array([0.5, 0.0])  # [angle, angular velocity]\ny_pend = euler_integrate(pendulum_ode, y0_pend, t_pend, omega_pend)\ntheta_obs = y_pend[:, 0] + np.random.normal(0, 0.02, len(t_pend))\nkinetic = 0.5 * y_pend[:, 1] ** 2\npotential = (omega_pend**2) * (1 - jnp.cos(y_pend[:, 0]))\ntotal_energy = kinetic + potential\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 3, 1)\nplt.plot(t_pend, y_pend[:, 0], \"g-\", linewidth=2, label=\"True angle\")\nplt.plot(t_pend, theta_obs, \"b.\", alpha=0.5, markersize=3, label=\"Observations\")\nplt.xlabel(\"Time (t)\")\nplt.ylabel(\"Angle \u03b8 (rad)\")\nplt.title(\"Pendulum Motion\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.subplot(1, 3, 2)\nplt.plot(y_pend[:, 0], y_pend[:, 1], \"g-\", linewidth=2)\nplt.xlabel(\"Angle \u03b8 (rad)\")\nplt.ylabel(\"Angular velocity d\u03b8/dt (rad/s)\")\nplt.title(\"Phase Space\")\nplt.grid(True, alpha=0.3)\nplt.subplot(1, 3, 3)\nplt.plot(t_pend, total_energy, \"r-\", linewidth=2, label=\"Total energy\")\nplt.axhline(y=jnp.mean(total_energy), color=\"k\", linestyle=\"--\", label=\"Mean energy\")\nplt.xlabel(\"Time (t)\")\nplt.ylabel(\"Energy (J)\")\nplt.title(\"Energy Conservation\")\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nfig_dir = Path(__file__).parent / \"figures\" / \"ml_integration_tutorial\"\nfig_dir.mkdir(parents=True, exist_ok=True)\nplt.savefig(fig_dir / \"fig_05.png\", dpi=300, bbox_inches=\"tight\")\nplt.close()\nenergy_std = float(jnp.std(total_energy))\nprint(f\"Energy conservation (std dev): {energy_std:.6f}\")\nprint(f\"Energy variation: {energy_std / jnp.mean(total_energy) * 100:.3f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Custom Physics-Informed Fitting\n",
        "\n",
        "While NLSQ doesn't directly support custom loss functions (it uses least squares), we can:\n",
        "1. Use NLSQ for standard parameter estimation\n",
        "2. Add physics penalty in post-processing\n",
        "3. Or use Optax for full physics-informed optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if FLAX_AVAILABLE:\n    print(\"=\" * 70)\n    print(\"PHYSICS-INFORMED OPTIMIZATION WITH OPTAX\")\n    print(\"=\" * 70)\n    def physics_informed_loss(params, t, theta_obs, lambda_physics=0.1):\n        \"\"\"Loss = data fit + energy conservation penalty.\"\"\"\n        omega, theta0, thetadot0 = params\n        y0 = jnp.array([theta0, thetadot0])\n        y = euler_integrate(pendulum_ode, y0, t, omega)\n        theta_pred = y[:, 0]\n        loss_data = jnp.mean((theta_pred - theta_obs) ** 2)\n        kinetic = 0.5 * y[:, 1] ** 2\n        potential = (omega**2) * (1 - jnp.cos(y[:, 0]))\n        total_energy = kinetic + potential\n        energy_var = jnp.var(total_energy)\n        loss_physics = lambda_physics * energy_var\n        return loss_data + loss_physics, {\n            \"loss_data\": loss_data,\n            \"loss_physics\": loss_physics,\n        }\n    params_init = jnp.array([1.5, 0.4, 0.1])  # [omega, theta0, thetadot0]\n    optimizer = optax.adam(learning_rate=0.01)\n    opt_state = optimizer.init(params_init)\n    @jit\n    def update_step(params, opt_state, t, theta_obs):\n        (loss_val, metrics), grads = jax.value_and_grad(\n            physics_informed_loss, has_aux=True\n        )(params, t, theta_obs)\n        updates, opt_state = optimizer.update(grads, opt_state)\n        params = optax.apply_updates(params, updates)\n        return params, opt_state, loss_val, metrics\n    params = params_init\n    n_steps = 1000\n    losses = []\n    print(\"\\n\ud83c\udfaf Training with physics-informed loss...\")\n    for step in range(n_steps):\n        params, opt_state, loss_val, metrics = update_step(\n            params, opt_state, t_pend, theta_obs\n        )\n        losses.append(float(loss_val))\n        if (step + 1) % 200 == 0:\n            print(\n                f\"   Step {step + 1}: Total={loss_val:.6f}, \"\n                f\"Data={metrics['loss_data']:.6f}, Physics={metrics['loss_physics']:.6f}\"\n            )\n    omega_pi, theta0_pi, thetadot0_pi = params\n    print(\"\\n\ud83d\udcca Fitted parameters:\")\n    print(f\"   \u03c9={omega_pi:.4f} (true: {omega_pend:.4f})\")\n    print(f\"   \u03b8\u2080={theta0_pi:.4f} (true: {y0_pend[0]:.4f})\")\n    print(f\"   d\u03b8\u2080/dt={thetadot0_pi:.4f} (true: {y0_pend[1]:.4f})\")\n    def pendulum_model(t, omega, theta0, thetadot0):\n        y0 = jnp.array([theta0, thetadot0])\n        y = euler_integrate(pendulum_ode, y0, t, omega)\n        return y[:, 0]\n    popt_std, _ = cf_ode.curve_fit(\n        pendulum_model, t_pend, theta_obs, p0=[1.5, 0.4, 0.1]\n    )\n    y_pi = euler_integrate(\n        pendulum_ode, jnp.array([theta0_pi, thetadot0_pi]), t_pend, omega_pi\n    )\n    y_std = euler_integrate(\n        pendulum_ode, jnp.array([popt_std[1], popt_std[2]]), t_pend, popt_std[0]\n    )\n    def compute_energy_std(y, omega):\n        kinetic = 0.5 * y[:, 1] ** 2\n        potential = (omega**2) * (1 - jnp.cos(y[:, 0]))\n        return float(jnp.std(kinetic + potential))\n    energy_std_pi = compute_energy_std(y_pi, omega_pi)\n    energy_std_std = compute_energy_std(y_std, popt_std[0])\n    print(\"\\n\u26a1 Energy Conservation:\")\n    print(f\"   Physics-informed: \u03c3_E = {energy_std_pi:.6f}\")\n    print(f\"   Standard NLSQ:    \u03c3_E = {energy_std_std:.6f}\")\n    print(f\"   Improvement: {(1 - energy_std_pi / energy_std_std) * 100:.1f}%\")\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 3, 1)\n    plt.plot(losses)\n    plt.xlabel(\"Optimization Step\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Physics-Informed Training\")\n    plt.yscale(\"log\")\n    plt.grid(True, alpha=0.3)\n    plt.subplot(1, 3, 2)\n    plt.plot(t_pend, theta_obs, \"b.\", alpha=0.5, markersize=3, label=\"Data\")\n    plt.plot(t_pend, y_std[:, 0], \"g--\", linewidth=2, label=\"Standard NLSQ\")\n    plt.plot(t_pend, y_pi[:, 0], \"r-\", linewidth=2, label=\"Physics-informed\")\n    plt.xlabel(\"Time (t)\")\n    plt.ylabel(\"Angle \u03b8 (rad)\")\n    plt.title(\"Fit Comparison\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.subplot(1, 3, 3)\n    energy_std_series = 0.5 * y_std[:, 1] ** 2 + (popt_std[0] ** 2) * (\n        1 - jnp.cos(y_std[:, 0])\n    )\n    energy_pi_series = 0.5 * y_pi[:, 1] ** 2 + (omega_pi**2) * (1 - jnp.cos(y_pi[:, 0]))\n    plt.plot(t_pend, energy_std_series, \"g--\", linewidth=2, label=\"Standard NLSQ\")\n    plt.plot(t_pend, energy_pi_series, \"r-\", linewidth=2, label=\"Physics-informed\")\n    plt.xlabel(\"Time (t)\")\n    plt.ylabel(\"Energy (J)\")\n    plt.title(\"Energy Conservation\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\nfig_dir = Path(__file__).parent / \"figures\" / \"ml_integration_tutorial\"\nfig_dir.mkdir(parents=True, exist_ok=True)\nplt.savefig(fig_dir / \"fig_06.png\", dpi=300, bbox_inches=\"tight\")\nplt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Summary and Best Practices\n",
        "\n",
        "Integration Strategies\n",
        "\n",
        "| Approach | NLSQ Role | ML Role | Best For |\n",
        "|----------|-----------|---------|----------|\n",
        "| **Two-Stage Hybrid** | Fit physics parameters | Learn residuals | Known physics + systematic deviations |\n",
        "| **Neural ODE** | Fit ODE parameters | (Optional) Learn dynamics | Parameter estimation in dynamical systems |\n",
        "| **Physics-Informed** | Pre-fit, then refine | Enforce constraints | Energy/mass conservation, PDEs |\n",
        "| **Joint Optimization** | Parameter estimation | Model flexibility | Complex coupled systems |\n",
        "\n",
        "Key Takeaways\n",
        "\n",
        "1. **NLSQ + JAX = Powerful Combo**:\n",
        "- Automatic differentiation through complex models\n",
        "- GPU acceleration for both fitting and ML\n",
        "- Seamless integration with JAX ecosystem\n",
        "\n",
        "2. **Hybrid Models Win**:\n",
        "- Better than pure physics (captures deviations)\n",
        "- Better than pure ML (data efficient, interpretable)\n",
        "- Best of both worlds\n",
        "\n",
        "3. **Physics Constraints Help**:\n",
        "- Regularize ML models\n",
        "- Improve extrapolation\n",
        "- Ensure physical plausibility\n",
        "\n",
        "4. **Choose the Right Tool**:\n",
        "- **NLSQ**: Parameter estimation, well-conditioned problems\n",
        "- **Optax**: Custom losses, physics-informed training\n",
        "- **Combined**: Two-stage fitting strategies\n",
        "\n",
        "Production Recommendations\n",
        "\n",
        "```python\n",
        "1. Use diffrax for robust ODE integration\n",
        "import diffrax\n",
        "solver = diffrax.Tsit5()\n",
        "\n",
        "2. Separate training and inference\n",
        "@jit\n",
        "def inference_model(params, x):\n",
        "Compiled inference only\n",
        "return model.apply(params, x)\n",
        "\n",
        "3. Use appropriate precision\n",
        "NLSQ uses float64 by default (good for physics)\n",
        "ML often uses float32 (faster, sufficient for NNs)\n",
        "\n",
        "4. Validate physics constraints\n",
        "def check_energy_conservation(y, params):\n",
        "energy = compute_energy(y, params)\n",
        "return jnp.std(energy) < threshold\n",
        "\n",
        "5. Profile and optimize\n",
        "Use MemoryPool for repeated fitting\n",
        "from nlsq import MemoryPool\n",
        "with MemoryPool() as pool:\n",
        "for data in datasets:\n",
        "popt, _ = cf.curve_fit(model, *data)\n",
        "```\n",
        "\n",
        "Next Steps\n",
        "\n",
        "- Explore `equinox` for more Pythonic neural network design\n",
        "- Try `diffrax` for production-grade ODE solving\n",
        "- Investigate `jaxopt` for more optimization algorithms\n",
        "- Read about **Universal Differential Equations** (UDEs)\n",
        "- Study **SciML (Scientific Machine Learning)** ecosystem\n",
        "\n",
        "References\n",
        "\n",
        "1. **Neural ODEs**: Chen et al., \"Neural Ordinary Differential Equations\", NeurIPS 2018\n",
        "2. **PINNs**: Raissi et al., \"Physics-informed neural networks\", JCP 2019\n",
        "3. **UDEs**: Rackauckas et al., \"Universal Differential Equations\", arXiv 2020\n",
        "4. **JAX Ecosystem**: https://github.com/n2cholas/awesome-jax\n",
        "\n",
        "---\n",
        "\n",
        "**Congratulations!** You've learned how to integrate NLSQ with the JAX ML ecosystem for hybrid scientific computing.\n",
        "\n",
        "**Continue Learning**:\n",
        "- [Research Workflow Case Study](research_workflow_case_study.ipynb) - Real experimental data\n",
        "- [Advanced Features Demo](advanced_features_demo.ipynb) - Diagnostics and optimization\n",
        "- [Performance Optimization Demo](performance_optimization_demo.ipynb) - Production-ready optimization\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
