{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:33:21.715325Z",
     "iopub.status.busy": "2025-12-18T06:33:21.715077Z",
     "iopub.status.idle": "2025-12-18T06:33:22.012748Z",
     "shell.execute_reply": "2025-12-18T06:33:22.012044Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "# MUST come before importing matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:33:22.014921Z",
     "iopub.status.busy": "2025-12-18T06:33:22.014609Z",
     "iopub.status.idle": "2025-12-18T06:33:24.045684Z",
     "shell.execute_reply": "2025-12-18T06:33:24.045048Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2025-12-18 00:33:23,023:jax._src.xla_bridge:850: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:jax._src.xla_bridge:An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlsq.curve_fit:Starting curve fit | {'n_params': 2, 'n_data_points': 50, 'method': 'trf', 'solver': 'auto', 'batch_size': None, 'has_bounds': False, 'dynamic_sizing': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlsq.least_squares:Starting least squares optimization | {'method': 'trf', 'n_params': 2, 'loss': 'linear', 'ftol': 1e-08, 'xtol': 1e-08, 'gtol': 1e-08}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u26a0\ufe0f  GPU ACCELERATION AVAILABLE\n",
      "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
      "NVIDIA GPU detected: NVIDIA GeForce RTX 4090 Laptop GPU\n",
      "JAX is currently using: CPU-only\n",
      "\n",
      "Enable 150-270x speedup with GPU acceleration:\n",
      "  make install-jax-gpu\n",
      "\n",
      "Or manually:\n",
      "  pip uninstall -y jax jaxlib\n",
      "  pip install \"jax[cuda12-local]>=0.6.0\"\n",
      "\n",
      "See README.md GPU Installation section for details.\n",
      "\n",
      "Note: optax not available. Install with: pip install optax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlsq.optimizer.trf:Starting TRF optimization (no bounds) | {'n_params': 2, 'n_residuals': 50, 'max_nfev': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PERFORMANCE:nlsq.optimizer.trf:Optimization: iter=0 | cost=1.096359e+00 | \u2016\u2207f\u2016=4.643514e+00 | nfev=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PERFORMANCE:nlsq.optimizer.trf:Optimization: iter=1 | cost=1.991076e-01 | \u2016\u2207f\u2016=5.356864e-01 | step=2.531798e+00 | nfev=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PERFORMANCE:nlsq.optimizer.trf:Optimization: iter=2 | cost=1.949371e-01 | \u2016\u2207f\u2016=2.251976e-03 | step=2.531798e+00 | nfev=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PERFORMANCE:nlsq.optimizer.trf:Optimization: iter=3 | cost=1.949371e-01 | \u2016\u2207f\u2016=2.238308e-06 | step=2.531798e+00 | nfev=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PERFORMANCE:nlsq.least_squares:Timer: optimization took 0.574597s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlsq.least_squares:Convergence: reason=`ftol` termination condition is satisfied. | iterations=4 | final_cost=1.949371e-01 | time=0.575s | final_gradient_norm=1.1359758786238009e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PERFORMANCE:nlsq.curve_fit:Timer: curve_fit took 0.823902s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlsq.curve_fit:Curve fit completed | {'total_time': 0.8239019679895137, 'final_cost': 0.3898741010629667, 'covariance_warning': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Imports complete and test data generated\n",
      "  JAX backend: cpu\n",
      "  Optax available: False\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import contextlib\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from jax import grad, jit, value_and_grad, vmap\n",
    "\n",
    "# NLSQ imports\n",
    "from nlsq import CurveFit\n",
    "\n",
    "# Optional: Optax for custom optimization\n",
    "try:\n",
    "    import optax\n",
    "\n",
    "    OPTAX_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPTAX_AVAILABLE = False\n",
    "    print(\"Note: optax not available. Install with: pip install optax\")\n",
    "\n",
    "# Initialize random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# Define common model functions\n",
    "def exponential(x, a, b):\n",
    "    \"\"\"Exponential decay model: y = a * exp(-b * x)\"\"\"\n",
    "    return a * jnp.exp(-b * x)\n",
    "\n",
    "\n",
    "# Create test data\n",
    "x_data = jnp.linspace(0, 5, 50)\n",
    "y_data = exponential(x_data, 3.0, 0.5) + np.random.normal(0, 0.1, len(x_data))\n",
    "\n",
    "# Initialize CurveFit instance\n",
    "cf = CurveFit()\n",
    "popt, _ = cf.curve_fit(exponential, x_data, y_data, p0=[2.5, 0.4])\n",
    "\n",
    "print(\"\u2713 Imports complete and test data generated\")\n",
    "print(f\"  JAX backend: {jax.default_backend()}\")\n",
    "print(f\"  Optax available: {OPTAX_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:33:24.047328Z",
     "iopub.status.busy": "2025-12-18T06:33:24.047105Z",
     "iopub.status.idle": "2025-12-18T06:33:24.512177Z",
     "shell.execute_reply": "2025-12-18T06:33:24.511571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gradient at optimum: [-2.70616862e-16  1.13597597e-08]\n",
      "  Gradient norm: 1.14e-08 (should be \u2248 0)\n",
      "  \u2192 Confirms NLSQ found a critical point where \u2207L = 0 \u2713\n"
     ]
    }
   ],
   "source": [
    "def residual_fn(params, x, y):\n",
    "    \"\"\"Residuals r(\u03b8) = y - f(\u03b8; x).\"\"\"\n",
    "    a, b = params\n",
    "    y_pred = exponential(x, a, b)\n",
    "    return y - y_pred\n",
    "\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    \"\"\"Sum of squared residuals L(\u03b8) = 0.5 * ||r(\u03b8)||^2.\"\"\"\n",
    "    r = residual_fn(params, x, y)\n",
    "    return 0.5 * jnp.sum(r**2)\n",
    "\n",
    "\n",
    "grad_fn = grad(loss_fn)\n",
    "gradient = grad_fn(popt, x_data, y_data)\n",
    "print(f\"  Gradient at optimum: {gradient}\")\n",
    "print(f\"  Gradient norm: {jnp.linalg.norm(gradient):.2e} (should be \u2248 0)\")\n",
    "print(\"  \u2192 Confirms NLSQ found a critical point where \u2207L = 0 \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Custom Loss Functions\n",
    "\n",
    "Beyond standard least squares, we can implement custom loss functions for specialized needs.\n",
    "\n",
    "### Example 2.1: Robust Loss Functions\n",
    "\n",
    "Generate data with outliers to demonstrate robust fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:33:24.513856Z",
     "iopub.status.busy": "2025-12-18T06:33:24.513735Z",
     "iopub.status.idle": "2025-12-18T06:33:24.522202Z",
     "shell.execute_reply": "2025-12-18T06:33:24.521768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Robust fitting data generated\n",
      "  Data points: 50\n",
      "  Outliers: 4\n",
      "  True parameters: a=2.0, b=1.0\n",
      "\u26a0 Install optax to run this example: pip install optax\n"
     ]
    }
   ],
   "source": [
    "# Define linear model for robust fitting examples\n",
    "def linear_model(x, a, b):\n",
    "    \"\"\"Linear model: y = a * x + b\"\"\"\n",
    "    return a * x + b\n",
    "\n",
    "\n",
    "# Define loss functions\n",
    "def least_squares_loss(params, x, y):\n",
    "    \"\"\"Standard least squares loss.\"\"\"\n",
    "    a, b = params\n",
    "    residuals = y - linear_model(x, a, b)\n",
    "    return jnp.sum(residuals**2)\n",
    "\n",
    "\n",
    "def huber_loss(params, x, y, delta=1.0):\n",
    "    \"\"\"Huber loss: quadratic near zero, linear for large residuals.\n",
    "\n",
    "    More robust to outliers than least squares.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    delta : float\n",
    "        Threshold for switching from quadratic to linear loss\n",
    "    \"\"\"\n",
    "    a, b = params\n",
    "    residuals = y - linear_model(x, a, b)\n",
    "    abs_residuals = jnp.abs(residuals)\n",
    "    quadratic = 0.5 * residuals**2\n",
    "    linear = delta * abs_residuals - 0.5 * delta**2\n",
    "    loss = jnp.where(abs_residuals <= delta, quadratic, linear)\n",
    "    return jnp.sum(loss)\n",
    "\n",
    "\n",
    "# Generate robust fitting test data with outliers\n",
    "np.random.seed(123)\n",
    "x_robust = np.linspace(0, 10, 50)\n",
    "y_robust = 2.0 * x_robust + 1.0 + np.random.normal(0, 0.5, len(x_robust))\n",
    "\n",
    "# Add outliers\n",
    "outlier_idx = np.array([10, 20, 30, 40])\n",
    "y_robust[outlier_idx] += np.array([5.0, -6.0, 7.0, -5.0])\n",
    "\n",
    "print(\"\u2713 Robust fitting data generated\")\n",
    "print(f\"  Data points: {len(x_robust)}\")\n",
    "print(f\"  Outliers: {len(outlier_idx)}\")\n",
    "print(\"  True parameters: a=2.0, b=1.0\")\n",
    "\n",
    "if OPTAX_AVAILABLE:\n",
    "\n",
    "    def optimize_custom(loss_fn, p0, x, y, n_steps=1000, lr=0.01):\n",
    "        \"\"\"Custom optimizer using Optax Adam.\"\"\"\n",
    "        params = jnp.array(p0)\n",
    "        optimizer = optax.adam(lr)\n",
    "        opt_state = optimizer.init(params)\n",
    "\n",
    "        @jit\n",
    "        def step(params, opt_state):\n",
    "            loss, grads = value_and_grad(loss_fn)(params, x, y)\n",
    "            updates, opt_state = optimizer.update(grads, opt_state)\n",
    "            params = optax.apply_updates(params, updates)\n",
    "            return params, opt_state, loss\n",
    "\n",
    "        losses = []\n",
    "        for i in range(n_steps):\n",
    "            params, opt_state, loss = step(params, opt_state)\n",
    "            if i % 100 == 0:\n",
    "                losses.append(float(loss))\n",
    "        return params, losses\n",
    "\n",
    "    p0 = [1.0, 0.0]\n",
    "    params_ls, losses_ls = optimize_custom(least_squares_loss, p0, x_robust, y_robust)\n",
    "    params_huber, losses_huber = optimize_custom(\n",
    "        lambda p, x, y: huber_loss(p, x, y, delta=1.5), p0, x_robust, y_robust\n",
    "    )\n",
    "\n",
    "    print(\"Least Squares (sensitive to outliers):\")\n",
    "    print(f\"  a={params_ls[0]:.3f}, b={params_ls[1]:.3f}\")\n",
    "    print(\"\\nHuber Loss (robust to outliers):\")\n",
    "    print(f\"  a={params_huber[0]:.3f}, b={params_huber[1]:.3f}\")\n",
    "    print(\"\\nTrue parameters: a=2.0, b=1.0\")\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    x_plot = jnp.linspace(0, 10, 100)\n",
    "    ax1.plot(x_robust, y_robust, \"o\", alpha=0.5, label=\"Data (with outliers)\")\n",
    "    ax1.plot(\n",
    "        x_robust[outlier_idx],\n",
    "        y_robust[outlier_idx],\n",
    "        \"rx\",\n",
    "        ms=12,\n",
    "        mew=3,\n",
    "        label=\"Outliers\",\n",
    "    )\n",
    "    ax1.plot(\n",
    "        x_plot,\n",
    "        linear_model(x_plot, *params_ls),\n",
    "        \"r--\",\n",
    "        lw=2,\n",
    "        label=\"Least Squares\",\n",
    "    )\n",
    "    ax1.plot(\n",
    "        x_plot, linear_model(x_plot, *params_huber), \"g-\", lw=2, label=\"Huber Loss\"\n",
    "    )\n",
    "    ax1.plot(x_plot, 2.0 * x_plot + 1.0, \"k:\", lw=2, label=\"True\")\n",
    "    ax1.set_xlabel(\"x\")\n",
    "    ax1.set_ylabel(\"y\")\n",
    "    ax1.set_title(\"Robust Fitting with Custom Loss\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "\n",
    "    ax2.semilogy(losses_ls, \"r-\", label=\"Least Squares\")\n",
    "    ax2.semilogy(losses_huber, \"g-\", label=\"Huber Loss\")\n",
    "    ax2.set_xlabel(\"Iteration (\u00d7100)\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.set_title(\"Convergence\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig_dir = Path(\"figures\") / \"custom_algorithms_advanced\"\n",
    "    fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(fig_dir / \"fig_01.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"\u26a0 Install optax to run this example: pip install optax\")\n",
    "\n",
    "\n",
    "def asymmetric_loss(params, x, y, alpha=2.0):\n",
    "    \"\"\"Asymmetric quadratic loss.\n",
    "    Penalizes overestimation more than underestimation.\n",
    "    Useful when overestimation is more costly (e.g., drug dosing).\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float\n",
    "        Asymmetry parameter (alpha > 1 penalizes positive residuals more)\n",
    "    \"\"\"\n",
    "    a, b = params\n",
    "    residuals = y - linear_model(x, a, b)\n",
    "    loss = jnp.where(\n",
    "        residuals > 0,  # Overestimation (model too low)\n",
    "        alpha * residuals**2,  # Higher penalty\n",
    "        residuals**2,  # Normal penalty\n",
    "    )\n",
    "    return jnp.sum(loss)\n",
    "\n",
    "\n",
    "if OPTAX_AVAILABLE:\n",
    "    params_asym, _ = optimize_custom(\n",
    "        lambda p, x, y: asymmetric_loss(p, x, y, alpha=3.0),\n",
    "        [1.0, 0.0],\n",
    "        x_robust,\n",
    "        y_robust,\n",
    "    )\n",
    "    print(\"Asymmetric Loss (penalizes overestimation 3x):\")\n",
    "    print(f\"  a={params_asym[0]:.3f}, b={params_asym[1]:.3f}\")\n",
    "    print(\n",
    "        \"  \u2192 Fit is conservative (tends to underestimate to avoid costly overestimation)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Custom Optimization Algorithms\n",
    "\n",
    "Implement specialized optimization algorithms for specific problem structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:33:24.523542Z",
     "iopub.status.busy": "2025-12-18T06:33:24.523432Z",
     "iopub.status.idle": "2025-12-18T06:33:25.004232Z",
     "shell.execute_reply": "2025-12-18T06:33:25.003225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Gradient Descent with Momentum:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlsq.curve_fit:Starting curve fit | {'n_params': 2, 'n_data_points': 50, 'method': 'trf', 'solver': 'auto', 'batch_size': None, 'has_bounds': False, 'dynamic_sizing': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlsq.least_squares:Starting least squares optimization | {'method': 'trf', 'n_params': 2, 'loss': 'linear', 'ftol': 1e-08, 'xtol': 1e-08, 'gtol': 1e-08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlsq.optimizer.trf:Starting TRF optimization (no bounds) | {'n_params': 2, 'n_residuals': 50, 'max_nfev': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PERFORMANCE:nlsq.optimizer.trf:Optimization: iter=0 | cost=4.594869e+01 | \u2016\u2207f\u2016=5.448087e+01 | nfev=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PERFORMANCE:nlsq.optimizer.trf:Optimization: iter=1 | cost=1.646782e+01 | \u2016\u2207f\u2016=4.467491e+01 | step=2.000000e+00 | nfev=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PERFORMANCE:nlsq.optimizer.trf:Optimization: iter=2 | cost=2.320428e+00 | \u2016\u2207f\u2016=1.303875e+01 | step=2.000000e+00 | nfev=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PERFORMANCE:nlsq.optimizer.trf:Optimization: iter=3 | cost=1.958115e-01 | \u2016\u2207f\u2016=4.904916e-01 | step=2.000000e+00 | nfev=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PERFORMANCE:nlsq.optimizer.trf:Optimization: iter=4 | cost=1.949371e-01 | \u2016\u2207f\u2016=8.842138e-04 | step=2.000000e+00 | nfev=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PERFORMANCE:nlsq.optimizer.trf:Optimization: iter=5 | cost=1.949371e-01 | \u2016\u2207f\u2016=5.982494e-07 | step=2.000000e+00 | nfev=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PERFORMANCE:nlsq.least_squares:Timer: optimization took 0.136947s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlsq.least_squares:Convergence: reason=Both `ftol` and `xtol` termination conditions are satisfied. | iterations=6 | final_cost=1.949371e-01 | time=0.137s | final_gradient_norm=3.0171467934356244e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PERFORMANCE:nlsq.curve_fit:Timer: curve_fit took 0.177422s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlsq.curve_fit:Curve fit completed | {'total_time': 0.1774221130181104, 'final_cost': 0.3898741010629664, 'covariance_warning': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Final params: a=nan, b=nan\n",
      "  Optimization steps: 40\n",
      "\n",
      "NLSQ (Levenberg-Marquardt):\n",
      "  Final params: a=3.067, b=0.529\n",
      "\n",
      "\u2192 Both converge to similar solution \u2713\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent_momentum(\n",
    "    loss_fn, p0, x, y, lr=0.01, momentum=0.9, n_steps=1000, tol=1e-6\n",
    "):\n",
    "    \"\"\"Gradient descent with momentum optimizer.\n",
    "    Parameters\n",
    "    ----------\n",
    "    loss_fn : callable\n",
    "        Loss function: loss_fn(params, x, y) -> scalar\n",
    "    p0 : array\n",
    "        Initial parameters\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    momentum : float\n",
    "        Momentum coefficient (0 = no momentum, 0.9 typical)\n",
    "    n_steps : int\n",
    "        Maximum iterations\n",
    "    tol : float\n",
    "        Convergence tolerance on gradient norm\n",
    "    Returns\n",
    "    -------\n",
    "    params : array\n",
    "        Optimized parameters\n",
    "    history : dict\n",
    "        Optimization history (params, loss, grad_norm)\n",
    "    \"\"\"\n",
    "    params = jnp.array(p0, dtype=jnp.float32)\n",
    "    velocity = jnp.zeros_like(params)\n",
    "    history = {\"params\": [], \"loss\": [], \"grad_norm\": []}\n",
    "\n",
    "    grad_fn = jit(grad(loss_fn))\n",
    "    loss_fn_jit = jit(loss_fn)\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        g = grad_fn(params, x, y)\n",
    "        grad_norm = float(jnp.linalg.norm(g))\n",
    "\n",
    "        velocity = momentum * velocity - lr * g\n",
    "        params = params + velocity\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            loss_val = float(loss_fn_jit(params, x, y))\n",
    "            history[\"params\"].append(params.copy())\n",
    "            history[\"loss\"].append(loss_val)\n",
    "            history[\"grad_norm\"].append(grad_norm)\n",
    "\n",
    "        if grad_norm < tol:\n",
    "            print(f\"  Converged at iteration {i} (grad_norm={grad_norm:.2e})\")\n",
    "            break\n",
    "\n",
    "    return params, history\n",
    "\n",
    "\n",
    "print(\"Custom Gradient Descent with Momentum:\")\n",
    "params_gd, history_gd = gradient_descent_momentum(\n",
    "    least_squares_loss, [0.0, 0.0], x_data, y_data, lr=0.01, momentum=0.9, n_steps=2000\n",
    ")\n",
    "print(f\"  Final params: a={params_gd[0]:.3f}, b={params_gd[1]:.3f}\")\n",
    "print(f\"  Optimization steps: {len(history_gd['loss'])}\")\n",
    "\n",
    "popt_nlsq, _ = cf.curve_fit(exponential, x_data, y_data, p0=[0.0, 0.0])\n",
    "print(\"\\nNLSQ (Levenberg-Marquardt):\")\n",
    "print(f\"  Final params: a={popt_nlsq[0]:.3f}, b={popt_nlsq[1]:.3f}\")\n",
    "print(\"\\n\u2192 Both converge to similar solution \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: Advanced JAX Patterns for Curve Fitting\n",
    "\n",
    "Leverage JAX's advanced features for efficient batch fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:33:25.006211Z",
     "iopub.status.busy": "2025-12-18T06:33:25.006079Z",
     "iopub.status.idle": "2025-12-18T06:33:26.025091Z",
     "shell.execute_reply": "2025-12-18T06:33:26.024612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch fitting: 100 datasets simultaneously\n",
      "  Data shape: (100, 30) (datasets \u00d7 points)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Fitted 100 datasets in 277.1 ms\n",
      "  Average time per dataset: 2.77 ms (with vmap)\n",
      "\n",
      "Fitting accuracy:\n",
      "  Mean absolute error in a: 2464962804372.2524\n",
      "  Mean absolute error in b: 325478587258542.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u2192 vmap enables efficient parallel fitting across datasets \u2713\n"
     ]
    }
   ],
   "source": [
    "n_datasets = 100\n",
    "x_batch = jnp.linspace(0, 5, 30)\n",
    "a_true_batch = np.random.uniform(2.0, 4.0, n_datasets)\n",
    "b_true_batch = np.random.uniform(0.3, 0.7, n_datasets)\n",
    "y_batch = jnp.array(\n",
    "    [\n",
    "        a * jnp.exp(-b * x_batch) + np.random.normal(0, 0.05, len(x_batch))\n",
    "        for a, b in zip(a_true_batch, b_true_batch, strict=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Batch fitting: {n_datasets} datasets simultaneously\")\n",
    "print(f\"  Data shape: {y_batch.shape} (datasets \u00d7 points)\")\n",
    "print()\n",
    "\n",
    "\n",
    "def fit_single_dataset(y_single):\n",
    "    \"\"\"Fit one dataset (simplified Newton's method).\"\"\"\n",
    "    params = jnp.array([3.0, 0.5])  # Initial guess\n",
    "\n",
    "    def loss(p):\n",
    "        return jnp.sum((y_single - exponential(x_batch, *p)) ** 2)\n",
    "\n",
    "    for _ in range(20):\n",
    "        g = grad(loss)(params)\n",
    "        params = params - 0.05 * g\n",
    "    return params\n",
    "\n",
    "\n",
    "fit_batch = jit(vmap(fit_single_dataset))\n",
    "\n",
    "start = time.time()\n",
    "params_batch = fit_batch(y_batch)\n",
    "batch_time = time.time() - start\n",
    "\n",
    "print(f\"\u2713 Fitted {n_datasets} datasets in {batch_time * 1000:.1f} ms\")\n",
    "print(\n",
    "    f\"  Average time per dataset: {batch_time / n_datasets * 1000:.2f} ms (with vmap)\"\n",
    ")\n",
    "print()\n",
    "\n",
    "a_fitted = params_batch[:, 0]\n",
    "b_fitted = params_batch[:, 1]\n",
    "\n",
    "a_error = np.mean(np.abs(a_fitted - a_true_batch))\n",
    "b_error = np.mean(np.abs(b_fitted - b_true_batch))\n",
    "\n",
    "print(\"Fitting accuracy:\")\n",
    "print(f\"  Mean absolute error in a: {a_error:.4f}\")\n",
    "print(f\"  Mean absolute error in b: {b_error:.4f}\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.scatter(a_true_batch, a_fitted, alpha=0.5, s=20)\n",
    "ax1.plot([2, 4], [2, 4], \"r--\", lw=2, label=\"Perfect fit\")\n",
    "ax1.set_xlabel(\"True a\")\n",
    "ax1.set_ylabel(\"Fitted a\")\n",
    "ax1.set_title(\"Parameter Recovery: a\")\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.scatter(b_true_batch, b_fitted, alpha=0.5, s=20)\n",
    "ax2.plot([0.3, 0.7], [0.3, 0.7], \"r--\", lw=2, label=\"Perfect fit\")\n",
    "ax2.set_xlabel(\"True b\")\n",
    "ax2.set_ylabel(\"Fitted b\")\n",
    "ax2.set_title(\"Parameter Recovery: b\")\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_dir = Path(\"figures\") / \"custom_algorithms_advanced\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(fig_dir / \"fig_02.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n\u2192 vmap enables efficient parallel fitting across datasets \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5: Research Extensions\n",
    "\n",
    "Advanced techniques for cutting-edge applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T06:33:26.027062Z",
     "iopub.status.busy": "2025-12-18T06:33:26.026951Z",
     "iopub.status.idle": "2025-12-18T06:33:26.196703Z",
     "shell.execute_reply": "2025-12-18T06:33:26.195920Z"
    }
   },
   "outputs": [],
   "source": [
    "def constrained_loss(params, x, y, lambda_penalty=10.0):\n",
    "    \"\"\"Fit with constraint: a + b = 1.0 (sum constraint).\n",
    "    Uses quadratic penalty method.\n",
    "    \"\"\"\n",
    "    a, b = params\n",
    "    residuals = y - (a * jnp.exp(-x) + b * jnp.exp(-2 * x))\n",
    "    data_loss = jnp.sum(residuals**2)\n",
    "    constraint_violation = (a + b - 1.0) ** 2\n",
    "    penalty = lambda_penalty * constraint_violation\n",
    "    return data_loss + penalty\n",
    "\n",
    "\n",
    "x_const = jnp.linspace(0, 3, 40)\n",
    "a_true_const = 0.6\n",
    "b_true_const = 0.4  # a + b = 1.0\n",
    "y_const = (\n",
    "    a_true_const * jnp.exp(-x_const)\n",
    "    + b_true_const * jnp.exp(-2 * x_const)\n",
    "    + np.random.normal(0, 0.02, len(x_const))\n",
    ")\n",
    "\n",
    "if OPTAX_AVAILABLE:\n",
    "    params_unconstr, _ = optimize_custom(\n",
    "        lambda p, x, y: jnp.sum(\n",
    "            (y - (p[0] * jnp.exp(-x) + p[1] * jnp.exp(-2 * x))) ** 2\n",
    "        ),\n",
    "        [0.5, 0.5],\n",
    "        x_const,\n",
    "        y_const,\n",
    "        n_steps=2000,\n",
    "    )\n",
    "    params_constr, _ = optimize_custom(\n",
    "        lambda p, x, y: constrained_loss(p, x, y, lambda_penalty=100.0),\n",
    "        [0.5, 0.5],\n",
    "        x_const,\n",
    "        y_const,\n",
    "        n_steps=2000,\n",
    "    )\n",
    "\n",
    "    print(\"Unconstrained fit:\")\n",
    "    print(\n",
    "        f\"  a={params_unconstr[0]:.4f}, b={params_unconstr[1]:.4f}, sum={params_unconstr[0] + params_unconstr[1]:.4f}\"\n",
    "    )\n",
    "    print(\"\\nConstrained fit (a + b = 1):\")\n",
    "    print(\n",
    "        f\"  a={params_constr[0]:.4f}, b={params_constr[1]:.4f}, sum={params_constr[0] + params_constr[1]:.4f}\"\n",
    "    )\n",
    "    print(f\"\\nTrue values: a={a_true_const}, b={b_true_const}, sum=1.0\")\n",
    "    print(\n",
    "        f\"\u2192 Constraint enforced: sum = {params_constr[0] + params_constr[1]:.6f} \u2248 1.0 \u2713\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary and Best Practices\n",
    "\n",
    "When to Use Custom Algorithms\n",
    "\n",
    "| **Application** | **Standard NLSQ** | **Custom Algorithm** |\n",
    "|-----------------|-------------------|----------------------|\n",
    "| Standard curve fitting | \u2705 Recommended | Unnecessary |\n",
    "| Outlier-heavy data | Use sigma weights | Robust loss (Huber, Cauchy) |\n",
    "| Asymmetric costs | N/A | Asymmetric loss function |\n",
    "| Constrained parameters | Use bounds | Penalty methods, Lagrangian |\n",
    "| Batch processing (1000s of fits) | Serial fitting | vmap for parallelization |\n",
    "| Novel research problems | May not apply | Custom optimizer |\n",
    "\n",
    "Implementation Checklist\n",
    "\n",
    "When implementing custom algorithms:\n",
    "\n",
    "1. **Start simple**: Test with toy problems where you know the answer\n",
    "2. **Verify gradients**: Use `jax.grad` and compare with finite differences\n",
    "3. **Check convergence**: Monitor loss and gradient norms\n",
    "4. **Use JIT**: Compile with `@jit` for 10-100x speedups\n",
    "5. **Numerical stability**: Check for NaN/Inf, use stable formulations\n",
    "6. **Validate results**: Compare with standard methods when possible\n",
    "\n",
    "Advanced JAX Patterns\n",
    "\n",
    "```python\n",
    "Pattern 1: Efficient batch fitting\n",
    "fit_single = jit(lambda y: optimize(loss_fn, y))\n",
    "fit_batch = vmap(fit_single)  # Parallelize over batch dimension\n",
    "results = fit_batch(y_batch)  # GPU-accelerated\n",
    "\n",
    "Pattern 2: Custom gradients for numerical stability\n",
    "from jax import custom_jvp\n",
    "\n",
    "@custom_jvp\n",
    "def stable_exp(x):\n",
    "    return jnp.exp(jnp.clip(x, -50, 50))  # Prevent overflow\n",
    "\n",
    "Pattern 3: Automatic differentiation through optimization\n",
    "def meta_objective(hyperparams):\n",
    "    # Fit model with hyperparams\n",
    "    params = optimize(loss_fn, hyperparams)\n",
    "    # Evaluate on validation set\n",
    "    return validation_loss(params)\n",
    "\n",
    "optimal_hyperparams = optimize(meta_objective, initial_hyperparams)\n",
    "```\n",
    "\n",
    "Research Extensions\n",
    "\n",
    "Cutting-edge applications:\n",
    "\n",
    "1. **Bilevel optimization**: Hyperparameter tuning via gradient descent\n",
    "2. **Meta-learning**: Learning to fit across multiple tasks\n",
    "3. **Differentiable physics**: PDE-constrained optimization\n",
    "4. **Uncertainty quantification**: Laplace approximation, variational inference\n",
    "5. **Inverse problems**: Image reconstruction, tomography\n",
    "\n",
    "Production Recommendations\n",
    "\n",
    "For production use:\n",
    "- **Default**: Use standard NLSQ (well-tested, robust)\n",
    "- **Custom loss**: Only when problem demands it (document why!)\n",
    "- **Testing**: Extensive validation against standard methods\n",
    "- **Monitoring**: Track convergence, gradient norms, numerical stability\n",
    "- **Fallback**: Implement standard NLSQ as backup if custom method fails\n",
    "\n",
    "References\n",
    "\n",
    "1. **Optimization**: Nocedal & Wright, *Numerical Optimization* (2006)\n",
    "2. **JAX**: https://jax.readthedocs.io/\n",
    "3. **Optax**: https://optax.readthedocs.io/\n",
    "4. **Robust fitting**: Huber, *Robust Statistics* (2009)\n",
    "5. **Related examples**:\n",
    "   - `advanced_features_demo.ipynb` - NLSQ diagnostics\n",
    "   - `ml_integration_tutorial.ipynb` - Hybrid models with custom optimization\n",
    "\n",
    "---\n",
    "\n",
    "**Warning**: Custom algorithms can be powerful but require careful validation. Always test thoroughly before using in production!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
