{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f62feeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T21:03:25.253377Z",
     "iopub.status.busy": "2025-12-18T21:03:25.252947Z",
     "iopub.status.idle": "2025-12-18T21:03:25.547226Z",
     "shell.execute_reply": "2025-12-18T21:03:25.546672Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "# MUST come before importing matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384d8adb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T21:03:25.549206Z",
     "iopub.status.busy": "2025-12-18T21:03:25.549063Z",
     "iopub.status.idle": "2025-12-18T21:03:26.291200Z",
     "shell.execute_reply": "2025-12-18T21:03:26.290448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Flax/Optax not available. Install with: pip install flax optax\n",
      "âœ“ Imports complete\n",
      "  JAX backend: gpu\n",
      "  Flax available: False\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from jax import grad, jit, value_and_grad, vmap\n",
    "\n",
    "# NLSQ imports\n",
    "from nlsq import CurveFit\n",
    "\n",
    "# Optional: Flax/Optax for neural networks\n",
    "try:\n",
    "    import flax.linen as nn\n",
    "    import optax\n",
    "    from flax.training import train_state\n",
    "\n",
    "    FLAX_AVAILABLE = True\n",
    "except ImportError:\n",
    "    FLAX_AVAILABLE = False\n",
    "    print(\"Note: Flax/Optax not available. Install with: pip install flax optax\")\n",
    "\n",
    "# Initialize\n",
    "np.random.seed(42)\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "print(\"âœ“ Imports complete\")\n",
    "print(f\"  JAX backend: {jax.default_backend()}\")\n",
    "print(f\"  Flax available: {FLAX_AVAILABLE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9b7a6df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T21:03:26.293168Z",
     "iopub.status.busy": "2025-12-18T21:03:26.292928Z",
     "iopub.status.idle": "2025-12-18T21:03:26.693053Z",
     "shell.execute_reply": "2025-12-18T21:03:26.692542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  Flax not available - ML integration examples will be skipped\n",
      "  Install with: pip install flax optax\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Dataset Statistics:\n",
      "   Data points: 200\n",
      "   Physics RMSE: 0.209\n",
      "   Correction amplitude: 0.429\n"
     ]
    }
   ],
   "source": [
    "if FLAX_AVAILABLE:\n",
    "    class CorrectionMLP(nn.Module):\n",
    "        features: list = (16, 16, 1)\n",
    "\n",
    "        @nn.compact\n",
    "        def __call__(self, x):\n",
    "            for feat in self.features[:-1]:\n",
    "                x = nn.Dense(feat)(x)\n",
    "                x = nn.relu(x)\n",
    "            x = nn.Dense(self.features[-1])(x)\n",
    "            return x.squeeze()\n",
    "\n",
    "    model = CorrectionMLP()\n",
    "    params = model.init(key, jnp.ones((1, 1)))\n",
    "    print(\"âœ… Correction MLP initialized\")\n",
    "    print(f\"   Parameter shapes: {jax.tree_util.tree_map(lambda x: x.shape, params)}\")\n",
    "else:\n",
    "    print(\"âš  Flax not available - ML integration examples will be skipped\")\n",
    "    print(\"  Install with: pip install flax optax\")\n",
    "\n",
    "# Generate hybrid model demonstration data\n",
    "x_data = np.linspace(0, 5, 200)\n",
    "true_a, true_b = 5.0, 1.2\n",
    "y_physics = true_a * np.exp(-true_b * x_data)\n",
    "y_correction = 0.5 * np.sin(3 * x_data) * np.exp(-0.3 * x_data)\n",
    "y_data = y_physics + y_correction + np.random.normal(0, 0.1, len(x_data))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x_data, y_physics, \"g-\", linewidth=2, label=\"Physics (exponential)\")\n",
    "plt.plot(x_data, y_data, \"b.\", alpha=0.5, markersize=3, label=\"Observed data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Data vs Physics Model\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(x_data, y_correction, \"r-\", linewidth=2, label=\"True correction\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Correction\")\n",
    "plt.title(\"Systematic Deviation\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "residuals = y_data - y_physics\n",
    "plt.plot(x_data, residuals, \"r.\", alpha=0.5, markersize=3, label=\"Residuals\")\n",
    "plt.plot(x_data, y_correction, \"k--\", linewidth=2, label=\"True correction\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.title(\"Physics Model Residuals\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_dir = Path(\"figures\") / \"ml_integration_tutorial\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(fig_dir / \"fig_01.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nðŸ“Š Dataset Statistics:\")\n",
    "print(f\"   Data points: {len(x_data)}\")\n",
    "print(f\"   Physics RMSE: {np.sqrt(np.mean((y_data - y_physics) ** 2)):.3f}\")\n",
    "print(f\"   Correction amplitude: {np.max(np.abs(y_correction)):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73926b6",
   "metadata": {},
   "source": [
    "Strategy 1: Two-Stage Fitting\n",
    "\n",
    "**Step 1**: Fit physics parameters with NLSQ\n",
    "**Step 2**: Train neural network on residuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "477885bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T21:03:26.694416Z",
     "iopub.status.busy": "2025-12-18T21:03:26.694286Z",
     "iopub.status.idle": "2025-12-18T21:03:26.703178Z",
     "shell.execute_reply": "2025-12-18T21:03:26.702772Z"
    }
   },
   "outputs": [],
   "source": [
    "if FLAX_AVAILABLE:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"TWO-STAGE HYBRID FITTING\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nðŸ”§ Stage 1: Fitting physics parameters with NLSQ...\")\n",
    "    def exponential_decay(x, a, b):\n",
    "        return a * jnp.exp(-b * x)\n",
    "    cf = CurveFit()\n",
    "    start_time = time.time()\n",
    "    popt_physics, pcov_physics = cf.curve_fit(\n",
    "        exponential_decay, x_data, y_data, p0=[4.0, 1.0]\n",
    "    )\n",
    "    physics_time = time.time() - start_time\n",
    "    a_fit, b_fit = popt_physics\n",
    "    print(f\"   Fitted parameters: a={a_fit:.3f}, b={b_fit:.3f}\")\n",
    "    print(f\"   True parameters:   a={true_a:.3f}, b={true_b:.3f}\")\n",
    "    print(f\"   Fit time: {physics_time:.3f}s\")\n",
    "    y_physics_fit = np.array(exponential_decay(x_data, *popt_physics))\n",
    "    residuals = y_data - y_physics_fit\n",
    "    physics_rmse = np.sqrt(np.mean(residuals**2))\n",
    "    print(f\"   Physics RMSE: {physics_rmse:.4f}\")\n",
    "    print(\"\\nðŸ§  Stage 2: Training neural network on residuals...\")\n",
    "    x_train = x_data.reshape(-1, 1).astype(np.float32)\n",
    "    y_train = residuals.astype(np.float32)\n",
    "    def create_train_state(rng, learning_rate=1e-3):\n",
    "        model_nn = CorrectionMLP()\n",
    "        params_nn = model_nn.init(rng, jnp.ones((1, 1)))\n",
    "        tx = optax.adam(learning_rate)\n",
    "        return train_state.TrainState.create(\n",
    "            apply_fn=model_nn.apply, params=params_nn, tx=tx\n",
    "        )\n",
    "    state = create_train_state(key, learning_rate=5e-3)\n",
    "    @jit\n",
    "    def train_step(state, x_batch, y_batch):\n",
    "        def loss_fn(params):\n",
    "            pred = state.apply_fn(params, x_batch)\n",
    "            return jnp.mean((pred - y_batch) ** 2)\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "        return state, loss\n",
    "    n_epochs = 500\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        state, loss = train_step(state, x_train, y_train)\n",
    "        losses.append(float(loss))\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"   Epoch {epoch + 1}/{n_epochs}, Loss: {loss:.6f}\")\n",
    "    nn_time = time.time() - start_time\n",
    "    print(f\"   NN training time: {nn_time:.3f}s\")\n",
    "    correction_pred = model.apply(state.params, x_train).squeeze()\n",
    "    y_hybrid = y_physics_fit + np.array(correction_pred)\n",
    "    hybrid_rmse = np.sqrt(np.mean((y_data - y_hybrid) ** 2))\n",
    "    print(\"\\nðŸ“Š Results:\")\n",
    "    print(f\"   Physics-only RMSE: {physics_rmse:.4f}\")\n",
    "    print(f\"   Hybrid model RMSE: {hybrid_rmse:.4f}\")\n",
    "    print(f\"   Improvement: {(1 - hybrid_rmse / physics_rmse) * 100:.1f}%\")\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.title(\"Neural Network Training\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(x_data, y_data, \"b.\", alpha=0.5, markersize=3, label=\"Data\")\n",
    "    plt.plot(x_data, y_physics_fit, \"g--\", linewidth=2, label=\"Physics only\")\n",
    "    plt.plot(x_data, y_hybrid, \"r-\", linewidth=2, label=\"Hybrid (Physics + NN)\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(\"Model Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(\n",
    "        x_data,\n",
    "        y_correction,\n",
    "        \"k--\",\n",
    "        linewidth=2,\n",
    "        label=\"True correction\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    plt.plot(x_data, correction_pred, \"r-\", linewidth=2, label=\"NN learned correction\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"Correction\")\n",
    "    plt.title(\"Learned vs True Correction\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    fig_dir = Path(\"figures\") / \"ml_integration_tutorial\"\n",
    "    fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(fig_dir / \"fig_02.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d5b5db",
   "metadata": {},
   "source": [
    "Key Insights\n",
    "\n",
    "1. **Physics provides structure**: The exponential decay captures the dominant behavior\n",
    "2. **NN learns deviations**: Small neural network captures systematic errors\n",
    "3. **Data efficiency**: Physics model requires fewer parameters than pure ML\n",
    "4. **Interpretability**: Physical parameters (a, b) have clear meaning\n",
    "5. **Better extrapolation**: Physics guides behavior outside training range\n",
    "\n",
    "**When to use this approach**:\n",
    "- âœ… Known physics with systematic deviations\n",
    "- âœ… Limited data (physics provides inductive bias)\n",
    "- âœ… Need interpretable parameters\n",
    "- âœ… Extrapolation is important\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe40720c",
   "metadata": {},
   "source": [
    "Part 2: Neural ODEs with NLSQ\n",
    "\n",
    "Concept: Learning Dynamics\n",
    "\n",
    "**Neural ODEs** parameterize the derivative of a system with a neural network:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dt} = f_{\\theta}(y, t)\n",
    "$$\n",
    "\n",
    "Where $f_{\\theta}$ is a neural network. We can then integrate this ODE to get predictions.\n",
    "\n",
    "**NLSQ Integration**: Use NLSQ to fit:\n",
    "1. Initial conditions\n",
    "2. ODE parameters (if partially mechanistic)\n",
    "3. Neural network parameters (jointly or in stages)\n",
    "\n",
    "Example 2.1: Damped Oscillator with Learned Damping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33623cca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T21:03:26.704298Z",
     "iopub.status.busy": "2025-12-18T21:03:26.704189Z",
     "iopub.status.idle": "2025-12-18T21:03:27.715319Z",
     "shell.execute_reply": "2025-12-18T21:03:27.714882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True parameters: Ï‰=2.0, Î³=0.3\n",
      "Initial state: xâ‚€=1.0, vâ‚€=0.0\n"
     ]
    }
   ],
   "source": [
    "def euler_integrate(f, y0, t, *args):\n",
    "    \"\"\"Simple Euler integration for demonstration.\"\"\"\n",
    "    dt = t[1] - t[0]\n",
    "    y = jnp.zeros((len(t), len(y0)))\n",
    "    y = y.at[0].set(y0)\n",
    "    for i in range(1, len(t)):\n",
    "        dydt = f(y[i - 1], t[i - 1], *args)\n",
    "        y = y.at[i].set(y[i - 1] + dt * dydt)\n",
    "    return y\n",
    "def damped_oscillator_ode(state, t, omega, gamma):\n",
    "    \"\"\"dy/dt for damped harmonic oscillator.\n",
    "    state = [position, velocity]\n",
    "    omega = natural frequency\n",
    "    gamma = damping coefficient\n",
    "    \"\"\"\n",
    "    x, v = state\n",
    "    dxdt = v\n",
    "    dvdt = -(omega**2) * x - 2 * gamma * v\n",
    "    return jnp.array([dxdt, dvdt])\n",
    "t_ode = np.linspace(0, 10, 200)\n",
    "omega_true = 2.0  # Natural frequency\n",
    "gamma_true = 0.3  # Damping\n",
    "y0_true = jnp.array([1.0, 0.0])  # Initial [position, velocity]\n",
    "y_true = euler_integrate(damped_oscillator_ode, y0_true, t_ode, omega_true, gamma_true)\n",
    "x_true = y_true[:, 0]  # Extract position\n",
    "x_obs = x_true + np.random.normal(0, 0.05, len(t_ode))\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(t_ode, x_true, \"g-\", linewidth=2, label=\"True dynamics\")\n",
    "plt.plot(t_ode, x_obs, \"b.\", alpha=0.5, markersize=3, label=\"Noisy observations\")\n",
    "plt.xlabel(\"Time (t)\")\n",
    "plt.ylabel(\"Position (x)\")\n",
    "plt.title(\"Damped Harmonic Oscillator\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(y_true[:, 0], y_true[:, 1], \"g-\", linewidth=2, label=\"Phase space\")\n",
    "plt.xlabel(\"Position (x)\")\n",
    "plt.ylabel(\"Velocity (v)\")\n",
    "plt.title(\"Phase Space Trajectory\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "fig_dir = Path(\"figures\") / \"ml_integration_tutorial\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(fig_dir / \"fig_03.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(f\"True parameters: Ï‰={omega_true}, Î³={gamma_true}\")\n",
    "print(f\"Initial state: xâ‚€={y0_true[0]}, vâ‚€={y0_true[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36a0962",
   "metadata": {},
   "source": [
    "Fitting ODE Parameters with NLSQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "952c47ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T21:03:27.716617Z",
     "iopub.status.busy": "2025-12-18T21:03:27.716479Z",
     "iopub.status.idle": "2025-12-18T21:04:02.611253Z",
     "shell.execute_reply": "2025-12-18T21:04:02.610675Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlsq.curve_fit:Starting curve fit | {'n_params': 4, 'n_data_points': 200, 'method': 'trf', 'solver': 'auto', 'batch_size': None, 'has_bounds': True, 'dynamic_sizing': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlsq.least_squares:Starting least squares optimization | {'method': 'trf', 'n_params': 4, 'loss': 'linear', 'ftol': 1e-08, 'xtol': 1e-08, 'gtol': 1e-08}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FITTING ODE PARAMETERS WITH NLSQ\n",
      "======================================================================\n",
      "\n",
      "ðŸ”§ Fitting ODE parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PERFORMANCE:nlsq.least_squares:Timer: optimization took 2.021949s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlsq.least_squares:Convergence: reason=`ftol` termination condition is satisfied. | iterations=10 | final_cost=2.352211e-01 | time=2.022s | final_gradient_norm=7.164226741499108e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PERFORMANCE:nlsq.curve_fit:Timer: curve_fit took 34.215419s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlsq.curve_fit:Curve fit completed | {'total_time': 34.21541919900119, 'final_cost': 0.4704422125438405, 'covariance_warning': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Results:\n",
      "   Fitted: Ï‰=2.0108, Î³=0.3062, xâ‚€=1.0152, vâ‚€=0.0437\n",
      "   True:   Ï‰=2.0000, Î³=0.3000, xâ‚€=1.0000, vâ‚€=0.0000\n",
      "   Fit time: 34.290s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RMSE: 0.04850\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FITTING ODE PARAMETERS WITH NLSQ\")\n",
    "print(\"=\" * 70)\n",
    "def oscillator_model(t, omega, gamma, x0, v0):\n",
    "    \"\"\"Model function that integrates ODE for given parameters.\"\"\"\n",
    "    y0 = jnp.array([x0, v0])\n",
    "    y = euler_integrate(damped_oscillator_ode, y0, t, omega, gamma)\n",
    "    return y[:, 0]  # Return position only\n",
    "print(\"\\nðŸ”§ Fitting ODE parameters...\")\n",
    "cf_ode = CurveFit()\n",
    "p0 = [1.5, 0.2, 0.8, 0.1]  # [omega, gamma, x0, v0]\n",
    "start_time = time.time()\n",
    "popt_ode, pcov_ode = cf_ode.curve_fit(\n",
    "    oscillator_model,\n",
    "    t_ode,\n",
    "    x_obs,\n",
    "    p0=p0,\n",
    "    bounds=([0, 0, -2, -2], [5, 2, 2, 2]),  # Reasonable physical bounds\n",
    ")\n",
    "ode_time = time.time() - start_time\n",
    "omega_fit, gamma_fit, x0_fit, v0_fit = popt_ode\n",
    "print(\"\\nðŸ“Š Results:\")\n",
    "print(\n",
    "    f\"   Fitted: Ï‰={omega_fit:.4f}, Î³={gamma_fit:.4f}, xâ‚€={x0_fit:.4f}, vâ‚€={v0_fit:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"   True:   Ï‰={omega_true:.4f}, Î³={gamma_true:.4f}, xâ‚€={y0_true[0]:.4f}, vâ‚€={y0_true[1]:.4f}\"\n",
    ")\n",
    "print(f\"   Fit time: {ode_time:.3f}s\")\n",
    "x_fit = oscillator_model(t_ode, *popt_ode)\n",
    "ode_rmse = np.sqrt(np.mean((x_obs - np.array(x_fit)) ** 2))\n",
    "print(f\"   RMSE: {ode_rmse:.5f}\")\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(t_ode, x_obs, \"b.\", alpha=0.5, markersize=3, label=\"Observations\")\n",
    "plt.plot(t_ode, x_true, \"g--\", linewidth=2, alpha=0.7, label=\"True dynamics\")\n",
    "plt.plot(t_ode, x_fit, \"r-\", linewidth=2, label=\"Fitted ODE\")\n",
    "plt.xlabel(\"Time (t)\")\n",
    "plt.ylabel(\"Position (x)\")\n",
    "plt.title(\"ODE Parameter Fitting\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals_ode = x_obs - np.array(x_fit)\n",
    "plt.plot(t_ode, residuals_ode, \"r.\", alpha=0.5, markersize=3)\n",
    "plt.axhline(y=0, color=\"k\", linestyle=\"-\", alpha=0.3)\n",
    "plt.xlabel(\"Time (t)\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.title(f\"Residuals (RMSE={ode_rmse:.5f})\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "fig_dir = Path(\"figures\") / \"ml_integration_tutorial\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(fig_dir / \"fig_04.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2c46e7",
   "metadata": {},
   "source": [
    "Key Takeaways\n",
    "\n",
    "1. **NLSQ handles ODEs naturally**: Just wrap ODE integration in model function\n",
    "2. **Automatic differentiation**: JAX computes gradients through ODE solver\n",
    "3. **Joint parameter estimation**: Fit dynamics parameters + initial conditions\n",
    "4. **Physical constraints**: Use bounds to enforce physically reasonable values\n",
    "\n",
    "**Production tip**: Use `diffrax` for more robust ODE integration:\n",
    "```python\n",
    "import diffrax\n",
    "solver = diffrax.Tsit5()  # Adaptive Runge-Kutta\n",
    "solution = diffrax.diffeqsolve(...)\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483c7ede",
   "metadata": {},
   "source": [
    "Part 3: Physics-Informed Loss Functions\n",
    "\n",
    "Concept: Incorporating Physical Constraints\n",
    "\n",
    "**Physics-informed fitting** adds physical constraints to the loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\mathcal{L}_{\\text{data}} + \\lambda \\mathcal{L}_{\\text{physics}}\n",
    "$$\n",
    "\n",
    "Examples:\n",
    "- **Conservation laws**: Energy, mass, momentum conservation\n",
    "- **PDE residuals**: Equations of motion, Maxwell's equations\n",
    "- **Boundary conditions**: Initial/final state constraints\n",
    "- **Symmetries**: Rotational, translational invariance\n",
    "\n",
    "Example 3.1: Energy-Conserving Pendulum\n",
    "\n",
    "For a frictionless pendulum, total energy should be conserved:\n",
    "\n",
    "$$\n",
    "E = \\frac{1}{2}mv^2 + mgh = \\text{constant}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03a58d87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T21:04:02.613196Z",
     "iopub.status.busy": "2025-12-18T21:04:02.613086Z",
     "iopub.status.idle": "2025-12-18T21:04:03.778248Z",
     "shell.execute_reply": "2025-12-18T21:04:03.777823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy conservation (std dev): 1.205909\n",
      "Energy variation: 60.915%\n"
     ]
    }
   ],
   "source": [
    "def pendulum_ode(state, t, omega):\n",
    "    \"\"\"Pendulum ODE: dÂ²Î¸/dtÂ² = -Ï‰Â² sin(Î¸)\"\"\"\n",
    "    theta, theta_dot = state\n",
    "    return jnp.array([theta_dot, -(omega**2) * jnp.sin(theta)])\n",
    "t_pend = np.linspace(0, 10, 150)\n",
    "omega_pend = 2.0\n",
    "y0_pend = jnp.array([0.5, 0.0])  # [angle, angular velocity]\n",
    "y_pend = euler_integrate(pendulum_ode, y0_pend, t_pend, omega_pend)\n",
    "theta_obs = y_pend[:, 0] + np.random.normal(0, 0.02, len(t_pend))\n",
    "kinetic = 0.5 * y_pend[:, 1] ** 2\n",
    "potential = (omega_pend**2) * (1 - jnp.cos(y_pend[:, 0]))\n",
    "total_energy = kinetic + potential\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(t_pend, y_pend[:, 0], \"g-\", linewidth=2, label=\"True angle\")\n",
    "plt.plot(t_pend, theta_obs, \"b.\", alpha=0.5, markersize=3, label=\"Observations\")\n",
    "plt.xlabel(\"Time (t)\")\n",
    "plt.ylabel(\"Angle Î¸ (rad)\")\n",
    "plt.title(\"Pendulum Motion\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(y_pend[:, 0], y_pend[:, 1], \"g-\", linewidth=2)\n",
    "plt.xlabel(\"Angle Î¸ (rad)\")\n",
    "plt.ylabel(\"Angular velocity dÎ¸/dt (rad/s)\")\n",
    "plt.title(\"Phase Space\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(t_pend, total_energy, \"r-\", linewidth=2, label=\"Total energy\")\n",
    "plt.axhline(y=jnp.mean(total_energy), color=\"k\", linestyle=\"--\", label=\"Mean energy\")\n",
    "plt.xlabel(\"Time (t)\")\n",
    "plt.ylabel(\"Energy (J)\")\n",
    "plt.title(\"Energy Conservation\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "fig_dir = Path(\"figures\") / \"ml_integration_tutorial\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(fig_dir / \"fig_05.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "energy_std = float(jnp.std(total_energy))\n",
    "print(f\"Energy conservation (std dev): {energy_std:.6f}\")\n",
    "print(f\"Energy variation: {energy_std / jnp.mean(total_energy) * 100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1248a45f",
   "metadata": {},
   "source": [
    "Custom Physics-Informed Fitting\n",
    "\n",
    "While NLSQ doesn't directly support custom loss functions (it uses least squares), we can:\n",
    "1. Use NLSQ for standard parameter estimation\n",
    "2. Add physics penalty in post-processing\n",
    "3. Or use Optax for full physics-informed optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30eb6563",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-18T21:04:03.780567Z",
     "iopub.status.busy": "2025-12-18T21:04:03.780360Z",
     "iopub.status.idle": "2025-12-18T21:04:03.791325Z",
     "shell.execute_reply": "2025-12-18T21:04:03.790665Z"
    }
   },
   "outputs": [],
   "source": [
    "if FLAX_AVAILABLE:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PHYSICS-INFORMED OPTIMIZATION WITH OPTAX\")\n",
    "    print(\"=\" * 70)\n",
    "    def physics_informed_loss(params, t, theta_obs, lambda_physics=0.1):\n",
    "        \"\"\"Loss = data fit + energy conservation penalty.\"\"\"\n",
    "        omega, theta0, thetadot0 = params\n",
    "        y0 = jnp.array([theta0, thetadot0])\n",
    "        y = euler_integrate(pendulum_ode, y0, t, omega)\n",
    "        theta_pred = y[:, 0]\n",
    "        loss_data = jnp.mean((theta_pred - theta_obs) ** 2)\n",
    "        kinetic = 0.5 * y[:, 1] ** 2\n",
    "        potential = (omega**2) * (1 - jnp.cos(y[:, 0]))\n",
    "        total_energy = kinetic + potential\n",
    "        energy_var = jnp.var(total_energy)\n",
    "        loss_physics = lambda_physics * energy_var\n",
    "        return loss_data + loss_physics, {\n",
    "            \"loss_data\": loss_data,\n",
    "            \"loss_physics\": loss_physics,\n",
    "        }\n",
    "    params_init = jnp.array([1.5, 0.4, 0.1])  # [omega, theta0, thetadot0]\n",
    "    optimizer = optax.adam(learning_rate=0.01)\n",
    "    opt_state = optimizer.init(params_init)\n",
    "    @jit\n",
    "    def update_step(params, opt_state, t, theta_obs):\n",
    "        (loss_val, metrics), grads = jax.value_and_grad(\n",
    "            physics_informed_loss, has_aux=True\n",
    "        )(params, t, theta_obs)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state, loss_val, metrics\n",
    "    params = params_init\n",
    "    n_steps = 1000\n",
    "    losses = []\n",
    "    print(\"\\nðŸŽ¯ Training with physics-informed loss...\")\n",
    "    for step in range(n_steps):\n",
    "        params, opt_state, loss_val, metrics = update_step(\n",
    "            params, opt_state, t_pend, theta_obs\n",
    "        )\n",
    "        losses.append(float(loss_val))\n",
    "        if (step + 1) % 200 == 0:\n",
    "            print(\n",
    "                f\"   Step {step + 1}: Total={loss_val:.6f}, \"\n",
    "                f\"Data={metrics['loss_data']:.6f}, Physics={metrics['loss_physics']:.6f}\"\n",
    "            )\n",
    "    omega_pi, theta0_pi, thetadot0_pi = params\n",
    "    print(\"\\nðŸ“Š Fitted parameters:\")\n",
    "    print(f\"   Ï‰={omega_pi:.4f} (true: {omega_pend:.4f})\")\n",
    "    print(f\"   Î¸â‚€={theta0_pi:.4f} (true: {y0_pend[0]:.4f})\")\n",
    "    print(f\"   dÎ¸â‚€/dt={thetadot0_pi:.4f} (true: {y0_pend[1]:.4f})\")\n",
    "    def pendulum_model(t, omega, theta0, thetadot0):\n",
    "        y0 = jnp.array([theta0, thetadot0])\n",
    "        y = euler_integrate(pendulum_ode, y0, t, omega)\n",
    "        return y[:, 0]\n",
    "    popt_std, _ = cf_ode.curve_fit(\n",
    "        pendulum_model, t_pend, theta_obs, p0=[1.5, 0.4, 0.1]\n",
    "    )\n",
    "    y_pi = euler_integrate(\n",
    "        pendulum_ode, jnp.array([theta0_pi, thetadot0_pi]), t_pend, omega_pi\n",
    "    )\n",
    "    y_std = euler_integrate(\n",
    "        pendulum_ode, jnp.array([popt_std[1], popt_std[2]]), t_pend, popt_std[0]\n",
    "    )\n",
    "    def compute_energy_std(y, omega):\n",
    "        kinetic = 0.5 * y[:, 1] ** 2\n",
    "        potential = (omega**2) * (1 - jnp.cos(y[:, 0]))\n",
    "        return float(jnp.std(kinetic + potential))\n",
    "    energy_std_pi = compute_energy_std(y_pi, omega_pi)\n",
    "    energy_std_std = compute_energy_std(y_std, popt_std[0])\n",
    "    print(\"\\nâš¡ Energy Conservation:\")\n",
    "    print(f\"   Physics-informed: Ïƒ_E = {energy_std_pi:.6f}\")\n",
    "    print(f\"   Standard NLSQ:    Ïƒ_E = {energy_std_std:.6f}\")\n",
    "    print(f\"   Improvement: {(1 - energy_std_pi / energy_std_std) * 100:.1f}%\")\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"Optimization Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Physics-Informed Training\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(t_pend, theta_obs, \"b.\", alpha=0.5, markersize=3, label=\"Data\")\n",
    "    plt.plot(t_pend, y_std[:, 0], \"g--\", linewidth=2, label=\"Standard NLSQ\")\n",
    "    plt.plot(t_pend, y_pi[:, 0], \"r-\", linewidth=2, label=\"Physics-informed\")\n",
    "    plt.xlabel(\"Time (t)\")\n",
    "    plt.ylabel(\"Angle Î¸ (rad)\")\n",
    "    plt.title(\"Fit Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    energy_std_series = 0.5 * y_std[:, 1] ** 2 + (popt_std[0] ** 2) * (\n",
    "        1 - jnp.cos(y_std[:, 0])\n",
    "    )\n",
    "    energy_pi_series = 0.5 * y_pi[:, 1] ** 2 + (omega_pi**2) * (1 - jnp.cos(y_pi[:, 0]))\n",
    "    plt.plot(t_pend, energy_std_series, \"g--\", linewidth=2, label=\"Standard NLSQ\")\n",
    "    plt.plot(t_pend, energy_pi_series, \"r-\", linewidth=2, label=\"Physics-informed\")\n",
    "    plt.xlabel(\"Time (t)\")\n",
    "    plt.ylabel(\"Energy (J)\")\n",
    "    plt.title(\"Energy Conservation\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    fig_dir = Path(\"figures\") / \"ml_integration_tutorial\"\n",
    "    fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(fig_dir / \"fig_06.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f88385",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Summary and Best Practices\n",
    "\n",
    "Integration Strategies\n",
    "\n",
    "| Approach | NLSQ Role | ML Role | Best For |\n",
    "|----------|-----------|---------|----------|\n",
    "| **Two-Stage Hybrid** | Fit physics parameters | Learn residuals | Known physics + systematic deviations |\n",
    "| **Neural ODE** | Fit ODE parameters | (Optional) Learn dynamics | Parameter estimation in dynamical systems |\n",
    "| **Physics-Informed** | Pre-fit, then refine | Enforce constraints | Energy/mass conservation, PDEs |\n",
    "| **Joint Optimization** | Parameter estimation | Model flexibility | Complex coupled systems |\n",
    "\n",
    "Key Takeaways\n",
    "\n",
    "1. **NLSQ + JAX = Powerful Combo**:\n",
    "- Automatic differentiation through complex models\n",
    "- GPU acceleration for both fitting and ML\n",
    "- Seamless integration with JAX ecosystem\n",
    "\n",
    "2. **Hybrid Models Win**:\n",
    "- Better than pure physics (captures deviations)\n",
    "- Better than pure ML (data efficient, interpretable)\n",
    "- Best of both worlds\n",
    "\n",
    "3. **Physics Constraints Help**:\n",
    "- Regularize ML models\n",
    "- Improve extrapolation\n",
    "- Ensure physical plausibility\n",
    "\n",
    "4. **Choose the Right Tool**:\n",
    "- **NLSQ**: Parameter estimation, well-conditioned problems\n",
    "- **Optax**: Custom losses, physics-informed training\n",
    "- **Combined**: Two-stage fitting strategies\n",
    "\n",
    "Production Recommendations\n",
    "\n",
    "```python\n",
    "1. Use diffrax for robust ODE integration\n",
    "import diffrax\n",
    "solver = diffrax.Tsit5()\n",
    "\n",
    "2. Separate training and inference\n",
    "@jit\n",
    "def inference_model(params, x):\n",
    "Compiled inference only\n",
    "return model.apply(params, x)\n",
    "\n",
    "3. Use appropriate precision\n",
    "NLSQ uses float64 by default (good for physics)\n",
    "ML often uses float32 (faster, sufficient for NNs)\n",
    "\n",
    "4. Validate physics constraints\n",
    "def check_energy_conservation(y, params):\n",
    "energy = compute_energy(y, params)\n",
    "return jnp.std(energy) < threshold\n",
    "\n",
    "5. Profile and optimize\n",
    "Use MemoryPool for repeated fitting\n",
    "from nlsq import MemoryPool\n",
    "with MemoryPool() as pool:\n",
    "for data in datasets:\n",
    "popt, _ = cf.curve_fit(model, *data)\n",
    "```\n",
    "\n",
    "Next Steps\n",
    "\n",
    "- Explore `equinox` for more Pythonic neural network design\n",
    "- Try `diffrax` for production-grade ODE solving\n",
    "- Investigate `jaxopt` for more optimization algorithms\n",
    "- Read about **Universal Differential Equations** (UDEs)\n",
    "- Study **SciML (Scientific Machine Learning)** ecosystem\n",
    "\n",
    "References\n",
    "\n",
    "1. **Neural ODEs**: Chen et al., \"Neural Ordinary Differential Equations\", NeurIPS 2018\n",
    "2. **PINNs**: Raissi et al., \"Physics-informed neural networks\", JCP 2019\n",
    "3. **UDEs**: Rackauckas et al., \"Universal Differential Equations\", arXiv 2020\n",
    "4. **JAX Ecosystem**: https://github.com/n2cholas/awesome-jax\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've learned how to integrate NLSQ with the JAX ML ecosystem for hybrid scientific computing.\n",
    "\n",
    "**Continue Learning**:\n",
    "- [Research Workflow Case Study](research_workflow_case_study.ipynb) - Real experimental data\n",
    "- [Advanced Features Demo](advanced_features_demo.ipynb) - Diagnostics and optimization\n",
    "- [Performance Optimization Demo](performance_optimization_demo.ipynb) - Production-ready optimization\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
