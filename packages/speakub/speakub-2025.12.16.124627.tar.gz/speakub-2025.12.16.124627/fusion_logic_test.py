#!/usr/bin/env python3
"""
Fusion Logic Test Script for SpeakUB

Tests the content-adaptive batching logic with different scenarios:
- Normal short content (uses base batch size based on config.json)
- Fragmented short content (auto-expands batch size)
- Long paragraph content (uses 3-item processing)
- Different config.json batch_size settings

Based on user's design philosophy:
- Base limit is 5 items (code iron law)
- config.json can dynamically adjust this base value
- Content evaluation still dynamically adjusts based on new base
"""

from speakub.tts.fusion_reservoir.batching_strategy import FusionBatchingStrategy
import sys
import os
from typing import Any, Dict, List

# Add project root to path for imports
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))


# Simulate minimal ConfigManager


class MockConfigManager:
    def __init__(self, base_batch_size: int = 5):
        self.base_batch_size = base_batch_size

    def get(self, key: str, default=None):
        if key == "tts.batch_size":
            return self.base_batch_size
        elif key == "tts.fusion.max_short_items":
            return 15
        elif key == "tts.fusion.long_paragraph_max_items":
            return 5
        elif key == "tts.fusion.char_limit":
            return 200
        elif key == "tts.fusion.enabled":
            return True
        elif key == "tts.preferred_engine":
            return "nanmai"  # For engine-specific char_limit
        return default

    def set_batch_size(self, size: int):
        self.base_batch_size = size


class FusionLogicTester:
    """Tests the Fusion logic under different scenarios using real FusionBatchingStrategy."""

    def __init__(self):
        self.test_scenarios = []

    def test_different_config_batch_sizes(self):
        """Test the effect of different config.json batch_size settings"""
        print("ðŸ”§ Test the effect of different config.json batch_size settings\n")

        scenarios = []
        for batch_size in [5, 7, 10]:
            config = MockConfigManager(base_batch_size=batch_size)
            strategy = FusionBatchingStrategy(config)  # type: ignore

            # Test the behavior of the same content under different batch_size settings
            test_candidates = [
                (0, "é€™æ˜¯ä¸€å€‹æ­£å¸¸çš„çŸ­å¥å­"),
                (1, "é€™æ˜¯å¦ä¸€å€‹çŸ­å¥å­å…§å®¹"),
                (2, "å¥å­ç‰‡æ®µæ¸¬è©¦å…§å®¹"),
                (3, "æ­£å¸¸å¥å­é•·åº¦æ¸¬è©¦"),
                (4, "æœ€å¾Œä¸€å€‹æ¸¬è©¦å¥å­")
            ]  # 5å€‹ä¸­ç­‰é•·åº¦çš„å¥å­

            selected_items, strategy_name = strategy.select_batch(
                test_candidates)

            result = {
                "strategy": strategy_name,
                "selected_count": len(selected_items),
                "candidates_count": len(test_candidates)
            }

            scenarios.append({
                "batch_size_setting": batch_size,
                "result": result,
                "candidates_info": f"{len(test_candidates)}é …ï¼Œå¹³å‡{sum(len(c[1]) for c in test_candidates)/len(test_candidates):.1f}å­—"
            })

        for scenario in scenarios:
            print(f"config.json batch_size = {scenario['batch_size_setting']}")
            print(f"  æ¸¬è©¦å…§å®¹ï¼š{scenario['candidates_info']}")
            print(
                f"  é¸æ“‡ï¼š{scenario['result']['selected_count']}é … ({scenario['result']['strategy']})")
            print(f"  é‚è¼¯ï¼šåŸºæ–¼{scenario['batch_size_setting']}çš„åŸºç¤Žé™åˆ¶\n")

    def test_content_type_scenarios(self):
        """Test processing of different content types"""
        print("ðŸ“‹ Test classification processing of different content types\n")

        # Use 5 as the base batch setting
        config = MockConfigManager(base_batch_size=5)
        strategy = FusionBatchingStrategy(config)  # type: ignore

        test_scenarios = [
            ("æ­£å¸¸çŸ­å…§å®¹", [
                (0, "é€™æ˜¯ä¸€å€‹æ­£å¸¸çš„çŸ­å¥å­"),
                (1, "é€™æ˜¯å¦ä¸€å€‹çŸ­å¥å­"),
                (2, "å¥å­å…§å®¹æ¸¬è©¦"),
                (3, "æ­£å¸¸å¥å­"),
                (4, "æœ€å¾Œä¸€å¥")
            ]),
            ("ç¢Žç‰‡åŒ–çŸ­å…§å®¹", [
                (i, text) for i, text in enumerate([
                    "çŸ­", "å¾ˆçŸ­", "æ¥µçŸ­", "è¶…çŸ­", "çŸ­ä¿ƒ", "ç°¡çŸ­", "ç‰‡æ®µ", "é›¶æ˜Ÿ", "æ•£ç¢Ž", "æ–·çºŒ"
                ])
            ]),
            ("é•·æ®µè½å…§å®¹", [
                (0, "é€™æ˜¯ä¸€å€‹éžå¸¸é•·çš„æ®µè½å…§å®¹åŒ…å«äº†å¤§é‡çš„æ–‡å­—è¶…å‡ºé€šå¸¸çš„å¥å­é•·åº¦é™åˆ¶æ¸¬è©¦èžåˆé‚è¼¯å¦‚ä½•è™•ç†é€™ç¨®æ¥µç«¯æƒ…æ³ç¢ºä¿ç³»çµ±èƒ½å¤ æ­£ç¢ºè­˜åˆ¥ä¸¦å€‹åˆ¥è™•ç†é€™ç¨®è¶…é•·å…§å®¹è€Œä¸æœƒå½±éŸ¿æ•´é«”æ€§èƒ½" +
                 "ç¹¼çºŒå¢žåŠ å…§å®¹ä½¿é€™å€‹æ®µè½æ›´åŠ é•·æ›´æŽ¥è¿‘çœŸå¯¦çš„ç”¨æˆ¶æƒ…æ³" * 10),
                (1, "é€™æ˜¯ä¸€å€‹æ­£å¸¸é•·åº¦çš„å¥å­"),
                (2, "é€™æ˜¯å¦ä¸€å€‹æ­£å¸¸å¥å­")
            ]),
            ("æ··åˆå…§å®¹", [
                (0, "é€™æ˜¯ä¸€å€‹æ­£å¸¸çš„çŸ­å¥å­"),
                (1, "çŸ­ä¸€å¥"),
                (2, "é€™æ˜¯ä¸€å€‹å¾ˆé•·çš„æ®µè½åŒ…å«è¶…é•·å…§å®¹æ¸¬è©¦èžåˆé‚è¼¯" * 20),
                (3, "çµå°¾çš„æ­£å¸¸å¥å­"),
                (4, "æœ€å¾Œä¸€å¥")
            ])
        ]

        for scenario_name, candidates in test_scenarios:
            print(f"ðŸŽ¯ æ¸¬è©¦å ´æ™¯ï¼š{scenario_name}")
            print(f"   å€™é¸é …ç›®æ•¸ï¼š{len(candidates)}é …")
            print(
                f"   å¹³å‡é•·åº¦ï¼š{sum(len(c[1]) for c in candidates)/len(candidates):.1f}å­—")

            selected_items, strategy_name = strategy.select_batch(candidates)
            result = {
                "strategy": strategy_name,
                "selected_count": len(selected_items),
                "candidates_count": len(candidates),
                "avg_length": sum(len(c[1]) for c in candidates) / len(candidates) if candidates else 0,
                "config_batch_size": 5  # Fixed for this test
            }

            print(f"   é¸æ“‡ç­–ç•¥ï¼š{result['strategy']}")
            print(f"   æœ€çµ‚é¸æ“‡ï¼š{result['selected_count']}é …")
            print(f"   è©³ç´°é‚è¼¯ï¼š{self._explain_logic(result)}")
            print()

    def _explain_logic(self, result: Dict[str, Any]) -> str:
        """Explain selection logic"""
        if result["strategy"] == "LONG_PARAGRAPH_MODE":
            return f"æª¢æ¸¬åˆ°è¶…é•·å…§å®¹ï¼Œé¸æ“‡è©²é•·æ®µè½+{result['selected_count']-1}å€‹æ­£å¸¸é …ç›®"
        elif result["strategy"] == "SHORT_CONTENT_MODE":
            if result["candidates_count"] >= 8 and result["avg_length"] < 20:
                return f"ç¢Žç‰‡åŒ–çŸ­å…§å®¹ï¼Œæ“´å±•è‡³{result['selected_count']}é …ï¼ˆåŸºç¤Ž{result['config_batch_size']}é …ï¼‰"
            else:
                return f"æ­£å¸¸çŸ­å…§å®¹ï¼Œå–åŸºç¤Žå€¼{result['config_batch_size']}é …èˆ‡å€™é¸æ•¸çš„è¼ƒå°å€¼"
        else:
            return f"æ®µè½æ¨¡å¼ï¼Œé¸æ“‡{result['selected_count']}é …"

    def test_edge_cases(self):
        """Test edge cases"""
        print("âš ï¸ Test edge cases\n")

        config = MockConfigManager(base_batch_size=5)
        strategy = FusionBatchingStrategy(config)  # type: ignore

        edge_cases = [
            ("ç©ºå€™é¸", []),
            ("å–®ä¸€é …ç›®", [(0, "åªæœ‰ä¸€å€‹å¥å­")]),
            ("æ¥µç«¯é•·åº¦ä¸å‡", [(i, text) for i, text in enumerate(
                ["çŸ­"] * 20 + ["è¶…é•·æ®µè½" * 50])]),
            ("å‡ç­‰é•·åº¦", [(i, "ä¸­ç­‰å¥å­" * 5) for i in range(10)]),
            ("æ¥µç«¯ç¢Žç‰‡åŒ–", [(i, "å­—") for i in range(50)]),  # çœŸçš„ç¢Žç‰‡åŒ–
        ]

        for case_name, candidates in edge_cases:
            print(f"ðŸ”¹ é‚Šç·£æ¡ˆä¾‹ï¼š{case_name}")
            print(f"   å€™é¸æ•¸ï¼š{len(candidates)}é …")

            selected_items, strategy_name = strategy.select_batch(candidates)
            result = {
                "strategy": strategy_name,
                "selected_count": len(selected_items),
                "candidates_count": len(candidates)
            }

            print(f"   çµæžœï¼š{result['strategy']} â†’ {result['selected_count']}é …")
            print()

    def test_verification_scenario(self):
        """Test your specific usage scenario"""
        print("ðŸŽ¯ Verify actual scenario in your testing\n")

        config = MockConfigManager(base_batch_size=5)
        strategy = FusionBatchingStrategy(config)  # type: ignore

        # Simulate your actual test content (based on log items)
        real_scenario_candidates = [
            (i, text) for i, text in enumerate([
                "æŒ¯å®‡æ€è€ƒäº†ä¸€ä¸‹ï¼Œéš¨å³æƒ³åˆ°æ‹¯æ•‘è»Šæµ·å°çš„æ–¹æ³•...",
                "ä»–æª¢æŸ¥è»Šæµ·å°çš„è‡‰è‰²ï¼ŒæŒºèµ·å½Žä½Žçš„èº«å­ï¼ŒæŽ¥è‘—...",
                "ã€Œå¯ä»¥é—œä¸€ä¸‹æ”å½±æ©Ÿå—Žï¼Ÿã€...",
                "æŒ¯å®‡æ²’æœ‰å›žç­”æ”å½±å¸«çš„æå•ã€‚...",
                "ã€Œâ€¦â€¦ã€...",
                "æ”å½±å¸«ç„¡æ³•è¼•æ˜“åšå‡ºæ±ºå®šã€‚æˆæŒ¯å®‡çµäººæ˜¯ä»–çš„...",
                "çœ‹åˆ°æ”å½±å¸«ç…©æƒ±ä¸”çŒ¶è±«ä¸æ±ºï¼ŒæŒ¯å®‡ä¾¿ç«‹åˆ»èªªï¼Œ...",
                "æŒ¯å®‡å†·æ¼ çš„èªžæ°£è®“æ”å½±å¸«æŠ–äº†ä¸€ä¸‹ã€‚å¦‚æžœæˆæŒ¯...",
                "ã€ŒçŸ¥ã€çŸ¥é“äº†ã€‚ã€æ”å½±å¸«æ‹¿ä¸‹æˆ´åœ¨é ­ä¸Šçš„æ”å½±...",
            ])
        ]

        print(f"å¯¦éš›æ¸¬è©¦å…§å®¹ï¼š{len(real_scenario_candidates)}å€‹å¥å­ç‰‡æ®µ")
        print(
            f"å…§å®¹çµ±è¨ˆï¼šå¹³å‡é•·åº¦{sum(len(c[1]) for c in real_scenario_candidates)/len(real_scenario_candidates):.1f}å­—")
        print()

        selected_items, strategy_name = strategy.select_batch(
            real_scenario_candidates)
        result = {
            "strategy": strategy_name,
            "selected_count": len(selected_items),
            "candidates_count": len(real_scenario_candidates),
            "avg_length": sum(len(c[1]) for c in real_scenario_candidates) / len(real_scenario_candidates) if real_scenario_candidates else 0,
            "config_batch_size": 5  # Fixed for this test
        }

        print(f"é æ¸¬çµæžœï¼š{result['strategy']} â†’ é¸æ“‡{result['selected_count']}é …")
        print(f"é‹ä½œé‚è¼¯ï¼š{self._explain_logic(result)}")
        print()

        return result

    def run_comprehensive_test(self):
        """é‹è¡Œå®Œæ•´çš„æ¸¬è©¦å¥—ä»¶"""
        print("ðŸ”¬ SpeakUB Fusion é‚è¼¯ç¶œåˆæ¸¬è©¦")
        print("=" * 60)
        print("è¨­è¨ˆç†å¿µï¼šåŸºæœ¬é™åˆ¶5å€‹é …ç›®ï¼Œconfigå¯å‹•æ…‹èª¿æ•´ï¼Œå…§å®¹è©•ä¼°å‹•æ…‹é©æ‡‰")
        print("=" * 60)
        print()

        # ä¸»è¦æ¸¬è©¦å ´æ™¯
        self.test_different_config_batch_sizes()
        self.test_content_type_scenarios()
        self.test_edge_cases()
        self.test_verification_scenario()

        print("ðŸ“Š æ¸¬è©¦ç¸½çµ")
        print("=" * 30)
        print("âœ… åŸºæœ¬é™åˆ¶ï¼šç¨‹å¼ç¢¼ä¸­å›ºå®š5å€‹é …ç›®çš„è¨­è¨ˆå“²å­¸")
        print("âœ… å‹•æ…‹èª¿æ•´ï¼šconfig.jsonå¯æå‡åŸºæœ¬å€¼ï¼ˆbatch_sizeï¼‰")
        print("âœ… å…§å®¹é©æ‡‰ï¼šåŸºæ–¼æ–°åŸºæœ¬å€¼é€²è¡Œå…§å®¹ç‰¹å¾µè©•ä¼°")
        print("âœ… é‚Šç·£ä¿è­·ï¼šç©ºå€™é¸å’Œæ¥µç«¯æƒ…æ³çš„æ­£ç¢ºè™•ç†")
        print("âœ… è¶…é•·ä¿è­·ï¼šé•·æ®µè½å€‹åˆ¥è™•ç†é¿å…ç³»çµ±é˜»å¡ž")
        print()
        print("ðŸŽ‰ Fusioné‚è¼¯å®Œå…¨ç¬¦åˆä½ çš„è¨­è¨ˆç†å¿µï¼")


def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    tester = FusionLogicTester()
    tester.run_comprehensive_test()


if __name__ == "__main__":
    main()
