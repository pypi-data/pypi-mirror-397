"""
Filter Images Job

This job filters images generated by the Transform Images job.
It is cumulative throughout the day - each run combines:
1. Previous filtered output (from latest successful manifest)
2. New transform_images output (that triggered this run)

Key principles:
- LAST DUPLICATE WINS: When the same image appears multiple times,
  the version from the most recent transform job is kept (ALWAYS)
- RECOVERY FRIENDLY: Job failures are expected; manifests enable easy recovery
- TIMEZONE BASED: Processing window is from start of day in configured timezone

See FILTER_IMAGES.md for detailed architecture documentation.
"""
from datetime import datetime, timezone
from typing import Any, Optional

from dwh.jobs.abstract_job import AbstractJob
from dwh.jobs.filter_images.extract.transform_output_reader import TransformOutputReader
from dwh.jobs.filter_images.extract.filtered_output_reader import FilteredOutputReader
from dwh.jobs.filter_images.transform.filter_transformer import FilterTransformer
from dwh.jobs.filter_images.load.filter_loader import FilterLoader
from dwh.jobs.filter_images.manifest.manifest_service import ManifestService
from dwh.jobs.filter_images.manifest.manifest_models import (
    FilterManifest,
    ManifestTimeRange,
    ManifestLineageEntry
)
from dwh.jobs.filter_images.metrics.job_metrics import FilterImagesJobMetrics
from dwh.services.quality.quality_checker import QualityChecker
from dwh.services.quality.quality_models import QualityResult, QualityCheckFailedError
from dwh.services.event.job_event_publisher import JobEventPublisher


class FilterImagesJob(AbstractJob):
    """
    Filter images job with manifest-based recovery and cumulative processing.

    Each run:
    1. Finds the latest successful manifest for today
    2. Reads previous filtered output (if exists) via FilteredOutputReader
    3. Reads new transform_images output via TransformOutputReader
    4. Combines and deduplicates (last wins)
    5. Writes output to run-specific directory
    6. Writes manifest (enables recovery)
    7. Updates latest.json pointer (on success)
    """

    def __init__(
        self,
        transform_output_reader: TransformOutputReader,
        filtered_output_reader: FilteredOutputReader,
        transformer: FilterTransformer,
        loader: FilterLoader,
        manifest_service: ManifestService,
        # Configuration
        base_path: str,
        timezone_name: str,
        # Time column for range tracking
        time_column: str = "eventTime",
        # Optional: Quality checking
        quality_checker: QualityChecker = None,
        # Optional: Event publishing
        event_publisher: JobEventPublisher = None,
    ):
        self.transform_output_reader = transform_output_reader
        self.filtered_output_reader = filtered_output_reader
        self.transformer = transformer
        self.loader = loader
        self.manifest_service = manifest_service

        # Configuration
        self.base_path = base_path
        self.timezone_name = timezone_name
        self.time_column = time_column

        # Optional services
        self.quality_checker = quality_checker
        self.event_publisher = event_publisher

        # Runtime state
        self._last_metrics: dict = {}
        self._last_quality: QualityResult = None
        self._current_manifest: FilterManifest = None

    def execute_job(
        self,
        transform_output_path: str,
        triggered_by_job: str,
        triggered_by_run_id: str,
        year: int,
        month: int,
        day: int,
        **kwargs
    ) -> Any:
        """
        Execute the filter images job.

        Args:
            transform_output_path: Path to new transform_images output
            triggered_by_job: Name of job that triggered this run
            triggered_by_run_id: Run ID of triggering job
            year, month, day: Date for this run's output
            **kwargs: Additional parameters (service_provider, environment, etc.)

        Returns:
            Job metrics as dict
        """
        # Initialize metrics
        metrics = FilterImagesJobMetrics()

        # Capture execution context
        metrics.timezone = self.timezone_name
        metrics.day = f"{year:04d}-{month:02d}-{day:02d}"
        metrics.triggered_by_job = triggered_by_job
        metrics.triggered_by_run_id = triggered_by_run_id
        metrics.service_provider = kwargs.get('service_provider')
        metrics.environment = kwargs.get('environment')
        metrics.region = kwargs.get('region')
        metrics.created_by = kwargs.get('created_by')

        # Generate run ID and create manifest
        run_id = self.manifest_service.generate_run_id()
        metrics.job_run_id = run_id

        current_time = datetime.now(timezone.utc)
        manifest = FilterManifest(
            run_id=run_id,
            status="IN_PROGRESS",
            created_at=current_time.isoformat(),
            timezone=self.timezone_name,
            day=metrics.day,
            triggered_by_job=triggered_by_job,
            triggered_by_run_id=triggered_by_run_id,
            transform_output_path=transform_output_path
        )
        self._current_manifest = manifest

        # Get output paths for this run
        output_path = self.manifest_service.get_output_path(
            self.base_path, self._get_tz_name(), year, month, day, run_id
        )
        latest_output_path = self.manifest_service.get_latest_output_path(
            self.base_path, self._get_tz_name(), year, month, day
        )
        manifest.output_path = output_path

        try:
            # === EXTRACT PHASE ===
            print("Extract: Starting...")
            extract_start = datetime.now(timezone.utc)
            metrics.extract_start_time = extract_start

            # Find latest successful manifest
            previous_manifest = self.manifest_service.get_latest_manifest(
                self.base_path, self._get_tz_name(), year, month, day
            )

            # Read previous filtered output (if exists)
            previous_df = None
            if previous_manifest:
                print(f"Extract: Found previous manifest: {previous_manifest.run_id}")
                manifest.previous_manifest = f"manifest-{previous_manifest.run_id}.json"
                manifest.previous_output_path = previous_manifest.output_path
                manifest.previous_record_count = previous_manifest.record_count
                # Inherit lineage from previous manifest
                manifest.lineage = list(previous_manifest.lineage)

                # Read previous filtered output
                if previous_manifest.output_path:
                    previous_df = self.filtered_output_reader.read(previous_manifest.output_path)
                    if previous_df is not None:
                        previous_count = previous_df.count()
                        metrics.previous_output_path = previous_manifest.output_path
                        metrics.previous_manifest_path = previous_manifest.run_id
                        metrics.previous_record_count = previous_count
                        print(f"Extract: Read {previous_count:,} records from previous filtered output")
                    else:
                        print("Extract: Previous output path exists but no data found")
            else:
                print("Extract: No previous manifest (first run of day)")

            # Read new transform_images output
            new_df = self.transform_output_reader.read(transform_output_path)
            new_count = new_df.count()
            metrics.transform_record_count = new_count
            metrics.transform_output_path = transform_output_path
            print(f"Extract: Read {new_count:,} records from transform output")

            # Calculate total records read
            metrics.records_read = metrics.previous_record_count + metrics.transform_record_count

            extract_end = datetime.now(timezone.utc)
            metrics.extract_end_time = extract_end
            metrics.extract_duration_seconds = (extract_end - extract_start).total_seconds()
            print("Extract: Complete")

            # === TRANSFORM PHASE ===
            print("Transform: Starting...")
            transform_start = datetime.now(timezone.utc)
            metrics.transform_start_time = transform_start

            # Get time range from new data for this run
            new_min_time, new_max_time = self.transformer.get_time_range(
                new_df, self.time_column
            )
            metrics.this_run_range_start = new_min_time
            metrics.this_run_range_end = new_max_time

            # Transform (combine + deduplicate)
            # Use current timestamp as source timestamp for new data
            new_source_timestamp = current_time.isoformat()
            deduplicated_df = self.transformer.transform(
                previous_df, new_df, new_source_timestamp, metrics
            )

            transform_end = datetime.now(timezone.utc)
            metrics.transform_end_time = transform_end
            metrics.transform_duration_seconds = (transform_end - transform_start).total_seconds()
            print("Transform: Complete")

            # Calculate cumulative time range
            cumulative_min, cumulative_max = self.transformer.get_time_range(
                deduplicated_df, self.time_column
            )
            metrics.cumulative_range_start = cumulative_min
            metrics.cumulative_range_end = cumulative_max

            # === LOAD PHASE ===
            print("Load: Starting...")
            load_start = datetime.now(timezone.utc)
            metrics.load_start_time = load_start

            # Write to run-specific path
            output_files = self.loader.load(deduplicated_df, output_path, metrics)

            # Write to "latest" path for external clients
            print("Load: Writing to 'latest' path...")
            self.loader.write_latest(deduplicated_df, latest_output_path, metrics)
            manifest.latest_output_path = latest_output_path

            load_end = datetime.now(timezone.utc)
            metrics.load_end_time = load_end
            metrics.load_duration_seconds = (load_end - load_start).total_seconds()
            print("Load: Complete")

            # === UPDATE MANIFEST ===
            end_time = datetime.now(timezone.utc)
            duration = (end_time - current_time).total_seconds()

            # Add this run to lineage
            manifest.lineage.append(ManifestLineageEntry(
                run_id=run_id,
                transform_run_id=triggered_by_run_id,
                time_range=ManifestTimeRange(
                    start=new_min_time or "",
                    end=new_max_time or ""
                ),
                records_contributed=metrics.transform_record_count
            ))

            # Update manifest with results
            manifest.cumulative_range = ManifestTimeRange(
                start=cumulative_min or "",
                end=cumulative_max or ""
            ) if cumulative_min else None

            manifest.this_run_added = ManifestTimeRange(
                start=new_min_time or "",
                end=new_max_time or ""
            ) if new_min_time else None

            manifest.output_files = output_files
            manifest.record_count = metrics.records_written
            manifest.bytes_written = metrics.bytes_written
            manifest.duplicates_removed = metrics.duplicates_removed
            manifest.transform_record_count = metrics.transform_record_count
            manifest.mark_success(end_time.isoformat(), duration)

            # Save manifest and update latest.json
            self.manifest_service.save_manifest(
                manifest, self.base_path, self._get_tz_name(),
                year, month, day, update_latest=True
            )

            # Finalize metrics
            metrics.job_status = "SUCCESS"
            metrics.complete()
            print(metrics.get_summary())

            # Store for event publishing
            self._last_metrics = metrics.get_json()

            # Run quality checks if configured
            if self.quality_checker:
                quality_metrics = {
                    "records_read": metrics.records_read,
                    "records_written": metrics.records_written,
                    "records_dropped": metrics.duplicates_removed,  # Duplicates as "dropped"
                    "bytes_written": metrics.bytes_written,
                }
                self._last_quality = self.quality_checker.check(quality_metrics)
                print(f"Quality check result: {self._last_quality.status}")

                if self._last_quality.status == "RED":
                    raise QualityCheckFailedError(self._last_quality)
            else:
                self._last_quality = QualityResult.green()

            return self._last_metrics

        except Exception as e:
            # Mark manifest as failed
            end_time = datetime.now(timezone.utc)
            duration = (end_time - current_time).total_seconds()
            manifest.mark_failed(end_time.isoformat(), duration)

            # Save failed manifest (don't update latest.json)
            self.manifest_service.save_manifest(
                manifest, self.base_path, self._get_tz_name(),
                year, month, day, update_latest=False
            )

            # Clean up partial output
            self.loader.delete_output(output_path)

            raise

    def on_success(self, results) -> None:
        """Handle successful job execution - publish event."""
        self._publish_job_event(error=None)

    def on_failure(self, error_message: str) -> None:
        """Handle job failure - publish event."""
        error = RuntimeError(error_message)
        self._publish_job_event(error=error)

    def _publish_job_event(self, error: Exception = None) -> None:
        """Publish job event to SNS."""
        if not self.event_publisher:
            return

        job_run_id = self._last_metrics.get("job_run_id", "unknown")
        quality = self._last_quality or QualityResult.green()

        self.event_publisher.publish(
            job_name="filter_images",
            job_run_id=job_run_id,
            metrics=self._last_metrics,
            quality=quality,
            error=error
        )

    def _get_tz_name(self) -> str:
        """Get sanitized timezone name for path construction."""
        # Convert "America/Los_Angeles" to "pacific" or similar
        tz_lower = self.timezone_name.lower()
        if "los_angeles" in tz_lower or "pacific" in tz_lower:
            return "pacific"
        elif "new_york" in tz_lower or "eastern" in tz_lower:
            return "eastern"
        elif "chicago" in tz_lower or "central" in tz_lower:
            return "central"
        elif "denver" in tz_lower or "mountain" in tz_lower:
            return "mountain"
        else:
            # Use last part of timezone, sanitized
            return self.timezone_name.split("/")[-1].lower().replace(" ", "_")
