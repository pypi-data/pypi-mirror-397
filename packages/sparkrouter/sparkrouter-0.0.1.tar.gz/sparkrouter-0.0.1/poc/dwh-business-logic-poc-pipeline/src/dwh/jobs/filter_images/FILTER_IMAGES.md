# Filter Images

This job filters images generated by the Transform Images job.

## Overview

This is not a traditional filter - throughout the day, it picks up files from the previous filtered job and combines them with the latest transform_images output to generate a cumulative set of filtered images.

**Key principles:**
1. **Timezone-based**: Processing window starts from beginning of day in configured timezone
2. **Triggered automatically**: Each successful transform_images job triggers a filter job
3. **Cumulative**: Each run builds on the previous run's output
4. **Last duplicate wins**: When the same image appears multiple times, the most recent version is kept (ALWAYS)
5. **Recovery-friendly**: Job failures are expected; recovery must be simple

## Example Flow (Pacific Time)

```
Run 1: 00:00-00:15 transform_images completes
       → filter_images reads transform output
       → writes filtered output covering 00:00-00:15
       → writes manifest

Run 2: 00:15-00:30 transform_images completes
       → filter_images reads:
         - previous filtered output (00:00-00:15)
         - new transform output (00:15-00:30)
       → deduplicates (last wins)
       → writes filtered output covering 00:00-00:30
       → writes manifest

Run 3: 00:30-00:45 transform_images completes
       → filter_images reads:
         - previous filtered output (00:00-00:30)
         - new transform output (00:30-00:45)
       → deduplicates (last wins)
       → writes filtered output covering 00:00-00:45
       → writes manifest
```

## Deduplication Strategy: Last Duplicate Wins

When combining previous filtered images with new transform output:
- Images are identified by a unique key (e.g., image_id, or composite key)
- If the same key appears in both datasets, the record from the MORE RECENT transform job is kept
- This ensures corrections/updates from later runs take precedence

```python
# Pseudocode for deduplication
combined = previous_filtered.union(new_transform)
deduplicated = combined.orderBy(desc("source_job_timestamp")).dropDuplicates(["image_key"])
```

## File Layout & Manifest Strategy

### Directory Structure

```
s3://bucket/filtered_images/{timezone}/year={YYYY}/month={MM}/day={DD}/
├── _manifests/
│   ├── latest.json                                    # Symlink/copy to most recent successful manifest
│   ├── manifest-{run_id}.json                         # Individual run manifests
│   └── manifest-{run_id}.json
├── {run_id}/
│   ├── part-00000.parquet
│   └── part-00001.parquet
└── {run_id}/
    └── ...
```

### Manifest Schema

```json
{
  "schema_version": "1.0",
  "run_id": "filter-20251124T003000Z-abc123",
  "status": "SUCCESS",

  "timing": {
    "created_at": "2025-11-24T00:35:00Z",
    "processing_duration_seconds": 45.2
  },

  "context": {
    "timezone": "America/Los_Angeles",
    "day": "2025-11-24",
    "day_start_utc": "2025-11-24T08:00:00Z"
  },

  "triggered_by": {
    "job_name": "transform_images",
    "job_run_id": "spark-application-xyz",
    "output_path": "s3://bucket/transformed/..."
  },

  "coverage": {
    "cumulative_range": {
      "start": "2025-11-24T08:00:00Z",
      "end": "2025-11-24T08:30:00Z"
    },
    "this_run_added": {
      "start": "2025-11-24T08:15:00Z",
      "end": "2025-11-24T08:30:00Z"
    },
    "gaps": []
  },

  "input": {
    "previous_manifest": "s3://bucket/.../manifest-filter-20251124T001500Z-def456.json",
    "previous_output_path": "s3://bucket/.../filter-20251124T001500Z-def456/",
    "previous_record_count": 8000,
    "transform_output_path": "s3://bucket/transformed/...",
    "transform_record_count": 7500
  },

  "output": {
    "path": "s3://bucket/filtered_images/pacific/year=2025/month=11/day=24/filter-20251124T003000Z-abc123/",
    "files": [
      "part-00000.parquet",
      "part-00001.parquet"
    ],
    "record_count": 15000,
    "bytes_written": 52428800,
    "duplicates_removed": 500
  },

  "lineage": [
    {
      "run_id": "filter-20251124T001500Z-def456",
      "transform_run_id": "spark-app-111",
      "time_range": {"start": "2025-11-24T08:00:00Z", "end": "2025-11-24T08:15:00Z"},
      "records_contributed": 8000
    },
    {
      "run_id": "filter-20251124T003000Z-abc123",
      "transform_run_id": "spark-app-222",
      "time_range": {"start": "2025-11-24T08:15:00Z", "end": "2025-11-24T08:30:00Z"},
      "records_contributed": 7500
    }
  ]
}
```

### Key Manifest Fields for Recovery

| Field | Purpose |
|-------|---------|
| `status` | SUCCESS/FAILED - only SUCCESS manifests are used as input |
| `coverage.cumulative_range` | What time range is fully covered |
| `coverage.gaps` | Any time periods that were skipped (for alerting) |
| `input.previous_manifest` | Which manifest was used as input (for lineage) |
| `lineage` | Full audit trail of all contributing transform runs |

## Recovery Scenarios

### Scenario 1: Filter job fails mid-write

```
Run 1: SUCCESS → manifest-run1.json, latest.json → run1
Run 2: FAILS mid-write (partial files in run2/)

Recovery:
1. Read latest.json → points to run1
2. Delete partial run2/ directory
3. Rerun filter job - it reads from run1's output
```

### Scenario 2: Transform job for 00:30 arrives before 00:15

```
Run 1 (00:30 data): SUCCESS → covers 00:00-00:30 (with gap 00:00-00:15)
                    manifest shows: gaps: [{start: 00:00, end: 00:15}]
Run 2 (00:15 data): SUCCESS → fills gap, covers 00:00-00:30
                    manifest shows: gaps: []

The gap tracking alerts operators that data may be incomplete.
```

### Scenario 3: Need to reprocess from scratch

```
1. Delete all manifests and output directories for the day
2. Trigger filter job with first transform output
3. Subsequent transform completions will rebuild cumulatively
```

## Finding What's NOT Been Processed

The manifest's `coverage` section makes this clear:

```json
"coverage": {
  "cumulative_range": {
    "start": "2025-11-24T08:00:00Z",
    "end": "2025-11-24T10:30:00Z"
  },
  "gaps": [
    {"start": "2025-11-24T09:00:00Z", "end": "2025-11-24T09:15:00Z", "reason": "transform job not yet complete"}
  ]
}
```

**Reading the coverage:**
- `cumulative_range.end` vs current time = how far behind we are
- `gaps` array = specific time windows that are missing
- Empty `gaps` + `cumulative_range.end` close to now = healthy state

## Integration with Downstream Orchestration

The filter_images job publishes to SNS on completion, enabling:
- Dashboard updates showing filter progress
- Alerts if gaps persist too long
- Downstream jobs that need the filtered output

## Configuration

```json
{
  "timezone": "America/Los_Angeles",
  "source_bucket": "dwh-data",
  "output_bucket": "dwh-data",
  "output_prefix": "filtered_images",
  "dedup_key_columns": ["image_id"],
  "dedup_order_column": "source_event_time",
  "triggered_by_job": "transform_images",
  "triggered_by_run_id": "spark-xxx"
}
```
