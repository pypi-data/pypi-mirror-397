#
# pproc_worker.py - DeGirum Python SDK: postprocessor workers factory
# Copyright DeGirum Corp. 2024
#
# Provides create_worker function to start workers in separate processes
#

import sys
import logging

if "darwin" in sys.platform:
    import site

    sys.path.append(site.getusersitepackages())

import importlib
import json

from queue import Queue
from threading import Thread, Event, Lock
from multiprocessing import set_start_method, set_executable
from subprocess import TimeoutExpired

import argparse
import os
import tempfile, shutil
from pathlib import Path
import signal

import subprocess
import psutil

import numpy as np

import msgpack
import zmq

if "linux" in sys.platform:
    import pyseccomp as seccomp
    import resource as rc

    MEMORY_LIMIT = 1024  # Mb
else:
    MEMORY_LIMIT = -1  # not set

import hashlib, uuid
import pickle

# modules allowed to import in postprocessors
allowed_modules = ["numpy", "cv2"]


class NumpyNotAllowed(Exception):
    def __init__(self, message="Numpy array is not allowed as output of a pythonic postprocessor!"):
        super().__init__(message)


def create_worker(
    parent_pid: int = 0,
    port_id: int = 5555,
    protocol: str = "tcp",
    ipc_dir: str = "tmp",
    memory_limit_MB: int = MEMORY_LIMIT,
    time_limit_sec: float = 1.0,
    log_level=logging.INFO,
):
    """
    Create a worker for postprocess execution
    parent_pid: int: parent process id; 0 if there is none
    port_id: int: port to bind the 0mq socket
    protocol: str: data transfer protocol to use
    ipc_dir: str: temp folder for ipc protocol
    memory_limit_MB: int: process address space limit, in Mb
    time_limit_sec: float: process execution time limit, in sec
    """
    set_start_method("spawn", force=True)

    command = [
        get_python(),
        os.path.abspath(__file__),
        "--parent_pid",
        str(parent_pid),
        "--port_id",
        str(port_id),
        "--protocol",
        protocol,
        "--ipc_dir",
        ipc_dir,
        "--memory_limit_MB",
        str(memory_limit_MB),
        "--time_limit_sec",
        str(time_limit_sec),
    ]

    # Start the process
    worker = subprocess.Popen(command)
    try:
        # Wait for process to finish (should not happen)
        worker.wait(timeout=1.0)
        # Process exited
        return None, 1
    except TimeoutExpired:
        # Process is running ok
        return worker, 0
    except Exception:
        # Unknown exception
        return None, 2


def is_worker_running(worker: subprocess.Popen) -> bool:
    """
    Checks if a subprocess is running
    Returns 1 if yes, 0 otherwise
    """
    return worker.poll() is None


def stop_worker(
    worker: subprocess.Popen, port_id: int, protocol: str, ipc_dir: str
) -> int:
    """
    Stop a worker.
    Input: worker: a subprocess worker to be stoped;
    port_id: what 0mq port the worker is using;
    protocol: wwhat protocol the worker is using (tcp or ipc);
    ipc_dir: what temp folder for ipc protocol the worker is using
    Returns 0 on success; 1 if process is not running in the first place
    """
    if worker.poll() is not None:
        return 1  # subprocess is not running

    context = zmq.Context()
    try:
        socket = context.socket(zmq.REQ)
        if protocol == "tcp":
            socket.connect(f"tcp://localhost:{port_id}")
        elif protocol == "ipc":
            socket.connect(f"ipc://{ipc_dir}")

        req = {"action": "poison_pill"}
        socket.send(msgpack.packb(req))
        reply = msgpack.unpackb(socket.recv())
        assert reply[PP_Worker.ret_val_key].lower() == "ok"
        worker.wait(1.0)
    except Exception:
        worker.kill()
        worker.wait(1.0)

    context.destroy()
    return 0


def tostr(x):
    if isinstance(x, bytes):
        return x.decode()
    else:
        return str(x)


def _unpack_dtype(dtype):
    """
    Unpack dtype descr, recursively unpacking nested structured dtypes.
    """

    if isinstance(dtype, (list, tuple)):
        # Unpack structured dtypes of the form: (name, type, *shape)
        dtype = [
            (subdtype[0], _unpack_dtype(subdtype[1])) + tuple(subdtype[2:])
            for subdtype in dtype
        ]
    return np.dtype(dtype)


def decode(obj, chain=None):
    """
    Decoder for deserializing numpy data types.
    """
    try:
        if b'nd' in obj:
            if obj[b'nd'] is True:

                # Check if b'kind' is in obj to enable decoding of data
                # serialized with older versions (#20) or data
                # that had dtype == 'O' (#46):
                if b'kind' in obj and obj[b'kind'] == b'V':
                    descr = [tuple(tostr(t) if type(t) is bytes else t for t in d)
                             for d in obj[b'type']]
                elif b'kind' in obj and obj[b'kind'] == b'O':
                    return pickle.loads(obj[b'data'])
                else:
                    descr = obj[b'type']
                return np.ndarray(buffer=obj[b'data'],
                                  dtype=_unpack_dtype(descr),
                                  shape=obj[b'shape'])
            else:
                descr = obj[b'type']
                return np.frombuffer(obj[b'data'], dtype=_unpack_dtype(descr))[0]
        elif b'complex' in obj:
            return complex(tostr(obj[b'data']))
        else:
            return obj if chain is None else chain(obj)
    except KeyError:
        return obj if chain is None else chain(obj)


def encode(obj, chain=None):
    """
    Data encoder for serializing numpy data types.
    - np.ndarray  -> custom dict with raw bytes (unchanged)
    - np.bool_    -> bool
    - np.integer  -> int
    - np.floating -> float
    - np.complexfloating / complex -> custom dict (real, imag)
    Fallback to `chain` if provided.
    """

    # --- Arrays: not allowed, raise NumpyNotAllowed exception ---
    if isinstance(obj, np.ndarray):
        raise NumpyNotAllowed

    # --- Scalars: convert to built-in Python types first ---
    if isinstance(obj, np.bool_):
        return bool(obj)

    if isinstance(obj, np.integer):
        return int(obj)

    if isinstance(obj, np.floating):
        return float(obj)

    # Complex scalars: encode explicitly (msgpack has no complex)
    if isinstance(obj, (np.complexfloating, complex)):
        c = complex(obj)
        return {b'complex': True, b'data': [c.real, c.imag]}

    # Anything else: try the next handler in the chain or return as-is
    return obj if chain is None else chain(obj)


def extract_exc_info(e):
    """
    Extracts exception information: file, function, line number and exception message.
    """
    tb = e.__traceback__
    while tb.tb_next:  # Traverse to the last frame
        tb = tb.tb_next
    frame = tb.tb_frame
    filename = os.path.basename(frame.f_code.co_filename)
    return f"{e} [{type(e).__name__}] in file '{filename}', function '{frame.f_code.co_name}', line {tb.tb_lineno}"


def to_ndarray(obj):
    """
    Decoder for deserializing numpy data types.
    """
    try:
        if "nd" in obj:
            if obj["nd"] is True:
                # Check if b'kind' is in obj to enable decoding of data
                # serialized with older versions (#20) or data
                # that had dtype == 'O' (#46):
                if "kind" in obj and obj["kind"] == "V":
                    descr = [
                        tuple(tostr(t) if type(t) is bytes else t for t in d)
                        for d in obj["type"]
                    ]
                elif "kind" in obj and obj["kind"] == b"O":
                    return pickle.loads(obj["data"])
                else:
                    descr = obj["type"]

                dtype_ = _unpack_dtype(descr)
                arr: np.ndarray = np.ndarray(
                    buffer=obj["data"], dtype=dtype_, shape=obj["shape"]
                )
                return arr
            else:
                descr = obj["type"]
                dtype_ = _unpack_dtype(descr)
                return np.frombuffer(obj["data"], dtype=dtype_)[0]
        elif "complex" in obj:
            return complex(tostr(obj[b"data"]))
        else:
            raise KeyError("no known key")
    except KeyError:
        return None


def create_module_key(fpath: Path) -> str:
    """
    create a unique id for a file
    Input:
    fpath: Path: file path
    Output:
    unique id for thr given file
    """
    hash = ""
    with open(fpath, "rb") as f:
        data = f.read()
        hash = hashlib.md5(data).hexdigest()
    return hash


def import_postprocessor(mod_path_str: str):
    mod_path: Path = Path(mod_path_str)
    mod_key: str = create_module_key(mod_path)
    mod_name: str = f"{mod_path.stem}_{str(mod_key)}"
    if mod_name in sys.modules:
        return sys.modules[mod_name]

    temp_dir = tempfile.mkdtemp()
    temp_file = Path(temp_dir, mod_name).with_suffix(mod_path.suffix)

    shutil.copy(str(mod_path), str(temp_file))
    sys.path.append(temp_dir)
    module = importlib.import_module(mod_name)

    sys.path.remove(temp_dir)
    shutil.rmtree(temp_dir)

    return module


class PP_Worker:
    """
    Store postpocess classes and execute them when requested.
    In case of runaway execution, monitor thread will terminate the process
    """

    poison_pill: str = "poison_pill"
    time_limit: float = 1.0  # limit postproc execution by 1 sec
    socket_timeout_sec: float = 1.0  # socket timeout

    # Return json keys
    ret_val_key: str = "rv"
    msg_key: str = "msg"
    data_key: str = "data"
    err_code_key: str = "err_code"

    # Error codes
    err_code_other = -1
    err_code_terminated = 1
    err_code_postprocessor_missing = 2
    err_code_postprocessor_unavailable = 3
    err_code_postprocessor_semantics = 4
    err_code_bad_output_data = 5

    logger = logging.getLogger(__name__)

    @classmethod
    def set_time_limit(cls, time_limit: float = 1.0):
        cls.time_limit = time_limit

    @classmethod
    def set_log_level(cls, log_level):
        cls.logger.setLevel(log_level)

    def __init__(
        self, parent_pid: int, port_id: int, protocol: str = "tcp", ipc_dir: str = "tmp"
    ):
        """
        port_id: int: port to bind the 0mq socket
        protocol: str: data transfer protocol to use
        ipc_dir: str : ipc data folder
        """
        protocols_supported = ["tcp", "ipc"] if "linux" in sys.platform else ["tcp"]
        if protocol not in protocols_supported:
            raise ValueError(f"PP_Worker: {protocol} not supported on {sys.platform}")

        self.parent_pid = parent_pid
        self.cache: dict = {}  # loaded postprocs

        # initialize 0mq
        self.binded_addr = ""
        self.zmq_context = zmq.Context()
        self.socket = self.zmq_context.socket(zmq.REP)
        self.socket.setsockopt(zmq.RCVTIMEO, int(1000 * PP_Worker.socket_timeout_sec))
        try:
            if protocol == "tcp":
                self.socket.bind(f"{protocol}://*:{port_id}")
                self.binded_addr = f"{protocol}://*:{port_id}"
            elif protocol == "ipc":
                self.socket.bind(f"{protocol}://*:{ipc_dir}")
                self.binded_addr = f"{protocol}://*:{port_id}"
        except Exception:
            self.release()

        self.stop_event = Event()  # set when process is being terminated by monitor
        self.send_lock = Lock()  # guards responce sending

        self.monitor_thread: Thread = Thread()  # monitoring thread; starts in run
        self.monitor_q: Queue = Queue()  # pass postprocessor id to monitor thread
        self.monitor_timeout = Event()  # terminate the worker's runaway process

    def release(self):
        """
        Release resources.
        """
        self.zmq_context.destroy()

    def _add_pp(self, data: dict):
        """
        Add a postpocessor, if it has a new id. Increase ref count, if it exists already.
        data : dict: should contains path and config
        """
        pp_id = str(uuid.uuid4())
        if pp_id is not None:
            self.monitor_q.put_nowait(pp_id)

        pp_path = data["pp_path"]
        json_config = data["json_config"]

        PP_Worker.logger.info(f"Adding postproc {pp_path}")

        if not os.path.isfile(pp_path):
            return PP_Worker.err_reply(
                f"{pp_path} is not available",
                err_code=PP_Worker.err_code_postprocessor_unavailable,
            )
        pp_module = import_postprocessor(pp_path)
        PP_Worker.logger.debug(f"imported postproc {pp_path}")

        if hasattr(pp_module.PostProcessor, "configure"):
            pp = pp_module.PostProcessor()
            PP_Worker.logger.debug("calling configure")
            pp.configure(json_config)
        else:
            if not isinstance(json_config, str):
                if isinstance(json_config, dict):
                    json_config_str = json.dumps(json_config)
                else:
                    json_config_str = str(json_config)
            PP_Worker.logger.debug("calling constructor")
            pp = pp_module.PostProcessor(json_config_str)

        self.cache[pp_id] = pp
        PP_Worker.logger.info(f"add_pp: pp_id: {pp_id}, {id(pp)}")
        return PP_Worker.ok_reply({"pp_id": pp_id})

    def _forward(self, data: dict):
        """
        Pass data to postprocessor's 'forward' function.
        data : dict: should contains target postprocessor id, tensor_list and details_list
        """
        # unpack the needed data
        pp_id = data["pp_id"]
        PP_Worker.logger.info(f"Forward: pp_id: {pp_id}")
        if pp_id not in self.cache.keys():
            PP_Worker.logger.info("Forward: postproc missing")
            return PP_Worker.err_reply(
                "PP_Worker._forward: Postprocessor is missing.",
                err_code=PP_Worker.err_code_postprocessor_missing,
            )

        basic_tensor_list = data.setdefault("tensor_list", [])
        details_list = data.setdefault("details_list", [])

        # if needed, convert basic tensors (decoded as dicts) to numpy arrays
        tensor_list = []
        if basic_tensor_list is not None:
            for t in basic_tensor_list:
                if isinstance(t, dict):
                    tensor_list.append(to_ndarray(t))
                else:
                    tensor_list.append(t)

        # get the requested postprocessor and call forward
        pp = self.cache[pp_id]
        PP_Worker.logger.info(f"Forward: calling forward: {id(pp)}, {self.cache}")
        reply = pp.forward(tensor_list, details_list)

        if isinstance(reply, str):
            # that's old-style postprocessor result, convert it to json
            reply_json = json.loads(reply)
        else:
            reply_json = reply

        return PP_Worker.ok_reply(reply_json)

    def _poison_pill(self, data: dict):
        """
        Process the poison pill.
        data : dict: unused
        """
        PP_Worker.logger.info("Got poison pill!")
        return PP_Worker.ok_reply("got poison pill")

    def _release_pp(self, data: dict):
        """
        Remove the postprocessor with the given id.
        data : dict: should contain id of the postprocessor to be deleted
        """
        pp_id = data["pp_id"]
        if pp_id in self.cache:
            del self.cache[pp_id]
            return PP_Worker.ok_reply({"pp_id": pp_id})
        else:
            return PP_Worker.err_reply(f"postprocessor {pp_id} is not loaded")

    def _get_info(self, data: dict):
        """
        Get worker info: a stub; more diagnostics will be added
        data : dict: unused
        """
        return PP_Worker.ok_reply(
            {
                "pproc_num": len(self.cache),
                "time_limit": PP_Worker.time_limit,
                "binded_addr": self.binded_addr,
                "pid": os.getpid()
            }
        )

    def check_parent(self):
        """
        Check For the existence of a unix pid.
        """
        if self.parent_pid == 0 or psutil.pid_exists(self.parent_pid):
            return True
        else:
            return False

    def run(self):
        """
        Server request/reply loop.
        """
        PP_Worker.logger.debug("---------- Entered PP_Worker.run! ---------------")
        PP_Worker.logger.debug(f"PP_Worker.run : timeout: {PP_Worker.time_limit}")

        self.monitor_thread = Thread(target=self.monitoring)
        self.monitor_thread.start()

        action: str = ""
        while action != PP_Worker.poison_pill:
            # wait for request
            PP_Worker.logger.debug("PP_Worker.run: wait for request")
            try:
                request = self.socket.recv()
            except Exception:
                if not self.check_parent():
                    break

            try:
                # convert to json
                data: dict = msgpack.unpackb(request, object_hook=decode)
                # extract action
                action = data["action"]
                PP_Worker.logger.debug(f"PP_Worker.run: action: {action}")
                pp_id = data.setdefault("pp_id", None)
                # NB: by adding like 'if action == "forward"' clause, you  may impose additional conditions
                # on what type of actions' execution is monitored.
                if pp_id is not None:
                    self.monitor_q.put_nowait(pp_id)
                # process requested self._action; self.data_err will be called if self._action is missing
                reply: bytes = getattr(self, "_" + action, self.data_err)(data)

            except Exception as e:
                msg = extract_exc_info(e)
                PP_Worker.logger.info(f"PP_Worker.run: {msg}")
                reply = PP_Worker.err_reply(msg)

            finally:
                # cancel monitor timer to avoid terminating by timeout
                self.monitor_q.join()
                self.monitor_timeout.set()
            # send reply
            if not self.stop_event.is_set():
                self.send_reply(reply)

        # stop monitor
        self.monitor_q.put_nowait(None)
        self.monitor_thread.join()

        # release resouces
        self.release()
        PP_Worker.logger.info("PP_Worker.run: Worker exits")

    def monitoring(self):
        """
        Monitoring thread function
        """
        while True:
            # wait indefinetely for a postprocessor to be run
            pp_id = self.monitor_q.get()
            if pp_id is None:
                # got the poison pill
                self.monitor_q.task_done()
                break
            # start timer
            self.monitor_timeout.clear()
            self.monitor_q.task_done()
            if not self.monitor_timeout.wait(PP_Worker.time_limit):
                self.terminate("Max running time exceeded")
            else:
                pass

    def terminate(self, reason: str = ""):
        """
        Send the 'reason' as the reply, release resources, then terminate the current process.
        """
        self.stop_event.set()
        if len(reason) > 0:
            reply = self.err_reply(
                f"Postprocessor terminated: {reason}", PP_Worker.err_code_terminated
            )
            self.send_reply(reply)
        self.release()
        pid = os.getpid()
        os.kill(pid, signal.SIGTERM)

    def send_reply(self, reply: bytes) -> bool:
        """
        Send reply via 0mq socket.
        reply: bytes: data to send
        Return: bool: True on success, False otherwise
        """
        with self.send_lock:
            try:
                self.socket.send(reply)
                return True
            finally:
                return False

    @classmethod
    def data_err(cls, data: dict) -> bytes:
        return PP_Worker.err_reply("wrong request format")

    @classmethod
    def err_reply(cls, err_msg: str, err_code=err_code_other) -> bytes:
        return msgpack.packb(
            {
                PP_Worker.ret_val_key: "Err",
                PP_Worker.msg_key: err_msg,
                PP_Worker.err_code_key: err_code,
            }
        )

    @classmethod
    def ok_reply(cls, data) -> bytes:
        try:
            res = {PP_Worker.ret_val_key: "OK", PP_Worker.data_key: data}
            msg_data = msgpack.packb(res, default=encode)
            return msg_data
        except NumpyNotAllowed as e:
            return cls.err_reply(str(e))
        except Exception as e:
            return cls.err_reply(f"PP_Worker.ok_reply throws exception {e}")


def get_python():
    """
    Returns the path of the Python interpreter to use when starting a child process.
    """
    if "win32" in sys.platform:
        py_exec_path = os.path.join(sys.exec_prefix, "python.exe")
    elif "linux" in sys.platform:
        py_exec_path = sys.exec_prefix + "/bin/python3"
    elif "darwin" in sys.platform:
        py_exec_path = sys.exec_prefix + "/bin/python3"
    else:
        py_exec_path = ""

    return py_exec_path


def configure_embed():
    """
    1. Sets the path of the Python interpreter to use when starting a child process.
    2. Set the method which should be used to start child processes as 'spawn'.
    """
    py_exec_path = get_python()

    if os.path.isfile(py_exec_path):
        set_executable(py_exec_path)
    else:
        PP_Worker.logger.error(f"cannot set_executable: {py_exec_path} is not a file")

    # set_start_method("spawn", force=True)
    try:
        set_start_method("spawn", force=False)
    except Exception as e:
        PP_Worker.logger.debug(f"set_start_method exception: {e}")


def set_mem_limit(memory_limit_bytes: int = -1):
    """
    Set memory usage limit
    memory_limit: int: process address space limit, in bytes
    """
    # set max size of Process Address Space
    if "linux" in sys.platform and memory_limit_bytes > 0:
        # load all allowed modules
        for module in allowed_modules:
            importlib.import_module(module)

        # find used heap size
        current_process = psutil.Process(os.getpid())
        memory_info = current_process.memory_info()
        # add user-defined cap
        allowed_memory_bytes = memory_info.data + memory_limit_bytes
        # set DATA limit
        rc.setrlimit(rc.RLIMIT_DATA, (allowed_memory_bytes, allowed_memory_bytes))  # type: ignore


def secure():
    if "linux" not in sys.platform:
        return
    syscall2deny = [
        "execve",
        "execveat",
        "fork",
        "vfork",
        "connect",
    ]
    # Create a seccomp filter with default action ALLOW
    filter = seccomp.SyscallFilter(defaction=seccomp.ALLOW)
    action_deny = seccomp.ERRNO(seccomp.errno.EPERM)
    # Deny specific syscalls
    for syscall in syscall2deny:
        filter.add_rule(action_deny, syscall)
    # Load the filter
    filter.load()


def run_worker(
    parent_pid: int = 0,
    port_id: int = 5555,
    protocol: str = "tcp",
    ipc_dir: str = "tmp",
    memory_limit_MB: int = MEMORY_LIMIT,
    time_limit_sec: float = 1.0,
    log_level=logging.INFO,
):
    """
    port_id: int: port to bind the 0mq socket
    protocol: str: data transfer protocol to use
    ipc_dir: str: temp folder for ipc protocol
    memory_limit_MB: int: process address space limit, in Mb
    time_limit_sec: float: process execution time limit, in sec
    """
    if memory_limit_MB > 0:
        memory_limit_bytes = memory_limit_MB * 1024 * 1024
        set_mem_limit(memory_limit_bytes)

    secure()

    PP_Worker.set_time_limit(time_limit_sec)
    PP_Worker.set_log_level(log_level)

    try:
        worker = PP_Worker(
            parent_pid=parent_pid, port_id=port_id, protocol=protocol, ipc_dir=ipc_dir
        )
        if worker.binded_addr != "":
            # binding success, report and start running
            PP_Worker.logger.info(
                f"PP_Worker.run: binding to {worker.binded_addr} OK, starting worker"
            )
            worker.run()
        else:
            # binding failed, report and exit process
            PP_Worker.logger.info(
                f"PP_Worker.run: binding to {port_id} failed, exiting worker"
            )
    except Exception as e:
        PP_Worker.logger.error(f"run_worker: PP_Worker constructor throws: {e}")


def start_worker():
    """
    Parse arguments and run worker
    """
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-d", "--parent_pid", help="parent process pid; 0 if there is none", default="0"
    )
    parser.add_argument(
        "-p", "--port_id", help="port to be used for ZeroMQ socket", default="5000"
    )
    parser.add_argument(
        "-r", "--protocol", help="protocol used by ZeroMQ socket", default="tcp"
    )
    parser.add_argument(
        "-i",
        "--ipc_dir",
        help="directory to be used by ZeroMQ socket for ipc communication",
        default="tmp",
    )
    parser.add_argument(
        "-m",
        "--memory_limit_MB",
        help="process address space limit, in Mb",
        default=str(MEMORY_LIMIT),
    )
    parser.add_argument(
        "-t",
        "--time_limit_sec",
        help=" process execution time limit, in sec",
        default=str(1.0),
    )

    try:
        args = parser.parse_args()
        parent_pid: int = int(args.parent_pid)
        port_id: int = int(args.port_id)
        protocol: str = args.protocol
        ipc_dir: str = args.ipc_dir
        memory_limit_MB: int = int(args.memory_limit_MB)
        time_limit_sec: float = float(args.time_limit_sec)
    except Exception as e:
        raise Exception("start_worker: " + str(e))

    log_level = logging.INFO

    run_worker(
        parent_pid,
        port_id,
        protocol,
        ipc_dir,
        memory_limit_MB,
        time_limit_sec,
        log_level,
    )


if __name__ == "__main__":
    start_worker()
