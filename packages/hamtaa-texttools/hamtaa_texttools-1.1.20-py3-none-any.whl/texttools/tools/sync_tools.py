from datetime import datetime
from typing import Literal
from collections.abc import Callable

from openai import OpenAI

from texttools.internals.sync_operator import Operator
import texttools.internals.models as Models
from texttools.internals.exceptions import (
    TextToolsError,
    PromptError,
    LLMError,
    ValidationError,
)
from texttools.internals.text_to_chunks import text_to_chunks


class TheTool:
    """
    Each method configures the operator with a specific YAML prompt,
    output schema, and flags, then delegates execution to `operator.run()`.
    """

    def __init__(
        self,
        client: OpenAI,
        model: str,
    ):
        self._operator = Operator(client=client, model=model)

    def categorize(
        self,
        text: str,
        categories: list[str] | Models.CategoryTree,
        with_analysis: bool = False,
        user_prompt: str | None = None,
        temperature: float | None = 0.0,
        logprobs: bool = False,
        top_logprobs: int = 3,
        mode: Literal["category_list", "category_tree"] = "category_list",
        validator: Callable[[object], bool] | None = None,
        max_validation_retries: int | None = None,
        priority: int | None = 0,
    ) -> Models.ToolOutput:
        """
        Categorize a text into a category / category tree.

        Important Note: category_tree mode is EXPERIMENTAL, you can use it but it isn't reliable.

        Arguments:
            text: The input text to categorize
            categories: The category / category_tree to give to LLM
            with_analysis: Whether to include detailed reasoning analysis
            user_prompt: Additional instructions for the categorization
            temperature: Controls randomness (0.0 = deterministic, 1.0 = creative)
            logprobs: Whether to return token probability information
            top_logprobs: Number of top token alternatives to return if logprobs enabled
            validator: Custom validation function to validate the output
            max_validation_retries: Maximum number of retry attempts if validation fails
            priority: Task execution priority (if enabled by vLLM and model)

        Returns:
            ToolOutput: Object containing:
                - result (str): The assigned category
                - logprobs (list | None): Probability data if logprobs enabled
                - analysis (str | None): Detailed reasoning if with_analysis enabled
                - process (str | None): Description of the process used
                - processed_at (datetime): Timestamp when the processing occurred
                - execution_time (float): Time taken for execution in seconds (-1.0 if not measured)
                - errors (list(str) | None): Errors occured during tool call

        """
        output = Models.ToolOutput()

        try:
            start = datetime.now()

            if mode == "category_tree":
                levels = categories.get_level_count()
                parent_id = 0
                final_categories = []
                analysis = ""
                logprobs = []

                for _ in range(levels):
                    # Get child nodes for current parent
                    parent_node = categories.get_node(parent_id)
                    children = categories.get_children(parent_node)

                    # Check if child nodes exist
                    if not children:
                        output.errors.append(
                            f"No categories found for parent_id {parent_id} in the tree"
                        )
                        end = datetime.now()
                        output.execution_time = (end - start).total_seconds()
                        return output

                    # Extract category names and descriptions
                    category_list = [
                        f"Category Name: {node.name}, Description: {node.description}"
                        for node in children
                    ]
                    category_names = [node.name for node in children]

                    # Run categorization for current level
                    level_output = self._operator.run(
                        # User parameters
                        text=text,
                        category_list=category_list,
                        with_analysis=with_analysis,
                        user_prompt=user_prompt,
                        temperature=temperature,
                        logprobs=logprobs,
                        top_logprobs=top_logprobs,
                        mode=mode,
                        validator=validator,
                        max_validation_retries=max_validation_retries,
                        priority=priority,
                        # Internal parameters
                        prompt_file="categorize.yaml",
                        output_model=Models.create_dynamic_model(category_names),
                        output_lang=None,
                    )

                    # Check for errors from operator
                    if level_output.errors:
                        output.errors.extend(level_output.errors)
                        end = datetime.now()
                        output.execution_time = (end - start).total_seconds()
                        return output

                    # Get the chosen category
                    chosen_category = level_output.result

                    # Find the corresponding node
                    parent_node = categories.get_node(chosen_category)
                    if parent_node is None:
                        output.errors.append(
                            f"Category '{chosen_category}' not found in tree after selection"
                        )
                        end = datetime.now()
                        output.execution_time = (end - start).total_seconds()
                        return output

                    parent_id = parent_node.node_id
                    final_categories.append(parent_node.name)

                    if with_analysis:
                        analysis += level_output.analysis
                    if logprobs:
                        logprobs += level_output.logprobs

                end = datetime.now()
                output = Models.ToolOutput(
                    result=final_categories,
                    logprobs=logprobs,
                    analysis=analysis,
                    process="categorize",
                    execution_time=(end - start).total_seconds(),
                )

                return output

            else:
                output = self._operator.run(
                    # User parameters
                    text=text,
                    category_list=categories,
                    with_analysis=with_analysis,
                    user_prompt=user_prompt,
                    temperature=temperature,
                    logprobs=logprobs,
                    top_logprobs=top_logprobs,
                    mode=mode,
                    validator=validator,
                    max_validation_retries=max_validation_retries,
                    priority=priority,
                    # Internal parameters
                    prompt_file="categorize.yaml",
                    output_model=Models.create_dynamic_model(categories),
                    output_lang=None,
                )
                end = datetime.now()
                output.execution_time = (end - start).total_seconds()
                return output

        except PromptError as e:
            output.errors.append(f"Prompt error: {e}")
        except LLMError as e:
            output.errors.append(f"LLM error: {e}")
        except ValidationError as e:
            output.errors.append(f"Validation error: {e}")
        except TextToolsError as e:
            output.errors.append(f"TextTools error: {e}")
        except Exception as e:
            output.errors.append(f"Unexpected error: {e}")

        return output

    def extract_keywords(
        self,
        text: str,
        with_analysis: bool = False,
        output_lang: str | None = None,
        user_prompt: str | None = None,
        temperature: float | None = 0.0,
        logprobs: bool = False,
        top_logprobs: int = 3,
        mode: Literal["auto", "threshold", "count"] = "auto",
        number_of_keywords: int | None = None,
        validator: Callable[[object], bool] | None = None,
        max_validation_retries: int | None = None,
        priority: int | None = 0,
    ) -> Models.ToolOutput:
        """
        Extract salient keywords from text.

        Arguments:
            text: The input text to extract keywords from
            with_analysis: Whether to include detailed reasoning analysis
            output_lang: Language for the output response
            user_prompt: Additional instructions for keyword extraction
            temperature: Controls randomness (0.0 = deterministic, 1.0 = creative)
            logprobs: Whether to return token probability information
            top_logprobs: Number of top token alternatives to return if logprobs enabled
            validator: Custom validation function to validate the output
            max_validation_retries: Maximum number of retry attempts if validation fails
            priority: Task execution priority (if enabled by vLLM and model)

        Returns:
            ToolOutput: Object containing:
                - result (list[str]): List of extracted keywords
                - logprobs (list | None): Probability data if logprobs enabled
                - analysis (str | None): Detailed reasoning if with_analysis enabled
                - process (str | None): Description of the process used
                - processed_at (datetime): Timestamp when the processing occurred
                - execution_time (float): Time taken for execution in seconds (-1.0 if not measured)
                - errors (list(str) | None): Errors occured during tool call
        """
        output = Models.ToolOutput()

        try:
            start = datetime.now()
            output = self._operator.run(
                # User parameters
                text=text,
                with_analysis=with_analysis,
                output_lang=output_lang,
                user_prompt=user_prompt,
                temperature=temperature,
                logprobs=logprobs,
                top_logprobs=top_logprobs,
                mode=mode,
                number_of_keywords=number_of_keywords,
                validator=validator,
                max_validation_retries=max_validation_retries,
                priority=priority,
                # Internal parameters
                prompt_file="extract_keywords.yaml",
                output_model=Models.ListStr,
            )
            end = datetime.now()
            output.execution_time = (end - start).total_seconds()
            return output

        except PromptError as e:
            output.errors.append(f"Prompt error: {e}")
        except LLMError as e:
            output.errors.append(f"LLM error: {e}")
        except ValidationError as e:
            output.errors.append(f"Validation error: {e}")
        except TextToolsError as e:
            output.errors.append(f"TextTools error: {e}")
        except Exception as e:
            output.errors.append(f"Unexpected error: {e}")

        return output

    def extract_entities(
        self,
        text: str,
        entities: list[str] | None = None,
        with_analysis: bool = False,
        output_lang: str | None = None,
        user_prompt: str | None = None,
        temperature: float | None = 0.0,
        logprobs: bool = False,
        top_logprobs: int = 3,
        validator: Callable[[object], bool] | None = None,
        max_validation_retries: int | None = None,
        priority: int | None = 0,
    ) -> Models.ToolOutput:
        """
        Perform Named Entity Recognition (NER) over the input text.

        Arguments:
            text: The input text to extract entities from
            entities: List of entities provided by user (Optional)
            with_analysis: Whether to include detailed reasoning analysis
            output_lang: Language for the output response
            user_prompt: Additional instructions for entity extraction
            temperature: Controls randomness (0.0 = deterministic, 1.0 = creative)
            logprobs: Whether to return token probability information
            top_logprobs: Number of top token alternatives to return if logprobs enabled
            validator: Custom validation function to validate the output
            max_validation_retries: Maximum number of retry attempts if validation fails
            priority: Task execution priority (if enabled by vLLM and model)

        Returns:
            ToolOutput: Object containing:
                - result (list[dict]): List of entities with 'text' and 'type' keys
                - logprobs (list | None): Probability data if logprobs enabled
                - analysis (str | None): Detailed reasoning if with_analysis enabled
                - process (str | None): Description of the process used
                - processed_at (datetime): Timestamp when the processing occurred
                - execution_time (float): Time taken for execution in seconds (-1.0 if not measured)
                - errors (list(str) | None): Errors occured during tool call
        """
        output = Models.ToolOutput()

        try:
            start = datetime.now()
            output = self._operator.run(
                # User parameters
                text=text,
                entities=entities
                or "all named entities (e.g., PER, ORG, LOC, DAT, etc.)",
                with_analysis=with_analysis,
                output_lang=output_lang,
                user_prompt=user_prompt,
                temperature=temperature,
                logprobs=logprobs,
                top_logprobs=top_logprobs,
                validator=validator,
                max_validation_retries=max_validation_retries,
                priority=priority,
                # Internal parameters
                prompt_file="extract_entities.yaml",
                output_model=Models.ListDictStrStr,
                mode=None,
            )
            end = datetime.now()
            output.execution_time = (end - start).total_seconds()
            return output

        except PromptError as e:
            output.errors.append(f"Prompt error: {e}")
        except LLMError as e:
            output.errors.append(f"LLM error: {e}")
        except ValidationError as e:
            output.errors.append(f"Validation error: {e}")
        except TextToolsError as e:
            output.errors.append(f"TextTools error: {e}")
        except Exception as e:
            output.errors.append(f"Unexpected error: {e}")

        return output

    def is_question(
        self,
        text: str,
        with_analysis: bool = False,
        user_prompt: str | None = None,
        temperature: float | None = 0.0,
        logprobs: bool = False,
        top_logprobs: int = 3,
        validator: Callable[[object], bool] | None = None,
        max_validation_retries: int | None = None,
        priority: int | None = 0,
    ) -> Models.ToolOutput:
        """
        Detect if the input is phrased as a question.

        Arguments:
            text: The input text to analyze
            with_analysis: Whether to include detailed reasoning analysis
            user_prompt: Additional instructions for question detection
            temperature: Controls randomness (0.0 = deterministic, 1.0 = creative)
            logprobs: Whether to return token probability information
            top_logprobs: Number of top token alternatives to return if logprobs enabled
            validator: Custom validation function to validate the output
            max_validation_retries: Maximum number of retry attempts if validation fails
            priority: Task execution priority (if enabled by vLLM and model)

        Returns:
            ToolOutput: Object containing:
                - result (bool): True if text is a question, False otherwise
                - logprobs (list | None): Probability data if logprobs enabled
                - analysis (str | None): Detailed reasoning if with_analysis enabled
                - process (str | None): Description of the process used
                - processed_at (datetime): Timestamp when the processing occurred
                - execution_time (float): Time taken for execution in seconds (-1.0 if not measured)
                - errors (list(str) | None): Errors occured during tool call
        """
        output = Models.ToolOutput()

        try:
            start = datetime.now()
            output = self._operator.run(
                # User parameters
                text=text,
                with_analysis=with_analysis,
                user_prompt=user_prompt,
                temperature=temperature,
                logprobs=logprobs,
                top_logprobs=top_logprobs,
                validator=validator,
                max_validation_retries=max_validation_retries,
                priority=priority,
                # Internal parameters
                prompt_file="is_question.yaml",
                output_model=Models.Bool,
                mode=None,
                output_lang=None,
            )
            end = datetime.now()
            output.execution_time = (end - start).total_seconds()
            return output

        except PromptError as e:
            output.errors.append(f"Prompt error: {e}")
        except LLMError as e:
            output.errors.append(f"LLM error: {e}")
        except ValidationError as e:
            output.errors.append(f"Validation error: {e}")
        except TextToolsError as e:
            output.errors.append(f"TextTools error: {e}")
        except Exception as e:
            output.errors.append(f"Unexpected error: {e}")

        return output

    def text_to_question(
        self,
        text: str,
        number_of_questions: int,
        with_analysis: bool = False,
        output_lang: str | None = None,
        user_prompt: str | None = None,
        temperature: float | None = 0.0,
        logprobs: bool = False,
        top_logprobs: int = 3,
        validator: Callable[[object], bool] | None = None,
        max_validation_retries: int | None = None,
        priority: int | None = 0,
    ) -> Models.ToolOutput:
        """
        Generate a single question from the given text.

        Arguments:
            text: The input text to generate a question from
            number_of_questions: Number of questions to generate
            with_analysis: Whether to include detailed reasoning analysis
            output_lang: Language for the output question
            user_prompt: Additional instructions for question generation
            temperature: Controls randomness (0.0 = deterministic, 1.0 = creative)
            logprobs: Whether to return token probability information
            top_logprobs: Number of top token alternatives to return if logprobs enabled
            validator: Custom validation function to validate the output
            max_validation_retries: Maximum number of retry attempts if validation fails
            priority: Task execution priority (if enabled by vLLM and model)

        Returns:
            ToolOutput: Object containing:
                - result (str): The generated question
                - logprobs (list | None): Probability data if logprobs enabled
                - analysis (str | None): Detailed reasoning if with_analysis enabled
                - process (str | None): Description of the process used
                - processed_at (datetime): Timestamp when the processing occurred
                - execution_time (float): Time taken for execution in seconds (-1.0 if not measured)
                - errors (list(str) | None): Errors occured during tool call
        """
        output = Models.ToolOutput()

        try:
            start = datetime.now()
            output = self._operator.run(
                # User parameters
                text=text,
                number_of_questions=number_of_questions,
                with_analysis=with_analysis,
                output_lang=output_lang,
                user_prompt=user_prompt,
                temperature=temperature,
                logprobs=logprobs,
                top_logprobs=top_logprobs,
                validator=validator,
                max_validation_retries=max_validation_retries,
                priority=priority,
                # Internal parameters
                prompt_file="text_to_question.yaml",
                output_model=Models.ReasonListStr,
                mode=None,
            )
            end = datetime.now()
            output.execution_time = (end - start).total_seconds()
            return output

        except PromptError as e:
            output.errors.append(f"Prompt error: {e}")
        except LLMError as e:
            output.errors.append(f"LLM error: {e}")
        except ValidationError as e:
            output.errors.append(f"Validation error: {e}")
        except TextToolsError as e:
            output.errors.append(f"TextTools error: {e}")
        except Exception as e:
            output.errors.append(f"Unexpected error: {e}")

        return output

    def merge_questions(
        self,
        text: list[str],
        with_analysis: bool = False,
        output_lang: str | None = None,
        user_prompt: str | None = None,
        temperature: float | None = 0.0,
        logprobs: bool = False,
        top_logprobs: int = 3,
        mode: Literal["default", "reason"] = "default",
        validator: Callable[[object], bool] | None = None,
        max_validation_retries: int | None = None,
        priority: int | None = 0,
    ) -> Models.ToolOutput:
        """
        Merge multiple questions into a single unified question.

        Arguments:
            text: List of questions to merge
            with_analysis: Whether to include detailed reasoning analysis
            output_lang: Language for the output merged question
            user_prompt: Additional instructions for question merging
            temperature: Controls randomness (0.0 = deterministic, 1.0 = creative)
            logprobs: Whether to return token probability information
            top_logprobs: Number of top token alternatives to return if logprobs enabled
            mode: Merging strategy - 'default' for direct merge, 'reason' for reasoned merge
            validator: Custom validation function to validate the output
            max_validation_retries: Maximum number of retry attempts if validation fails
            priority: Task execution priority (if enabled by vLLM and model)

        Returns:
            ToolOutput: Object containing:
                - result (str): The merged question
                - logprobs (list | None): Probability data if logprobs enabled
                - analysis (str | None): Detailed reasoning if with_analysis enabled
                - process (str | None): Description of the process used
                - processed_at (datetime): Timestamp when the processing occurred
                - execution_time (float): Time taken for execution in seconds (-1.0 if not measured)
                - errors (list(str) | None): Errors occured during tool call
        """
        output = Models.ToolOutput()

        try:
            start = datetime.now()
            text = ", ".join(text)
            output = self._operator.run(
                # User parameters
                text=text,
                with_analysis=with_analysis,
                output_lang=output_lang,
                user_prompt=user_prompt,
                temperature=temperature,
                logprobs=logprobs,
                top_logprobs=top_logprobs,
                validator=validator,
                max_validation_retries=max_validation_retries,
                priority=priority,
                # Internal parameters
                prompt_file="merge_questions.yaml",
                output_model=Models.Str,
                mode=mode,
            )
            end = datetime.now()
            output.execution_time = (end - start).total_seconds()
            return output

        except PromptError as e:
            output.errors.append(f"Prompt error: {e}")
        except LLMError as e:
            output.errors.append(f"LLM error: {e}")
        except ValidationError as e:
            output.errors.append(f"Validation error: {e}")
        except TextToolsError as e:
            output.errors.append(f"TextTools error: {e}")
        except Exception as e:
            output.errors.append(f"Unexpected error: {e}")

        return output

    def rewrite(
        self,
        text: str,
        with_analysis: bool = False,
        output_lang: str | None = None,
        user_prompt: str | None = None,
        temperature: float | None = 0.0,
        logprobs: bool = False,
        top_logprobs: int = 3,
        mode: Literal["positive", "negative", "hard_negative"] = "positive",
        validator: Callable[[object], bool] | None = None,
        max_validation_retries: int | None = None,
        priority: int | None = 0,
    ) -> Models.ToolOutput:
        """
        Rewrite a text with different modes.

        Arguments:
            text: The input text to rewrite
            with_analysis: Whether to include detailed reasoning analysis
            output_lang: Language for the output rewritten text
            user_prompt: Additional instructions for rewriting
            temperature: Controls randomness (0.0 = deterministic, 1.0 = creative)
            logprobs: Whether to return token probability information
            top_logprobs: Number of top token alternatives to return if logprobs enabled
            mode: Rewriting mode - 'positive', 'negative', or 'hard_negative'
            validator: Custom validation function to validate the output
            max_validation_retries: Maximum number of retry attempts if validation fails
            priority: Task execution priority (if enabled by vLLM and model)

        Returns:
            ToolOutput: Object containing:
                - result (str): The rewritten text
                - logprobs (list | None): Probability data if logprobs enabled
                - analysis (str | None): Detailed reasoning if with_analysis enabled
                - process (str | None): Description of the process used
                - processed_at (datetime): Timestamp when the processing occurred
                - execution_time (float): Time taken for execution in seconds (-1.0 if not measured)
                - errors (list(str) | None): Errors occured during tool call
        """
        output = Models.ToolOutput()

        try:
            start = datetime.now()
            output = self._operator.run(
                # User parameters
                text=text,
                with_analysis=with_analysis,
                output_lang=output_lang,
                user_prompt=user_prompt,
                temperature=temperature,
                logprobs=logprobs,
                top_logprobs=top_logprobs,
                validator=validator,
                max_validation_retries=max_validation_retries,
                priority=priority,
                # Internal parameters
                prompt_file="rewrite.yaml",
                output_model=Models.Str,
                mode=mode,
            )
            end = datetime.now()
            output.execution_time = (end - start).total_seconds()
            return output

        except PromptError as e:
            output.errors.append(f"Prompt error: {e}")
        except LLMError as e:
            output.errors.append(f"LLM error: {e}")
        except ValidationError as e:
            output.errors.append(f"Validation error: {e}")
        except TextToolsError as e:
            output.errors.append(f"TextTools error: {e}")
        except Exception as e:
            output.errors.append(f"Unexpected error: {e}")

        return output

    def subject_to_question(
        self,
        text: str,
        number_of_questions: int,
        with_analysis: bool = False,
        output_lang: str | None = None,
        user_prompt: str | None = None,
        temperature: float | None = 0.0,
        logprobs: bool = False,
        top_logprobs: int = 3,
        validator: Callable[[object], bool] | None = None,
        max_validation_retries: int | None = None,
        priority: int | None = 0,
    ) -> Models.ToolOutput:
        """
        Generate a list of questions about a subject.

        Arguments:
            text: The subject text to generate questions about
            number_of_questions: Number of questions to generate
            with_analysis: Whether to include detailed reasoning analysis
            output_lang: Language for the output questions
            user_prompt: Additional instructions for question generation
            temperature: Controls randomness (0.0 = deterministic, 1.0 = creative)
            logprobs: Whether to return token probability information
            top_logprobs: Number of top token alternatives to return if logprobs enabled
            validator: Custom validation function to validate the output
            max_validation_retries: Maximum number of retry attempts if validation fails
            priority: Task execution priority (if enabled by vLLM and model)

        Returns:
            ToolOutput: Object containing:
                - result (list[str]): List of generated questions
                - logprobs (list | None): Probability data if logprobs enabled
                - analysis (str | None): Detailed reasoning if with_analysis enabled
                - process (str | None): Description of the process used
                - processed_at (datetime): Timestamp when the processing occurred
                - execution_time (float): Time taken for execution in seconds (-1.0 if not measured)
                - errors (list(str) | None): Errors occured during tool call
        """
        output = Models.ToolOutput()

        try:
            start = datetime.now()
            output = self._operator.run(
                # User parameters
                text=text,
                number_of_questions=number_of_questions,
                with_analysis=with_analysis,
                output_lang=output_lang,
                user_prompt=user_prompt,
                temperature=temperature,
                logprobs=logprobs,
                top_logprobs=top_logprobs,
                validator=validator,
                max_validation_retries=max_validation_retries,
                priority=priority,
                # Internal parameters
                prompt_file="subject_to_question.yaml",
                output_model=Models.ReasonListStr,
                mode=None,
            )
            end = datetime.now()
            output.execution_time = (end - start).total_seconds()
            return output

        except PromptError as e:
            output.errors.append(f"Prompt error: {e}")
        except LLMError as e:
            output.errors.append(f"LLM error: {e}")
        except ValidationError as e:
            output.errors.append(f"Validation error: {e}")
        except TextToolsError as e:
            output.errors.append(f"TextTools error: {e}")
        except Exception as e:
            output.errors.append(f"Unexpected error: {e}")

        return output

    def summarize(
        self,
        text: str,
        with_analysis: bool = False,
        output_lang: str | None = None,
        user_prompt: str | None = None,
        temperature: float | None = 0.0,
        logprobs: bool = False,
        top_logprobs: int = 3,
        validator: Callable[[object], bool] | None = None,
        max_validation_retries: int | None = None,
        priority: int | None = 0,
    ) -> Models.ToolOutput:
        """
        Summarize the given subject text.

        Arguments:
            text: The input text to summarize
            with_analysis: Whether to include detailed reasoning analysis
            output_lang: Language for the output summary
            user_prompt: Additional instructions for summarization
            temperature: Controls randomness (0.0 = deterministic, 1.0 = creative)
            logprobs: Whether to return token probability information
            top_logprobs: Number of top token alternatives to return if logprobs enabled
            validator: Custom validation function to validate the output
            max_validation_retries: Maximum number of retry attempts if validation fails
            priority: Task execution priority (if enabled by vLLM and model)

        Returns:
            ToolOutput: Object containing:
                - result (str): The summary text
                - logprobs (list | None): Probability data if logprobs enabled
                - analysis (str | None): Detailed reasoning if with_analysis enabled
                - process (str | None): Description of the process used
                - processed_at (datetime): Timestamp when the processing occurred
                - execution_time (float): Time taken for execution in seconds (-1.0 if not measured)
                - errors (list(str) | None): Errors occured during tool call
        """
        output = Models.ToolOutput()

        try:
            start = datetime.now()
            output = self._operator.run(
                # User parameters
                text=text,
                with_analysis=with_analysis,
                output_lang=output_lang,
                user_prompt=user_prompt,
                temperature=temperature,
                logprobs=logprobs,
                top_logprobs=top_logprobs,
                validator=validator,
                max_validation_retries=max_validation_retries,
                priority=priority,
                # Internal parameters
                prompt_file="summarize.yaml",
                output_model=Models.Str,
                mode=None,
            )
            end = datetime.now()
            output.execution_time = (end - start).total_seconds()
            return output

        except PromptError as e:
            output.errors.append(f"Prompt error: {e}")
        except LLMError as e:
            output.errors.append(f"LLM error: {e}")
        except ValidationError as e:
            output.errors.append(f"Validation error: {e}")
        except TextToolsError as e:
            output.errors.append(f"TextTools error: {e}")
        except Exception as e:
            output.errors.append(f"Unexpected error: {e}")

        return output

    def translate(
        self,
        text: str,
        target_language: str,
        use_chunker: bool = True,
        with_analysis: bool = False,
        user_prompt: str | None = None,
        temperature: float | None = 0.0,
        logprobs: bool = False,
        top_logprobs: int = 3,
        validator: Callable[[object], bool] | None = None,
        max_validation_retries: int | None = None,
        priority: int | None = 0,
    ) -> Models.ToolOutput:
        """
        Translate text between languages.

        Important Note: This tool is EXPERIMENTAL, you can use it but it isn't reliable.

        Arguments:
            text: The input text to translate
            target_language: The target language for translation
            use_chunker: Whether to use text chunker for text length bigger than 1500
            with_analysis: Whether to include detailed reasoning analysis
            user_prompt: Additional instructions for translation
            temperature: Controls randomness (0.0 = deterministic, 1.0 = creative)
            logprobs: Whether to return token probability information
            top_logprobs: Number of top token alternatives to return if logprobs enabled
            validator: Custom validation function to validate the output
            max_validation_retries: Maximum number of retry attempts if validation fails
            priority: Task execution priority (if enabled by vLLM and model)

        Returns:
            ToolOutput: Object containing:
                - result (str): The translated text
                - logprobs (list | None): Probability data if logprobs enabled
                - analysis (str | None): Detailed reasoning if with_analysis enabled
                - process (str | None): Description of the process used
                - processed_at (datetime): Timestamp when the processing occurred
                - execution_time (float): Time taken for execution in seconds (-1.0 if not measured)
                - errors (list(str) | None): Errors occured during tool call
        """
        output = Models.ToolOutput()

        try:
            start = datetime.now()

            if len(text.split(" ")) > 1500 and use_chunker:
                chunks = text_to_chunks(text, 1200, 0)

                translation = ""
                analysis = ""
                logprobs = []

                # Run translation for each chunk
                for chunk in chunks:
                    chunk_output = self._operator.run(
                        # User parameters
                        text=chunk,
                        target_language=target_language,
                        with_analysis=with_analysis,
                        user_prompt=user_prompt,
                        temperature=temperature,
                        logprobs=logprobs,
                        top_logprobs=top_logprobs,
                        validator=validator,
                        max_validation_retries=max_validation_retries,
                        priority=priority,
                        # Internal parameters
                        prompt_file="translate.yaml",
                        output_model=Models.Str,
                        mode=None,
                        output_lang=None,
                    )

                    # Check for errors from operator
                    if chunk_output.errors:
                        output.errors.extend(chunk_output.errors)
                        end = datetime.now()
                        output.execution_time = (end - start).total_seconds()
                        return output

                    # Concatenate the outputs
                    translation += chunk_output.result + "\n"
                    if with_analysis:
                        analysis += chunk_output.analysis
                    if logprobs:
                        logprobs += chunk_output.logprobs

                end = datetime.now()
                output = Models.ToolOutput(
                    result=translation,
                    logprobs=logprobs,
                    analysis=analysis,
                    process="translate",
                    execution_time=(end - start).total_seconds(),
                )
                return output

            else:
                output = self._operator.run(
                    # User parameters
                    text=text,
                    target_language=target_language,
                    with_analysis=with_analysis,
                    user_prompt=user_prompt,
                    temperature=temperature,
                    logprobs=logprobs,
                    top_logprobs=top_logprobs,
                    validator=validator,
                    max_validation_retries=max_validation_retries,
                    priority=priority,
                    # Internal parameters
                    prompt_file="translate.yaml",
                    output_model=Models.Str,
                    mode=None,
                    output_lang=None,
                )
                end = datetime.now()
                output.execution_time = (end - start).total_seconds()
                return output

        except PromptError as e:
            output.errors.append(f"Prompt error: {e}")
        except LLMError as e:
            output.errors.append(f"LLM error: {e}")
        except ValidationError as e:
            output.errors.append(f"Validation error: {e}")
        except TextToolsError as e:
            output.errors.append(f"TextTools error: {e}")
        except Exception as e:
            output.errors.append(f"Unexpected error: {e}")

        return output

    def propositionize(
        self,
        text: str,
        with_analysis: bool = False,
        output_lang: str | None = None,
        user_prompt: str | None = None,
        temperature: float | None = 0.0,
        logprobs: bool = False,
        top_logprobs: int = 3,
        validator: Callable[[object], bool] | None = None,
        max_validation_retries: int | None = None,
        priority: int | None = 0,
    ) -> Models.ToolOutput:
        """
        Proposition input text to meaningful sentences.

        Important Note: This tool is EXPERIMENTAL, you can use it but it isn't reliable.

        Arguments:
            text: The input text
            with_analysis: Whether to include detailed reasoning analysis
            output_lang: Language for the output summary
            user_prompt: Additional instructions for summarization
            temperature: Controls randomness (0.0 = deterministic, 1.0 = creative)
            logprobs: Whether to return token probability information
            top_logprobs: Number of top token alternatives to return if logprobs enabled
            validator: Custom validation function to validate the output
            max_validation_retries: Maximum number of retry attempts if validation fails
            priority: Task execution priority (if enabled by vLLM and model)

        Returns:
            ToolOutput: Object containing:
                - result (list[str]): The propositions
                - logprobs (list | None): Probability data if logprobs enabled
                - analysis (str | None): Detailed reasoning if with_analysis enabled
                - process (str | None): Description of the process used
                - processed_at (datetime): Timestamp when the processing occurred
                - execution_time (float): Time taken for execution in seconds (-1.0 if not measured)
                - errors (list(str) | None): Errors occured during tool call
        """
        output = Models.ToolOutput()

        try:
            start = datetime.now()
            output = self._operator.run(
                # User parameters
                text=text,
                with_analysis=with_analysis,
                output_lang=output_lang,
                user_prompt=user_prompt,
                temperature=temperature,
                logprobs=logprobs,
                top_logprobs=top_logprobs,
                validator=validator,
                max_validation_retries=max_validation_retries,
                priority=priority,
                # Internal parameters
                prompt_file="propositionize.yaml",
                output_model=Models.ListStr,
                mode=None,
            )
            end = datetime.now()
            output.execution_time = (end - start).total_seconds()
            return output

        except PromptError as e:
            output.errors.append(f"Prompt error: {e}")
        except LLMError as e:
            output.errors.append(f"LLM error: {e}")
        except ValidationError as e:
            output.errors.append(f"Validation error: {e}")
        except TextToolsError as e:
            output.errors.append(f"TextTools error: {e}")
        except Exception as e:
            output.errors.append(f"Unexpected error: {e}")

        return output

    def check_fact(
        self,
        text: str,
        source_text: str,
        with_analysis: bool = False,
        output_lang: str | None = None,
        user_prompt: str | None = None,
        temperature: float | None = 0.0,
        logprobs: bool = False,
        top_logprobs: int = 3,
        validator: Callable[[object], bool] | None = None,
        max_validation_retries: int | None = None,
        priority: int | None = 0,
    ) -> Models.ToolOutput:
        """
        Checks wheather a statement is relevant to the source text or not.

        Important Note: This tool is EXPERIMENTAL, you can use it but it isn't reliable.

        Arguments:
            text: The input text
            source_text: the source text that we want to check relation of text to it
            with_analysis: Whether to include detailed reasoning analysis
            output_lang: Language for the output summary
            user_prompt: Additional instructions for summarization
            temperature: Controls randomness (0.0 = deterministic, 1.0 = creative)
            logprobs: Whether to return token probability information
            top_logprobs: Number of top token alternatives to return if logprobs enabled
            validator: Custom validation function to validate the output
            max_validation_retries: Maximum number of retry attempts if validation fails
            priority: Task execution priority (if enabled by vLLM and model)

        Returns:
            ToolOutput: Object containing:
                - result (bool): statement is relevant to source text or not
                - logprobs (list | None): Probability data if logprobs enabled
                - analysis (str | None): Detailed reasoning if with_analysis enabled
                - process (str | None): Description of the process used
                - processed_at (datetime): Timestamp when the processing occurred
                - execution_time (float): Time taken for execution in seconds (-1.0 if not measured)
                - errors (list(str) | None): Errors occured during tool call
        """
        output = Models.ToolOutput()
        try:
            start = datetime.now()
            output = self._operator.run(
                # User parameters
                text=text,
                with_analysis=with_analysis,
                output_lang=output_lang,
                user_prompt=user_prompt,
                temperature=temperature,
                logprobs=logprobs,
                top_logprobs=top_logprobs,
                validator=validator,
                max_validation_retries=max_validation_retries,
                priority=priority,
                # Internal parameters
                prompt_file="check_fact.yaml",
                output_model=Models.Bool,
                mode=None,
                source_text=source_text,
            )
            end = datetime.now()
            output.execution_time = (end - start).total_seconds()
            return output

        except PromptError as e:
            output.errors.append(f"Prompt error: {e}")
        except LLMError as e:
            output.errors.append(f"LLM error: {e}")
        except ValidationError as e:
            output.errors.append(f"Validation error: {e}")
        except TextToolsError as e:
            output.errors.append(f"TextTools error: {e}")
        except Exception as e:
            output.errors.append(f"Unexpected error: {e}")

        return output

    def run_custom(
        self,
        prompt: str,
        output_model: object,
        with_analysis: bool = False,
        analyze_template: str | None = None,
        output_lang: str | None = None,
        temperature: float | None = None,
        logprobs: bool | None = None,
        top_logprobs: int = 3,
        validator: Callable[[object], bool] | None = None,
        max_validation_retries: int | None = None,
        priority: int | None = 0,
    ) -> Models.ToolOutput:
        """
        Custom tool that can do almost anything!

        Important Note: This tool is EXPERIMENTAL, you can use it but it isn't reliable.

        Arguments:
            prompt: The user prompt
            output_model: Pydantic BaseModel used for structured output
            with_analysis: Whether to include detailed reasoning analysis
            analyze_template: The analyze template used for reasoning analysis
            output_lang: Language for the output summary
            temperature: Controls randomness (0.0 = deterministic, 1.0 = creative)
            logprobs: Whether to return token probability information
            top_logprobs: Number of top token alternatives to return if logprobs enabled
            validator: Custom validation function to validate the output
            max_validation_retries: Maximum number of retry attempts if validation fails
            priority: Task execution priority (if enabled by vLLM and model)

        Returns:
            ToolOutput: Object containing:
                - result (str): The translated text
                - logprobs (list | None): Probability data if logprobs enabled
                - analysis (str | None): Detailed reasoning if with_analysis enabled
                - process (str | None): Description of the process used
                - processed_at (datetime): Timestamp when the processing occurred
                - execution_time (float): Time taken for execution in seconds (-1.0 if not measured)
                - errors (list(str) | None): Errors occured during tool call
        """
        output = Models.ToolOutput()

        try:
            start = datetime.now()
            output = self._operator.run(
                # User paramaeters
                text=prompt,
                output_model=output_model,
                with_analysis=with_analysis,
                analyze_template=analyze_template,
                output_model_str=output_model.model_json_schema(),
                output_lang=output_lang,
                temperature=temperature,
                logprobs=logprobs,
                top_logprobs=top_logprobs,
                validator=validator,
                max_validation_retries=max_validation_retries,
                priority=priority,
                # Internal parameters
                prompt_file="run_custom.yaml",
                user_prompt=None,
                mode=None,
            )
            end = datetime.now()
            output.execution_time = (end - start).total_seconds()
            return output

        except PromptError as e:
            output.errors.append(f"Prompt error: {e}")
        except LLMError as e:
            output.errors.append(f"LLM error: {e}")
        except ValidationError as e:
            output.errors.append(f"Validation error: {e}")
        except TextToolsError as e:
            output.errors.append(f"TextTools error: {e}")
        except Exception as e:
            output.errors.append(f"Unexpected error: {e}")

        return output
