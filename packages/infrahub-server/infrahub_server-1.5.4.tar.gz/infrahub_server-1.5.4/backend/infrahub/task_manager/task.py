import asyncio
import hashlib
import json
from datetime import datetime, timedelta, timezone
from typing import Any
from uuid import UUID

from prefect import State
from prefect.client.orchestration import PrefectClient, get_client
from prefect.client.schemas.filters import (
    ArtifactFilter,
    ArtifactFilterType,
    FlowFilter,
    FlowFilterId,
    FlowFilterName,
    FlowRunFilter,
    FlowRunFilterId,
    FlowRunFilterName,
    FlowRunFilterStartTime,
    FlowRunFilterState,
    FlowRunFilterStateType,
    FlowRunFilterTags,
    LogFilter,
    LogFilterFlowRunId,
)
from prefect.client.schemas.objects import Flow, FlowRun, StateType
from prefect.client.schemas.sorting import (
    FlowRunSort,
)

from infrahub import config
from infrahub.core.constants import TaskConclusion
from infrahub.core.query.node import NodeGetKindQuery
from infrahub.database import InfrahubDatabase
from infrahub.log import get_logger
from infrahub.message_bus.types import KVTTL
from infrahub.utils import get_nested_dict
from infrahub.workers.dependencies import get_cache
from infrahub.workflows.constants import TAG_NAMESPACE, WorkflowTag

from .constants import CONCLUSION_STATE_MAPPING
from .models import FlowLogs, FlowProgress, RelatedNodesInfo

log = get_logger()

NB_LOGS_LIMIT = 10_000
PREFECT_MAX_LOGS_PER_CALL = 200


class PrefectTask:
    @staticmethod
    def _build_flow_run_count_cache_key(body: dict[str, Any]) -> str:
        serialized = json.dumps(body, sort_keys=True, separators=(",", ":"))
        hashed = hashlib.sha256(serialized.encode()).hexdigest()
        return f"task_manager:flow_run_count:{hashed}"

    @classmethod
    async def count_flow_runs(
        cls,
        client: PrefectClient,
        flow_filter: FlowFilter | None = None,
        flow_run_filter: FlowRunFilter | None = None,
    ) -> int:
        """
        Method to count the number of flow runs based on a flow_run_filter.
        The format of the body is the same as the one generated in read_flow_runs
        """
        body = {
            "flows": flow_filter.model_dump(mode="json") if flow_filter else None,
            "flow_runs": (flow_run_filter.model_dump(mode="json", exclude_unset=True) if flow_run_filter else None),
        }
        cache_key = cls._build_flow_run_count_cache_key(body)

        cache = await get_cache()
        cached_value_raw = await cache.get(key=cache_key)
        if cached_value_raw is not None:
            try:
                return int(cached_value_raw)
            except (TypeError, ValueError):
                await cache.delete(key=cache_key)

        response = await client._client.post("/flow_runs/count", json=body)
        response.raise_for_status()
        count_value = int(response.json())

        if count_value >= config.SETTINGS.workflow.flow_run_count_cache_threshold:
            await cache.set(key=cache_key, value=str(count_value), expires=KVTTL.ONE_MINUTE)

        return count_value

    @classmethod
    async def _get_related_nodes(cls, db: InfrahubDatabase, flows: list[FlowRun]) -> RelatedNodesInfo:
        related_nodes = RelatedNodesInfo()

        # Extract all related nodes ID from tags
        for flow in flows:
            related_node_tag_prefix = WorkflowTag.RELATED_NODE.render(identifier="")
            related_node_ids = [
                tag.replace(related_node_tag_prefix, "") for tag in flow.tags if tag.startswith(related_node_tag_prefix)
            ]
            if not related_node_ids:
                continue
            related_nodes.add_nodes(flow_id=flow.id, node_ids=related_node_ids)

        if unique_related_node_ids := related_nodes.get_unique_related_node_ids():
            query = await NodeGetKindQuery.init(db=db, ids=unique_related_node_ids)
            await query.execute(db=db)
            unique_related_node_ids_kind = await query.get_node_kind_map()

            for node_id, node_kind in unique_related_node_ids_kind.items():
                if node_id in related_nodes.nodes:
                    related_nodes.nodes[node_id].kind = node_kind

        return related_nodes

    @classmethod
    async def _get_logs(
        cls, client: PrefectClient, flow_ids: list[UUID], log_limit: int | None, log_offset: int | None
    ) -> FlowLogs:
        """
        Return the logs for a flow run, based on log_limit and log_offset.
        At most, NB_LOGS_LIMIT logs will be returned per flow.
        """

        logs_flow = FlowLogs()

        log_limit = log_limit if log_limit is not None else NB_LOGS_LIMIT
        log_offset = log_offset or 0
        current_offset = log_offset

        if log_limit > NB_LOGS_LIMIT:
            raise ValueError(f"log_limit cannot be greater than {NB_LOGS_LIMIT}")

        all_logs = []

        # Fetch the logs in batches of PREFECT_MAX_LOGS_PER_CALL, as prefect does not allow to fetch more logs at once.
        remaining = min(log_limit, NB_LOGS_LIMIT)
        while remaining > 0:
            batch_limit = min(PREFECT_MAX_LOGS_PER_CALL, remaining)
            logs_batch = await client.read_logs(
                log_filter=LogFilter(flow_run_id=LogFilterFlowRunId(any_=flow_ids)),
                offset=current_offset,
                limit=batch_limit,
            )
            all_logs.extend(logs_batch)
            nb_fetched = len(logs_batch)
            if nb_fetched < batch_limit:
                break  # No more logs to fetch

            current_offset += nb_fetched
            remaining -= nb_fetched

        for flow_log in all_logs:
            if flow_log.flow_run_id and flow_log.message not in ["Finished in state Completed()"]:
                logs_flow.logs[flow_log.flow_run_id].append(flow_log)

        return logs_flow

    @classmethod
    async def _get_flows(
        cls, client: PrefectClient, ids: list[UUID] | None = None, names: list[str] | None = None
    ) -> list[Flow]:
        if not names and not ids:
            return await client.read_flows()

        flow_filter = FlowFilter()
        flow_filter.name = FlowFilterName(any_=names) if names else None
        flow_filter.id = FlowFilterId(any_=ids) if ids else None
        return await client.read_flows(flow_filter=flow_filter)

    @classmethod
    async def _get_progress(cls, client: PrefectClient, flow_ids: list[UUID]) -> FlowProgress:
        artifacts = await client.read_artifacts(
            artifact_filter=ArtifactFilter(type=ArtifactFilterType(any_=["progress"])),
            flow_run_filter=FlowRunFilter(id=FlowRunFilterId(any_=flow_ids)),
        )
        flow_progress = FlowProgress()
        for artifact in artifacts:
            if artifact.flow_run_id in flow_progress.data:
                log.warning(
                    f"Multiple Progress Artifact found for the flow_run {artifact.flow_run_id}, keeping the first one"
                )
                continue
            if artifact.flow_run_id and isinstance(artifact.data, float):
                flow_progress.data[artifact.flow_run_id] = artifact.data

        return flow_progress

    @classmethod
    async def _extract_branch_name(cls, flow: FlowRun) -> str | None:
        branch_name = [
            tag.replace(WorkflowTag.BRANCH.render(identifier=""), "")
            for tag in flow.tags
            if tag.startswith(WorkflowTag.BRANCH.render(identifier=""))
        ]

        return branch_name[0] if branch_name else None

    @classmethod
    def _generate_flow_filter(cls, workflows: list[str] | None = None) -> FlowFilter:
        flow_filter = FlowFilter()
        if workflows:
            flow_filter.name = FlowFilterName(any_=workflows)
        return flow_filter

    @classmethod
    def _generate_flow_run_filter(
        cls,
        q: str | None = None,
        ids: list[str] | None = None,
        related_nodes: list[str] | None = None,
        statuses: list[StateType] | None = None,
        tags: list[str] | None = None,
        branch: str | None = None,
    ) -> FlowRunFilter:
        filter_tags = [TAG_NAMESPACE]

        if tags:
            filter_tags.extend(tags)
        if branch:
            filter_tags.append(WorkflowTag.BRANCH.render(identifier=branch))
        # We only support one related node for now, need to investigate HOW (and IF) we can support more

        if related_nodes:
            filter_tags.append(WorkflowTag.RELATED_NODE.render(identifier=related_nodes[0]))

        flow_run_filter = FlowRunFilter(
            tags=FlowRunFilterTags(all_=filter_tags),
        )
        if ids:
            flow_run_filter.id = FlowRunFilterId(any_=[UUID(id) for id in ids])

        if statuses:
            flow_run_filter.state = FlowRunFilterState(type=FlowRunFilterStateType(any_=statuses))

        if q:
            flow_run_filter.name = FlowRunFilterName(like_=q)

        return flow_run_filter

    @classmethod
    async def query(
        cls,
        db: InfrahubDatabase,
        fields: dict[str, Any],
        q: str | None = None,
        ids: list[str] | None = None,
        related_nodes: list[str] | None = None,
        statuses: list[StateType] | None = None,
        workflows: list[str] | None = None,
        tags: list[str] | None = None,
        branch: str | None = None,
        limit: int | None = None,
        offset: int | None = None,
        log_limit: int | None = None,
        log_offset: int | None = None,
    ) -> dict[str, Any]:
        nodes: list[dict] = []
        count = None

        node_fields = get_nested_dict(nested_dict=fields, keys=["edges", "node"])
        log_fields = get_nested_dict(nested_dict=fields, keys=["edges", "node", "logs", "edges", "node"])
        logs_flow = FlowLogs()
        progress_flow = FlowProgress()
        workflow_names: dict[UUID, str] = {}
        related_nodes_info = RelatedNodesInfo()

        async with get_client(sync_client=False) as client:
            flow_filter = cls._generate_flow_filter(workflows=workflows)
            flow_run_filter = cls._generate_flow_run_filter(
                q=q, ids=ids, related_nodes=related_nodes, statuses=statuses, tags=tags, branch=branch
            )

            if "count" in fields:
                count = await cls.count_flow_runs(
                    client=client, flow_filter=flow_filter, flow_run_filter=flow_run_filter
                )

            if node_fields:
                flows = await client.read_flow_runs(
                    flow_filter=flow_filter,
                    flow_run_filter=flow_run_filter,
                    limit=limit,
                    offset=offset or 0,
                    sort=FlowRunSort.START_TIME_DESC,
                )
                if log_fields:
                    logs_flow = await cls._get_logs(
                        client=client, flow_ids=[flow.id for flow in flows], log_limit=log_limit, log_offset=log_offset
                    )

                if "progress" in node_fields:
                    progress_flow = await cls._get_progress(client=client, flow_ids=[flow.id for flow in flows])

                if (
                    "related_nodes" in node_fields
                    or "related_node" in node_fields
                    or "related_node_kind" in node_fields
                ):
                    related_nodes_info = await cls._get_related_nodes(db=db, flows=flows)

                if "workflow" in node_fields:
                    unique_flow_ids = {flow.flow_id for flow in flows}
                    workflow_names = {
                        flow.id: flow.name for flow in await cls._get_flows(client=client, ids=list(unique_flow_ids))
                    }

                for flow in flows:
                    logs = []

                    if log_fields:
                        logs = logs_flow.to_graphql(flow_id=flow.id)

                    related_node = related_nodes_info.get_first_related_node(flow_id=flow.id)

                    nodes.append(
                        {
                            "node": {
                                "title": flow.name,
                                "conclusion": CONCLUSION_STATE_MAPPING.get(
                                    str(flow.state_name), TaskConclusion.UNKNOWN
                                ).value,
                                "state": flow.state_type,
                                "progress": progress_flow.data.get(flow.id, None),
                                "parameters": flow.parameters,
                                "branch": await cls._extract_branch_name(flow=flow),
                                "tags": flow.tags,
                                "workflow": workflow_names.get(flow.flow_id, None),
                                "related_node": related_node.id if related_node else None,
                                "related_node_kind": related_node.kind if related_node else None,
                                "related_nodes": related_nodes_info.get_related_nodes_as_dict(flow_id=flow.id),
                                "created_at": flow.created.to_iso8601_string(),  # type: ignore
                                "updated_at": flow.updated.to_iso8601_string(),  # type: ignore
                                "start_time": flow.start_time.to_iso8601_string() if flow.start_time else None,
                                "id": flow.id,
                                "logs": {"edges": logs, "count": len(logs)},
                            }
                        }
                    )

        return {"count": count or 0, "edges": nodes}

    @classmethod
    async def delete_flow_runs(
        cls,
        states: list[StateType] = [StateType.COMPLETED, StateType.FAILED, StateType.CANCELLED],  # noqa: B006
        delete: bool = True,
        days_to_keep: int = 2,
        batch_size: int = 100,
    ) -> None:
        """Delete flow runs in the specified states and older than specified days."""

        logger = get_logger()

        async with get_client(sync_client=False) as client:
            cutoff = datetime.now(timezone.utc) - timedelta(days=days_to_keep)

            flow_run_filter = FlowRunFilter(
                start_time=FlowRunFilterStartTime(before_=cutoff),  # type: ignore[arg-type]
                state=FlowRunFilterState(type=FlowRunFilterStateType(any_=states)),
            )

            # Get flow runs to delete
            flow_runs = await client.read_flow_runs(flow_run_filter=flow_run_filter, limit=batch_size)

            deleted_total = 0

            while True:
                batch_deleted = 0
                failed_deletes = []

                # Delete each flow run through the API
                for flow_run in flow_runs:
                    try:
                        if delete:
                            await client.delete_flow_run(flow_run_id=flow_run.id)
                        else:
                            await client.set_flow_run_state(
                                flow_run_id=flow_run.id,
                                state=State(type=StateType.CRASHED),
                                force=True,
                            )
                        deleted_total += 1
                        batch_deleted += 1
                    except Exception as e:
                        logger.warning(f"Failed to delete flow run {flow_run.id}: {e}")
                        failed_deletes.append(flow_run.id)

                    # Rate limiting
                    if batch_deleted % 10 == 0:
                        await asyncio.sleep(0.5)

                logger.info(f"Delete {batch_deleted}/{len(flow_runs)} flow runs (total: {deleted_total})")

                # Get next batch
                previous_flow_run_ids = [fr.id for fr in flow_runs]
                flow_runs = await client.read_flow_runs(flow_run_filter=flow_run_filter, limit=batch_size)

                if not flow_runs:
                    logger.info("No more flow runs to delete")
                    break

                if previous_flow_run_ids == [fr.id for fr in flow_runs]:
                    logger.info("Found same flow runs to delete, aborting")
                    break

                # Delay between batches to avoid overwhelming the API
                await asyncio.sleep(1.0)

            logger.info(f"Retention complete. Total deleted tasks: {deleted_total}")
