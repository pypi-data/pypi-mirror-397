"""
CLI å‘½ä»¤å®ç°
"""
import json
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Literal, Optional

from ..core import DataTransformer, DictWrapper
from ..presets import get_preset, list_presets
from ..storage.io import load_data, save_data, sample_file


# æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
SUPPORTED_FORMATS = {".csv", ".jsonl", ".json", ".xlsx", ".xls", ".parquet", ".arrow", ".feather"}


def _check_file_format(filepath: Path) -> bool:
    """æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ”¯æŒï¼Œä¸æ”¯æŒåˆ™æ‰“å°é”™è¯¯ä¿¡æ¯å¹¶è¿”å› False"""
    ext = filepath.suffix.lower()
    if ext not in SUPPORTED_FORMATS:
        print(f"é”™è¯¯: ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ - {ext}")
        print(f"æ”¯æŒçš„æ ¼å¼: {', '.join(sorted(SUPPORTED_FORMATS))}")
        return False
    return True


def sample(
    filename: str,
    num: int = 10,
    sample_type: Literal["random", "head", "tail"] = "head",
    output: Optional[str] = None,
    seed: Optional[int] = None,
    by: Optional[str] = None,
    uniform: bool = False,
) -> None:
    """
    ä»æ•°æ®æ–‡ä»¶ä¸­é‡‡æ ·æŒ‡å®šæ•°é‡çš„æ•°æ®ã€‚

    Args:
        filename: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒ csv/excel/jsonl/json/parquet/arrow/feather æ ¼å¼
        num: é‡‡æ ·æ•°é‡ï¼Œé»˜è®¤ 10
            - num > 0: é‡‡æ ·æŒ‡å®šæ•°é‡
            - num = 0: é‡‡æ ·æ‰€æœ‰æ•°æ®
            - num < 0: Python åˆ‡ç‰‡é£æ ¼ï¼ˆå¦‚ -1 è¡¨ç¤ºæœ€å 1 æ¡ï¼Œ-10 è¡¨ç¤ºæœ€å 10 æ¡ï¼‰
        sample_type: é‡‡æ ·æ–¹å¼ï¼Œå¯é€‰ random/head/tailï¼Œé»˜è®¤ head
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¸æŒ‡å®šåˆ™æ‰“å°åˆ°æ§åˆ¶å°
        seed: éšæœºç§å­ï¼ˆä»…åœ¨ sample_type=random æ—¶æœ‰æ•ˆï¼‰
        by: åˆ†å±‚é‡‡æ ·å­—æ®µåï¼ŒæŒ‰è¯¥å­—æ®µçš„å€¼åˆ†ç»„é‡‡æ ·
        uniform: å‡åŒ€é‡‡æ ·æ¨¡å¼ï¼ˆéœ€é…åˆ --by ä½¿ç”¨ï¼‰ï¼Œå„ç»„é‡‡æ ·ç›¸åŒæ•°é‡

    Examples:
        dt sample data.jsonl 5
        dt sample data.csv 100 --sample_type=head
        dt sample data.xlsx 50 --output=sampled.jsonl
        dt sample data.jsonl 0   # é‡‡æ ·æ‰€æœ‰æ•°æ®
        dt sample data.jsonl -10 # æœ€å 10 æ¡æ•°æ®
        dt sample data.jsonl 1000 --by=category           # æŒ‰æ¯”ä¾‹åˆ†å±‚é‡‡æ ·
        dt sample data.jsonl 1000 --by=category --uniform # å‡åŒ€åˆ†å±‚é‡‡æ ·
    """
    filepath = Path(filename)

    if not filepath.exists():
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {filename}")
        return

    if not _check_file_format(filepath):
        return

    # uniform å¿…é¡»é…åˆ by ä½¿ç”¨
    if uniform and not by:
        print("é”™è¯¯: --uniform å¿…é¡»é…åˆ --by ä½¿ç”¨")
        return

    # åˆ†å±‚é‡‡æ ·æ¨¡å¼
    if by:
        try:
            sampled = _stratified_sample(
                filepath, num, by, uniform, seed, sample_type
            )
        except Exception as e:
            print(f"é”™è¯¯: {e}")
            return
    else:
        # æ™®é€šé‡‡æ ·
        try:
            sampled = sample_file(
                str(filepath),
                num=num,
                sample_type=sample_type,
                seed=seed,
                output=None,  # å…ˆä¸ä¿å­˜ï¼Œç»Ÿä¸€åœ¨æœ€åå¤„ç†
            )
        except Exception as e:
            print(f"é”™è¯¯: {e}")
            return

    # è¾“å‡ºç»“æœ
    if output:
        save_data(sampled, output)
        print(f"å·²ä¿å­˜ {len(sampled)} æ¡æ•°æ®åˆ° {output}")
    else:
        _print_samples(sampled)


def _stratified_sample(
    filepath: Path,
    num: int,
    stratify_field: str,
    uniform: bool,
    seed: Optional[int],
    sample_type: str,
) -> List[Dict]:
    """
    åˆ†å±‚é‡‡æ ·å®ç°ã€‚

    Args:
        filepath: æ–‡ä»¶è·¯å¾„
        num: ç›®æ ‡é‡‡æ ·æ€»æ•°
        stratify_field: åˆ†å±‚å­—æ®µ
        uniform: æ˜¯å¦å‡åŒ€é‡‡æ ·ï¼ˆå„ç»„ç›¸åŒæ•°é‡ï¼‰
        seed: éšæœºç§å­
        sample_type: é‡‡æ ·æ–¹å¼ï¼ˆç”¨äºç»„å†…é‡‡æ ·ï¼‰

    Returns:
        é‡‡æ ·åçš„æ•°æ®åˆ—è¡¨
    """
    import random
    from collections import defaultdict

    if seed is not None:
        random.seed(seed)

    # åŠ è½½æ•°æ®
    data = load_data(str(filepath))
    total = len(data)

    if num <= 0 or num > total:
        num = total

    # æŒ‰å­—æ®µåˆ†ç»„
    groups: Dict[Any, List[Dict]] = defaultdict(list)
    for item in data:
        key = item.get(stratify_field, "__null__")
        groups[key].append(item)

    group_keys = list(groups.keys())
    num_groups = len(group_keys)

    # æ‰“å°åˆ†ç»„ä¿¡æ¯
    print(f"ğŸ“Š åˆ†å±‚é‡‡æ ·: å­—æ®µ={stratify_field}, å…± {num_groups} ç»„")
    for key in sorted(group_keys, key=lambda x: -len(groups[x])):
        count = len(groups[key])
        pct = count / total * 100
        display_key = key if key != "__null__" else "[ç©ºå€¼]"
        print(f"   {display_key}: {count} æ¡ ({pct:.1f}%)")

    # è®¡ç®—å„ç»„é‡‡æ ·æ•°é‡
    if uniform:
        # å‡åŒ€é‡‡æ ·ï¼šå„ç»„æ•°é‡ç›¸ç­‰
        per_group = num // num_groups
        remainder = num % num_groups
        sample_counts = {key: per_group for key in group_keys}
        # ä½™æ•°åˆ†é…ç»™æ•°æ®é‡æœ€å¤šçš„ç»„
        for key in sorted(group_keys, key=lambda x: -len(groups[x]))[:remainder]:
            sample_counts[key] += 1
    else:
        # æŒ‰æ¯”ä¾‹é‡‡æ ·ï¼šä¿æŒåŸæœ‰æ¯”ä¾‹
        sample_counts = {}
        allocated = 0
        # æŒ‰ç»„å¤§å°é™åºå¤„ç†ï¼Œç¡®ä¿å°ç»„ä¹Ÿèƒ½åˆ†åˆ°
        sorted_keys = sorted(group_keys, key=lambda x: -len(groups[x]))
        for i, key in enumerate(sorted_keys):
            if i == len(sorted_keys) - 1:
                # æœ€åä¸€ç»„åˆ†é…å‰©ä½™
                sample_counts[key] = num - allocated
            else:
                # æŒ‰æ¯”ä¾‹è®¡ç®—
                ratio = len(groups[key]) / total
                count = int(num * ratio)
                # ç¡®ä¿è‡³å°‘ 1 æ¡ï¼ˆå¦‚æœç»„æœ‰æ•°æ®ï¼‰
                count = max(1, count) if groups[key] else 0
                sample_counts[key] = count
                allocated += count

    # æ‰§è¡Œå„ç»„é‡‡æ ·
    result = []
    print(f"ğŸ”„ æ‰§è¡Œé‡‡æ ·...")
    for key in group_keys:
        group_data = groups[key]
        target = min(sample_counts[key], len(group_data))

        if target <= 0:
            continue

        # ç»„å†…é‡‡æ ·
        if sample_type == "random":
            sampled = random.sample(group_data, target)
        elif sample_type == "head":
            sampled = group_data[:target]
        else:  # tail
            sampled = group_data[-target:]

        result.extend(sampled)

    # æ‰“å°é‡‡æ ·ç»“æœ
    print(f"\nğŸ“‹ é‡‡æ ·ç»“æœ:")
    result_groups: Dict[Any, int] = defaultdict(int)
    for item in result:
        key = item.get(stratify_field, "__null__")
        result_groups[key] += 1

    for key in sorted(group_keys, key=lambda x: -len(groups[x])):
        orig = len(groups[key])
        sampled_count = result_groups.get(key, 0)
        display_key = key if key != "__null__" else "[ç©ºå€¼]"
        print(f"   {display_key}: {orig} â†’ {sampled_count}")

    print(f"\nâœ… æ€»è®¡: {total} â†’ {len(result)} æ¡")

    return result


def head(
    filename: str,
    num: int = 10,
    output: Optional[str] = None,
) -> None:
    """
    æ˜¾ç¤ºæ–‡ä»¶çš„å‰ N æ¡æ•°æ®ï¼ˆdt sample --sample_type=head çš„å¿«æ·æ–¹å¼ï¼‰ã€‚

    Args:
        filename: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒ csv/excel/jsonl/json/parquet/arrow/feather æ ¼å¼
        num: æ˜¾ç¤ºæ•°é‡ï¼Œé»˜è®¤ 10
            - num > 0: æ˜¾ç¤ºæŒ‡å®šæ•°é‡
            - num = 0: æ˜¾ç¤ºæ‰€æœ‰æ•°æ®
            - num < 0: Python åˆ‡ç‰‡é£æ ¼ï¼ˆå¦‚ -10 è¡¨ç¤ºæœ€å 10 æ¡ï¼‰
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¸æŒ‡å®šåˆ™æ‰“å°åˆ°æ§åˆ¶å°

    Examples:
        dt head data.jsonl          # æ˜¾ç¤ºå‰ 10 æ¡
        dt head data.jsonl 20       # æ˜¾ç¤ºå‰ 20 æ¡
        dt head data.csv 0          # æ˜¾ç¤ºæ‰€æœ‰æ•°æ®
        dt head data.xlsx --output=head.jsonl
    """
    sample(filename, num=num, sample_type="head", output=output)


def tail(
    filename: str,
    num: int = 10,
    output: Optional[str] = None,
) -> None:
    """
    æ˜¾ç¤ºæ–‡ä»¶çš„å N æ¡æ•°æ®ï¼ˆdt sample --sample_type=tail çš„å¿«æ·æ–¹å¼ï¼‰ã€‚

    Args:
        filename: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒ csv/excel/jsonl/json/parquet/arrow/feather æ ¼å¼
        num: æ˜¾ç¤ºæ•°é‡ï¼Œé»˜è®¤ 10
            - num > 0: æ˜¾ç¤ºæŒ‡å®šæ•°é‡
            - num = 0: æ˜¾ç¤ºæ‰€æœ‰æ•°æ®
            - num < 0: Python åˆ‡ç‰‡é£æ ¼ï¼ˆå¦‚ -10 è¡¨ç¤ºæœ€å 10 æ¡ï¼‰
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¸æŒ‡å®šåˆ™æ‰“å°åˆ°æ§åˆ¶å°

    Examples:
        dt tail data.jsonl          # æ˜¾ç¤ºå 10 æ¡
        dt tail data.jsonl 20       # æ˜¾ç¤ºå 20 æ¡
        dt tail data.csv 0          # æ˜¾ç¤ºæ‰€æœ‰æ•°æ®
        dt tail data.xlsx --output=tail.jsonl
    """
    sample(filename, num=num, sample_type="tail", output=output)


def _print_samples(samples: list) -> None:
    """æ‰“å°é‡‡æ ·ç»“æœã€‚"""
    if not samples:
        print("æ²¡æœ‰æ•°æ®")
        return

    try:
        from rich.console import Console
        from rich.json import JSON
        from rich.table import Table

        console = Console()

        # å°è¯•ä»¥è¡¨æ ¼å½¢å¼å±•ç¤º
        if isinstance(samples[0], dict):
            keys = list(samples[0].keys())
            # é€‚åˆè¡¨æ ¼å±•ç¤ºï¼šå­—æ®µä¸å¤ªå¤šä¸”å€¼ä¸å¤ªé•¿
            if len(keys) <= 5 and all(
                len(str(s.get(k, ""))) < 100 for s in samples[:3] for k in keys
            ):
                table = Table(title=f"é‡‡æ ·ç»“æœ ({len(samples)} æ¡)")
                for key in keys:
                    table.add_column(key, overflow="fold")
                for item in samples:
                    table.add_row(*[str(item.get(k, "")) for k in keys])
                console.print(table)
                return

        # ä»¥ JSON å½¢å¼å±•ç¤º
        for i, item in enumerate(samples, 1):
            console.print(f"\n[bold cyan]--- ç¬¬ {i} æ¡ ---[/bold cyan]")
            console.print(JSON.from_data(item))

    except ImportError:
        # æ²¡æœ‰ richï¼Œä½¿ç”¨æ™®é€šæ‰“å°
        import json

        for i, item in enumerate(samples, 1):
            print(f"\n--- ç¬¬ {i} æ¡ ---")
            print(json.dumps(item, ensure_ascii=False, indent=2))

    print(f"\nå…± {len(samples)} æ¡æ•°æ®")


# ============ Transform Command ============

CONFIG_DIR = ".dt"


def _get_config_path(input_path: Path, config_override: Optional[str] = None) -> Path:
    """è·å–é…ç½®æ–‡ä»¶è·¯å¾„"""
    if config_override:
        return Path(config_override)

    # ä½¿ç”¨è¾“å…¥æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ä½œä¸ºé…ç½®æ–‡ä»¶å
    config_name = input_path.stem + ".py"
    return input_path.parent / CONFIG_DIR / config_name


def transform(
    filename: str,
    num: Optional[int] = None,
    preset: Optional[str] = None,
    config: Optional[str] = None,
    output: Optional[str] = None,
) -> None:
    """
    è½¬æ¢æ•°æ®æ ¼å¼ã€‚

    ä¸¤ç§ä½¿ç”¨æ–¹å¼ï¼š
    1. é…ç½®æ–‡ä»¶æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰ï¼šè‡ªåŠ¨ç”Ÿæˆé…ç½®æ–‡ä»¶ï¼Œç¼–è¾‘åå†æ¬¡è¿è¡Œ
    2. é¢„è®¾æ¨¡å¼ï¼šä½¿ç”¨ --preset ç›´æ¥è½¬æ¢

    Args:
        filename: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒ csv/excel/jsonl/json/parquet/arrow/feather æ ¼å¼
        num: åªè½¬æ¢å‰ N æ¡æ•°æ®ï¼ˆå¯é€‰ï¼‰
        preset: ä½¿ç”¨é¢„è®¾æ¨¡æ¿ï¼ˆopenai_chat, alpaca, sharegpt, dpo_pair, simple_qaï¼‰
        config: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ .dt/<filename>.pyï¼‰
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„

    Examples:
        dt transform data.jsonl                        # é¦–æ¬¡ç”Ÿæˆé…ç½®
        dt transform data.jsonl 10                     # åªè½¬æ¢å‰ 10 æ¡
        dt transform data.jsonl --preset=openai_chat   # ä½¿ç”¨é¢„è®¾
        dt transform data.jsonl 100 --preset=alpaca    # é¢„è®¾ + é™åˆ¶æ•°é‡
    """
    filepath = Path(filename)
    if not filepath.exists():
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {filename}")
        return

    if not _check_file_format(filepath):
        return

    # é¢„è®¾æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨é¢„è®¾è½¬æ¢
    if preset:
        _execute_preset_transform(filepath, preset, output, num)
        return

    # é…ç½®æ–‡ä»¶æ¨¡å¼
    config_path = _get_config_path(filepath, config)

    if not config_path.exists():
        _generate_config(filepath, config_path)
    else:
        _execute_transform(filepath, config_path, output, num)


def _generate_config(input_path: Path, config_path: Path) -> None:
    """åˆ†æè¾“å…¥æ•°æ®å¹¶ç”Ÿæˆé…ç½®æ–‡ä»¶"""
    print(f"ğŸ“Š åˆ†æè¾“å…¥æ•°æ®: {input_path}")

    # è¯»å–æ•°æ®
    try:
        data = load_data(str(input_path))
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ - {e}")
        return

    if not data:
        print("é”™è¯¯: æ–‡ä»¶ä¸ºç©º")
        return

    total_count = len(data)
    sample_item = data[0]

    print(f"   æ£€æµ‹åˆ° {total_count} æ¡æ•°æ®")

    # ç”Ÿæˆé…ç½®å†…å®¹
    config_content = _build_config_content(sample_item, input_path.name, total_count)

    # ç¡®ä¿é…ç½®ç›®å½•å­˜åœ¨
    config_path.parent.mkdir(parents=True, exist_ok=True)

    # å†™å…¥é…ç½®æ–‡ä»¶
    config_path.write_text(config_content, encoding="utf-8")

    print(f"\nğŸ“ å·²ç”Ÿæˆé…ç½®æ–‡ä»¶: {config_path}")
    print("\nğŸ‘‰ ä¸‹ä¸€æ­¥:")
    print(f"   1. ç¼–è¾‘ {config_path}ï¼Œå®šä¹‰ transform å‡½æ•°")
    print(f"   2. å†æ¬¡æ‰§è¡Œ dt transform {input_path.name} å®Œæˆè½¬æ¢")


def _build_config_content(sample: Dict[str, Any], filename: str, total: int) -> str:
    """æ„å»ºé…ç½®æ–‡ä»¶å†…å®¹"""
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # ç”Ÿæˆ Item ç±»çš„å­—æ®µå®šä¹‰
    fields_def = _generate_fields_definition(sample)

    # ç”Ÿæˆé»˜è®¤çš„ transform å‡½æ•°ï¼ˆç®€å•é‡å‘½åï¼‰
    field_names = list(sample.keys())

    # ç”Ÿæˆè§„èŒƒåŒ–çš„å­—æ®µåç”¨äºç¤ºä¾‹
    safe_field1 = _sanitize_field_name(field_names[0])[0] if field_names else "field1"
    safe_field2 = _sanitize_field_name(field_names[1])[0] if len(field_names) > 1 else "field2"

    # ç”Ÿæˆé»˜è®¤è¾“å‡ºæ–‡ä»¶å
    base_name = Path(filename).stem
    output_filename = f"{base_name}_output.jsonl"

    config = f'''"""
DataTransformer é…ç½®æ–‡ä»¶
ç”Ÿæˆæ—¶é—´: {now}
è¾“å…¥æ–‡ä»¶: {filename} ({total} æ¡)
"""


# ===== è¾“å…¥æ•°æ®ç»“æ„ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼ŒIDE å¯è¡¥å…¨ï¼‰=====

class Item:
{fields_def}


# ===== å®šä¹‰è½¬æ¢é€»è¾‘ =====
# æç¤ºï¼šè¾“å…¥ item. å IDE ä¼šè‡ªåŠ¨è¡¥å…¨å¯ç”¨å­—æ®µ

def transform(item: Item):
    return {{
{_generate_default_transform(field_names)}
    }}


# è¾“å‡ºæ–‡ä»¶è·¯å¾„
output = "{output_filename}"


# ===== ç¤ºä¾‹ =====
#
# ç¤ºä¾‹1: æ„å»º OpenAI Chat æ ¼å¼
# def transform(item: Item):
#     return {{
#         "messages": [
#             {{"role": "user", "content": item.{safe_field1}}},
#             {{"role": "assistant", "content": item.{safe_field2}}},
#         ]
#     }}
#
# ç¤ºä¾‹2: Alpaca æ ¼å¼
# def transform(item: Item):
#     return {{
#         "instruction": item.{safe_field1},
#         "input": "",
#         "output": item.{safe_field2},
#     }}
'''
    return config


def _generate_fields_definition(sample: Dict[str, Any], indent: int = 4) -> str:
    """ç”Ÿæˆ Item ç±»çš„å­—æ®µå®šä¹‰"""
    lines = []
    prefix = " " * indent

    for key, value in sample.items():
        type_name = _get_type_name(value)
        example = _format_example_value(value)
        safe_key, changed = _sanitize_field_name(key)
        comment = f"  # åŸå­—æ®µå: {key}" if changed else ""
        lines.append(f"{prefix}{safe_key}: {type_name} = {example}{comment}")

    return "\n".join(lines) if lines else f"{prefix}pass"


def _get_type_name(value: Any) -> str:
    """è·å–å€¼çš„ç±»å‹åç§°"""
    if value is None:
        return "str"
    if isinstance(value, str):
        return "str"
    if isinstance(value, bool):
        return "bool"
    if isinstance(value, int):
        return "int"
    if isinstance(value, float):
        return "float"
    if isinstance(value, list):
        return "list"
    if isinstance(value, dict):
        return "dict"
    return "str"


def _format_example_value(value: Any, max_len: int = 50) -> str:
    """æ ¼å¼åŒ–ç¤ºä¾‹å€¼"""
    if value is None:
        return '""'
    if isinstance(value, str):
        # æˆªæ–­é•¿å­—ç¬¦ä¸²
        if len(value) > max_len:
            value = value[:max_len] + "..."
        # è½¬ä¹‰å¹¶åŠ å¼•å·
        escaped = value.replace("\\", "\\\\").replace('"', '\\"').replace("\n", "\\n")
        return f'"{escaped}"'
    if isinstance(value, bool):
        return str(value)
    if isinstance(value, (int, float)):
        return str(value)
    if isinstance(value, (list, dict)):
        s = json.dumps(value, ensure_ascii=False)
        if len(s) > max_len:
            return f"{s[:max_len]}..."
        return s
    return '""'


def _sanitize_field_name(name: str) -> tuple:
    """
    å°†å­—æ®µåè§„èŒƒåŒ–ä¸ºåˆæ³•çš„ Python æ ‡è¯†ç¬¦ã€‚

    Returns:
        tuple: (è§„èŒƒåŒ–åçš„åç§°, æ˜¯å¦è¢«ä¿®æ”¹)
    """
    if name.isidentifier():
        return name, False

    # æ›¿æ¢å¸¸è§çš„éæ³•å­—ç¬¦
    sanitized = name.replace("-", "_").replace(" ", "_").replace(".", "_")

    # å¦‚æœä»¥æ•°å­—å¼€å¤´ï¼Œæ·»åŠ å‰ç¼€
    if sanitized and sanitized[0].isdigit():
        sanitized = "f_" + sanitized

    # ç§»é™¤å…¶ä»–éæ³•å­—ç¬¦
    sanitized = "".join(c if c.isalnum() or c == "_" else "_" for c in sanitized)

    # ç¡®ä¿ä¸ä¸ºç©º
    if not sanitized:
        sanitized = "field"

    return sanitized, True


def _generate_default_transform(field_names: List[str]) -> str:
    """ç”Ÿæˆé»˜è®¤çš„ transform å‡½æ•°ä½“"""
    lines = []
    for name in field_names[:5]:  # æœ€å¤šæ˜¾ç¤º 5 ä¸ªå­—æ®µ
        safe_name, _ = _sanitize_field_name(name)
        lines.append(f'        "{name}": item.{safe_name},')
    return "\n".join(lines) if lines else '        # åœ¨è¿™é‡Œå®šä¹‰è¾“å‡ºå­—æ®µ'


def _execute_transform(
    input_path: Path,
    config_path: Path,
    output_override: Optional[str],
    num: Optional[int],
) -> None:
    """æ‰§è¡Œæ•°æ®è½¬æ¢"""
    print(f"ğŸ“‚ åŠ è½½é…ç½®: {config_path}")

    # åŠ¨æ€åŠ è½½é…ç½®æ–‡ä»¶
    try:
        config_ns = _load_config(config_path)
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ - {e}")
        return

    # è·å– transform å‡½æ•°
    if "transform" not in config_ns:
        print("é”™è¯¯: é…ç½®æ–‡ä»¶ä¸­æœªå®šä¹‰ transform å‡½æ•°")
        return

    transform_func = config_ns["transform"]

    # è·å–è¾“å‡ºè·¯å¾„
    output_path = output_override or config_ns.get("output", "output.jsonl")

    # åŠ è½½æ•°æ®å¹¶ä½¿ç”¨ DataTransformer æ‰§è¡Œè½¬æ¢
    print(f"ğŸ“Š åŠ è½½æ•°æ®: {input_path}")
    try:
        dt = DataTransformer.load(str(input_path))
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ - {e}")
        return

    total = len(dt)
    if num:
        dt = DataTransformer(dt.data[:num])
        print(f"   å¤„ç†å‰ {len(dt)}/{total} æ¡æ•°æ®")
    else:
        print(f"   å…± {total} æ¡æ•°æ®")

    # æ‰§è¡Œè½¬æ¢ï¼ˆä½¿ç”¨ Core çš„ to æ–¹æ³•ï¼Œè‡ªåŠ¨æ”¯æŒå±æ€§è®¿é—®ï¼‰
    print("ğŸ”„ æ‰§è¡Œè½¬æ¢...")
    try:
        results = dt.to(transform_func)
    except Exception as e:
        print(f"é”™è¯¯: è½¬æ¢å¤±è´¥ - {e}")
        import traceback
        traceback.print_exc()
        return

    # ä¿å­˜ç»“æœ
    print(f"ğŸ’¾ ä¿å­˜ç»“æœ: {output_path}")
    try:
        save_data(results, output_path)
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•ä¿å­˜æ–‡ä»¶ - {e}")
        return

    print(f"\nâœ… å®Œæˆ! å·²è½¬æ¢ {len(results)} æ¡æ•°æ®åˆ° {output_path}")


def _execute_preset_transform(
    input_path: Path,
    preset_name: str,
    output_override: Optional[str],
    num: Optional[int],
) -> None:
    """ä½¿ç”¨é¢„è®¾æ¨¡æ¿æ‰§è¡Œè½¬æ¢"""
    print(f"ğŸ“‚ ä½¿ç”¨é¢„è®¾: {preset_name}")

    # è·å–é¢„è®¾å‡½æ•°
    try:
        transform_func = get_preset(preset_name)
    except ValueError as e:
        print(f"é”™è¯¯: {e}")
        print(f"å¯ç”¨é¢„è®¾: {', '.join(list_presets())}")
        return

    # åŠ è½½æ•°æ®
    print(f"ğŸ“Š åŠ è½½æ•°æ®: {input_path}")
    try:
        dt = DataTransformer.load(str(input_path))
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ - {e}")
        return

    total = len(dt)
    if num:
        dt = DataTransformer(dt.data[:num])
        print(f"   å¤„ç†å‰ {len(dt)}/{total} æ¡æ•°æ®")
    else:
        print(f"   å…± {total} æ¡æ•°æ®")

    # æ‰§è¡Œè½¬æ¢
    print("ğŸ”„ æ‰§è¡Œè½¬æ¢...")
    try:
        results = dt.to(transform_func)
    except Exception as e:
        print(f"é”™è¯¯: è½¬æ¢å¤±è´¥ - {e}")
        import traceback
        traceback.print_exc()
        return

    # ä¿å­˜ç»“æœ
    output_path = output_override or f"{input_path.stem}_{preset_name}.jsonl"
    print(f"ğŸ’¾ ä¿å­˜ç»“æœ: {output_path}")
    try:
        save_data(results, output_path)
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•ä¿å­˜æ–‡ä»¶ - {e}")
        return

    print(f"\nâœ… å®Œæˆ! å·²è½¬æ¢ {len(results)} æ¡æ•°æ®åˆ° {output_path}")


def _load_config(config_path: Path) -> Dict[str, Any]:
    """åŠ¨æ€åŠ è½½ Python é…ç½®æ–‡ä»¶"""
    import importlib.util

    spec = importlib.util.spec_from_file_location("dt_config", config_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)

    return {name: getattr(module, name) for name in dir(module) if not name.startswith("_")}


# ============ Dedupe Command ============


def dedupe(
    filename: str,
    key: Optional[str] = None,
    similar: Optional[float] = None,
    output: Optional[str] = None,
) -> None:
    """
    æ•°æ®å»é‡ã€‚

    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. ç²¾ç¡®å»é‡ï¼ˆé»˜è®¤ï¼‰ï¼šå®Œå…¨ç›¸åŒçš„æ•°æ®æ‰å»é‡
    2. ç›¸ä¼¼åº¦å»é‡ï¼šä½¿ç”¨ MinHash+LSH ç®—æ³•ï¼Œç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼åˆ™å»é‡

    Args:
        filename: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒ csv/excel/jsonl/json/parquet/arrow/feather æ ¼å¼
        key: å»é‡ä¾æ®å­—æ®µï¼Œå¤šä¸ªå­—æ®µç”¨é€—å·åˆ†éš”ã€‚ä¸æŒ‡å®šåˆ™å…¨é‡å»é‡
        similar: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰ï¼ŒæŒ‡å®šåå¯ç”¨ç›¸ä¼¼åº¦å»é‡æ¨¡å¼ï¼Œéœ€è¦æŒ‡å®š --key
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¸æŒ‡å®šåˆ™è¦†ç›–åŸæ–‡ä»¶

    Examples:
        dt dedupe data.jsonl                       # å…¨é‡ç²¾ç¡®å»é‡
        dt dedupe data.jsonl --key=text            # æŒ‰ text å­—æ®µç²¾ç¡®å»é‡
        dt dedupe data.jsonl --key=user,timestamp  # æŒ‰å¤šå­—æ®µç»„åˆç²¾ç¡®å»é‡
        dt dedupe data.jsonl --key=text --similar=0.8   # ç›¸ä¼¼åº¦å»é‡
        dt dedupe data.jsonl --output=clean.jsonl  # æŒ‡å®šè¾“å‡ºæ–‡ä»¶
    """
    filepath = Path(filename)

    if not filepath.exists():
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {filename}")
        return

    if not _check_file_format(filepath):
        return

    # ç›¸ä¼¼åº¦å»é‡æ¨¡å¼å¿…é¡»æŒ‡å®š key
    if similar is not None and not key:
        print("é”™è¯¯: ç›¸ä¼¼åº¦å»é‡éœ€è¦æŒ‡å®š --key å‚æ•°")
        return

    if similar is not None and (similar <= 0 or similar > 1):
        print("é”™è¯¯: --similar å‚æ•°å¿…é¡»åœ¨ 0-1 ä¹‹é—´")
        return

    # åŠ è½½æ•°æ®
    print(f"ğŸ“Š åŠ è½½æ•°æ®: {filepath}")
    try:
        dt = DataTransformer.load(str(filepath))
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ - {e}")
        return

    original_count = len(dt)
    print(f"   å…± {original_count} æ¡æ•°æ®")

    # æ‰§è¡Œå»é‡
    if similar is not None:
        # ç›¸ä¼¼åº¦å»é‡æ¨¡å¼
        print(f"ğŸ”‘ ç›¸ä¼¼åº¦å»é‡: å­—æ®µ={key}, é˜ˆå€¼={similar}")
        print("ğŸ”„ æ‰§è¡Œå»é‡ï¼ˆMinHash+LSHï¼‰...")
        try:
            result = dt.dedupe_similar(key, threshold=similar)
        except ImportError as e:
            print(f"é”™è¯¯: {e}")
            return
    else:
        # ç²¾ç¡®å»é‡æ¨¡å¼
        dedupe_key: Any = None
        if key:
            keys = [k.strip() for k in key.split(",")]
            if len(keys) == 1:
                dedupe_key = keys[0]
                print(f"ğŸ”‘ æŒ‰å­—æ®µç²¾ç¡®å»é‡: {dedupe_key}")
            else:
                dedupe_key = keys
                print(f"ğŸ”‘ æŒ‰å¤šå­—æ®µç»„åˆç²¾ç¡®å»é‡: {', '.join(dedupe_key)}")
        else:
            print("ğŸ”‘ å…¨é‡ç²¾ç¡®å»é‡")

        print("ğŸ”„ æ‰§è¡Œå»é‡...")
        result = dt.dedupe(dedupe_key)

    dedupe_count = len(result)
    removed_count = original_count - dedupe_count

    # ä¿å­˜ç»“æœ
    output_path = output or str(filepath)
    print(f"ğŸ’¾ ä¿å­˜ç»“æœ: {output_path}")
    try:
        result.save(output_path)
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•ä¿å­˜æ–‡ä»¶ - {e}")
        return

    print(f"\nâœ… å®Œæˆ! å»é™¤ {removed_count} æ¡é‡å¤æ•°æ®ï¼Œå‰©ä½™ {dedupe_count} æ¡")


# ============ Concat Command ============


def concat(
    *files: str,
    output: Optional[str] = None,
    strict: bool = False,
) -> None:
    """
    æ‹¼æ¥å¤šä¸ªæ•°æ®æ–‡ä»¶ã€‚

    Args:
        *files: è¾“å…¥æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œæ”¯æŒ csv/excel/jsonl/json/parquet/arrow/feather æ ¼å¼
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¿…é¡»æŒ‡å®š
        strict: ä¸¥æ ¼æ¨¡å¼ï¼Œå­—æ®µå¿…é¡»å®Œå…¨ä¸€è‡´ï¼Œå¦åˆ™æŠ¥é”™

    Examples:
        dt concat a.jsonl b.jsonl -o merged.jsonl
        dt concat data1.csv data2.csv data3.csv -o all.jsonl
        dt concat a.jsonl b.jsonl --strict -o merged.jsonl
    """
    if len(files) < 2:
        print("é”™è¯¯: è‡³å°‘éœ€è¦ä¸¤ä¸ªæ–‡ä»¶")
        return

    if not output:
        print("é”™è¯¯: å¿…é¡»æŒ‡å®šè¾“å‡ºæ–‡ä»¶ (-o/--output)")
        return

    # éªŒè¯æ‰€æœ‰æ–‡ä»¶
    file_paths = []
    for f in files:
        filepath = Path(f)
        if not filepath.exists():
            print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {f}")
            return
        if not _check_file_format(filepath):
            return
        file_paths.append(filepath)

    # åˆ†æå„æ–‡ä»¶çš„å­—æ®µ
    print("ğŸ“Š æ–‡ä»¶å­—æ®µåˆ†æ:")
    file_infos = []  # [(filepath, data, fields, count)]

    for filepath in file_paths:
        try:
            data = load_data(str(filepath))
        except Exception as e:
            print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ {filepath} - {e}")
            return

        if not data:
            print(f"è­¦å‘Š: æ–‡ä»¶ä¸ºç©º - {filepath}")
            fields = set()
        else:
            fields = set(data[0].keys())

        file_infos.append((filepath, data, fields, len(data)))
        fields_str = ", ".join(sorted(fields)) if fields else "(ç©º)"
        print(f"   {filepath.name}: {fields_str} ({len(data)} æ¡)")

    # åˆ†æå­—æ®µå·®å¼‚
    all_fields = set()
    common_fields = None
    for _, _, fields, _ in file_infos:
        all_fields.update(fields)
        if common_fields is None:
            common_fields = fields.copy()
        else:
            common_fields &= fields

    common_fields = common_fields or set()
    diff_fields = all_fields - common_fields

    if diff_fields:
        if strict:
            print(f"\nâŒ ä¸¥æ ¼æ¨¡å¼: å­—æ®µä¸ä¸€è‡´")
            print(f"   å…±åŒå­—æ®µ: {', '.join(sorted(common_fields)) or '(æ— )'}")
            print(f"   å·®å¼‚å­—æ®µ: {', '.join(sorted(diff_fields))}")
            return
        else:
            print(f"\nâš  å­—æ®µå·®å¼‚: {', '.join(sorted(diff_fields))} ä»…åœ¨éƒ¨åˆ†æ–‡ä»¶ä¸­å­˜åœ¨")

    # æ‰§è¡Œæ‹¼æ¥
    print("\nğŸ”„ æ‰§è¡Œæ‹¼æ¥...")
    all_data = []
    for _, data, _, _ in file_infos:
        all_data.extend(data)

    # ä¿å­˜ç»“æœ
    print(f"ğŸ’¾ ä¿å­˜ç»“æœ: {output}")
    try:
        save_data(all_data, output)
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•ä¿å­˜æ–‡ä»¶ - {e}")
        return

    total_count = len(all_data)
    file_count = len(files)
    print(f"\nâœ… å®Œæˆ! å·²åˆå¹¶ {file_count} ä¸ªæ–‡ä»¶ï¼Œå…± {total_count} æ¡æ•°æ®åˆ° {output}")


# ============ Stats Command ============


def stats(
    filename: str,
    top: int = 10,
) -> None:
    """
    æ˜¾ç¤ºæ•°æ®æ–‡ä»¶çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆç±»ä¼¼ pandas df.info() + df.describe()ï¼‰ã€‚

    Args:
        filename: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒ csv/excel/jsonl/json/parquet/arrow/feather æ ¼å¼
        top: æ˜¾ç¤ºé¢‘ç‡æœ€é«˜çš„å‰ N ä¸ªå€¼ï¼Œé»˜è®¤ 10

    Examples:
        dt stats data.jsonl
        dt stats data.csv --top=5
    """
    filepath = Path(filename)

    if not filepath.exists():
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {filename}")
        return

    if not _check_file_format(filepath):
        return

    # åŠ è½½æ•°æ®
    try:
        data = load_data(str(filepath))
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ - {e}")
        return

    if not data:
        print("æ–‡ä»¶ä¸ºç©º")
        return

    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total = len(data)
    field_stats = _compute_field_stats(data, top)

    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    _print_stats(filepath.name, total, field_stats)


def _compute_field_stats(data: List[Dict], top: int) -> List[Dict[str, Any]]:
    """
    å•æ¬¡éå†è®¡ç®—æ¯ä¸ªå­—æ®µçš„ç»Ÿè®¡ä¿¡æ¯ã€‚

    ä¼˜åŒ–ï¼šå°†å¤šæ¬¡éå†åˆå¹¶ä¸ºå•æ¬¡éå†ï¼Œåœ¨éå†è¿‡ç¨‹ä¸­åŒæ—¶æ”¶é›†æ‰€æœ‰ç»Ÿè®¡æ•°æ®ã€‚
    """
    from collections import Counter, defaultdict

    if not data:
        return []

    total = len(data)

    # å•æ¬¡éå†æ”¶é›†æ‰€æœ‰å­—æ®µçš„å€¼å’Œç»Ÿè®¡ä¿¡æ¯
    field_values = defaultdict(list)  # å­˜å‚¨æ¯ä¸ªå­—æ®µçš„æ‰€æœ‰å€¼
    field_counters = defaultdict(Counter)  # å­˜å‚¨æ¯ä¸ªå­—æ®µçš„å€¼é¢‘ç‡ï¼ˆç”¨äº top Nï¼‰

    for item in data:
        for k, v in item.items():
            field_values[k].append(v)
            # å¯¹å€¼è¿›è¡Œæˆªæ–­åè®¡æ•°ï¼ˆç”¨äº top N æ˜¾ç¤ºï¼‰
            displayable = _truncate(v if v is not None else "", 30)
            field_counters[k][displayable] += 1

    # æ ¹æ®æ”¶é›†çš„æ•°æ®è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    stats_list = []
    for field in sorted(field_values.keys()):
        values = field_values[field]
        non_null = [v for v in values if v is not None and v != ""]
        non_null_count = len(non_null)

        # æ¨æ–­ç±»å‹ï¼ˆä»ç¬¬ä¸€ä¸ªéç©ºå€¼ï¼‰
        field_type = _infer_type(non_null)

        # åŸºç¡€ç»Ÿè®¡
        stat = {
            "field": field,
            "non_null": non_null_count,
            "null_rate": f"{(total - non_null_count) / total * 100:.1f}%",
            "type": field_type,
        }

        # ç±»å‹ç‰¹å®šç»Ÿè®¡
        if non_null:
            # å”¯ä¸€å€¼è®¡æ•°
            stat["unique"] = len(set(str(v) for v in non_null))

            # å­—ç¬¦ä¸²ç±»å‹ï¼šè®¡ç®—é•¿åº¦ç»Ÿè®¡
            if field_type == "str":
                lengths = [len(str(v)) for v in non_null]
                stat["len_min"] = min(lengths)
                stat["len_max"] = max(lengths)
                stat["len_avg"] = sum(lengths) / len(lengths)

            # æ•°å€¼ç±»å‹ï¼šè®¡ç®—æ•°å€¼ç»Ÿè®¡
            elif field_type in ("int", "float"):
                nums = [float(v) for v in non_null if _is_numeric(v)]
                if nums:
                    stat["min"] = min(nums)
                    stat["max"] = max(nums)
                    stat["avg"] = sum(nums) / len(nums)

            # åˆ—è¡¨ç±»å‹ï¼šè®¡ç®—é•¿åº¦ç»Ÿè®¡
            elif field_type == "list":
                lengths = [len(v) if isinstance(v, list) else 0 for v in non_null]
                stat["len_min"] = min(lengths)
                stat["len_max"] = max(lengths)
                stat["len_avg"] = sum(lengths) / len(lengths)

            # Top N å€¼ï¼ˆå·²åœ¨éå†æ—¶æ”¶é›†ï¼‰
            stat["top_values"] = field_counters[field].most_common(top)

        stats_list.append(stat)

    return stats_list


def _infer_type(values: List[Any]) -> str:
    """æ¨æ–­å­—æ®µç±»å‹"""
    if not values:
        return "unknown"

    sample = values[0]
    if isinstance(sample, bool):
        return "bool"
    if isinstance(sample, int):
        return "int"
    if isinstance(sample, float):
        return "float"
    if isinstance(sample, list):
        return "list"
    if isinstance(sample, dict):
        return "dict"
    return "str"


def _is_numeric(v: Any) -> bool:
    """æ£€æŸ¥å€¼æ˜¯å¦ä¸ºæ•°å€¼"""
    if isinstance(v, (int, float)) and not isinstance(v, bool):
        return True
    return False


def _truncate(v: Any, max_width: int) -> str:
    """æŒ‰æ˜¾ç¤ºå®½åº¦æˆªæ–­å€¼ï¼ˆä¸­æ–‡å­—ç¬¦ç®— 2 å®½åº¦ï¼‰"""
    s = str(v)
    width = 0
    result = []
    for char in s:
        # CJK å­—ç¬¦èŒƒå›´
        if '\u4e00' <= char <= '\u9fff' or '\u3000' <= char <= '\u303f' or '\uff00' <= char <= '\uffef':
            char_width = 2
        else:
            char_width = 1
        if width + char_width > max_width - 3:  # é¢„ç•™ ... çš„å®½åº¦
            return ''.join(result) + "..."
        result.append(char)
        width += char_width
    return s


def _display_width(s: str) -> int:
    """è®¡ç®—å­—ç¬¦ä¸²çš„æ˜¾ç¤ºå®½åº¦ï¼ˆä¸­æ–‡å­—ç¬¦ç®— 2ï¼ŒASCII å­—ç¬¦ç®— 1ï¼‰"""
    width = 0
    for char in s:
        # CJK å­—ç¬¦èŒƒå›´
        if '\u4e00' <= char <= '\u9fff' or '\u3000' <= char <= '\u303f' or '\uff00' <= char <= '\uffef':
            width += 2
        else:
            width += 1
    return width


def _pad_to_width(s: str, target_width: int) -> str:
    """å°†å­—ç¬¦ä¸²å¡«å……åˆ°æŒ‡å®šçš„æ˜¾ç¤ºå®½åº¦"""
    current_width = _display_width(s)
    if current_width >= target_width:
        return s
    return s + ' ' * (target_width - current_width)


def _print_stats(filename: str, total: int, field_stats: List[Dict[str, Any]]) -> None:
    """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
    try:
        from rich.console import Console
        from rich.table import Table
        from rich.panel import Panel

        console = Console()

        # æ¦‚è§ˆ
        console.print(Panel(
            f"[bold]æ–‡ä»¶:[/bold] {filename}\n"
            f"[bold]æ€»æ•°:[/bold] {total:,} æ¡\n"
            f"[bold]å­—æ®µ:[/bold] {len(field_stats)} ä¸ª",
            title="ğŸ“Š æ•°æ®æ¦‚è§ˆ",
            expand=False,
        ))

        # å­—æ®µç»Ÿè®¡è¡¨
        table = Table(title="ğŸ“‹ å­—æ®µç»Ÿè®¡", show_header=True, header_style="bold cyan")
        table.add_column("å­—æ®µ", style="green")
        table.add_column("ç±»å‹", style="yellow")
        table.add_column("éç©ºç‡", justify="right")
        table.add_column("å”¯ä¸€å€¼", justify="right")
        table.add_column("ç»Ÿè®¡", style="dim")

        for stat in field_stats:
            non_null_rate = f"{stat['non_null'] / total * 100:.0f}%"
            unique = str(stat.get("unique", "-"))

            # æ„å»ºç»Ÿè®¡ä¿¡æ¯å­—ç¬¦ä¸²
            extra = []
            if "len_avg" in stat:
                extra.append(f"é•¿åº¦: {stat['len_min']}-{stat['len_max']} (avg {stat['len_avg']:.0f})")
            if "avg" in stat:
                if stat["type"] == "int":
                    extra.append(f"èŒƒå›´: {int(stat['min'])}-{int(stat['max'])} (avg {stat['avg']:.1f})")
                else:
                    extra.append(f"èŒƒå›´: {stat['min']:.2f}-{stat['max']:.2f} (avg {stat['avg']:.2f})")

            table.add_row(
                stat["field"],
                stat["type"],
                non_null_rate,
                unique,
                "; ".join(extra) if extra else "-",
            )

        console.print(table)

        # Top å€¼ç»Ÿè®¡ï¼ˆä»…æ˜¾ç¤ºæœ‰æ„ä¹‰çš„å­—æ®µï¼‰
        for stat in field_stats:
            top_values = stat.get("top_values", [])
            if not top_values:
                continue

            # è·³è¿‡æ•°å€¼ç±»å‹ï¼ˆmin/max/avg å·²è¶³å¤Ÿï¼‰
            if stat["type"] in ("int", "float"):
                continue

            # è·³è¿‡å”¯ä¸€å€¼è¿‡å¤šçš„å­—æ®µï¼ˆåŸºæœ¬éƒ½æ˜¯å”¯ä¸€çš„ï¼‰
            unique_ratio = stat.get("unique", 0) / total if total > 0 else 0
            if unique_ratio > 0.9 and stat.get("unique", 0) > 100:
                continue

            console.print(f"\n[bold cyan]{stat['field']}[/bold cyan] å€¼åˆ†å¸ƒ (Top {len(top_values)}):")
            max_count = max(c for _, c in top_values) if top_values else 1
            for value, count in top_values:
                pct = count / total * 100
                bar_len = int(count / max_count * 20)  # æŒ‰ç›¸å¯¹æ¯”ä¾‹ï¼Œæœ€é•¿ 20 å­—ç¬¦
                bar = "â–ˆ" * bar_len
                display_value = value if value else "[ç©º]"
                # ä½¿ç”¨æ˜¾ç¤ºå®½åº¦å¯¹é½ï¼ˆå¤„ç†ä¸­æ–‡å­—ç¬¦ï¼‰
                padded_value = _pad_to_width(display_value, 32)
                console.print(f"  {padded_value} {count:>6} ({pct:>5.1f}%) {bar}")

    except ImportError:
        # æ²¡æœ‰ richï¼Œä½¿ç”¨æ™®é€šæ‰“å°
        print(f"\n{'=' * 50}")
        print(f"ğŸ“Š æ•°æ®æ¦‚è§ˆ")
        print(f"{'=' * 50}")
        print(f"æ–‡ä»¶: {filename}")
        print(f"æ€»æ•°: {total:,} æ¡")
        print(f"å­—æ®µ: {len(field_stats)} ä¸ª")

        print(f"\n{'=' * 50}")
        print(f"ğŸ“‹ å­—æ®µç»Ÿè®¡")
        print(f"{'=' * 50}")
        print(f"{'å­—æ®µ':<20} {'ç±»å‹':<8} {'éç©ºç‡':<8} {'å”¯ä¸€å€¼':<8}")
        print("-" * 50)

        for stat in field_stats:
            non_null_rate = f"{stat['non_null'] / total * 100:.0f}%"
            unique = str(stat.get("unique", "-"))
            print(f"{stat['field']:<20} {stat['type']:<8} {non_null_rate:<8} {unique:<8}")


# ============ Clean Command ============


def clean(
    filename: str,
    drop_empty: Optional[str] = None,
    min_len: Optional[str] = None,
    max_len: Optional[str] = None,
    keep: Optional[str] = None,
    drop: Optional[str] = None,
    strip: bool = False,
    output: Optional[str] = None,
) -> None:
    """
    æ•°æ®æ¸…æ´—ã€‚

    Args:
        filename: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒ csv/excel/jsonl/json/parquet/arrow/feather æ ¼å¼
        drop_empty: åˆ é™¤ç©ºå€¼è®°å½•
            - ä¸å¸¦å€¼ï¼šåˆ é™¤ä»»æ„å­—æ®µä¸ºç©ºçš„è®°å½•
            - æŒ‡å®šå­—æ®µï¼šåˆ é™¤æŒ‡å®šå­—æ®µä¸ºç©ºçš„è®°å½•ï¼ˆé€—å·åˆ†éš”ï¼‰
        min_len: æœ€å°é•¿åº¦è¿‡æ»¤ï¼Œæ ¼å¼ "å­—æ®µ:é•¿åº¦"ï¼ˆå¦‚ text:10ï¼‰
        max_len: æœ€å¤§é•¿åº¦è¿‡æ»¤ï¼Œæ ¼å¼ "å­—æ®µ:é•¿åº¦"ï¼ˆå¦‚ text:1000ï¼‰
        keep: åªä¿ç•™æŒ‡å®šå­—æ®µï¼ˆé€—å·åˆ†éš”ï¼‰
        drop: åˆ é™¤æŒ‡å®šå­—æ®µï¼ˆé€—å·åˆ†éš”ï¼‰
        strip: å»é™¤æ‰€æœ‰å­—ç¬¦ä¸²å­—æ®µçš„é¦–å°¾ç©ºç™½
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¸æŒ‡å®šåˆ™è¦†ç›–åŸæ–‡ä»¶

    Examples:
        dt clean data.jsonl --drop-empty                    # åˆ é™¤ä»»æ„ç©ºå€¼è®°å½•
        dt clean data.jsonl --drop-empty=text,answer        # åˆ é™¤æŒ‡å®šå­—æ®µä¸ºç©ºçš„è®°å½•
        dt clean data.jsonl --min-len=text:10               # text å­—æ®µæœ€å°‘ 10 å­—ç¬¦
        dt clean data.jsonl --max-len=text:1000             # text å­—æ®µæœ€å¤š 1000 å­—ç¬¦
        dt clean data.jsonl --keep=question,answer          # åªä¿ç•™è¿™äº›å­—æ®µ
        dt clean data.jsonl --drop=metadata,timestamp       # åˆ é™¤è¿™äº›å­—æ®µ
        dt clean data.jsonl --strip                         # å»é™¤å­—ç¬¦ä¸²é¦–å°¾ç©ºç™½
        dt clean data.jsonl --drop-empty --strip -o out.jsonl
    """
    filepath = Path(filename)

    if not filepath.exists():
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {filename}")
        return

    if not _check_file_format(filepath):
        return

    # åŠ è½½æ•°æ®
    print(f"ğŸ“Š åŠ è½½æ•°æ®: {filepath}")
    try:
        dt = DataTransformer.load(str(filepath))
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ - {e}")
        return

    original_count = len(dt)
    print(f"   å…± {original_count} æ¡æ•°æ®")

    # è§£æå‚æ•°ï¼ˆfire å¯èƒ½ä¼šå°†é€—å·åˆ†éš”çš„å€¼è§£æä¸ºå…ƒç»„ï¼‰
    min_len_field, min_len_value = _parse_len_param(min_len) if min_len else (None, None)
    max_len_field, max_len_value = _parse_len_param(max_len) if max_len else (None, None)
    keep_fields = _parse_field_list(keep) if keep else None
    drop_fields = _parse_field_list(drop) if drop else None

    # æ„å»ºæ¸…æ´—é…ç½®
    empty_fields = None
    if drop_empty is not None:
        if drop_empty == "" or drop_empty is True:
            print("ğŸ”„ åˆ é™¤ä»»æ„å­—æ®µä¸ºç©ºçš„è®°å½•...")
            empty_fields = []  # ç©ºåˆ—è¡¨è¡¨ç¤ºæ£€æŸ¥æ‰€æœ‰å­—æ®µ
        else:
            empty_fields = _parse_field_list(drop_empty)
            print(f"ğŸ”„ åˆ é™¤å­—æ®µä¸ºç©ºçš„è®°å½•: {', '.join(empty_fields)}")

    if strip:
        print("ğŸ”„ å»é™¤å­—ç¬¦ä¸²é¦–å°¾ç©ºç™½...")
    if min_len_field:
        print(f"ğŸ”„ è¿‡æ»¤ {min_len_field} é•¿åº¦ < {min_len_value} çš„è®°å½•...")
    if max_len_field:
        print(f"ğŸ”„ è¿‡æ»¤ {max_len_field} é•¿åº¦ > {max_len_value} çš„è®°å½•...")
    if keep_fields:
        print(f"ğŸ”„ åªä¿ç•™å­—æ®µ: {', '.join(keep_fields)}")
    if drop_fields:
        print(f"ğŸ”„ åˆ é™¤å­—æ®µ: {', '.join(drop_fields)}")

    # å•æ¬¡éå†æ‰§è¡Œæ‰€æœ‰æ¸…æ´—æ“ä½œ
    data, step_stats = _clean_data_single_pass(
        dt.data,
        strip=strip,
        empty_fields=empty_fields,
        min_len_field=min_len_field,
        min_len_value=min_len_value,
        max_len_field=max_len_field,
        max_len_value=max_len_value,
        keep_fields=keep_fields,
        drop_fields=set(drop_fields) if drop_fields else None,
    )

    # ä¿å­˜ç»“æœ
    final_count = len(data)
    output_path = output or str(filepath)
    print(f"ğŸ’¾ ä¿å­˜ç»“æœ: {output_path}")

    try:
        save_data(data, output_path)
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•ä¿å­˜æ–‡ä»¶ - {e}")
        return

    # æ‰“å°ç»Ÿè®¡
    removed_count = original_count - final_count
    print(f"\nâœ… å®Œæˆ!")
    print(f"   åŸå§‹: {original_count} æ¡ -> æ¸…æ´—å: {final_count} æ¡ (åˆ é™¤ {removed_count} æ¡)")
    if step_stats:
        print(f"   æ­¥éª¤: {' | '.join(step_stats)}")


def _parse_len_param(param: str) -> tuple:
    """è§£æé•¿åº¦å‚æ•°ï¼Œæ ¼å¼ 'field:length'"""
    if ":" not in param:
        raise ValueError(f"é•¿åº¦å‚æ•°æ ¼å¼é”™è¯¯: {param}ï¼Œåº”ä¸º 'å­—æ®µ:é•¿åº¦'")
    parts = param.split(":", 1)
    field = parts[0].strip()
    try:
        length = int(parts[1].strip())
    except ValueError:
        raise ValueError(f"é•¿åº¦å¿…é¡»æ˜¯æ•´æ•°: {parts[1]}")
    return field, length


def _parse_field_list(value: Any) -> List[str]:
    """è§£æå­—æ®µåˆ—è¡¨å‚æ•°ï¼ˆå¤„ç† fire å°†é€—å·åˆ†éš”çš„å€¼è§£æä¸ºå…ƒç»„çš„æƒ…å†µï¼‰"""
    if isinstance(value, (list, tuple)):
        return [str(f).strip() for f in value]
    elif isinstance(value, str):
        return [f.strip() for f in value.split(",")]
    else:
        return [str(value)]


def _is_empty_value(v: Any) -> bool:
    """åˆ¤æ–­å€¼æ˜¯å¦ä¸ºç©º"""
    if v is None:
        return True
    if isinstance(v, str) and v.strip() == "":
        return True
    if isinstance(v, (list, dict)) and len(v) == 0:
        return True
    return False


def _get_value_len(value: Any) -> int:
    """è·å–å€¼çš„é•¿åº¦"""
    if value is None:
        return 0
    if isinstance(value, (str, list, dict)):
        return len(value)
    return len(str(value))


def _clean_data_single_pass(
    data: List[Dict],
    strip: bool = False,
    empty_fields: Optional[List[str]] = None,
    min_len_field: Optional[str] = None,
    min_len_value: Optional[int] = None,
    max_len_field: Optional[str] = None,
    max_len_value: Optional[int] = None,
    keep_fields: Optional[List[str]] = None,
    drop_fields: Optional[set] = None,
) -> tuple:
    """
    å•æ¬¡éå†æ‰§è¡Œæ‰€æœ‰æ¸…æ´—æ“ä½œã€‚

    Args:
        data: åŸå§‹æ•°æ®åˆ—è¡¨
        strip: æ˜¯å¦å»é™¤å­—ç¬¦ä¸²é¦–å°¾ç©ºç™½
        empty_fields: æ£€æŸ¥ç©ºå€¼çš„å­—æ®µåˆ—è¡¨ï¼Œç©ºåˆ—è¡¨è¡¨ç¤ºæ£€æŸ¥æ‰€æœ‰å­—æ®µï¼ŒNone è¡¨ç¤ºä¸æ£€æŸ¥
        min_len_field: æœ€å°é•¿åº¦æ£€æŸ¥çš„å­—æ®µ
        min_len_value: æœ€å°é•¿åº¦å€¼
        max_len_field: æœ€å¤§é•¿åº¦æ£€æŸ¥çš„å­—æ®µ
        max_len_value: æœ€å¤§é•¿åº¦å€¼
        keep_fields: åªä¿ç•™çš„å­—æ®µåˆ—è¡¨
        drop_fields: è¦åˆ é™¤çš„å­—æ®µé›†åˆ

    Returns:
        (æ¸…æ´—åçš„æ•°æ®, ç»Ÿè®¡ä¿¡æ¯åˆ—è¡¨)
    """
    result = []
    stats = {
        "drop_empty": 0,
        "min_len": 0,
        "max_len": 0,
    }

    # é¢„å…ˆè®¡ç®— keep_fields é›†åˆï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    keep_set = set(keep_fields) if keep_fields else None

    for item in data:
        # 1. strip å¤„ç†ï¼ˆåœ¨è¿‡æ»¤å‰æ‰§è¡Œï¼Œè¿™æ ·ç©ºå€¼æ£€æµ‹æ›´å‡†ç¡®ï¼‰
        if strip:
            item = {k: v.strip() if isinstance(v, str) else v for k, v in item.items()}

        # 2. ç©ºå€¼è¿‡æ»¤
        if empty_fields is not None:
            if len(empty_fields) == 0:
                # æ£€æŸ¥æ‰€æœ‰å­—æ®µ
                if any(_is_empty_value(v) for v in item.values()):
                    stats["drop_empty"] += 1
                    continue
            else:
                # æ£€æŸ¥æŒ‡å®šå­—æ®µ
                if any(_is_empty_value(item.get(f)) for f in empty_fields):
                    stats["drop_empty"] += 1
                    continue

        # 3. æœ€å°é•¿åº¦è¿‡æ»¤
        if min_len_field is not None:
            if _get_value_len(item.get(min_len_field, "")) < min_len_value:
                stats["min_len"] += 1
                continue

        # 4. æœ€å¤§é•¿åº¦è¿‡æ»¤
        if max_len_field is not None:
            if _get_value_len(item.get(max_len_field, "")) > max_len_value:
                stats["max_len"] += 1
                continue

        # 5. å­—æ®µç®¡ç†ï¼ˆkeep/dropï¼‰
        if keep_set is not None:
            item = {k: v for k, v in item.items() if k in keep_set}
        elif drop_fields is not None:
            item = {k: v for k, v in item.items() if k not in drop_fields}

        result.append(item)

    # æ„å»ºç»Ÿè®¡ä¿¡æ¯å­—ç¬¦ä¸²åˆ—è¡¨
    step_stats = []
    if strip:
        step_stats.append("strip")
    if stats["drop_empty"] > 0:
        step_stats.append(f"drop-empty: -{stats['drop_empty']}")
    if stats["min_len"] > 0:
        step_stats.append(f"min-len: -{stats['min_len']}")
    if stats["max_len"] > 0:
        step_stats.append(f"max-len: -{stats['max_len']}")
    if keep_fields:
        step_stats.append(f"keep: {len(keep_fields)} å­—æ®µ")
    if drop_fields:
        step_stats.append(f"drop: {len(drop_fields)} å­—æ®µ")

    return result, step_stats
