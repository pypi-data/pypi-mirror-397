# ============================================================================
# intronIC v2.0 - Configuration
# ============================================================================
# Default configuration for intronIC (classify and train modes)
# CLI arguments override these settings
#
# Auto-loaded from (in order of precedence):
#   1. --config PATH (explicit CLI argument)
#   2. ./.intronIC.yaml (current directory)
#   3. ~/.config/intronIC/config.yaml (XDG config dir)
#   4. ~/.intronIC.yaml (user home)
#   5. <install_dir>/config/config.yaml (built-in defaults)
#
# Quick profiles:
#   --config config/profiles/quick.yaml      (fast testing)
#   --config config/profiles/production.yaml (full production)
#
# ============================================================================

# ============================================================================
# SCORING PARAMETERS
# ============================================================================
scoring:
  # Classification threshold (0-100%)
  # Introns with SVM probability >= threshold are classified as U12-type
  # Lower = more sensitive (more U12s found, more false positives)
  # Higher = more specific (fewer U12s, fewer false positives)
  # Recommended: 90 for standard analysis, 95 for high-confidence set
  threshold: 90.0

  # Feature type to extract introns from
  # - "cds": Only coding sequence features (excludes UTR introns)
  # - "exon": All exon features (includes UTR introns)
  # - "both": Use CDS where available, fall back to exons (recommended)
  feature_type: "both"

  # Exclude non-canonical introns from scoring
  # When true, only GT-AG, GC-AG, and AT-AC introns are scored
  # When false, all intron types are scored (including rare boundaries)
  # Note: Most U12 introns are AT-AC, so excluding NC may miss some U12s
  exclude_noncanonical: false

  # Pseudocount for PWM scoring (log(0) prevention)
  # Small value added to matrix frequencies to avoid log(0) errors
  # Affects score magnitude but not relative rankings
  pseudocount: 0.0001

  # Ignore terminal dinucleotides when scoring non-canonical introns
  # When true, only scores internal motif regions for NC introns
  # Prevents biasing scores based on non-canonical boundaries
  ignore_nc_dinucleotides: true

  # Enable U12 boundary correction for non-canonical introns
  # Searches ±6bp around splice sites for strong U12 motifs like ATATCCTT
  # If found at offset position, adjusts boundaries by same shift
  # Only applies if result is canonical (GT-AG, GC-AG, or AT-AC)
  # Corrected introns marked with [c:shift] tag (e.g., [c:-2] = 2bp upstream)
  # Useful for fixing annotation errors where U12 introns are off by 1-6bp
  u12_boundary_correction: true

  # Scoring regions (PWM application windows)
  # Coordinates: negative = exonic, positive = intronic, 0 = boundary
  regions:
    # 5' splice site (donor)
    # Typical U12 motif: RTATCCTT (extends into intron)
    five_prime:
      start: -3   # 3bp exonic context
      end: 9      # 9bp intronic motif

    # Branch point search region (upstream of acceptor)
    # U12 BP motif: TCCTTAAC (7-8bp), located 10-50bp upstream of 3' site
    # Scans this window to find best-scoring BP position
    branch_point:
      start: -55  # Start search 55bp upstream
      end: -5     # End search 5bp upstream

    # 3' splice site (acceptor)
    # Typical U12 motif: YAC (polypyrimidine tract less pronounced than U2)
    three_prime:
      start: -6   # 6bp intronic context
      end: 4      # 4bp exonic

# ============================================================================
# TRAINING PARAMETERS
# ============================================================================
training:
  # Model evaluation mode
  # How to evaluate classifier performance on reference data:
  # - "nested_cv": Nested cross-validation (honest evaluation, RECOMMENDED)
  #   Most rigorous but slowest
  # - "split": Simple train/test split (faster, less rigorous)
  #   Single 80/20 split for quick evaluation
  # - "none": Skip evaluation (fastest, use when loading pretrained model)
  eval_mode: "nested_cv"

  # Number of folds for OUTER nested CV loop (evaluation only)
  # Only used when eval_mode = "nested_cv"
  # IMPORTANT: Should match optimizer.cv_folds (below) per expert guidance
  # Both outer and inner loops should use k=5 for optimal balance
  # Recommendation: Keep synchronized with optimizer.cv_folds
  n_cv_folds: 7  # Test with per-multiplier C grid fix

  # Test set fraction for split evaluation
  # Only used when eval_mode = "split"
  # Fraction of reference data held out for testing (0.0-1.0)
  test_fraction: 0.2

  # Number of optimization rounds for C parameter search
  # Controls iterative refinement rounds during hyperparameter optimization
  # Each round narrows search range around best C value
  # More rounds = finer granularity but longer optimization time
  n_optimization_rounds: 10

  # Fixed SVM C parameter (soft-margin penalty)
  # If set, skips hyperparameter optimization and uses this value directly
  # Useful for reproducing previous runs or when you know optimal C
  # If not set (commented out), C is optimized via grid search
  # fixed_C: 50.0

  # Ensemble configuration
  ensemble:
    # Number of models in ensemble
    # - 1 = single model (faster, less robust)
    # - 3-5 = typical ensemble (good balance)
    # - 10+ = very robust but slower
    # Recommendation: 10 for production, 1 for fast testing
    n_models: 16

    # Whether to subsample U2 set for each model
    # - true: Each model sees different random U2 subset (more diversity)
    # - false: All models see same U2 set (faster, less diverse)
    # Only applies when n_models > 1
    subsample_u2: true

    # Fraction of U2 introns to use per model (if subsample_u2=true)
    # - 0.8 = use 80% of U2s for each model
    # - 1.0 = use all U2s (no subsampling)
    # Only applies when subsample_u2=true
    # Recommendation: 0.7-0.9 for good diversity without losing too much data
    subsample_ratio: 0.8

    # Maximum iterations for LinearSVC convergence
    # Separate from optimizer.max_iter (which is for parameter search)
    # Used during final ensemble training
    # Production: 40000-50000, Fast testing: 3000-10000
    max_iter: 50000

    # Random seed for ensemble training
    # Controls U2 subsampling randomness
    # Set to same value as optimizer.random_state for full reproducibility
    random_state: 42

  # Use fold-averaged hyperparameters from nested CV
  # - false: Re-optimize C and calibration on full dataset after nested CV
  #   Maximizes performance on training data
  #   May overfit to training species composition
  # - true: Use geometric mean of fold-specific C values and majority-vote calibration
  #   More conservative, favors generalization over training fit
  #   RECOMMENDED for cross-species applications
  #   Expected to reduce FPs on distant species (e.g., C. elegans: 2 FPs vs 6 FPs)
  # Only applies when eval_mode='nested_cv'
  use_fold_averaged_params: false

# ============================================================================
# HYPERPARAMETER OPTIMIZATION
# ============================================================================
optimizer:
  # Number of geometric refinement rounds (1-5)
  # - Round 1: Coarse grid over full C range
  # - Rounds 2+: Refine around best C from previous round
  # Production: 3-5 rounds, Fast testing: 1-2 rounds
  n_rounds: 7

  # Initial grid points for round 1 (7-13 typical)
  # Number of C values to test in first round
  # More points = better coverage but slower
  # Production: 11-13 points, Fast testing: 3-7 points
  n_points_initial: 20

  # Refinement grid points for rounds 2+ (20-100 typical)
  # Number of C values to test in refinement rounds
  # Higher = finer granularity around optimal region
  # Production: 50-100 points, Fast testing: 20-30 points
  n_points_refine: 100

  # Cross-validation folds for INNER loop (hyperparameter search)
  # Used during grid search to evaluate each parameter combination
  # IMPORTANT: Should match training.n_cv_folds (above) per expert guidance
  # With hundreds-thousands of U12s, k=5 provides sufficient validation
  # k=10 doesn't give significant benefits but costs 2× computation
  # Recommendation: 5 for most datasets, 3 for very small reference sets
  # Note: CalibratedClassifierCV adds another 5-fold internal CV
  # Total model fits per parameter set ≈ cv_folds × 5 (calibration wrapper)
  # KEEP SYNCHRONIZED: training.n_cv_folds = optimizer.cv_folds = 5
  cv_folds: 7  # Test with per-multiplier C grid fix

  # Random seed for reproducibility
  # Ensures same train/test splits across runs
  random_state: 42

  # Number of parallel jobs for GridSearchCV hyperparameter optimization
  # - -1: use all available CPU cores (recommended for speed)
  # - 1: sequential (useful for debugging)
  # - N: use N cores (balance speed and resource usage)
  #
  # Higher parallelism = faster training but more memory usage
  # Recommended: -1 for maximum speed on most systems
  n_jobs: -1

  # Print detailed progress messages
  # - true: show optimization progress, CV results, timings
  # - false: quiet mode (only warnings/errors)
  verbose: true

  # Maximum iterations for LinearSVC convergence during optimization
  # Higher = allows convergence on difficult datasets
  # Lower = faster but may not converge (warns if hit)
  # Production: 60000-75000, Fast testing: 3000-10000
  max_iter: 65000

  # Scoring metric for hyperparameter optimization
  # Metric used during grid search to evaluate parameter combinations
  # Options:
  # - 'balanced_accuracy': (TPR + TNR) / 2 - treats both classes equally (DEFAULT)
  #   Good for imbalanced data, no precision/recall bias
  # - 'f_beta_0.5': F_0.5 score - heavily weights precision over recall
  #   Best for minimizing false positives (conservative U12 calls)
  # - 'f_beta_0.75': F_0.75 score - slightly weights precision over recall
  #   Moderate precision focus, closer to balanced
  #
  # Expert recommendation: Try 'f_beta_0.5' for precision-focused training
  # This encourages the optimizer to find hyperparameters that minimize false positives
  # while still maintaining reasonable recall on true U12s.
  scoring_metric: 'balanced_accuracy'

  # Robustness improvements (2025-01-19)
  # Penalty types to search (L1 = feature selection, L2 = full weighting)
  # WARNING: L1 can be 10-20× slower than L2 and may cause memory issues with high parallelism
  # L1 with max_iter=65000 can take minutes per fit vs seconds for L2
  # Recommendation: Use ['l2'] only for production (sufficient for robustness)
  # Advanced: Try ['l1', 'l2'] with reduced max_iter (10000) and lower n_jobs (4-8)
  penalty_options: ['l1', 'l2']

  # Loss functions to search
  # IMPORTANT: With dual=False (our default), only 'squared_hinge' is supported
  # The 'hinge' loss requires dual=True which is slower for our feature count
  # Default: ['squared_hinge'] (only valid option for dual=False)
  # This parameter is kept for potential future dual=True support
  loss_options: ['squared_hinge']

  # Class weight multipliers to search (controls precision-recall tradeoff)
  # Multiplies ONLY the positive class (U12) weight, keeping U2 weight fixed
  # This changes the relative importance: w_pos/w_neg ratio
  #
  # - alpha < 1.0 (e.g., 0.8): Lower U12 penalty → HIGHER PRECISION (fewer false positives)
  # - alpha = 1.0: Standard balanced weighting (equal importance)
  # - alpha > 1.0 (e.g., 1.2): Higher U12 penalty → HIGHER RECALL (find more U12s)
  #
  # Grid search explores (C, alpha) combinations to find best balanced accuracy
  # Default: [0.8, 1.0, 1.2] to test precision/recall tradeoffs
  # For fastest search or to disable multiplier search: [1.0]
  class_weight_multipliers: [0.8, 0.85, 0.9, 0.95, 1.0, 1.05, 1.1, 1.15, 1.2]

  # Tie-breaking for class_weight_multiplier selection
  # When multiple multipliers achieve the same best score:
  # - true (default): Prefer multiplier closest to 1.0 (standard balanced weighting)
  # - false: Use GridSearchCV default (first tie in parameter list order)
  #
  # Rationale: When precision/recall tradeoffs don't affect performance (tied scores),
  # selecting 1.0 is more interpretable than arbitrarily selecting the first value.
  # Set to false if you want deterministic first-tie behavior regardless of performance.
  use_multiplier_tiebreaker: true

  # Feature transformation configuration (BothEndsStrongTransformer)
  # Controls which composite features are included in the model
  # Base features (ALWAYS included): s5, sBP, s3 (3D z-scores)
  #
  # Specify list of composite features to add (beyond base 3D features):
  # Available features:
  #   - 'min_5_bp':      min(s5, sBP) - both 5' and BP must be strong
  #   - 'min_5_3':       min(s5, s3) - both 5' and 3' must be strong
  #   - 'min_all':       min(s5, sBP, s3) - ALL THREE must be strong
  #   - 'absdiff_5_bp':  |s5 - sBP| - penalty for 5'/BP imbalance (expects negative coef)
  #   - 'absdiff_5_3':   |s5 - s3| - penalty for 5'/3' imbalance (expects negative coef)
  #   - 'absdiff_bp_3':  |sBP - s3| - penalty for BP/3' imbalance (expects negative coef)
  #   - 'max_5_bp':      max(s5, sBP) - at least one of 5'/BP strong
  #   - 'max_5_3':       max(s5, s3) - at least one of 5'/3' strong
  #
  # Note: Old models used 'neg_absdiff_*' (negative values) which caused models
  # to learn positive coefficients that rewarded imbalance. New models use
  # 'absdiff_*' (positive values) so negative coefficients properly penalize it.
  #
  # Standard (4D): Base 3 z-scores + BP/3' imbalance penalty (RECOMMENDED)
  #   Based on L1 regularization analysis showing absdiff_bp_3 is most informative
  # Extended (7D): Adds min_all and all three imbalance terms
  # Custom: Specify exactly which features you want
  #
  # Examples:
  #   # Standard 4D (RECOMMENDED - based on L1 coefficient analysis)
  #   features: ['absdiff_bp_3']
  #
  #   # Extended 7D (more features, may help edge cases)
  #   features: ['min_all', 'absdiff_5_bp', 'absdiff_5_3', 'absdiff_bp_3']
  #
  #   # Full 11D (all composite features - for experimentation)
  #   features: ['min_5_bp', 'min_5_3', 'min_all', 'absdiff_5_bp',
  #              'absdiff_5_3', 'absdiff_bp_3', 'max_5_bp', 'max_5_3']
  feature_transform:
    # List of composite features to include (standard: 4D recommended set)
    features: ['absdiff_bp_3', 'absdiff_5_bp']

  # Gamma scaling for imbalance features (nudges L2 toward L1 behavior)
  # Scales all absdiff_* features by this factor before training
  # Higher gamma = imbalance is more costly in the margin
  # Set to null to disable grid search (uses gamma=1.0 always)
  # Set to list of values to grid search: e.g., [1.0, 2.0, 4.0]
  #
  # Intuition: L1 naturally emphasizes imbalance terms by zeroing others.
  # L2 with gamma>1.0 achieves similar effect without L1's computational cost.
  #
  # Example: gamma=2.0 doubles the weight of imbalance penalties,
  # making the model more sensitive to signal inconsistencies.
  # gamma_imbalance_options: [1.0, 2.0, 3.0]

  # C parameter bounds (optional - uses auto-computed bounds if not specified)
  # Controls range of C (soft-margin penalty) values to search
  # By default, C bounds are computed automatically from class weights
  # Leave commented to use auto-computed bounds (RECOMMENDED)
  #
  # AUTO-COMPUTED BOUNDS (default):
  # - Range: [1e-3, 1e3] effective C for positive class (U12)
  # - Adjusted by class imbalance (typically ~1:50 U12:U2 ratio)
  # - Results in weight-aware search range appropriate for dataset
  #
  # MANUAL OVERRIDE (advanced):
  # c_bounds:
  #   # Effective C range for positive class (U12)
  #   eff_C_pos_range: [0.001, 1000.0]
  #   # Optional maximum effective C for negative class (U2)
  #   # null = no limit, float = cap at this value
  #   eff_C_neg_max: null

# ============================================================================
# PARAMETER GRID (Advanced - for custom search space)
# ============================================================================
# UPDATED ARCHITECTURE (2025-01-19):
# - ScoreNormalizer: RobustScaler(with_centering=True) - single scaling step
# - Pipeline: BothEndsStrongTransformer → LinearSVC (Stage 1) → CalibratedClassifierCV (Stage 2)
# - Two-stage optimization:
#   * Stage 1: Optimize C, penalty, loss, class_weight_mult (metric: balanced_accuracy)
#   * Stage 2: Select calibration method sigmoid vs isotonic (metric: log-loss)
#
# GRID-SEARCHED PARAMETERS (configurable above):
# - C: Auto-optimized via geometric grid search (balanced_accuracy)
# - penalty: L1 (feature selection) vs L2 (full weighting)
# - loss: squared_hinge only (hinge requires dual=True)
# - class_weight_multiplier: 0.8 (conservative) vs 1.0 (balanced) vs 1.2 (aggressive)
# - calibration_method: Auto-selected (sigmoid vs isotonic) via log-loss
#
# FIXED ARCHITECTURE POINTS:
# - Standard 4D features: base 3 z-scores + absdiff_bp_3 (BP/3' imbalance penalty)
# - dual: false (primal formulation, faster for n_features << n_samples)
# - Stratified U2 sampling: 2D binning (length × GC content)
# - Ensemble diversity: Different stratified U2 samples per model
#
# GRID SIZE: ~6× larger than previous
# Example Round 1: 13 C values × 2 penalties × 1 loss × 3 multipliers = 78 combos
#
# Leave param_grid empty (uses optimizer config above).
# Only override for advanced testing (requires code changes).
#
# References: SCALER_CENTERING_FIX_COMPLETE.md, test_corrected_architecture.py
param_grid: {}

# ============================================================================
# PERFORMANCE OPTIONS
# ============================================================================
performance:
  # Number of parallel processes for classification
  # Uses multiprocessing to speed up SVM prediction on experimental introns
  # Set to number of CPU cores for maximum speed (e.g., 8 for 8-core system)
  # Memory usage scales linearly with processes (~1-2GB per process)
  processes: 5

  # Number of processes for cross-validation (if different from classification)
  # Controls parallelization during hyperparameter optimization
  # If not set, uses same value as 'processes' above
  # Grid search is very parallel-friendly - can benefit from many cores
  # cv_processes: 8

  # Minimum intron length in base pairs
  # Introns shorter than this are marked with [o:s] (omitted: short) tag
  # Still written to output but not scored/classified
  # Scoring requires sufficient sequence for all three regions (5', BP, 3')
  # Typical valid range: 30-70bp depending on scoring region configuration
  min_intron_length: 30

# ============================================================================
# ISOFORM SELECTION
# ============================================================================
isoform:
  # Include non-longest isoforms in analysis
  # - false: Only longest transcript per gene (reduces redundancy)
  # - true: All isoforms included (may result in many similar introns)
  # Recommendation: Keep false to avoid scoring ~5x more introns
  allow_multiple_isoforms: false

  # Exclude overlapping introns from different isoforms
  # Some isoforms have introns with overlapping coordinates
  # - true: Only one representative from each overlapping set
  # - false: All overlapping introns kept (may cause duplicates)
  exclude_overlapping: false

  # Include duplicate introns in output files
  # Duplicates are introns with identical coordinates (chr, strand, start, stop)
  # - false: Only first occurrence written (cleaner output)
  # - true: All occurrences written (useful for tracking gene families)
  # Note: Duplicates marked with [d] tag and linked to representative
  include_duplicates: false

# ============================================================================
# OUTPUT OPTIONS
# ============================================================================
output:
  # Remove "transcript:" and "gene:" prefixes from IDs
  # Makes output files more readable
  clean_names: true

# ============================================================================
# EXTRACTION OPTIONS
# ============================================================================
extraction:
  # Length of exonic flanks to extract (bp)
  # Flanking exonic sequence extracted on both sides of each intron
  # Used for context in output files and potential future analyses
  # Does not affect scoring (scoring uses internal coordinates only)
  # Larger values = more context, larger output files
  flank_length: 50

# ============================================================================
# ADVANCED OPTIONS
# ============================================================================
advanced:
  # Global random seed for reproducibility
  # Seeds all random number generators (train/test splits, subsampling, etc.)
  # Using same seed guarantees identical results across runs
  # Change this value to get different random splits/samples
  random_seed: 42

  # Suppress non-essential output
  # When true, reduces console output to warnings and errors only
  # Useful for running in scripts or batch processing
  # Log file still contains full details regardless of this setting
  quiet: false

  # Enable debug logging
  # When true, writes detailed debugging information to log file
  # Includes feature extraction details, score calculations, etc.
  # Produces very large log files - only use for troubleshooting
  debug: false
