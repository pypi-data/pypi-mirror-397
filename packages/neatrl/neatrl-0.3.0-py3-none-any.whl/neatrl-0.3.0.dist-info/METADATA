Metadata-Version: 2.4
Name: neatrl
Version: 0.3.0
Summary: A Python library for reinforcement learning algorithms
Author-email: Yuvraj Singh <yuvraj.mist@gmail.com>
License: MIT
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: torch
Requires-Dist: numpy
Requires-Dist: gymnasium
Requires-Dist: stable-baselines3
Requires-Dist: tqdm
Requires-Dist: wandb
Requires-Dist: imageio[ffmpeg]
Requires-Dist: opencv-python
Requires-Dist: wandb[media]
Provides-Extra: atari
Requires-Dist: gymnasium[atari]; extra == "atari"
Requires-Dist: ale-py; extra == "atari"
Provides-Extra: classic
Requires-Dist: gymnasium[classic-control]; extra == "classic"
Provides-Extra: box2d
Requires-Dist: swig; extra == "box2d"
Requires-Dist: gymnasium[box2d]; extra == "box2d"
Provides-Extra: dev
Requires-Dist: ruff; extra == "dev"
Requires-Dist: mypy; extra == "dev"
Requires-Dist: black; extra == "dev"
Requires-Dist: build; extra == "dev"
Requires-Dist: twine; extra == "dev"
Dynamic: license-file

# ğŸ¯ NeatRL

**A clean, modern Python library for reinforcement learning algorithms**

NeatRL provides high-quality implementations of popular RL algorithms with a focus on simplicity, performance, and ease of use. Built with PyTorch and designed for both research and production use.

## âœ¨ Features

- ğŸ“Š **Experiment Tracking**: Built-in support for Weights & Biases logging
- ğŸ® **Gymnasium Compatible**: Works with Gymnasium environments and adding many more!
- ğŸ”§ **Easy to Extend**: Modular design for adding new algorithms
- ğŸ“ˆ **State-of-the-Art**: Implements modern RL techniques and best practices

## ğŸ—ï¸ Supported Algorithms

### Current Implementations
- **DQN** (Deep Q-Network) - Classic value-based RL algorithm
- **Dueling DQN** - Enhanced DQN with separate value and advantage streams
- **REINFORCE** - Policy gradient method for discrete and continuous action spaces
- *More algorithms coming soon...*

## ğŸ“¦ Installation

```bash
python -m venv neatrl-env
source neatrl-env/bin/activate  # On Windows use `neatrl-env\Scripts

pip install neatrl"[classic,box2d,atari]""[classic,box2d,atari]"
```

## ğŸš€ Quick Start

Train a DQN agent on CartPole in 3 lines:

```python
from neatrl import train_dqn

model = train_dqn(
    env_id="CartPole-v1",
    total_timesteps=10000,
    seed=42
)
```

## ğŸ“š Documentation

ğŸ“– **[Complete Documentation](https://github.com/YuvrajSingh-mist/NeatRL/tree/master/neatrl/docs)**

The docs include:
- Detailed usage examples
- Hyperparameter tuning guides
- Environment compatibility
- Experiment tracking setup
- Troubleshooting tips

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### Development Setup
```bash
git clone https://github.com/YuvrajSingh-mist/NeatRL.git
cd NeatRL
pip install -e .[dev]
```

## ğŸ“‹ Changelog

### [0.1.4] - 2025-12-13
- **Added**: Custom agent support for DQN training
- **Added**: Network architecture display using torchinfo
- **Improved**: Error handling for custom agent constructors
- **Changed**: Agent parameter now accepts nn.Module subclasses

### [0.1.3] - 2025-12-01
- Initial release with DQN implementation
- Weights & Biases integration
- Video recording capabilities
- Comprehensive documentation

For the complete changelog, see [CHANGELOG.md](CHANGELOG.md).

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

**Made with â¤ï¸ for the RL community**
