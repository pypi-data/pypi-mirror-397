# AI Code Review Configuration
# Copy this file to .env and customize the values for your setup

#==============================================================================
# GitLab Configuration
#==============================================================================

# GitLab Personal Access Token (required)
# Create one at: https://gitlab.com/-/profile/personal_access_tokens
# Required scopes: api, read_user, read_repository
GITLAB_TOKEN=glpat-xxxxxxxxxxxxxxxxxxxx

# GitLab instance URL
# Default: https://gitlab.com
# Examples: https://gitlab.company.com, http://localhost:8080
# Note: In CI/CD, CI_SERVER_URL takes precedence if available
GITLAB_URL=https://gitlab.com

# SSL Configuration (for custom or internal GitLab instances)
# SSL certificate verification (default: true)
# Set to false to disable SSL verification (NOT recommended for production)
SSL_VERIFY=true

# Option 1: Path to existing SSL certificate file (manual approach)
# Use this for internal GitLab instances with self-signed or custom CA certificates
# Example: SSL_CERT_PATH=/path/to/company-ca.crt
SSL_CERT_PATH=

# Option 2: URL to download SSL certificate automatically (recommended for CI/CD)
# Tool will download and cache certificate automatically
# Example: SSL_CERT_URL=https://internal-gitlab.company.com/ca-bundle.crt
SSL_CERT_URL=

# Directory to cache downloaded SSL certificates (default: .ssl_cache)
# Only used when SSL_CERT_URL is set
SSL_CERT_CACHE_DIR=.ssl_cache

#==============================================================================
# GitLab CI/CD Automatic Variables
#==============================================================================
# These variables are automatically set by GitLab CI/CD and take precedence
# over manual configuration. Do not set these manually unless testing.

# GitLab CI project path (format: "group/subgroup/project")
# Automatically set by GitLab CI/CD as CI_PROJECT_PATH
# Only set manually for local testing of CI mode
CI_PROJECT_PATH=

# GitLab CI merge request IID (internal ID, not global ID)
# Automatically set by GitLab CI/CD as CI_MERGE_REQUEST_IID
# Only set manually for local testing of CI mode
CI_MERGE_REQUEST_IID=

# GitLab CI server URL
# Automatically set by GitLab CI/CD as CI_SERVER_URL
# Takes precedence over GITLAB_URL when available
CI_SERVER_URL=

#==============================================================================
# GitHub Configuration
#==============================================================================

# GitHub Personal Access Token (required for GitHub platform)
# Create one at: https://github.com/settings/tokens
# Required scopes: repo, read:org
GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx

# GitHub API URL
# Default: https://api.github.com
# For GitHub Enterprise: https://your-github-enterprise.com/api/v3
GITHUB_URL=https://api.github.com

#==============================================================================
# GitHub Actions Automatic Variables
#==============================================================================
# These variables are automatically set by GitHub Actions and take precedence
# over manual configuration. Do not set these manually unless testing.

# GitHub repository path (format: "owner/repository")
# Automatically set by GitHub Actions as GITHUB_REPOSITORY
# Only set manually for local testing of CI mode
GITHUB_REPOSITORY=

# GitHub Actions server URL
# Automatically set by GitHub Actions as GITHUB_SERVER_URL
# Takes precedence over GITHUB_URL when available
GITHUB_SERVER_URL=

#==============================================================================
# AI Provider Configuration
#==============================================================================

# AI provider to use
# Possible values: gemini, openai, anthropic, ollama
# Default: gemini (production), ollama (local development)
AI_PROVIDER=gemini

# AI model name
# For Gemini: gemini-3-pro-preview, gemini-2.5-pro, gemini-1.5-pro, gemini-2.0-flash-exp
# For OpenAI: gpt-4, gpt-3.5-turbo, gpt-4-turbo
# For Anthropic: claude-sonnet-4-20250514, claude-3-5-sonnet-20241022, claude-3-sonnet-20240229, claude-3-haiku-20240307
# For Ollama: qwen2.5-coder:7b, llama3.1:8b, codellama:13b
# Default: gemini-3-pro-preview
AI_MODEL=gemini-3-pro-preview

# API key for cloud AI providers (Gemini, OpenAI, Anthropic)
# Required for cloud providers, not needed for Ollama
# Get Google AI API key at: https://makersuite.google.com/app/apikey
# Get Anthropic API key at: https://console.anthropic.com/account/keys
# Default: None
AI_API_KEY=your_api_key_here

#==============================================================================
# Ollama Configuration (for local AI)
#==============================================================================

# Ollama server URL
# Default: http://localhost:11434
# Examples: http://localhost:11434, http://ollama-server:11434
OLLAMA_BASE_URL=http://localhost:11434

# HTTP request timeout in seconds for API calls
# Range: > 0.0 (recommended: 5.0-30.0)
# Default: 5.0
HTTP_TIMEOUT=5.0

# LLM API timeout in seconds for cloud providers
# Range: > 0.0 (recommended: 60.0-300.0)
# Default: 60.0
# This controls how long to wait for LLM API responses before timing out
# For CI/CD: Lower values (60-90s) fail faster and prevent blocking pipelines
# For local use: Higher values (120-300s) allow more time for complex reviews
LLM_TIMEOUT=60.0

# Maximum retries for LLM API calls on timeout or server errors
# Range: >= 0 (recommended: 1-3)
# Default: 2
# For CI/CD: Lower values (1-2) fail faster on server issues
# For local use: Higher values (3-5) may help with transient network issues
# Note: Each retry uses the LLM_TIMEOUT, so total wait = LLM_TIMEOUT * (LLM_MAX_RETRIES + 1)
LLM_MAX_RETRIES=2

#==============================================================================
# AI Model Parameters
#==============================================================================

# Temperature for AI responses
# Range: 0.0-2.0 (lower = more deterministic, higher = more creative)
# Default: 0.1 (recommended for code reviews)
TEMPERATURE=0.1

# Maximum tokens for AI response generation
# Range: 1-8192 (depending on model)
# Default: 8000
# Note: Higher values allow longer responses but may increase processing time
MAX_TOKENS=8000

#==============================================================================
# Content Processing Limits
#==============================================================================

# Maximum characters to process from diff
# Default: 100000 (100K characters)
# Increase for larger codebases, decrease for faster processing
MAX_CHARS=100000

# Maximum number of files to process in a single review
# Default: 100
# Increase for larger changesets, decrease for faster processing
MAX_FILES=100

#==============================================================================
# Review Context Configuration (Intelligent Synthesis)
#==============================================================================

# Enable fetching previous reviews and comments for context
# Possible values: true, false
# Default: true
# When enabled, fetches all reviews and comments from previous AI reviews
# to avoid repeating suggestions that were already addressed
ENABLE_REVIEW_CONTEXT=true

# Enable two-phase synthesis preprocessing
# Possible values: true, false
# Default: true
# When enabled, uses a fast model to synthesize comments before main review
# Benefits: reduces tokens, lowers costs, better context quality
# Automatically skips when no comments/reviews exist
ENABLE_REVIEW_SYNTHESIS=true

# Model for review synthesis (preprocessing phase)
# Default: auto-selects fast model based on AI_PROVIDER
#   - Gemini: gemini-2.5-flash
#   - Anthropic: claude-3-5-haiku-20241022
#   - OpenAI: gpt-4o-mini
#   - Ollama: qwen2.5-coder:7b (same as main model)
# Examples: gemini-2.5-flash, gpt-4o-mini, claude-3-5-haiku-20241022
SYNTHESIS_MODEL=

# Maximum tokens for synthesis output
# Range: 1-10000
# Default: 2000
# The synthesis model will generate a summary of this maximum length
SYNTHESIS_MAX_TOKENS=2000

# Maximum comments to fetch from platform API
# Range: 1-1000
# Default: 30
# Limits the number of discussions/comments fetched to prevent performance
# issues on PRs with hundreds of comments. The most recent N are fetched.
# Increase for projects with very active discussions (rare).
MAX_COMMENTS_TO_FETCH=30

#==============================================================================
# Optional Features
#==============================================================================

# Programming language hint for better context
# Examples: python, javascript, go, rust, java
# Default: None (auto-detect from file extensions)
LANGUAGE_HINT=

#==============================================================================
# Project Context Configuration
#==============================================================================

# Enable loading project context from file
# Possible values: true, false
# Default: true (loads context if file exists)
# When enabled, includes project-specific information in AI prompts
ENABLE_PROJECT_CONTEXT=true

# Team/organization context file (optional)
# Can be a local path or remote URL
# This context has higher priority than project context
# Use for organization-wide standards, security requirements, and team conventions
# Examples:
#   TEAM_CONTEXT_FILE=https://gitlab.com/org/standards/-/raw/main/review-context.md
#   TEAM_CONTEXT_FILE=../team-standards.md
#   TEAM_CONTEXT_FILE=https://company.com/standards/python-review.md
# TEAM_CONTEXT_FILE=

# Enable CI/CD documentation fetching for context generation
# Possible values: true, false
# Default: false (disabled to reduce token consumption)
# When enabled, fetches official CI/CD documentation (GitLab CI, GitHub Actions)
# and includes it in the generated context for better CI configuration reviews.
# Recommended only for projects that heavily rely on CI/CD pipelines.
# Note: This feature is used by ai-generate-context tool, not the main review tool
ENABLE_CI_DOCS=false

# Context7 API Key for fetching library documentation
# Required for Context7 integration (optional feature)
# Sign up at: https://context7.com
# When enabled, fetches official documentation for project dependencies
# to enhance AI code reviews with authoritative API information.
# Note: This feature is used by ai-generate-context tool, not the main review tool
CONTEXT7_API_KEY=

# Path to project context file (relative to repository root)
# Default: .ai_review/project.md
# Examples: docs/ai-context.md, .cursorrules, README.md
# Use this file to provide project-specific context to the AI:
# - Architecture patterns, coding conventions
# - Domain knowledge, business rules
# - Intentional "weird" code patterns that are legitimate
# - External dependencies not visible in diffs
PROJECT_CONTEXT_FILE=.ai_review/project.md

# Include MR Summary section in reviews
# Set to false for shorter, code-focused reviews without executive summary
# Possible values: true, false
# Default: true (includes both MR Summary and Detailed Code Review)
# When disabled, only shows Detailed Code Review and Summary sections
INCLUDE_MR_SUMMARY=true

#==============================================================================
# Configuration File Options
#==============================================================================

# Skip loading configuration file (auto-detected .ai_review/config.yml or custom path)
# Possible values: true, false
# Default: false (automatically loads config file if it exists)
# When set to true, ignores both auto-detected and custom config files
NO_CONFIG_FILE=false

# Custom configuration file path
# Default: auto-detect .ai_review/config.yml if it exists
# Examples: .ai_review/config.yml, configs/custom.yml, ../shared-config.yml
# This file can contain any of the above environment variables in YAML format
# CLI arguments and environment variables take precedence over config file values
CONFIG_FILE=

#==============================================================================
# Execution Options
#==============================================================================

# Dry run mode - no actual API calls made, uses mock responses
# Possible values: true, false
# Default: false
# Useful for testing configuration without making actual AI API calls
DRY_RUN=false

# Force large context window (24K tokens) for processing big diffs
# Possible values: true, false
# Default: false
# Note: Auto-activated when diffs exceed 60K characters
# Useful for very large MRs that need maximum context
BIG_DIFFS=false

#==============================================================================
# Logging Configuration
#==============================================================================

# Logging level
# Possible values: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Default: INFO
LOG_LEVEL=INFO

#==============================================================================
# File Filtering Configuration
#==============================================================================

# Glob patterns for files to exclude from AI review
# Comma-separated list of patterns (supports wildcards and ** for recursive)
# Default: includes common lockfiles, build artifacts, and dependency folders
# Examples:
#   "*.lock,*.min.js" - exclude lockfiles and minified JS
#   "dist/**,build/**" - exclude build directories
#   "" - disable filtering (include all files)
#
# Default patterns include:
# - All lockfiles (*.lock, package-lock.json, yarn.lock, etc.)
# - Minified files (*.min.js, *.min.css, *.map)
# - Build directories (dist/**, build/**)
# - Dependency folders (node_modules/**, __pycache__/**)
# - Generated files (*.egg-info/**)
#
# Uncomment to override with custom patterns:
# EXCLUDE_PATTERNS=*.lock,package-lock.json,yarn.lock,*.min.js,node_modules/**,dist/**

#==============================================================================
# Usage Examples
#==============================================================================

# Production with Gemini (default):
# AI_PROVIDER=gemini
# AI_MODEL=gemini-3-pro-preview
# AI_API_KEY=your_gemini_api_key_here

# Local development with Ollama:
# AI_PROVIDER=ollama
# AI_MODEL=qwen2.5-coder:7b
# OLLAMA_BASE_URL=http://localhost:11434
# AI_API_KEY=  # Not needed for Ollama

# Production with Anthropic:
# AI_PROVIDER=anthropic
# AI_MODEL=claude-sonnet-4-20250514
# AI_API_KEY=sk-ant-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Production with OpenAI:
# AI_PROVIDER=openai
# AI_MODEL=gpt-4
# AI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Testing/development mode:
# DRY_RUN=true
# LOG_LEVEL=DEBUG

# Context generation with Context7 and CI/CD docs:
# AI_PROVIDER=gemini
# AI_API_KEY=your_gemini_api_key_here
# CONTEXT7_API_KEY=your_context7_api_key_here
# ENABLE_CI_DOCS=true  # Only for CI-heavy projects
