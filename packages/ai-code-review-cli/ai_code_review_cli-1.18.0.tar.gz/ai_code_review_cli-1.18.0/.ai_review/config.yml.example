# AI Code Review Configuration File Example
# Copy this file to .ai_review/config.yml and customize for your project
#
# ‚ö†Ô∏è  PRIORITY ORDER: CLI args > Environment variables > Config file > Field defaults
# Values in this file are overridden by environment variables and CLI arguments
#
# üìù Note: All environment variables from env.example can be used here in YAML format
# Remove the leading variable names and use lowercase with underscores for keys

#==============================================================================
# Platform Configuration
#==============================================================================

# Platform provider: gitlab, github, or local
# Auto-detected in CI/CD environments, set manually for local use
platform_provider: gitlab

# Platform URLs (for self-hosted instances)
gitlab_url: https://gitlab.com
github_url: https://api.github.com

# Platform tokens (‚ö†Ô∏è NOT RECOMMENDED: use environment variables instead for security)
# gitlab_token: glpat_xxxxxxxxxxxxxxxxxxxx
# github_token: ghp_xxxxxxxxxxxxxxxxxxxx

#==============================================================================
# AI Provider Configuration
#==============================================================================

# AI provider: gemini, anthropic, ollama
ai_provider: gemini

# AI model name for selected provider
ai_model: gemini-3-pro-preview

# API key (‚ö†Ô∏è NOT RECOMMENDED: use AI_API_KEY environment variable instead for security)
# ai_api_key: your_api_key_here

# AI model parameters
temperature: 0.1        # 0.0-2.0, lower = more deterministic
max_tokens: 8000        # Maximum response tokens
http_timeout: 5.0       # HTTP timeout in seconds

#==============================================================================
# Ollama Configuration (for local AI)
#==============================================================================

# Ollama server URL (when using ollama provider)
ollama_base_url: http://localhost:11434

#==============================================================================
# SSL Configuration (for internal/self-hosted instances)
#==============================================================================

# SSL certificate verification
ssl_verify: true

# Custom SSL certificate path (for internal instances)
# ssl_cert_path: /path/to/cert.pem

#==============================================================================
# Processing Configuration
#==============================================================================

# Processing limits
max_chars: 100000       # Max characters from diff
max_files: 100          # Max files to process per review

# Diff download timeout (for GitLab/GitHub complete diff fetching)
# Adjust for very large repositories or slow networks
# Default: 120 seconds (2 minutes)
diff_download_timeout: 120

# File filtering (glob patterns)
exclude_patterns:
  - "*.lock"              # All lock files
  - "package-lock.json"   # npm lock file
  - "yarn.lock"           # Yarn lock file
  - "Pipfile.lock"        # Pipenv lock file
  - "poetry.lock"         # Poetry lock file
  - "pnpm-lock.yaml"      # PNPM lock file
  - "*.min.js"            # Minified JavaScript
  - "*.min.css"           # Minified CSS
  - "*.map"               # Source map files
  - "node_modules/**"     # Node modules (top level)
  - "**/node_modules/**"  # Node modules (nested)
  - "__pycache__/**"      # Python cache (top level)
  - "**/__pycache__/**"   # Python cache (nested)
  - "dist/**"             # Build distributions
  - "build/**"            # Build directories
  - "*.egg-info/**"       # Python egg info

#==============================================================================
# Project Context Configuration
#==============================================================================

# Enable loading project context from file
enable_project_context: true

# Path to project context file (relative to repository root)
project_context_file: .ai_review/project.md

# Team/organization context file (optional)
# Can be a local path or remote URL (e.g., https://company.com/standards/review.md)
# This context has higher priority than project context
# Use for organization-wide standards, security requirements, and team conventions
# team_context_file: https://gitlab.com/org/standards/-/raw/main/review-context.md
# team_context_file: ../team-standards.md

# Review format options
include_mr_summary: true    # Include MR Summary section in reviews

#==============================================================================
# Review Context Configuration (Intelligent Synthesis)
#==============================================================================

# Enable fetching previous reviews and comments for context
# When enabled, fetches all reviews and comments from previous AI reviews
# to avoid repeating suggestions that were already addressed
enable_review_context: true

# Enable two-phase synthesis preprocessing
# When enabled, uses a fast model to synthesize comments before main review
# Benefits: reduces tokens, lowers costs, better context quality
# Automatically skips when no comments/reviews exist
enable_review_synthesis: true

# Model for review synthesis (preprocessing phase)
# Default: auto-selects fast model based on ai_provider
#   - Gemini: gemini-2.5-flash
#   - Anthropic: claude-3-5-haiku-20241022
#   - OpenAI: gpt-4o-mini
#   - Ollama: qwen2.5-coder:7b (same as main model)
# synthesis_model: gemini-2.5-flash

# Maximum tokens for synthesis output (1-10000, default: 2000)
synthesis_max_tokens: 2000

#==============================================================================
# Development/Testing Options
#==============================================================================

# Dry run mode (no actual API calls, uses mock responses)
dry_run: false

# Force large context window (24K tokens) for processing big diffs
# Note: Auto-activated when diffs exceed 60K characters
big_diffs: false

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
log_level: INFO

#==============================================================================
# Example Configurations for Different Scenarios
#==============================================================================

# Example 1: Local development with Ollama
# ai_provider: ollama
# ai_model: qwen2.5-coder:7b
# ollama_base_url: http://localhost:11434
# platform_provider: local
# dry_run: false

# Example 2: GitHub with Anthropic Claude
# ai_provider: anthropic
# ai_model: claude-3-5-sonnet-20241022
# platform_provider: github
# github_url: https://api.github.com
# max_tokens: 4000

# Example 3: Self-hosted GitLab with custom SSL
# platform_provider: gitlab
# gitlab_url: https://gitlab.company.com
# ssl_verify: true
# ssl_cert_path: /etc/ssl/certs/company-ca.pem
# ai_provider: gemini
# ai_model: gemini-3-pro-preview

# Example 4: Minimal config for basic usage
# ai_provider: gemini
# ai_model: gemini-3-pro-preview
# platform_provider: gitlab

#==============================================================================
# Context7 Integration (Optional)
#==============================================================================

# Context7 provides official library documentation to enhance AI code reviews
# Requires Context7 API key (set via CONTEXT7_API_KEY environment variable)
# Sign up at https://context7.com for API access

context7:
  # Enable Context7 integration for library documentation
  enabled: false

  # Maximum number of libraries to fetch documentation for (default: 3)
  # Limits API calls and processing time. Can be overridden by MAX_LIBRARIES env var
  # Recommended: 3-5 for balanced performance vs coverage
  max_libraries: 3

  # Maximum tokens to fetch per library (100-10000)
  max_tokens_per_library: 2000

  # Timeout for Context7 API calls in seconds (1-60)
  timeout_seconds: 10

  # Priority libraries to fetch documentation for
  # If empty, will auto-detect important libraries from your dependencies
  priority_libraries: []

  # Example configurations for different project types:

  # Web API Project (high priority libraries)
  # max_libraries: 5
  # priority_libraries:
  #   - fastapi
  #   - pydantic
  #   - sqlalchemy
  #   - uvicorn
  #   - aiohttp

  # Data Science Project (focus on core libraries)
  # max_libraries: 4
  # priority_libraries:
  #   - pandas
  #   - numpy
  #   - scikit-learn
  #   - matplotlib

  # Django Project (web framework focused)
  # max_libraries: 5
  # priority_libraries:
  #   - django
  #   - djangorestframework
  #   - celery
  #   - redis
  #   - psycopg2

  # LangChain/AI Project (AI/ML focused)
  # max_libraries: 4
  # priority_libraries:
  #   - langchain
  #   - langchain-community
  #   - openai
  #   - anthropic

  # Performance-focused (minimal API calls)
  # max_libraries: 2
  # priority_libraries:
  #   - fastapi
  #   - pydantic

  # Comprehensive coverage (more API calls, slower)
  # max_libraries: 7
  # priority_libraries: []  # Auto-detect all important libraries
