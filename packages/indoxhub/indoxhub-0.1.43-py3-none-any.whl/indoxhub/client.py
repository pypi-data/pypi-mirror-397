"""
IndoxHub Client Module

This module provides a client for interacting with the IndoxHub API, which serves as a unified
interface to multiple AI providers and models. The client handles authentication, rate limiting,
error handling, and provides a standardized response format across different AI services.

IMPORTANT: The IndoxHub server now supports only cookie-based authentication. This client
automatically handles authentication by exchanging your API key for a JWT token through the login endpoint.

The Client class offers methods for:
- Authentication and session management
- Making API requests with automatic token refresh
- Accessing AI capabilities: chat completions, text completions, embeddings, image generation, video generation, and text-to-speech
- Retrieving information about available providers and models
- Monitoring usage statistics and credit consumption
- BYOK (Bring Your Own Key) support for using your own provider API keys

Usage example:
    ```python
    from IndoxHub import Client

    # Initialize client with API key
    client = Client(api_key="your_api_key")

    # Get available models
    models = client.models()

    # Generate a chat completion
    response = client.chat([
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Tell me a joke."}
    ], model="openai/gpt-4o-mini")

    # Generate text embeddings
    embeddings = client.embeddings("This is a sample text", model="openai/text-embedding-ada-002")

    # Generate images
    images = client.images("A futuristic city", model="openai/dall-e-3")

    # Generate videos (Google - synchronous)
    videos = client.videos("A neon hologram of a cat", model="google/veo-3.0-generate-001", duration=8)

    # Generate videos (OpenAI - asynchronous)
    response = client.videos("A cat on a motorcycle", model="openai/sora-2", duration=4)
    job_id = response["data"]["job_id"]
    # Wait for completion
    final_status = client.wait_for_video_job(job_id)
    video_url = final_status["result"]["video_url"]

    # Generate text-to-speech audio
    audio = client.text_to_speech("Hello, welcome to IndoxHub!", model="openai/tts-1", voice="alloy")

    # Transcribe audio to text using speech-to-text
    transcription = client.speech_to_text("path/to/audio.mp3", model="openai/whisper-1")

    # Using BYOK (Bring Your Own Key)
    response = client.chat([
        {"role": "user", "content": "Hello!"}
    ], model="openai/gpt-4", byok_api_key="sk-your-openai-key-here")

    # Clean up resources when done
    client.close()
    ```

The client can also be used as a context manager:
    ```python
    with Client(api_key="your_api_key") as client:
        response = client.chat([{"role": "user", "content": "Hello!"}], model="openai/gpt-4o-mini")
    ```

BYOK (Bring Your Own Key) Support:
    The client supports BYOK, allowing you to use your own API keys for AI providers:

    - No credit deduction from your IndoxHub account
    - No rate limiting from the platform
    - Direct provider access with your own API keys
    - Cost control - you pay providers directly at their rates

    Example:
        response = client.chat(
            messages=[{"role": "user", "content": "Hello!"}],
            model="openai/gpt-4",
            byok_api_key="sk-your-openai-key-here"
        )
"""

import os
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union
import requests
import json

from .exceptions import (
    AuthenticationError,
    NetworkError,
    ProviderNotFoundError,
    ModelNotFoundError,
    ModelNotAvailableError,
    InvalidParametersError,
    RateLimitError,
    ProviderError,
    RequestError,
    InsufficientCreditsError,
    ValidationError,
    APIError,
)
from .constants import (
    DEFAULT_BASE_URL,
    DEFAULT_TIMEOUT,
    DEFAULT_MODEL,
    DEFAULT_EMBEDDING_MODEL,
    DEFAULT_IMAGE_MODEL,
    DEFAULT_TTS_MODEL,
    DEFAULT_STT_MODEL,
    DEFAULT_VIDEO_MODEL,
    CHAT_ENDPOINT,
    COMPLETION_ENDPOINT,
    EMBEDDING_ENDPOINT,
    IMAGE_ENDPOINT,
    VIDEO_ENDPOINT,
    TTS_ENDPOINT,
    STT_ENDPOINT,
    STT_TRANSLATION_ENDPOINT,
    MODEL_ENDPOINT,
    USAGE_ENDPOINT,
    USE_COOKIES,
)

logger = logging.getLogger(__name__)


class Client:
    """
    Client for interacting with the IndoxHub API.
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        timeout: int = DEFAULT_TIMEOUT,
        base_url: Optional[str] = None,
    ):
        """
        Initialize the client.

        Args:
            api_key: API key for authentication. If not provided, the client will look for the
                INDOX_ROUTER_API_KEY environment variable.
            timeout: Request timeout in seconds.
            base_url: Base URL for the API. If not provided, the client will use the default URL.
        """

        use_cookies = USE_COOKIES
        self.api_key = api_key or os.environ.get("INDOX_ROUTER_API_KEY")
        if not self.api_key:
            raise ValueError(
                "API key must be provided either as an argument or as the INDOX_ROUTER_API_KEY environment variable."
            )

        self.base_url = base_url if base_url is not None else DEFAULT_BASE_URL

        if self.base_url.endswith("/"):
            self.base_url = self.base_url.rstrip("/")

        self.timeout = timeout
        self.use_cookies = use_cookies
        self.session = requests.Session()

        # Authenticate and get JWT tokens
        self._authenticate()

    def _authenticate(self):
        """
        Authenticate with the server and get JWT tokens.
        This uses the /auth/token endpoint to get JWT tokens using the API key.
        """
        try:
            # First try with the dedicated API key endpoint
            logger.debug("Authenticating with dedicated API key endpoint")
            response = self.session.post(
                f"{self.base_url}/api/v1/auth/api-key",
                headers={"X-API-Key": self.api_key},
                timeout=self.timeout,
            )

            if response.status_code != 200:
                # If dedicated endpoint fails, try using the API key as a username
                logger.debug("API key endpoint failed, trying with API key as username")
                response = self.session.post(
                    f"{self.base_url}/api/v1/auth/token",
                    data={
                        "username": self.api_key,
                        "password": self.api_key,  # Try using API key as both username and password
                    },
                    timeout=self.timeout,
                )

                if response.status_code != 200:
                    # Try one more method - the token endpoint with different format
                    logger.debug("Trying with API key as token parameter")
                    response = self.session.post(
                        f"{self.base_url}/api/v1/auth/token",
                        data={
                            "username": "pip_client",
                            "password": self.api_key,
                        },
                        timeout=self.timeout,
                    )

            if response.status_code != 200:
                error_data = {}
                try:
                    error_data = response.json()
                except:
                    error_data = {"detail": response.text}

                raise AuthenticationError(
                    f"Authentication failed: {error_data.get('detail', 'Unknown error')}"
                )

            # Check if we have a token in the response body
            try:
                response_data = response.json()
                if "access_token" in response_data:
                    # Store token in the session object for later use
                    self.access_token = response_data["access_token"]
                    logger.debug("Retrieved access token from response body")
            except:
                # If we couldn't parse JSON, that's fine - we'll rely on cookies
                logger.debug("No token found in response body, will rely on cookies")

            # At this point, the cookies should be set in the session
            logger.debug("Authentication successful")

            # Check if we have the cookies we need
            if "access_token" not in self.session.cookies:
                logger.warning(
                    "Authentication succeeded but no access_token cookie was set"
                )

        except requests.RequestException as e:
            logger.error(f"Authentication request failed: {str(e)}")
            raise NetworkError(f"Network error during authentication: {str(e)}")

    def _get_domain(self):
        """
        Extract domain from the base URL for cookie setting.
        """
        try:
            from urllib.parse import urlparse

            parsed_url = urlparse(self.base_url)
            return parsed_url.netloc
        except Exception:
            # If parsing fails, return a default value
            return ""

    def enable_debug(self, level=logging.DEBUG):
        """
        Enable debug logging for the client.

        Args:
            level: Logging level (default: logging.DEBUG)
        """
        handler = logging.StreamHandler()
        handler.setFormatter(
            logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        )
        logger.addHandler(handler)
        logger.setLevel(level)
        logger.debug("Debug logging enabled")

    def _request(
        self,
        method: str,
        endpoint: str,
        data: Optional[Dict[str, Any]] = None,
        stream: bool = False,
        files: Optional[Dict[str, Any]] = None,
    ) -> Any:
        """
        Make a request to the API.

        Args:
            method: HTTP method (GET, POST, etc.)
            endpoint: API endpoint
            data: Request data
            stream: Whether to stream the response
            files: Files to upload (for multipart/form-data requests)

        Returns:
            Response data
        """
        # Add API version prefix if not already present
        if not endpoint.startswith("api/v1/") and not endpoint.startswith("/api/v1/"):
            endpoint = f"api/v1/{endpoint}"

        # Remove any leading slash for consistent URL construction
        if endpoint.startswith("/"):
            endpoint = endpoint[1:]

        url = f"{self.base_url}/{endpoint}"

        # Set headers based on whether we're uploading files
        if files:
            # For multipart/form-data, don't set Content-Type header
            # requests will set it automatically with boundary
            headers = {}
        else:
            headers = {"Content-Type": "application/json"}

        # Add Authorization header if we have an access token
        if hasattr(self, "access_token") and self.access_token:
            headers["Authorization"] = f"Bearer {self.access_token}"

        # logger.debug(f"Making {method} request to {url}")
        # if data:
        #     logger.debug(f"Request data: {json.dumps(data, indent=2)}")

        # Diagnose potential issues with the request (only for non-file uploads)
        if method == "POST" and data and not files:
            diagnosis = self.diagnose_request(endpoint, data)
            if not diagnosis["is_valid"]:
                issues_str = "\n".join([f"- {issue}" for issue in diagnosis["issues"]])
                logger.warning(f"Request validation issues:\n{issues_str}")
                # We'll still send the request, but log the issues

        try:
            # Prepare request parameters
            request_params = {
                "method": method,
                "url": url,
                "headers": headers,
                "timeout": self.timeout,
                "stream": stream,
            }

            # Add data based on request type
            if files:
                # For file uploads, use form data
                request_params["data"] = data
                request_params["files"] = files
            else:
                # For regular requests, use JSON
                request_params["json"] = data

            response = self.session.request(**request_params)

            # Check if we need to reauthenticate (401 Unauthorized) - for both streaming and non-streaming
            if response.status_code == 401:
                logger.debug("Received 401, attempting to reauthenticate")
                self._authenticate()

                # Update Authorization header with new token if available
                if hasattr(self, "access_token") and self.access_token:
                    headers["Authorization"] = f"Bearer {self.access_token}"
                    request_params["headers"] = headers

                # Retry the request after reauthentication
                response = self.session.request(**request_params)

            # For streaming requests, check if the response is successful before returning
            if stream:
                if response.status_code >= 400:
                    # If streaming request still fails after retry, raise an exception
                    response.raise_for_status()
                return response

            response.raise_for_status()
            return response.json()
        except requests.HTTPError as e:
            error_data = {}
            try:
                error_data = e.response.json()
                logger.error(f"HTTP error response: {json.dumps(error_data, indent=2)}")
            except (ValueError, AttributeError):
                error_data = {"detail": str(e)}
                logger.error(f"HTTP error (no JSON response): {str(e)}")

            status_code = getattr(e.response, "status_code", 500)
            error_message = error_data.get("detail", str(e))

            if status_code == 401:
                raise AuthenticationError(f"Authentication failed: {error_message}")
            elif status_code == 404:
                if "provider" in error_message.lower():
                    raise ProviderNotFoundError(error_message)
                elif "model" in error_message.lower():
                    # Check if it's a model not found vs model not available
                    if (
                        "not supported" in error_message.lower()
                        or "disabled" in error_message.lower()
                        or "unavailable" in error_message.lower()
                    ):
                        raise ModelNotAvailableError(error_message)
                    else:
                        raise ModelNotFoundError(error_message)
                else:
                    raise APIError(f"Resource not found: {error_message} (URL: {url})")
            elif status_code == 429:
                raise RateLimitError(f"Rate limit exceeded: {error_message}")
            elif status_code == 400:
                # Check if it's a validation error or invalid parameters
                if (
                    "validation" in error_message.lower()
                    or "invalid format" in error_message.lower()
                ):
                    raise ValidationError(f"Request validation failed: {error_message}")
                else:
                    raise InvalidParametersError(f"Invalid parameters: {error_message}")
            elif status_code == 402:
                raise InsufficientCreditsError(f"Insufficient credits: {error_message}")
            elif status_code == 422:
                # Unprocessable Entity - typically validation errors
                raise ValidationError(f"Request validation failed: {error_message}")
            elif status_code == 503:
                # Service Unavailable - model might be temporarily unavailable
                if "model" in error_message.lower():
                    raise ModelNotAvailableError(
                        f"Model temporarily unavailable: {error_message}"
                    )
                else:
                    raise APIError(f"Service unavailable: {error_message}")
            elif status_code == 500:
                # Provide more detailed information for server errors
                error_detail = error_data.get("detail", "No details provided")
                # Include the request data in the error message for better debugging
                request_data_str = json.dumps(data, indent=2) if data else "None"
                raise RequestError(
                    f"Server error (500): {error_detail}. URL: {url}.\n"
                    f"Request data: {request_data_str}\n"
                    f"This may indicate an issue with the server configuration or a problem with the provider service."
                )
            elif status_code >= 400 and status_code < 500:
                # Client errors
                raise APIError(f"Client error ({status_code}): {error_message}")
            else:
                # Server errors
                raise RequestError(f"Server error ({status_code}): {error_message}")
        except requests.RequestException as e:
            logger.error(f"Request exception: {str(e)}")
            raise NetworkError(f"Network error: {str(e)}")

    def _format_model_string(self, model: str) -> str:
        """
        Format the model string in a way that the server expects.

        The server might be expecting a different format than "provider/model".
        This method handles different formatting requirements.

        Args:
            model: Model string in the format "provider/model"

        Returns:
            Formatted model string
        """
        if not model or "/" not in model:
            return model

        # The standard format is "provider/model"
        # But the server might be expecting something different
        provider, model_name = model.split("/", 1)

        # For now, return the original format as it seems the server
        # is having issues with JSON formatted model strings
        return model

    def _format_image_size_for_provider(
        self, size: str, provider: str, model: str
    ) -> str:
        """
        Format the image size parameter based on the provider's requirements.

        Google requires aspect ratios like "1:1", "4:3", etc. while OpenAI uses pixel dimensions
        like "1024x1024", "512x512", etc.

        Args:
            size: The size parameter (e.g., "1024x1024")
            provider: The provider name (e.g., "google", "openai")
            model: The model name

        Returns:
            Formatted size parameter appropriate for the provider
        """
        if provider.lower() == "google":
            # Google uses aspect ratios instead of pixel dimensions
            # Convert common pixel dimensions to aspect ratios
            size_to_aspect_ratio = {
                "1024x1024": "1:1",
                "512x512": "1:1",
                "256x256": "1:1",
                "1024x768": "4:3",
                "768x1024": "3:4",
                "1024x1536": "2:3",
                "1536x1024": "3:2",
                "1792x1024": "16:9",
                "1024x1792": "9:16",
            }

            # Check if size is already in aspect ratio format (contains a colon)
            if ":" in size:
                return size

            # Convert to aspect ratio if we have a mapping, otherwise use default 1:1
            return size_to_aspect_ratio.get(size, "1:1")

        # For other providers, return the original size
        return size

    def chat(
        self,
        messages: List[Dict[str, str]],
        model: str = DEFAULT_MODEL,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        stream: bool = False,
        byok_api_key: Optional[str] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        Generate a chat completion.

        Args:
            messages: List of messages in the conversation
            model: Model to use in the format "provider/model" (e.g., "openai/gpt-4o-mini")
            temperature: Sampling temperature
            max_tokens: Maximum number of tokens to generate
            stream: Whether to stream the response
            byok_api_key: Your own API key for the provider (BYOK - Bring Your Own Key)
            **kwargs: Additional parameters to pass to the API

        Returns:
            Response data
        """
        # Format the model string
        formatted_model = self._format_model_string(model)

        # Filter out problematic parameters
        filtered_kwargs = {}
        for key, value in kwargs.items():
            if key not in ["return_generator"]:  # List of parameters to exclude
                filtered_kwargs[key] = value

        data = {
            "messages": messages,
            "model": formatted_model,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": stream,
            "byok_api_key": byok_api_key,
            "additional_params": filtered_kwargs,
        }

        if stream:
            response = self._request("POST", CHAT_ENDPOINT, data, stream=True)
            return self._handle_streaming_response(response)
        else:
            return self._request("POST", CHAT_ENDPOINT, data)

    def completion(
        self,
        prompt: str,
        model: str = DEFAULT_MODEL,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        stream: bool = False,
        byok_api_key: Optional[str] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        Generate a text completion.

        Args:
            prompt: Text prompt
            model: Model to use in the format "provider/model" (e.g., "openai/gpt-4o-mini")
            temperature: Sampling temperature
            max_tokens: Maximum number of tokens to generate
            stream: Whether to stream the response
            byok_api_key: Your own API key for the provider (BYOK - Bring Your Own Key)
            **kwargs: Additional parameters to pass to the API

        Returns:
            Response data
        """
        # Format the model string
        formatted_model = self._format_model_string(model)

        # Filter out problematic parameters
        filtered_kwargs = {}
        for key, value in kwargs.items():
            if key not in ["return_generator"]:  # List of parameters to exclude
                filtered_kwargs[key] = value

        data = {
            "prompt": prompt,
            "model": formatted_model,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": stream,
            "byok_api_key": byok_api_key,
            "additional_params": filtered_kwargs,
        }

        if stream:
            response = self._request("POST", COMPLETION_ENDPOINT, data, stream=True)
            return self._handle_streaming_response(response)
        else:
            return self._request("POST", COMPLETION_ENDPOINT, data)

    def embeddings(
        self,
        text: Union[str, List[str]],
        model: str = DEFAULT_EMBEDDING_MODEL,
        byok_api_key: Optional[str] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        Generate embeddings for text.

        Args:
            text: Text to embed (string or list of strings)
            model: Model to use in the format "provider/model" (e.g., "openai/text-embedding-ada-002")
            byok_api_key: Your own API key for the provider (BYOK - Bring Your Own Key)
            **kwargs: Additional parameters to pass to the API

        Returns:
            Response data with embeddings
        """
        # Format the model string
        formatted_model = self._format_model_string(model)

        # Filter out problematic parameters
        filtered_kwargs = {}
        for key, value in kwargs.items():
            if key not in ["return_generator"]:  # List of parameters to exclude
                filtered_kwargs[key] = value

        data = {
            "text": text if isinstance(text, list) else [text],
            "model": formatted_model,
            "byok_api_key": byok_api_key,
            "additional_params": filtered_kwargs,
        }

        return self._request("POST", EMBEDDING_ENDPOINT, data)

    def images(
        self,
        prompt: str,
        model: str = DEFAULT_IMAGE_MODEL,
        size: Optional[str] = None,
        n: Optional[int] = None,
        quality: Optional[str] = None,
        style: Optional[str] = None,
        # Standard parameters
        response_format: Optional[str] = None,
        user: Optional[str] = None,
        # OpenAI-specific parameters
        background: Optional[str] = None,
        moderation: Optional[str] = None,
        output_compression: Optional[int] = None,
        output_format: Optional[str] = None,
        # Google-specific parameters
        negative_prompt: Optional[str] = None,
        guidance_scale: Optional[float] = None,
        seed: Optional[int] = None,
        safety_filter_level: Optional[str] = None,
        person_generation: Optional[str] = None,
        include_safety_attributes: Optional[bool] = None,
        include_rai_reason: Optional[bool] = None,
        language: Optional[str] = None,
        output_mime_type: Optional[str] = None,
        add_watermark: Optional[bool] = None,
        enhance_prompt: Optional[bool] = None,
        # Google-specific direct parameters
        aspect_ratio: Optional[str] = None,
        byok_api_key: Optional[str] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        Generate images from a prompt.

        Args:
            prompt: Text prompt
            model: Model to use in the format "provider/model" (e.g., "openai/dall-e-3", "google/imagen-3.0-generate-002")

            # Provider-specific parameters - will only be included if explicitly provided
            # Note: Different providers support different parameters
            size: Image size - For OpenAI: "1024x1024", "512x512", etc. For Google: use aspect_ratio instead
            n: Number of images to generate
            quality: Image quality (e.g., "standard", "hd") - supported by some providers
            style: Image style (e.g., "vivid", "natural") - supported by some providers

            # Standard parameters
            response_format: Format of the response - "url" or "b64_json"
            user: A unique identifier for the end-user

            # OpenAI-specific parameters
            background: Background style - "transparent", "opaque", or "auto"
            moderation: Moderation level - "low" or "auto"
            output_compression: Compression quality for output images (0-100)
            output_format: Output format - "png", "jpeg", or "webp"

            # Google-specific parameters
            negative_prompt: Description of what to discourage in the generated images
            guidance_scale: Controls how much the model adheres to the prompt
            seed: Random seed for image generation
            safety_filter_level: Filter level for safety filtering
            person_generation: Controls generation of people ("dont_allow", "allow_adult", "allow_all")
            include_safety_attributes: Whether to report safety scores of generated images
            include_rai_reason: Whether to include filter reason if the image is filtered
            language: Language of the text in the prompt
            output_mime_type: MIME type of the generated image
            add_watermark: Whether to add a watermark to the generated images
            enhance_prompt: Whether to use prompt rewriting logic
            aspect_ratio: Aspect ratio for Google models (e.g., "1:1", "16:9") - preferred over size

            byok_api_key: Your own API key for the provider (BYOK - Bring Your Own Key)

            **kwargs: Additional parameters to pass to the API

        Returns:
            Response data with image URLs
        """
        # Format the model string
        formatted_model = self._format_model_string(model)

        # Extract provider and model name from model string if present
        provider = "openai"  # Default provider
        model_name = model
        if "/" in model:
            provider, model_name = model.split("/", 1)

        # Filter out problematic parameters
        filtered_kwargs = {}
        for key, value in kwargs.items():
            if key not in ["return_generator"]:  # List of parameters to exclude
                filtered_kwargs[key] = value

        # Create the base request data with only the required parameters
        data = {
            "prompt": prompt,
            "model": formatted_model,
            "byok_api_key": byok_api_key,
        }

        # Add optional parameters only if they are explicitly provided
        if n is not None:
            data["n"] = n

        # Handle size/aspect_ratio parameters based on provider
        if provider.lower() == "google":
            # For Google, use aspect_ratio instead of size
            if aspect_ratio is not None:
                # Google's imagen-3 has specific supported aspect ratios
                if model_name == "imagen-3.0-generate-002" and aspect_ratio not in [
                    "1:1",
                    "3:4",
                    "4:3",
                    "9:16",
                    "16:9",
                ]:
                    aspect_ratio = "1:1"  # Default to 1:1 if not supported
                data["aspect_ratio"] = aspect_ratio
            elif size is not None:
                # Convert size to aspect_ratio
                formatted_size = self._format_image_size_for_provider(
                    size, provider, model_name
                )
                data["aspect_ratio"] = formatted_size
            else:
                # Default aspect_ratio for Google
                data["aspect_ratio"] = "1:1"
        elif provider.lower() == "xai":
            # xAI doesn't support size parameter - do not include it
            pass
        elif size is not None and provider.lower() != "xai":
            # For other providers (like OpenAI), use size as is
            data["size"] = size

        if quality is not None:
            data["quality"] = quality
        if style is not None:
            data["style"] = style

        # Add standard parameters if provided
        if response_format is not None:
            # Only add response_format if explicitly provided by the user
            data["response_format"] = response_format

        if user is not None:
            data["user"] = user

        # Add OpenAI-specific parameters if provided
        if background is not None:
            data["background"] = background
        if moderation is not None:
            data["moderation"] = moderation
        if output_compression is not None:
            data["output_compression"] = output_compression
        if output_format is not None:
            data["output_format"] = output_format

        # Add Google-specific parameters if provided
        if negative_prompt is not None:
            data["negative_prompt"] = negative_prompt
        if guidance_scale is not None:
            data["guidance_scale"] = guidance_scale
        if seed is not None:
            data["seed"] = seed
        if safety_filter_level is not None:
            data["safety_filter_level"] = safety_filter_level
        if person_generation is not None:
            data["person_generation"] = person_generation
        if include_safety_attributes is not None:
            data["include_safety_attributes"] = include_safety_attributes
        if include_rai_reason is not None:
            data["include_rai_reason"] = include_rai_reason
        if language is not None:
            data["language"] = language
        if output_mime_type is not None:
            data["output_mime_type"] = output_mime_type
        if add_watermark is not None:
            data["add_watermark"] = add_watermark
        if enhance_prompt is not None:
            data["enhance_prompt"] = enhance_prompt

        # Add any remaining parameters
        if filtered_kwargs:
            data["additional_params"] = filtered_kwargs

        # Special case handling for specific models and providers
        # Only include parameters supported by each model based on their JSON definitions
        if provider.lower() == "openai" and "gpt-image" in model_name.lower():
            # For OpenAI's gpt-image models, don't automatically add response_format
            if "response_format" in data and response_format is None:
                del data["response_format"]

        if provider.lower() == "xai" and "grok-2-image" in model_name.lower():
            # For xAI's grok-2-image models, ensure size is not included
            if "size" in data:
                del data["size"]

        # Clean up any parameters that shouldn't be sent to specific providers
        # This ensures we only send parameters that each provider supports
        supported_params = self._get_supported_parameters_for_model(
            provider, model_name
        )
        if supported_params:
            for param in list(data.keys()):
                if param not in ["prompt", "model"] and param not in supported_params:
                    del data[param]

        return self._request("POST", IMAGE_ENDPOINT, data)

    def videos(
        self,
        prompt: str,
        model: str = DEFAULT_VIDEO_MODEL,
        # Common parameters
        aspect_ratio: Optional[str] = None,
        resolution: Optional[str] = None,
        duration: Optional[int] = None,
        n: Optional[int] = None,
        # OpenAI-specific parameters
        size: Optional[str] = None,
        # Google-specific parameters
        input_image: Optional[str] = None,
        reference_image: Optional[str] = None,
        reference_images: Optional[List[str]] = None,
        generate_audio: Optional[bool] = None,
        negative_prompt: Optional[str] = None,
        person_generation: Optional[str] = None,
        last_frame: Optional[str] = None,
        video: Optional[str] = None,
        # General parameters
        response_format: Optional[str] = None,
        byok_api_key: Optional[str] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        Generate videos from a prompt.

        Args:
            prompt: Text prompt for video generation
            model: Model to use in the format "provider/model" (e.g., "google/veo-3.0-generate-001")

            # Common parameters
            aspect_ratio: Video aspect ratio (e.g., "16:9", "9:16", "1:1")
            resolution: Video resolution (e.g., "720p", "1080p")
            duration: Duration of the video in seconds (e.g., 4, 6, 8)
            n: Number of videos to generate

            # OpenAI-specific parameters
            size: Video size specification for OpenAI models

            # Google Veo-specific parameters
            input_image: Base64 encoded image or URL for image-to-video
            reference_image: Base64 encoded reference image (for style reference)
            reference_images: List of base64 encoded reference images (up to 3 for Veo 3.1)
            generate_audio: Whether to generate synchronized audio with video (Veo 3+)
            negative_prompt: Description of what NOT to include in the video (Veo 3.1+)
            person_generation: Control generation of people - "dont_allow", "allow_adult", "allow_all"
            last_frame: Base64 encoded last frame for frame interpolation
            video: Base64 encoded video or GCS URI for video extension

            # General parameters
            response_format: Format of the response ("url" or "b64_json")
            byok_api_key: Your own API key for the provider (BYOK - Bring Your Own Key)

            **kwargs: Additional parameters to pass to the API

        Returns:
            Response data with video URLs

        Examples:
            Basic text-to-video:
                response = client.videos(
                    "A neon hologram of a cat driving at top speed",
                    model="google/veo-3.0-generate-001",
                    duration=8,
                    resolution="720p"
                )

            Text-to-video with audio:
                response = client.videos(
                    "A butterfly lands on a flower",
                    model="google/veo-3.0-generate-001",
                    duration=6,
                    generate_audio=True
                )

            Portrait mode video:
                response = client.videos(
                    "A vertical video of a waterfall",
                    model="google/veo-3.0-generate-001",
                    aspect_ratio="9:16",
                    resolution="720p",
                    duration=6
                )

            High resolution with Veo 3:
                response = client.videos(
                    "A futuristic city at sunset",
                    model="google/veo-3.0-generate-001",
                    aspect_ratio="16:9",
                    resolution="1080p",
                    duration=4
                )

            Fast generation with Veo 3 Fast:
                response = client.videos(
                    "Waves crashing on a beach",
                    model="google/veo-3.0-fast-generate-001",
                    duration=8
                )

            Advanced Veo 3.1 with negative prompt:
                response = client.videos(
                    "A beautiful garden with flowers",
                    model="google/veo-3.1-generate-preview",
                    duration=8,
                    negative_prompt="rain, storm, dark clouds",
                    person_generation="allow_adult"
                )

            Image-to-video:
                response = client.videos(
                    "Animate this image with gentle movement",
                    model="google/veo-3.0-generate-001",
                    input_image="data:image/png;base64,iVBOR...",
                    duration=6
                )

            Using BYOK (Bring Your Own Key):
                response = client.videos(
                    "A scenic landscape",
                    model="google/veo-3.0-generate-001",
                    byok_api_key="your-google-api-key-here"
                )
        """
        # Format the model string
        formatted_model = self._format_model_string(model)

        # Filter out problematic parameters
        filtered_kwargs = {}
        for key, value in kwargs.items():
            if key not in ["return_generator"]:  # List of parameters to exclude
                filtered_kwargs[key] = value

        # Create the base request data with required parameters
        data = {
            "prompt": prompt,
            "model": formatted_model,
        }

        # Add optional parameters only if they are explicitly provided
        if n is not None:
            data["n"] = n
        if aspect_ratio is not None:
            data["aspect_ratio"] = aspect_ratio
        if resolution is not None:
            data["resolution"] = resolution
        if duration is not None:
            data["duration"] = duration

        # Add OpenAI-specific parameters if provided
        if size is not None:
            data["size"] = size

        # Add Google-specific parameters if provided
        if input_image is not None:
            data["input_image"] = input_image
        if reference_image is not None:
            data["reference_image"] = reference_image
        if reference_images is not None:
            data["reference_images"] = reference_images
        if generate_audio is not None:
            data["generate_audio"] = generate_audio
        if negative_prompt is not None:
            data["negative_prompt"] = negative_prompt
        if person_generation is not None:
            data["person_generation"] = person_generation
        if last_frame is not None:
            data["last_frame"] = last_frame
        if video is not None:
            data["video"] = video

        # Add general parameters if provided
        if response_format is not None:
            data["response_format"] = response_format

        # Add any remaining parameters
        if filtered_kwargs:
            data["additional_params"] = filtered_kwargs

        # Add BYOK API key if provided
        if byok_api_key:
            data["byok_api_key"] = byok_api_key

        return self._request("POST", VIDEO_ENDPOINT, data)

    def get_video_job_status(self, job_id: str) -> Dict[str, Any]:
        """
        Get the status of an async video generation job.

        Args:
            job_id: The job ID returned from the videos() call

        Returns:
            Job status information including completion status and video URL when ready

        Example:
            # Start video generation
            response = client.videos(
                "A cat on a motorcycle",
                model="openai/sora-2",
                duration=4
            )

            # Check if it's an async job
            if isinstance(response.get("data"), dict) and response["data"].get("is_async"):
                job_id = response["data"]["job_id"]

                # Get status
                status = client.get_video_job_status(job_id)
                print(f"Status: {status['status']}, Progress: {status['progress']}%")

                # When completed
                if status["status"] == "completed":
                    video_url = status["result"]["video_url"]
                    print(f"Video ready: {video_url}")
        """
        return self._request("GET", f"videos/jobs/{job_id}")

    def list_video_jobs(self, limit: int = 20, skip: int = 0) -> Dict[str, Any]:
        """
        List all video generation jobs for the current user.

        Args:
            limit: Maximum number of jobs to return
            skip: Number of jobs to skip (for pagination)

        Returns:
            Dictionary with jobs list and pagination info

        Example:
            jobs = client.list_video_jobs(limit=10)
            for job in jobs["jobs"]:
                print(f"Job {job['job_id']}: {job['status']}")
        """
        return self._request("GET", f"videos/jobs?limit={limit}&skip={skip}")

    def cancel_video_job(self, job_id: str) -> Dict[str, Any]:
        """
        Cancel a pending video generation job.

        Args:
            job_id: The job ID to cancel

        Returns:
            Cancellation status

        Example:
            response = client.videos("Test", model="openai/sora-2")
            job_id = response["data"]["job_id"]

            # Cancel if needed
            result = client.cancel_video_job(job_id)
            print(result["message"])
        """
        return self._request("POST", f"videos/jobs/{job_id}/cancel")

    def wait_for_video_job(
        self,
        job_id: str,
        check_interval: int = 15,
        max_wait_time: int = 600,
        callback: Optional[callable] = None,
    ) -> Dict[str, Any]:
        """
        Wait for a video generation job to complete.

        This method polls the job status until it's completed or failed.

        Args:
            job_id: The job ID to wait for
            check_interval: Seconds between status checks (default: 15)
            max_wait_time: Maximum time to wait in seconds (default: 600 = 10 minutes)
            callback: Optional callback function called on each status update.
                      Signature: callback(status_dict)

        Returns:
            Final job status with video URL if successful

        Raises:
            TimeoutError: If max_wait_time is exceeded
            APIError: If the job fails

        Example:
            # Simple usage
            response = client.videos("A cat", model="openai/sora-2", duration=4)
            job_id = response["data"]["job_id"]

            final_status = client.wait_for_video_job(job_id)
            video_url = final_status["result"]["video_url"]

            # With progress callback
            def on_progress(status):
                print(f"Progress: {status['progress']}%")

            final_status = client.wait_for_video_job(
                job_id,
                check_interval=10,
                callback=on_progress
            )
        """
        import time

        start_time = time.time()

        while True:
            # Check if we've exceeded max wait time
            elapsed = time.time() - start_time
            if elapsed > max_wait_time:
                raise TimeoutError(
                    f"Video job did not complete within {max_wait_time} seconds"
                )

            # Get current status
            status = self.get_video_job_status(job_id)

            # Call callback if provided
            if callback:
                callback(status)

            # Check if job is in final state
            if status["status"] == "completed":
                return status
            elif status["status"] == "failed":
                error_msg = status.get("error", "Unknown error")
                raise APIError(f"Video generation failed: {error_msg}")

            # Wait before next check
            time.sleep(check_interval)

    def text_to_speech(
        self,
        input: str,
        model: str = DEFAULT_TTS_MODEL,
        voice: Optional[str] = None,
        response_format: Optional[str] = None,
        speed: Optional[float] = None,
        instructions: Optional[str] = None,
        byok_api_key: Optional[str] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        Generate audio from text using text-to-speech models.

        Args:
            input: The text to generate audio for
            model: Model to use in the format "provider/model" (e.g., "openai/tts-1")
            voice: Voice to use for the audio generation (provider-specific)
            response_format: Format of the audio response (e.g., "mp3", "opus", "aac", "flac")
            speed: Speed of the generated audio (0.25 to 4.0)
            instructions: Optional instructions for the TTS generation
            byok_api_key: Your own API key for the provider (BYOK - Bring Your Own Key)
            **kwargs: Additional parameters to pass to the API

        Returns:
            Response data with audio content

        Examples:
            Basic usage:
                response = client.text_to_speech("Hello, world!")

            With specific voice and format:
                response = client.text_to_speech(
                    "Hello, world!",
                    model="openai/tts-1",
                    voice="alloy",
                    response_format="mp3",
                    speed=1.0
                )

            For different providers (when available):
                response = client.text_to_speech(
                    "Hello, world!",
                    model="provider/model-name",
                    voice="provider-specific-voice"
                )

            Using BYOK (Bring Your Own Key):
                response = client.text_to_speech(
                    "Hello, world!",
                    model="openai/tts-1",
                    byok_api_key="sk-your-openai-key-here"
                )
        """
        # Format the model string
        formatted_model = self._format_model_string(model)

        # Filter out problematic parameters
        filtered_kwargs = {}
        for key, value in kwargs.items():
            if key not in ["return_generator"]:  # List of parameters to exclude
                filtered_kwargs[key] = value

        # Create the base request data with required parameters
        data = {
            "input": input,
            "model": formatted_model,
        }

        # Add optional parameters only if they are explicitly provided
        if voice is not None:
            data["voice"] = voice
        if response_format is not None:
            data["response_format"] = response_format
        if speed is not None:
            data["speed"] = speed
        if instructions is not None and instructions.strip():
            data["instructions"] = instructions

        # Add any additional parameters from kwargs
        if filtered_kwargs:
            data["additional_params"] = filtered_kwargs

        # Add BYOK API key if provided
        if byok_api_key:
            data["byok_api_key"] = byok_api_key

        return self._request("POST", TTS_ENDPOINT, data)

    def speech_to_text(
        self,
        file: Union[str, bytes],
        model: str = DEFAULT_STT_MODEL,
        language: Optional[str] = None,
        prompt: Optional[str] = None,
        response_format: Optional[str] = "json",
        temperature: Optional[float] = 0.0,
        timestamp_granularities: Optional[List[str]] = None,
        byok_api_key: Optional[str] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        Transcribe audio to text using speech-to-text models.

        Args:
            file: Audio file path (str) or audio file data (bytes)
            model: Model to use in the format "provider/model" (e.g., "openai/whisper-1")
            language: Language code for the audio (e.g., "en", "es", "fr")
            prompt: Optional text to guide the model's style
            response_format: Format of the response ("json", "text", "srt", "verbose_json", "vtt")
            temperature: Temperature for transcription (0.0 to 1.0)
            timestamp_granularities: List of timestamp granularities (["word", "segment"])
            byok_api_key: Your own API key for the provider (BYOK - Bring Your Own Key)
            **kwargs: Additional parameters to pass to the API

        Returns:
            Response data with transcription text

        Examples:
            Basic usage with file path:
                response = client.speech_to_text("path/to/audio.mp3")

            Basic usage with file bytes:
                with open("audio.mp3", "rb") as f:
                    audio_data = f.read()
                response = client.speech_to_text(audio_data)

            With specific model and language:
                response = client.speech_to_text(
                    "path/to/audio.wav",
                    model="openai/whisper-1",
                    language="en",
                    response_format="json"
                )

            With timestamps for detailed analysis:
                response = client.speech_to_text(
                    "path/to/audio.mp3",
                    model="openai/whisper-1",
                    response_format="verbose_json",
                    timestamp_granularities=["word", "segment"]
                )

            Using BYOK (Bring Your Own Key):
                response = client.speech_to_text(
                    "path/to/audio.mp3",
                    model="openai/whisper-1",
                    byok_api_key="sk-your-openai-key-here"
                )
        """
        # Format the model string
        formatted_model = self._format_model_string(model)

        # Handle file input - can be a file path (str) or file data (bytes)
        if isinstance(file, str):
            # It's a file path, read the file
            try:
                with open(file, "rb") as f:
                    file_data = f.read()
                filename = os.path.basename(file)
            except FileNotFoundError:
                raise InvalidParametersError(f"File not found: {file}")
            except Exception as e:
                raise InvalidParametersError(f"Error reading file {file}: {str(e)}")
        elif isinstance(file, bytes):
            # It's file data
            file_data = file
            filename = kwargs.get("filename", "audio_file")
        else:
            raise InvalidParametersError(
                "File must be either a file path (str) or file data (bytes)"
            )

        # Prepare form data for multipart upload
        files = {"file": (filename, file_data, "audio/*")}

        # Create the form data with required parameters
        data = {
            "model": formatted_model,
        }

        # Add optional parameters only if they are provided
        if language is not None:
            data["language"] = language
        if prompt is not None:
            data["prompt"] = prompt
        if response_format is not None:
            data["response_format"] = response_format
        if temperature is not None:
            data["temperature"] = temperature
        if timestamp_granularities is not None:
            # Convert to JSON string as expected by the API
            data["timestamp_granularities"] = json.dumps(timestamp_granularities)

        # Add BYOK API key if provided
        if byok_api_key:
            data["byok_api_key"] = byok_api_key

        # Filter out problematic parameters from kwargs
        filtered_kwargs = {}
        for key, value in kwargs.items():
            if key not in [
                "filename",
                "return_generator",
            ]:  # List of parameters to exclude
                filtered_kwargs[key] = value

        # Add any additional parameters from kwargs
        if filtered_kwargs:
            data.update(filtered_kwargs)

        return self._request("POST", STT_ENDPOINT, data, files=files)

    def translate_audio(
        self,
        file: Union[str, bytes],
        model: str = DEFAULT_STT_MODEL,
        prompt: Optional[str] = None,
        response_format: Optional[str] = "json",
        temperature: Optional[float] = 0.0,
        byok_api_key: Optional[str] = None,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        Translate audio to English text using speech-to-text models.

        Args:
            file: Audio file path (str) or audio file data (bytes)
            model: Model to use in the format "provider/model" (e.g., "openai/whisper-1")
            prompt: Optional text to guide the model's style
            response_format: Format of the response ("json", "text", "srt", "verbose_json", "vtt")
            temperature: Temperature for translation (0.0 to 1.0)
            byok_api_key: Your own API key for the provider (BYOK - Bring Your Own Key)
            **kwargs: Additional parameters to pass to the API

        Returns:
            Response data with translated text in English

        Examples:
            Basic usage with file path:
                response = client.translate_audio("path/to/spanish_audio.mp3")

            With specific response format:
                response = client.translate_audio(
                    "path/to/french_audio.wav",
                    model="openai/whisper-1",
                    response_format="text"
                )

            Using BYOK (Bring Your Own Key):
                response = client.translate_audio(
                    "path/to/audio.mp3",
                    model="openai/whisper-1",
                    byok_api_key="sk-your-openai-key-here"
                )
        """
        # Format the model string
        formatted_model = self._format_model_string(model)

        # Handle file input - can be a file path (str) or file data (bytes)
        if isinstance(file, str):
            # It's a file path, read the file
            try:
                with open(file, "rb") as f:
                    file_data = f.read()
                filename = os.path.basename(file)
            except FileNotFoundError:
                raise InvalidParametersError(f"File not found: {file}")
            except Exception as e:
                raise InvalidParametersError(f"Error reading file {file}: {str(e)}")
        elif isinstance(file, bytes):
            # It's file data
            file_data = file
            filename = kwargs.get("filename", "audio_file")
        else:
            raise InvalidParametersError(
                "File must be either a file path (str) or file data (bytes)"
            )

        # Prepare form data for multipart upload
        files = {"file": (filename, file_data, "audio/*")}

        # Create the form data with required parameters
        data = {
            "model": formatted_model,
        }

        # Add optional parameters only if they are provided
        if prompt is not None:
            data["prompt"] = prompt
        if response_format is not None:
            data["response_format"] = response_format
        if temperature is not None:
            data["temperature"] = temperature

        # Add BYOK API key if provided
        if byok_api_key:
            data["byok_api_key"] = byok_api_key

        # Filter out problematic parameters from kwargs
        filtered_kwargs = {}
        for key, value in kwargs.items():
            if key not in [
                "filename",
                "return_generator",
            ]:  # List of parameters to exclude
                filtered_kwargs[key] = value

        # Add any additional parameters from kwargs
        if filtered_kwargs:
            data.update(filtered_kwargs)

        return self._request("POST", STT_TRANSLATION_ENDPOINT, data, files=files)

    def _get_supported_parameters_for_model(
        self, provider: str, model_name: str
    ) -> List[str]:
        """
        Get the list of supported parameters for a specific model.
        This helps avoid sending unsupported parameters to providers.

        Args:
            provider: The provider name (e.g., 'openai', 'google', 'xai')
            model_name: The model name (e.g., 'gpt-image-1', 'imagen-3.0-generate-002')

        Returns:
            List of parameter names supported by the model
        """
        if provider.lower() == "openai" and "gpt-image" in model_name.lower():
            return [
                "prompt",
                "size",
                "quality",
                "n",
                "user",
                "background",
                "moderation",
                "output_compression",
                "output_format",
                "style",
            ]

        elif provider.lower() == "google" and "imagen" in model_name.lower():
            return [
                "prompt",
                "n",
                "negative_prompt",
                "aspect_ratio",
                "guidance_scale",
                "seed",
                "safety_filter_level",
                "person_generation",
                "include_safety_attributes",
                "include_rai_reason",
                "language",
                "output_mime_type",
                "output_compression_quality",
                "add_watermark",
                "enhance_prompt",
                "response_format",
            ]

        elif provider.lower() == "xai" and "grok-2-image" in model_name.lower():
            return ["prompt", "n", "response_format"]

        # Default case - allow all parameters
        return []

    def models(self, provider: Optional[str] = None) -> Dict[str, Any]:
        """
        Get available models.

        Args:
            provider: Provider to filter by

        Returns:
            List of available models with pricing information
        """
        endpoint = MODEL_ENDPOINT
        if provider:
            endpoint = f"{MODEL_ENDPOINT}/{provider}"

        return self._request("GET", endpoint)

    def get_model_info(self, provider: str, model: str) -> Dict[str, Any]:
        """
        Get information about a specific model.

        Args:
            provider: Provider ID
            model: Model ID

        Returns:
            Model information including pricing
        """
        return self._request("GET", f"{MODEL_ENDPOINT}/{provider}/{model}")

    def get_usage(self) -> Dict[str, Any]:
        """
        Get usage statistics for the current user.

        Returns:
            Usage statistics
        """
        return self._request("GET", USAGE_ENDPOINT)

    def test_connection(self) -> Dict[str, Any]:
        """
        Test the connection to the server and return server status information.

        This method can be used to diagnose connection issues and verify that
        the server is accessible and properly configured.

        Returns:
            Dictionary containing server status information
        """
        try:
            # Try to access the base URL
            response = self.session.get(self.base_url, timeout=self.timeout)

            # Try to get server info if available
            server_info = {}
            try:
                if response.headers.get("Content-Type", "").startswith(
                    "application/json"
                ):
                    server_info = response.json()
            except:
                pass

            return {
                "status": "connected",
                "url": self.base_url,
                "status_code": response.status_code,
                "server_info": server_info,
                "headers": dict(response.headers),
            }
        except requests.RequestException as e:
            return {
                "status": "error",
                "url": self.base_url,
                "error": str(e),
                "error_type": type(e).__name__,
            }

    def diagnose_request(self, endpoint: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Diagnose potential issues with a request before sending it to the server.

        This method checks for common issues like malformed model strings,
        invalid message formats, or missing required parameters.

        Args:
            endpoint: API endpoint
            data: Request data

        Returns:
            Dictionary with diagnosis results
        """
        issues = []
        warnings = []

        # Check if this is a chat request
        if endpoint == CHAT_ENDPOINT:
            # Check model format
            if "model" in data:
                model = data["model"]
                # Check if the model is already formatted as JSON
                if (
                    isinstance(model, str)
                    and model.startswith("{")
                    and model.endswith("}")
                ):
                    try:
                        model_json = json.loads(model)
                        if (
                            not isinstance(model_json, dict)
                            or "provider" not in model_json
                            or "model" not in model_json
                        ):
                            issues.append(f"Invalid model JSON format: {model}")
                    except json.JSONDecodeError:
                        issues.append(f"Invalid model JSON format: {model}")
                elif not isinstance(model, str):
                    issues.append(f"Model must be a string, got {type(model).__name__}")
                elif "/" not in model:
                    issues.append(
                        f"Model '{model}' is missing provider prefix (should be 'provider/model')"
                    )
                else:
                    provider, model_name = model.split("/", 1)
                    if not provider or not model_name:
                        issues.append(
                            f"Invalid model format: '{model}'. Should be 'provider/model'"
                        )
            else:
                warnings.append("No model specified, will use default model")

            # Check messages format
            if "messages" in data:
                messages = data["messages"]
                if not isinstance(messages, list):
                    issues.append(
                        f"Messages must be a list, got {type(messages).__name__}"
                    )
                elif not messages:
                    issues.append("Messages list is empty")
                else:
                    for i, msg in enumerate(messages):
                        if not isinstance(msg, dict):
                            issues.append(
                                f"Message {i} must be a dictionary, got {type(msg).__name__}"
                            )
                        elif "role" not in msg:
                            issues.append(f"Message {i} is missing 'role' field")
                        elif "content" not in msg:
                            issues.append(f"Message {i} is missing 'content' field")
            else:
                issues.append("No messages specified")

        # Check if this is a completion request
        elif endpoint == COMPLETION_ENDPOINT:
            # Check model format (same as chat)
            if "model" in data:
                model = data["model"]
                if not isinstance(model, str):
                    issues.append(f"Model must be a string, got {type(model).__name__}")
                elif "/" not in model:
                    issues.append(
                        f"Model '{model}' is missing provider prefix (should be 'provider/model')"
                    )
            else:
                warnings.append("No model specified, will use default model")

            # Check prompt
            if "prompt" not in data:
                issues.append("No prompt specified")
            elif not isinstance(data["prompt"], str):
                issues.append(
                    f"Prompt must be a string, got {type(data['prompt']).__name__}"
                )

        # Return diagnosis results
        return {
            "endpoint": endpoint,
            "issues": issues,
            "warnings": warnings,
            "is_valid": len(issues) == 0,
            "data": data,
        }

    def _handle_streaming_response(self, response):
        """
        Handle a streaming response.

        Supports both the new OpenAI Responses API format and legacy format for backward compatibility.

        Args:
            response: Streaming response

        Returns:
            Generator yielding response chunks
        """
        accumulated_text = ""
        try:
            for line in response.iter_lines():
                if line:
                    line = line.decode("utf-8")
                    if line.startswith("data: "):
                        data = line[6:]
                        if data == "[DONE]":
                            break
                        try:
                            # Parse JSON chunk
                            chunk = json.loads(data)

                            # Check if this is an error chunk
                            if "error" in chunk:
                                # Extract error details
                                error_info = chunk["error"]
                                if isinstance(error_info, str):
                                    # Try to parse error details from the string
                                    if "Status 401" in error_info:
                                        raise AuthenticationError(
                                            f"Authentication failed during streaming: {error_info}"
                                        )
                                    else:
                                        raise APIError(
                                            f"API error during streaming: {error_info}"
                                        )
                                else:
                                    raise APIError(f"Streaming error: {error_info}")

                            # Handle new OpenAI Responses API format
                            event_type = chunk.get("type", "")

                            # Handle response.created event
                            if event_type == "response.created":
                                yield chunk
                                continue

                            # Handle response.output_item.added event
                            if event_type == "response.output_item.added":
                                yield chunk
                                continue

                            # Handle response.content_part.added event
                            if event_type == "response.content_part.added":
                                yield chunk
                                continue

                            # Handle reasoning started event
                            if event_type == "response.reasoning.started":
                                yield chunk
                                continue

                            # Handle reasoning delta events (comes first, before text)
                            if event_type == "response.reasoning.delta":
                                reasoning_delta = chunk.get("delta", "")
                                # Yield with backward-compatible data field
                                yield {
                                    **chunk,
                                    "data": reasoning_delta,  # For backward compatibility
                                    "reasoning": True,  # Flag to identify reasoning chunks
                                }
                                continue

                            # Handle response.content_part.delta event (text streaming)
                            if event_type == "response.content_part.delta":
                                delta_text = chunk.get("delta", "")
                                accumulated_text += delta_text
                                # Yield with backward-compatible data field
                                yield {
                                    **chunk,
                                    "data": delta_text,  # For backward compatibility
                                }
                                continue

                            # Handle response.output_item.done event
                            if event_type == "response.output_item.done":
                                # Extract full text from the item
                                item = chunk.get("item", {})
                                content = item.get("content", [])
                                if content and len(content) > 0:
                                    text_content = content[0].get("text", "")
                                    accumulated_text = text_content

                                # Build response with backward-compatible fields
                                response_chunk = {
                                    **chunk,
                                    "data": accumulated_text,  # For backward compatibility
                                }

                                # Add reasoning if available
                                if "reasoning" in item:
                                    response_chunk["reasoning"] = item["reasoning"]

                                yield response_chunk
                                continue

                            # Handle response.done event
                            if event_type == "response.done":
                                yield chunk
                                continue

                            # Handle image generation call events
                            if event_type.startswith("response.image_generation_call."):
                                yield chunk
                                continue

                            # Handle legacy format (backward compatibility)
                            # Handle image chunks
                            if "images" in chunk:
                                # This is an image chunk - yield it as-is for the user to handle
                                yield chunk
                                continue

                            # For legacy chat responses with choices
                            if "choices" in chunk:
                                # For delta responses (streaming)
                                choice = chunk["choices"][0]
                                if "delta" in choice and "content" in choice["delta"]:
                                    # Add a data field for backward compatibility
                                    chunk["data"] = choice["delta"]["content"]
                                # For text responses (completion)
                                elif "text" in choice:
                                    chunk["data"] = choice["text"]

                            yield chunk
                        except json.JSONDecodeError:
                            # For raw text responses
                            yield {"data": data}
        finally:
            response.close()

    def close(self):
        """Close the session."""
        self.session.close()

    def __enter__(self):
        """Enter context manager."""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Exit context manager."""
        self.close()

    def set_base_url(self, base_url: str) -> None:
        """
        Set a new base URL for the API.

        Args:
            base_url: New base URL for the API.
        """
        self.base_url = base_url
        logger.debug(f"Base URL set to {base_url}")


IndoxHub = Client
