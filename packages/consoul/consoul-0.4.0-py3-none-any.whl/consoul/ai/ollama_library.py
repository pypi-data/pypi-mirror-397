"""Ollama Library - Discover and browse models from ollama.com."""

from __future__ import annotations

import json
import os
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Literal

import requests
from bs4 import BeautifulSoup


@dataclass
class OllamaLibraryModel:
    """Represents a model from the Ollama library."""

    name: str
    description: str
    model_url: str
    url: str
    num_pulls: str = ""
    num_tags: str = ""
    updated: str = ""
    tags: list[str] = field(default_factory=list)
    supports_vision: bool = False
    supports_tools: bool = False
    supports_reasoning: bool = False


@dataclass
class OllamaLibraryData:
    """Container for library models with cache metadata."""

    models: list[OllamaLibraryModel]
    last_update: datetime


def get_cache_dir() -> Path:
    """Get the cache directory path."""
    cache_dir = Path.home() / ".consoul" / "cache"
    cache_dir.mkdir(parents=True, exist_ok=True)
    return cache_dir


def get_static_data_path() -> Path:
    """Get path to static ollama models data file.

    Returns:
        Path to data/ollama_models.json in the project root directory
    """
    # Get the project root (consoul/ai -> consoul -> src -> project root -> data)
    # __file__ is .../consoul/src/consoul/ai/ollama_library.py
    # We want .../consoul/data/ollama_models.json
    project_root = Path(__file__).parent.parent.parent.parent
    return project_root / "data" / "ollama_models.json"


def load_static_models() -> list[OllamaLibraryModel]:
    """Load models from static data file (data/ollama_models.json).

    This provides a fallback when ollama.com is unreachable or as a faster
    alternative to web scraping. The static file is generated by
    scripts/scrape_ollama_library.py.

    Returns:
        List of OllamaLibraryModel objects from static data, or empty list if not found
    """
    static_file = get_static_data_path()

    if not static_file.exists():
        return []

    try:
        with open(static_file, encoding="utf-8") as f:
            data = json.load(f)

        # Convert dict format to OllamaLibraryModel
        models = []
        for item in data:
            # Get model_url - might be in different fields
            model_url = item.get("model_url", "")
            if not model_url and "url" in item:
                # Extract path from full URL
                url_str = item["url"]
                if "/library/" in url_str:
                    model_url = url_str.split("ollama.com")[-1]

            model = OllamaLibraryModel(
                name=item.get("name", ""),
                description=item.get("description", ""),
                model_url=model_url,
                url=item.get("url", ""),
                num_pulls=item.get("num_pulls", ""),
                num_tags=item.get("num_tags", ""),
                updated=item.get("updated", ""),
                tags=item.get("tags", []),
                supports_vision=item.get("supports_vision", False),
                supports_tools=item.get("supports_tools", False),
                supports_reasoning=item.get("supports_reasoning", False),
            )
            models.append(model)

        return models
    except (json.JSONDecodeError, KeyError, ValueError, OSError):
        # Failed to load static data
        return []


def fetch_library_models(
    namespace: str = "library",
    category: Literal["popular", "featured", "newest"] | None = None,
    force_refresh: bool = False,
    timeout: int = 10,
    use_static_fallback: bool = True,
) -> list[OllamaLibraryModel]:
    """
    Fetch available models from ollama.com.

    Tries multiple sources in order:
    1. Local cache (if not force_refresh and < 24 hours old)
    2. Web scraping from ollama.com
    3. Static data file (if use_static_fallback=True and web fails)

    Args:
        namespace: The namespace to fetch models from (default: "library")
        category: Optional category filter ("popular", "featured", "newest")
        force_refresh: Force refresh even if cache is valid
        timeout: Request timeout in seconds
        use_static_fallback: Use static data file if web scraping fails (default: True)

    Returns:
        List of OllamaLibraryModel objects

    Raises:
        requests.RequestException: If the request fails and no fallback available
    """
    # Sanitize namespace
    if not namespace:
        namespace = "library"
    namespace = os.path.basename(namespace)

    # For library namespace, prefer static data (has capabilities from scraper)
    # Web scraping doesn't extract capabilities, only the scraper script does
    if namespace == "library" and use_static_fallback:
        static_models = load_static_models()
        if static_models:
            return static_models

    # Check cache first
    cache_file = get_cache_dir() / f"ollama_library_{namespace}.json"
    if not force_refresh and cache_file.exists():
        try:
            with open(cache_file, encoding="utf-8") as f:
                data = json.load(f)
            cache_data = OllamaLibraryData(
                models=[OllamaLibraryModel(**m) for m in data["models"]],
                last_update=datetime.fromisoformat(data["last_update"]),
            )
            # Check if cache is less than 24 hours old
            age = datetime.now(timezone.utc) - cache_data.last_update.replace(
                tzinfo=timezone.utc
            )
            if age.total_seconds() < 86400:  # 24 hours
                return cache_data.models
        except (json.JSONDecodeError, KeyError, ValueError):
            # Invalid cache, will refresh
            pass

    # Try to fetch from ollama.com
    models: list[OllamaLibraryModel] = []

    try:
        url_base = f"https://ollama.com/{namespace}"
        categories = [category] if category else ["popular", "featured", "newest"]
        seen_names: set[str] = set()

        for cat in categories:
            url = url_base
            if namespace == "models":
                url += f"?sort={cat}"

            response = requests.get(
                url, headers={"User-Agent": "Mozilla/5.0"}, timeout=timeout
            )
            response.raise_for_status()

            soup = BeautifulSoup(response.content.decode("utf-8"), "html.parser")

            for card in soup.find_all("li", class_="items-baseline"):
                # Extract basic info
                h2 = card.find("h2")
                p = card.find("p")
                a = card.find("a")

                if not h2 or not p or not a:
                    continue

                name = h2.text.strip()
                if name in seen_names:
                    continue
                seen_names.add(name)

                meta_data = {
                    "name": name,
                    "description": p.text.strip(),
                    "model_url": a["href"],
                    "url": f"https://ollama.com{a['href']}",
                    "num_pulls": "",
                    "num_tags": "",
                    "updated": "",
                    "tags": [],
                }

                # Extract metadata spans
                spans = card.find_all(
                    "span", class_=["flex", "items-center"], recursive=True
                )
                span_texts = [s.text.strip() for s in spans if s.text.strip()]

                for text in span_texts:
                    if "Pulls" in text:
                        meta_data["num_pulls"] = text.split("\n")[0].strip()
                    elif "Tags" in text:
                        meta_data["num_tags"] = text.split("\xa0")[0].strip()
                    elif "Updated" in text:
                        meta_data["updated"] = text.split("\xa0")[-1].strip()

                # Extract tags
                tag_spans = card.find_all(
                    "span", class_=["text-blue-600"], recursive=True
                )
                tags = [t.text.strip() for t in tag_spans if t.text.strip()]
                meta_data["tags"] = tags

                models.append(OllamaLibraryModel(**meta_data))

            # For library namespace, only fetch once (not per category)
            if namespace != "models":
                break

        # Save to cache if we got models from web
        if models:
            cache_data = OllamaLibraryData(
                models=models, last_update=datetime.now(timezone.utc)
            )
            with open(cache_file, "w", encoding="utf-8") as f:
                json.dump(
                    {
                        "models": [
                            {
                                "name": m.name,
                                "description": m.description,
                                "model_url": m.model_url,
                                "url": m.url,
                                "num_pulls": m.num_pulls,
                                "num_tags": m.num_tags,
                                "updated": m.updated,
                                "tags": m.tags,
                            }
                            for m in cache_data.models
                        ],
                        "last_update": cache_data.last_update.isoformat(),
                    },
                    f,
                    indent=2,
                )

    except (requests.RequestException, Exception):
        # Web scraping failed, try static fallback
        if use_static_fallback and namespace == "library":
            static_models = load_static_models()
            if static_models:
                return static_models
        # If no static fallback or it failed, re-raise
        if not use_static_fallback:
            raise

    # Return models (from web or empty if failed and no fallback)
    return models
