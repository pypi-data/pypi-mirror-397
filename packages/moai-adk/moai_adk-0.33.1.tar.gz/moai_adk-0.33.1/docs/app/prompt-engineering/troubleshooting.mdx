---
title: "í”„ë¡¬í”„íŠ¸ ë¬¸ì œ í•´ê²°"
description: "í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì‹œ ë°œìƒí•˜ëŠ” ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²° ë°©ë²•, ë””ë²„ê¹… ë° ìµœì í™” ê¸°ë²•"
---

# í”„ë¡¬í”„íŠ¸ ë¬¸ì œ í•´ê²°

í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê³¼ì •ì—ì„œ ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤ì„ ì§„ë‹¨í•˜ê³  í•´ê²°í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤. ì²´ê³„ì ì¸ ë¬¸ì œ í•´ê²° ì ‘ê·¼ë²•ì„ í†µí•´ í”„ë¡¬í”„íŠ¸ í’ˆì§ˆì„ ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ” ë¬¸ì œ ì§„ë‹¨ í”„ë ˆì„ì›Œí¬

### 1. ë¬¸ì œ ë¶„ë¥˜ ì²´ê³„

```python
# í”„ë¡¬í”„íŠ¸ ë¬¸ì œ ë¶„ë¥˜
class PromptProblemClassifier:
    def __init__(self):
        self.problem_categories = {
            "ambiguity": "ëª¨í˜¸ì„± ë¬¸ì œ",
            "insufficient_context": "ì»¨í…ìŠ¤íŠ¸ ë¶€ì¡±",
            "scope_issues": "ë²”ìœ„ ë¬¸ì œ",
            "format_problems": "í˜•ì‹ ë¬¸ì œ",
            "clarity_issues": "ëª…í™•ì„± ë¬¸ì œ",
            "complexity_mismatch": "ë³µì¡ë„ ë¶ˆì¼ì¹˜"
        }

    def classify_problem(self, prompt, response, user_feedback=None):
        """í”„ë¡¬í”„íŠ¸ ë¬¸ì œ ë¶„ë¥˜"""
        problems = []

        # ëª¨í˜¸ì„± ë¬¸ì œ ì§„ë‹¨
        if self.detect_ambiguity(prompt):
            problems.append({
                "type": "ambiguity",
                "severity": "high",
                "description": "ì—¬ëŸ¬ í•´ì„ì´ ê°€ëŠ¥í•œ í‘œí˜„ì´ ìˆìŠµë‹ˆë‹¤",
                "suggestions": self.get_ambiguity_suggestions(prompt)
            })

        # ì»¨í…ìŠ¤íŠ¸ ë¶€ì¡± ì§„ë‹¨
        if self.detect_insufficient_context(prompt):
            problems.append({
                "type": "insufficient_context",
                "severity": "medium",
                "description": "ì¶©ë¶„í•œ ë°°ê²½ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤",
                "suggestions": self.get_context_suggestions(prompt)
            })

        # ë²”ìœ„ ë¬¸ì œ ì§„ë‹¨
        if self.detect_scope_issues(prompt, response):
            problems.append({
                "type": "scope_issues",
                "severity": "medium",
                "description": "ìš”ì²­ ë²”ìœ„ê°€ ë„ˆë¬´ ë„“ê±°ë‚˜ ì¢ìŠµë‹ˆë‹¤",
                "suggestions": self.get_scope_suggestions(prompt, response)
            })

        return problems

    def detect_ambiguity(self, prompt):
        """ëª¨í˜¸ì„± íƒì§€"""
        ambiguity_indicators = [
            "ì¢‹ì€", "ë‚˜ìœ", "ì ì ˆí•œ", "íš¨ìœ¨ì ì¸",  # ì£¼ê´€ì  í‘œí˜„
            "ì–´ë–¤", "ëª‡ëª‡", "ëŒ€ëµ", "ì•½ê°„",  # ë¶ˆëª…í™•í•œ ìˆ˜ëŸ‰
            "ê°œì„ ", "ìµœì í™”", "í–¥ìƒ"  # ì¶”ìƒì  ëª©í‘œ
        ]

        prompt_lower = prompt.lower()
        return any(indicator in prompt_lower for indicator in ambiguity_indicators)

    def detect_insufficient_context(self, prompt):
        """ì»¨í…ìŠ¤íŠ¸ ë¶€ì¡± íƒì§€"""
        context_elements = [
            "í”„ë¡œì íŠ¸", "ëª©í‘œ", "ìš”êµ¬ì‚¬í•­", "ì œì•½ì‚¬í•­",
            "í™˜ê²½", "ê¸°ìˆ  ìŠ¤íƒ", "íŒ€ ê·œëª¨", "ë§ˆê°ì¼"
        ]

        prompt_lower = prompt.lower()
        context_count = sum(element in prompt_lower for element in context_elements)

        return context_count < 2  # ì»¨í…ìŠ¤íŠ¸ ìš”ì†Œê°€ 2ê°œ ë¯¸ë§Œì´ë©´ ë¶€ì¡±

    def detect_scope_issues(self, prompt, response):
        """ë²”ìœ„ ë¬¸ì œ íƒì§€"""
        # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ì™€ ì‘ë‹µ ê¸¸ì´ ë¹„êµ
        prompt_length = len(prompt.split())
        response_length = len(response.split())

        # ë„ˆë¬´ ì§§ì€ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ë„ˆë¬´ ê¸´ ì‘ë‹µ
        if prompt_length < 50 and response_length > 1000:
            return True

        # ë„ˆë¬´ ê´‘ë²”ìœ„í•œ ìš”ì²­ í‚¤ì›Œë“œ
        broad_requests = ["ëª¨ë“  ê²ƒ", "ì „ì²´ ì‹œìŠ¤í…œ", "ì™„ì „í•œ ì†”ë£¨ì…˜", "ëª¨ë“  ê¸°ëŠ¥"]
        prompt_lower = prompt.lower()

        return any(request in prompt_lower for request in broad_requests)

# ë¬¸ì œ ë¶„ë¥˜ ì˜ˆì‹œ
classifier = PromptProblemClassifier()

problem_prompt = "ì¢‹ì€ APIë¥¼ ë§Œë“¤ì–´ì¤˜"
problem_response = "[ë§¤ìš° ê¸´ API ì„¤ê³„ì™€ ì½”ë“œ...]"

problems = classifier.classify_problem(problem_prompt, problem_response)
for problem in problems:
    print(f"{problem['type']}: {problem['description']}")
```

### 2. ì‘ë‹µ í’ˆì§ˆ ë¶„ì„

```python
# ì‘ë‹µ í’ˆì§ˆ ë¶„ì„ê¸°
class ResponseQualityAnalyzer:
    def __init__(self):
        self.quality_metrics = {
            "completeness": "ì™„ì„±ë„",
            "accuracy": "ì •í™•ì„±",
            "relevance": "ê´€ë ¨ì„±",
            "clarity": "ëª…í™•ì„±",
            "consistency": "ì¼ê´€ì„±"
        }

    def analyze_response_quality(self, prompt, response, expected_aspects=None):
        """ì‘ë‹µ í’ˆì§ˆ ë¶„ì„"""
        analysis = {}

        # ì™„ì„±ë„ ë¶„ì„
        analysis["completeness"] = self.assess_completeness(
            response, expected_aspects
        )

        # ê´€ë ¨ì„± ë¶„ì„
        analysis["relevance"] = self.assess_relevance(prompt, response)

        # ëª…í™•ì„± ë¶„ì„
        analysis["clarity"] = self.assess_clarity(response)

        # ì¼ê´€ì„± ë¶„ì„
        analysis["consistency"] = self.assess_consistency(response)

        # ì¢…í•© ì ìˆ˜
        analysis["overall_score"] = self.calculate_overall_score(analysis)

        return analysis

    def assess_completeness(self, response, expected_aspects):
        """ì™„ì„±ë„ í‰ê°€"""
        if not expected_aspects:
            return {"score": 0.8, "feedback": "ê¸°ëŒ€ ìš”ì†Œ ë¯¸ì •ì˜"}

        response_lower = response.lower()
        found_aspects = []

        for aspect in expected_aspects:
            if aspect.lower() in response_lower:
                found_aspects.append(aspect)

        completeness_score = len(found_aspects) / len(expected_aspects)

        return {
            "score": completeness_score,
            "found_aspects": found_aspects,
            "missing_aspects": [a for a in expected_aspects if a not in found_aspects],
            "feedback": self.get_completeness_feedback(completeness_score)
        }

    def assess_relevance(self, prompt, response):
        """ê´€ë ¨ì„± í‰ê°€"""
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ì„± ì¸¡ì •
        prompt_keywords = self.extract_keywords(prompt)
        response_keywords = self.extract_keywords(response)

        common_keywords = set(prompt_keywords) & set(response_keywords)
        relevance_score = len(common_keywords) / max(len(prompt_keywords), 1)

        return {
            "score": relevance_score,
            "prompt_keywords": prompt_keywords,
            "response_keywords": response_keywords,
            "common_keywords": list(common_keywords),
            "feedback": self.get_relevance_feedback(relevance_score)
        }

    def assess_clarity(self, response):
        """ëª…í™•ì„± í‰ê°€"""
        clarity_issues = []

        # ë„ˆë¬´ ê¸´ ë¬¸ì¥ ê²€ì‚¬
        sentences = response.split('.')
        long_sentences = [s for s in sentences if len(s.split()) > 30]
        if long_sentences:
            clarity_issues.append(f"ë„ˆë¬´ ê¸´ ë¬¸ì¥ {len(long_sentences)}ê°œ")

        # ëª¨í˜¸í•œ í‘œí˜„ ê²€ì‚¬
        ambiguous_terms = ["ì•½ê°„", "ì–´ëŠ ì •ë„", "ëŒ€ëµ", "ì•„ë§ˆë„"]
        for term in ambiguous_terms:
            if term in response:
                clarity_issues.append(f"ëª¨í˜¸í•œ í‘œí˜„: {term}")

        clarity_score = max(0.3, 1.0 - len(clarity_issues) * 0.2)

        return {
            "score": clarity_score,
            "issues": clarity_issues,
            "feedback": self.get_clarity_feedback(clarity_score)
        }

    def extract_keywords(self, text):
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê¶Œì¥)
        import re
        # ëª…ì‚¬ì™€ ê¸°ìˆ  ìš©ì–´ ì¶”ì¶œ
        words = re.findall(r'\b[A-Z][a-z]+|[A-Z]{2,}|[a-z]+(?:-[a-z]+)*\b', text)
        # ë¶ˆìš©ì–´ ì œê±°
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for'}
        return [word for word in words if word.lower() not in stop_words and len(word) > 2]
```

## ğŸ› ï¸ ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²° ë°©ë²•

### 1. ëª¨í˜¸ì„± ë¬¸ì œ

#### ë¬¸ì œ ì¦ìƒ
```python
# ëª¨í˜¸ì„± ë¬¸ì œ ì˜ˆì‹œ
AMBIGUOUS_PROMPTS = [
    "ì¢‹ì€ ì½”ë“œë¥¼ ì‘ì„±í•´ì¤˜",  # "ì¢‹ì€"ì´ ëª¨í˜¸í•¨
    "ì„±ëŠ¥ì„ ê°œì„ í•´ì¤˜",       # ì–´ë–¤ ì„±ëŠ¥ì¸ì§€ ëª…ì‹œë˜ì§€ ì•ŠìŒ
    "ì‚¬ìš©ì ì¹œí™”ì ì¸ UI ë§Œë“¤ì–´ì¤˜",  # "ì‚¬ìš©ì ì¹œí™”ì " ê¸°ì¤€ ë¶ˆëª…í™•
    "íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•´ì¤˜",  # "íš¨ìœ¨ì " ê¸°ì¤€ ë¶€ì¬
]
```

#### í•´ê²° ë°©ë²•
```python
# ëª¨í˜¸ì„± í•´ê²° í…œí”Œë¦¿
CLARIFICATION_TEMPLATE = """
# êµ¬ì²´í™”ëœ ìš”ì²­
ëª¨í˜¸í•œ ìš”ì²­: "{vague_request}"

# ëª…í™•í•œ ìš”ì²­ìœ¼ë¡œ ê°œì„ 
ê°œì„ ëœ ìš”ì²­:
{improved_request}

# ê°œì„  í¬ì¸íŠ¸
{improvements}

# ì¶”ê°€ êµ¬ì²´í™” ë°©ì•ˆ
{further_clarification}
"""

# ëª¨í˜¸ì„± í•´ê²° ì˜ˆì‹œ
def resolve_ambiguity(vague_request):
    """ëª¨í˜¸ì„± í•´ê²°"""
    ambiguity_solutions = {
        "ì¢‹ì€ ì½”ë“œ": """
        ëª…í™•í•œ ìš”ì²­:
        "ë‹¤ìŒ ê¸°ì¤€ì„ ë§Œì¡±í•˜ëŠ” Python ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:
        - PEP 8 ì½”ë”© ìŠ¤íƒ€ì¼ ì¤€ìˆ˜
        - íƒ€ì… íŒíŠ¸ í¬í•¨
        - ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 90% ì´ìƒ
        - ì‹¤í–‰ ì‹œê°„ O(n) ì´ë‚´"
        """,

        "ì„±ëŠ¥ ê°œì„ ": """
        ëª…í™•í•œ ìš”ì²­:
        "ë‹¤ìŒ ì„±ëŠ¥ ì§€í‘œë¥¼ ê°œì„ í•˜ëŠ” ìµœì í™”ë¥¼ ì§„í–‰í•´ì£¼ì„¸ìš”:
        - í˜„ì¬ ì‘ë‹µ ì‹œê°„: 2.5ì´ˆ â†’ ëª©í‘œ: 200ms
        - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 500MB â†’ ëª©í‘œ: 100MB
        - CPU ì‚¬ìš©ë¥ : 80% â†’ ëª©í‘œ: 50%"
        """
    }

    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ ë§¤ì¹­ ë¡œì§ í•„ìš”
    for vague_term, solution in ambiguity_solutions.items():
        if vague_term in vague_request:
            return solution

    return "êµ¬ì²´ì ì¸ ê°œì„  ìš”êµ¬ì‚¬í•­ì„ ëª…ì‹œí•´ì£¼ì„¸ìš”."
```

### 2. ì»¨í…ìŠ¤íŠ¸ ë¶€ì¡± ë¬¸ì œ

#### ë¬¸ì œ ì¦ìƒ
```python
# ì»¨í…ìŠ¤íŠ¸ ë¶€ì¡± ì˜ˆì‹œ
INSUFFICIENT_CONTEXT_EXAMPLES = [
    {
        "prompt": "ë¡œê·¸ì¸ ê¸°ëŠ¥ì„ ë§Œë“¤ì–´ì¤˜",
        "missing_context": ["ê¸°ìˆ  ìŠ¤íƒ", "ë°ì´í„°ë² ì´ìŠ¤", "ë³´ì•ˆ ìš”êµ¬ì‚¬í•­", "ì‚¬ìš©ì ìœ í˜•"]
    },
    {
        "prompt": "ë°ì´í„° ë¶„ì„ì„ í•´ì¤˜",
        "missing_context": ["ë°ì´í„° ì†ŒìŠ¤", "ë¶„ì„ ëª©í‘œ", "ì˜ˆìƒ ê²°ê³¼ë¬¼", "ì‹œê°„ ì œì•½"]
    }
]
```

#### í•´ê²° ë°©ë²•
```python
# ì»¨í…ìŠ¤íŠ¸ ë³´ê°• í…œí”Œë¦¿
CONTEXT_ENHANCEMENT_TEMPLATE = """
# ì»¨í…ìŠ¤íŠ¸ ë¶€ì¡± ë¬¸ì œ í•´ê²°

ì›ë³¸ ìš”ì²­: "{original_prompt}"

# í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
{required_context}

# ì»¨í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ê°œì„ ëœ ìš”ì²­
{enhanced_prompt}
"""

def enhance_context(prompt):
    """ì»¨í…ìŠ¤íŠ¸ ë³´ê°•"""
    context_checklist = {
        "í”„ë¡œì íŠ¸ ì •ë³´": ["í”„ë¡œì íŠ¸ëª…", "ëª©í‘œ", "ê·œëª¨"],
        "ê¸°ìˆ ì  ìš”êµ¬ì‚¬í•­": ["ì–¸ì–´/ë²„ì „", "í”„ë ˆì„ì›Œí¬", "ë°ì´í„°ë² ì´ìŠ¤"],
        "ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­": ["íƒ€ê²Ÿ ì‚¬ìš©ì", "ì£¼ìš” ê¸°ëŠ¥", "ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­"],
        "ì œì•½ì‚¬í•­": ["ë§ˆê°ì¼", "ì˜ˆì‚°", "íŒ€ ê·œëª¨", "ê¸°ì¡´ ì‹œìŠ¤í…œ"]
    }

    # í˜„ì¬ í”„ë¡¬í”„íŠ¸ì—ì„œ ì°¾ì€ ì»¨í…ìŠ¤íŠ¸
    found_context = {}
    missing_context = {}

    for category, items in context_checklist.items():
        found_items = []
        for item in items:
            if item.lower() in prompt.lower():
                found_items.append(item)

        if found_items:
            found_context[category] = found_items
        else:
            missing_context[category] = items

    # ì»¨í…ìŠ¤íŠ¸ ì§ˆë¬¸ ìƒì„±
    context_questions = []
    for category, items in missing_context.items():
        if items:  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì§ˆë¬¸ ì¶”ê°€
            question = f"{category}ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš” (ì˜ˆ: {', '.join(items[:2])})"
            context_questions.append(question)

    return {
        "found_context": found_context,
        "missing_context": missing_context,
        "clarification_questions": context_questions,
        "enhancement_suggestion": generate_enhancement_suggestion(
            prompt, context_questions
        )
    }
```

### 3. ë²”ìœ„ ë¬¸ì œ

#### ë¬¸ì œ ì¦ìƒ
```python
# ë²”ìœ„ ë¬¸ì œ ìœ í˜•
SCOPE_ISSUES = {
    "too_broad": [
        "ì™„ì „í•œ ì „ììƒê±°ë˜ í”Œë«í¼ ë§Œë“¤ì–´ì¤˜",
        "ì¸ê³µì§€ëŠ¥ ì‹œìŠ¤í…œ ì„¤ê³„í•´ì¤˜",
        "ëª¨ë“  ê²ƒì„ ìµœì í™”í•´ì¤˜"
    ],
    "too_narrow": [
        "ì„¸ë¯¸ì½œë¡  í•˜ë‚˜ ì¶”ê°€í•´ì¤˜",
        "ë³€ìˆ˜ëª…ë§Œ ë°”ê¿”ì¤˜",
        "ì£¼ì„ í•œ ì¤„ ì¶”ê°€í•´ì¤˜"
    ]
}
```

#### í•´ê²° ë°©ë²•
```python
# ë²”ìœ„ ì¡°ì • í…œí”Œë¦¿
SCOPE_ADJUSTMENT_TEMPLATE = """
# ë²”ìœ„ ì¡°ì • ê°€ì´ë“œ

ì›ë³¸ ìš”ì²­: "{original_prompt}"
ë¬¸ì œ ìœ í˜•: "{scope_problem_type}"

# ì¡°ì •ëœ ìš”ì²­
{adjusted_request}

# ì¡°ì • ì´ìœ 
{adjustment_reasoning}

# ë‹¨ê³„ë³„ ì ‘ê·¼ ì œì•ˆ
{phased_approach}
"""

def adjust_scope(prompt, problem_type):
    """ë²”ìœ„ ì¡°ì •"""
    if problem_type == "too_broad":
        return break_down_complex_task(prompt)
    elif problem_type == "too_narrow":
        return broaden_simple_task(prompt)
    else:
        return prompt

def break_down_complex_task(complex_prompt):
    """ë³µì¡í•œ ê³¼ì—… ë¶„í•´"""
    breakdown_strategies = {
        "ì „ììƒê±°ë˜ í”Œë«í¼": """
        ë‹¨ê³„ë³„ ì ‘ê·¼:
        1ë‹¨ê³„: ì‚¬ìš©ì ì¸ì¦ ì‹œìŠ¤í…œ (ê°€ì¥ ê¸°ë³¸ì )
        2ë‹¨ê³„: ìƒí’ˆ ê´€ë¦¬ ì‹œìŠ¤í…œ
        3ë‹¨ê³„: ì¥ë°”êµ¬ë‹ˆ ê¸°ëŠ¥
        4ë‹¨ê³„: ê²°ì œ ì‹œìŠ¤í…œ
        5ë‹¨ê³„: ì£¼ë¬¸ ê´€ë¦¬

        í˜„ì¬ëŠ” ì–´ë–¤ ë‹¨ê³„ë¥¼ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?
        """,
        "ì¸ê³µì§€ëŠ¥ ì‹œìŠ¤í…œ": """
        êµ¬ì²´ì ì¸ ë²”ìœ„ ì§€ì •ì´ í•„ìš”í•©ë‹ˆë‹¤:
        - ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        - ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€
        - ì˜ˆì¸¡ API ê°œë°œ
        - ëª¨ë¸ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
        - ê²°ê³¼ ì‹œê°í™” ëŒ€ì‹œë³´ë“œ

        ì–´ëŠ ë¶€ë¶„ì— ì§‘ì¤‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ?
        """
    }

    return breakdown_strategies.get(complex_prompt, "êµ¬ì²´ì ì¸ ê¸°ëŠ¥ ëª©ë¡ì„ ì œê³µí•´ì£¼ì„¸ìš”.")

def broaden_simple_task(simple_prompt):
    """ë‹¨ìˆœí•œ ê³¼ì—… í™•ì¥"""
    broadening_strategies = {
        "ì„¸ë¯¸ì½œë¡  ì¶”ê°€": """
        ë” í¬ê´„ì ì¸ ì½”ë“œ ë¦¬ë·° ë° ê°œì„  ì œì•ˆ:
        - ì½”ë“œ ìŠ¤íƒ€ì¼ ê²€í† 
        - ì ì¬ì  ë²„ê·¸ ì‹ë³„
        - ì„±ëŠ¥ ê°œì„  ì œì•ˆ
        - ê°€ë…ì„± í–¥ìƒ
        - í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¶”ê°€ ì œì•ˆ
        """,
        "ë³€ìˆ˜ëª… ë³€ê²½": """
        ì „ì²´ì ì¸ ì½”ë“œ í’ˆì§ˆ ê°œì„ :
        - ëª…ëª… ê·œì¹™ ì¼ê´€ì„± ê²€í† 
        - í•¨ìˆ˜/í´ë˜ìŠ¤ êµ¬ì¡° ê°œì„ 
        - íƒ€ì… íŒíŠ¸ ì¶”ê°€
        - ë¬¸ì„œí™” ê°œì„ 
        - ì•„í‚¤í…ì²˜ íŒ¨í„´ ì œì•ˆ
        """
    }

    return broadening_strategies.get(simple_prompt, "ë” í° ë§¥ë½ì—ì„œì˜ ê°œì„ ì„ ì œì•ˆí•©ë‹ˆë‹¤.")
```

### 4. í˜•ì‹ ë¬¸ì œ

#### ë¬¸ì œ ì¦ìƒ
```python
# í˜•ì‹ ë¬¸ì œ ì˜ˆì‹œ
FORMAT_ISSUES = {
    "missing_structure": "êµ¬ì¡°í™”ëœ ì‘ë‹µ í˜•ì‹ì„ ì§€ì •í•˜ì§€ ì•ŠìŒ",
    "unclear_output": "ì›í•˜ëŠ” ì¶œë ¥ í˜•ì‹ì„ ëª…ì‹œí•˜ì§€ ì•ŠìŒ",
    "inconsistent_formatting": "ì¼ê´€ë˜ì§€ ì•Šì€ í˜•ì‹ ìš”êµ¬"
}
```

#### í•´ê²° ë°©ë²•
```python
# í˜•ì‹ í‘œì¤€í™” í…œí”Œë¦¿
FORMAT_STANDARDIZATION_TEMPLATE = """
# í˜•ì‹ í‘œì¤€í™” ê°€ì´ë“œ

## ì¶œë ¥ í˜•ì‹ ì§€ì •
ì›ë³¸ ìš”ì²­: "{original_prompt}"
ì¶œë ¥ í˜•ì‹: "{specified_format}"

## êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸
{structured_prompt}

## í˜•ì‹ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸
{format_checklist}
"""

def standardize_format(prompt):
    """í˜•ì‹ í‘œì¤€í™”"""
    format_patterns = {
        "ì½”ë“œ": """
        # ì½”ë“œ ìš”ì²­ í˜•ì‹
        ì–¸ì–´: [Python/JavaScript/Java ë“±]
        í”„ë ˆì„ì›Œí¬: [FastAPI/React/Spring ë“±]
        íŒŒì¼ êµ¬ì¡°: [ë‹¨ì¼ íŒŒì¼/ëª¨ë“ˆ/íŒ¨í‚¤ì§€]
        í…ŒìŠ¤íŠ¸: [í¬í•¨ ì—¬ë¶€]

        # ì¶œë ¥ í˜•ì‹
        ```[ì–¸ì–´]
        # ì—¬ê¸°ì— ì½”ë“œ
        ```

        # ê´€ë ¨ ì„¤ëª…
        - ì‚¬ìš©ë²•
        - ì£¼ìš” íŠ¹ì§•
        - ì˜ì¡´ì„±
        """,

        "ë¶„ì„": """
        # ë¶„ì„ ìš”ì²­ í˜•ì‹
        ë¶„ì„ ëŒ€ìƒ: [ë°ì´í„°/ì½”ë“œ/ì‹œìŠ¤í…œ]
        ë¶„ì„ ëª©í‘œ: [ì„±ëŠ¥/ë³´ì•ˆ/êµ¬ì¡°]
        ë¶„ì„ ë²”ìœ„: [ì „ì²´/íŠ¹ì • ëª¨ë“ˆ]

        # ì¶œë ¥ í˜•ì‹
        ## 1. ê°œìš”
        ## 2. ìƒì„¸ ë¶„ì„
        ## 3. ë¬¸ì œì 
        ## 4. ê°œì„  ì œì•ˆ
        ## 5. ê²°ë¡ 
        """,

        "ì„¤ê³„": """
        # ì„¤ê³„ ìš”ì²­ í˜•ì‹
        ì„¤ê³„ ëŒ€ìƒ: [ì•„í‚¤í…ì²˜/API/UI/ë°ì´í„°ë² ì´ìŠ¤]
        ìš”êµ¬ì‚¬í•­: [ê¸°ëŠ¥/ë¹„ê¸°ëŠ¥]
        ì œì•½ì‚¬í•­: [ê¸°ìˆ /ì‹œê°„/ì˜ˆì‚°]

        # ì¶œë ¥ í˜•ì‹
        ## 1. ì„¤ê³„ ëª©í‘œ
        ## 2. ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨
        ## 3. ìƒì„¸ ì„¤ê³„
        ## 4. ê¸°ìˆ  ì„ íƒ ê·¼ê±°
        ## 5. êµ¬í˜„ ê³„íš
        """
    }

    # ìš”ì²­ ìœ í˜• ê°ì§€
    request_type = detect_request_type(prompt)
    if request_type in format_patterns:
        return format_patterns[request_type]

    return "ì›í•˜ëŠ” ì¶œë ¥ í˜•ì‹ì„ êµ¬ì²´ì ìœ¼ë¡œ ì§€ì •í•´ì£¼ì„¸ìš” (ì˜ˆ: ë§ˆí¬ë‹¤ìš´, JSON, í‘œ ë“±)."
```

## ğŸ”„ ë°˜ë³µì  ê°œì„  í”„ë¡œì„¸ìŠ¤

### 1. í”„ë¡¬í”„íŠ¸ ìµœì í™” ë£¨í”„

```python
# í”„ë¡¬í”„íŠ¸ ìµœì í™” ì‹œìŠ¤í…œ
class PromptOptimizer:
    def __init__(self):
        self.optimization_history = []
        self.performance_metrics = {}

    def optimize_prompt(self, original_prompt, response, feedback, target_metrics=None):
        """í”„ë¡¬í”„íŠ¸ ìµœì í™”"""
        # 1. í˜„ì¬ ì„±ëŠ¥ ë¶„ì„
        current_performance = self.analyze_performance(original_prompt, response, feedback)

        # 2. ë¬¸ì œì  ì‹ë³„
        problems = self.identify_problems(current_performance, target_metrics)

        # 3. ê°œì„  ì „ëµ ìˆ˜ë¦½
        improvement_strategies = self.develop_strategies(problems)

        # 4. ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
        optimized_prompt = self.generate_optimized_prompt(
            original_prompt, improvement_strategies
        )

        # 5. ê¸°ë¡
        self.record_optimization(original_prompt, optimized_prompt, problems)

        return {
            "original_prompt": original_prompt,
            "optimized_prompt": optimized_prompt,
            "identified_problems": problems,
            "improvement_strategies": improvement_strategies,
            "expected_improvements": self.predict_improvements(improvement_strategies)
        }

    def analyze_performance(self, prompt, response, feedback):
        """ì„±ëŠ¥ ë¶„ì„"""
        return {
            "response_quality": self.evaluate_response_quality(response),
            "user_satisfaction": self.parse_user_feedback(feedback),
            "efficiency": self.measure_efficiency(prompt, response),
            "reproducibility": self.assess_reproducibility(prompt, response)
        }

    def identify_problems(self, performance, targets):
        """ë¬¸ì œì  ì‹ë³„"""
        problems = []

        if targets:
            for metric, target_value in targets.items():
                current_value = performance.get(metric, 0)
                if current_value < target_value:
                    problems.append({
                        "metric": metric,
                        "current": current_value,
                        "target": target_value,
                        "gap": target_value - current_value
                    })

        return problems

    def develop_strategies(self, problems):
        """ê°œì„  ì „ëµ ìˆ˜ë¦½"""
        strategies = []

        for problem in problems:
            if problem["metric"] == "response_quality":
                strategies.extend(self.get_quality_improvement_strategies())
            elif problem["metric"] == "user_satisfaction":
                strategies.extend(self.get_satisfaction_improvement_strategies())
            elif problem["metric"] == "efficiency":
                strategies.extend(self.get_efficiency_improvement_strategies())

        return strategies

    def generate_optimized_prompt(self, original_prompt, strategies):
        """ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        optimized = original_prompt

        for strategy in strategies:
            optimized = self.apply_strategy(optimized, strategy)

        return optimized

    def apply_strategy(self, prompt, strategy):
        """ê°œì„  ì „ëµ ì ìš©"""
        strategy_implementations = {
            "add_context": self.add_context,
            "clarify_requirements": self.clarify_requirements,
            "specify_format": self.specify_format,
            "break_down_task": self.break_down_task,
            "add_examples": self.add_examples
        }

        if strategy["type"] in strategy_implementations:
            return strategy_implementations[strategy["type"]](prompt, strategy)

        return prompt
```

### 2. A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬

```python
# í”„ë¡¬í”„íŠ¸ A/B í…ŒìŠ¤íŠ¸
class PromptABTest:
    def __init__(self):
        self.test_results = {}

    def create_test(self, prompt_a, prompt_b, test_input, evaluation_criteria):
        """A/B í…ŒìŠ¤íŠ¸ ìƒì„±"""
        test_id = f"test_{len(self.test_results) + 1}"

        self.test_results[test_id] = {
            "prompt_a": prompt_a,
            "prompt_b": prompt_b,
            "test_input": test_input,
            "criteria": evaluation_criteria,
            "results_a": None,
            "results_b": None,
            "winner": None
        }

        return test_id

    def run_test(self, test_id):
        """A/B í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        test = self.test_results[test_id]

        # ë‘ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰
        response_a = self.execute_prompt(test["prompt_a"], test["test_input"])
        response_b = self.execute_prompt(test["prompt_b"], test["test_input"])

        # ê²°ê³¼ í‰ê°€
        score_a = self.evaluate_response(response_a, test["criteria"])
        score_b = self.evaluate_response(response_b, test["criteria"])

        # ê²°ê³¼ ì €ì¥
        test["results_a"] = {"response": response_a, "score": score_a}
        test["results_b"] = {"response": response_b, "score": score_b}

        # ìŠ¹ì ê²°ì •
        test["winner"] = "A" if score_a > score_b else "B"

        return self.generate_test_report(test_id)

    def evaluate_response(self, response, criteria):
        """ì‘ë‹µ í‰ê°€"""
        total_score = 0
        detailed_scores = {}

        for criterion, weight in criteria.items():
            if criterion == "completeness":
                score = self.assess_completeness(response)
            elif criterion == "accuracy":
                score = self.assess_accuracy(response)
            elif criterion == "clarity":
                score = self.assess_clarity(response)
            else:
                score = 0.5  # ê¸°ë³¸ ì ìˆ˜

            detailed_scores[criterion] = score
            total_score += score * weight

        return {
            "total_score": total_score,
            "detailed_scores": detailed_scores
        }

    def generate_test_report(self, test_id):
        """í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        test = self.test_results[test_id]

        return f"""
# í”„ë¡¬í”„íŠ¸ A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼

## í…ŒìŠ¤íŠ¸ ì„¤ì •
- í”„ë¡¬í”„íŠ¸ A: {test['prompt_a'][:100]}...
- í”„ë¡¬í”„íŠ¸ B: {test['prompt_b'][:100]}...
- í…ŒìŠ¤íŠ¸ ì…ë ¥: {test['test_input']}

## ê²°ê³¼ ë¹„êµ
### í”„ë¡¬í”„íŠ¸ A
- ì´ì : {test['results_a']['score']['total_score']}
- ìƒì„¸ ì ìˆ˜: {test['results_a']['score']['detailed_scores']}

### í”„ë¡¬í”„íŠ¸ B
- ì´ì : {test['results_b']['score']['total_score']}
- ìƒì„¸ ì ìˆ˜: {test['results_b']['score']['detailed_scores']}

## ìŠ¹ì
í”„ë¡¬í”„íŠ¸ {test['winner']}ê°€ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.

## ê°œì„  ì œì•ˆ
{self.generate_improvement_suggestions(test)}
        """
```

## ğŸ“Š ì„±ëŠ¥ ì¸¡ì • ë° ëª¨ë‹ˆí„°ë§

### 1. í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­

```python
# í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ì¸¡ì •
class PromptPerformanceMonitor:
    def __init__(self):
        self.metrics_history = []
        self.baseline_metrics = {}

    def measure_performance(self, prompt_id, prompt, response, context=None):
        """ì„±ëŠ¥ ì¸¡ì •"""
        metrics = {
            "prompt_id": prompt_id,
            "timestamp": datetime.now(),
            "prompt_length": len(prompt),
            "response_length": len(response),
            "response_time": self.measure_response_time(prompt, response),
            "quality_score": self.calculate_quality_score(prompt, response),
            "token_efficiency": self.calculate_token_efficiency(prompt, response),
            "reusability": self.assess_reusability(prompt),
            "user_satisfaction": context.get("user_feedback", 0) if context else 0
        }

        self.metrics_history.append(metrics)
        return metrics

    def calculate_quality_score(self, prompt, response):
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        quality_indicators = {
            "completeness": self.check_completeness(prompt, response),
            "relevance": self.check_relevance(prompt, response),
            "clarity": self.check_clarity(response),
            "accuracy": self.check_accuracy(response),
            "structure": self.check_structure(response)
        }

        # ê°€ì¤‘ í‰ê· 
        weights = {"completeness": 0.3, "relevance": 0.25, "clarity": 0.2,
                  "accuracy": 0.15, "structure": 0.1}

        total_score = sum(
            quality_indicators[metric] * weights[metric]
            for metric in quality_indicators
        )

        return {
            "total_score": total_score,
            "indicators": quality_indicators
        }

    def calculate_token_efficiency(self, prompt, response):
        """í† í° íš¨ìœ¨ì„± ê³„ì‚°"""
        prompt_tokens = self.count_tokens(prompt)
        response_tokens = self.count_tokens(response)

        # í† í° íš¨ìœ¨ì„± = ì‘ë‹µ í’ˆì§ˆ / ì‚¬ìš©ëœ í† í° ìˆ˜
        efficiency_score = len(response.split()) / (prompt_tokens + response_tokens)

        return {
            "prompt_tokens": prompt_tokens,
            "response_tokens": response_tokens,
            "total_tokens": prompt_tokens + response_tokens,
            "efficiency_score": efficiency_score
        }

    def generate_performance_report(self, prompt_id=None, time_range=None):
        """ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        if prompt_id:
            metrics = [m for m in self.metrics_history if m["prompt_id"] == prompt_id]
        else:
            metrics = self.metrics_history

        if time_range:
            start_date, end_date = time_range
            metrics = [
                m for m in metrics
                if start_date <= m["timestamp"] <= end_date
            ]

        if not metrics:
            return "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        # í†µê³„ ê³„ì‚°
        avg_quality = sum(m["quality_score"]["total_score"] for m in metrics) / len(metrics)
        avg_efficiency = sum(m["token_efficiency"]["efficiency_score"] for m in metrics) / len(metrics)
        avg_response_time = sum(m["response_time"] for m in metrics) / len(metrics)

        # íŠ¸ë Œë“œ ë¶„ì„
        trend_analysis = self.analyze_trends(metrics)

        return f"""
# í”„ë¡¬í”„íŠ¸ ì„±ëŠ¥ ë³´ê³ ì„œ

## ê¸°ë³¸ í†µê³„
- ë¶„ì„ëœ í”„ë¡¬í”„íŠ¸ ìˆ˜: {len(metrics)}
- í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_quality:.2f}
- í‰ê·  í† í° íš¨ìœ¨ì„±: {avg_efficiency:.4f}
- í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_response_time:.2f}ì´ˆ

## íŠ¸ë Œë“œ ë¶„ì„
{trend_analysis}

## ê°œì„  ì œì•ˆ
{self.generate_improvement_recommendations(metrics)}
        """
```

## ğŸ“– ë‹¤ìŒ ë‹¨ê³„

ì´ì œ í”„ë¡¬í”„íŠ¸ ë¬¸ì œ í•´ê²° ë°©ë²•ì„ í•™ìŠµí–ˆìœ¼ë‹ˆ, ë‹¤ìŒìœ¼ë¡œ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. **[ëª¨ë²” ì‚¬ë¡€](./best-practices)** - Claude 4.5 ìµœì í™”ì™€ ì „ë¬¸ê°€ íŒ

---

## ğŸ”— ê´€ë ¨ ìë£Œ

- [Anthropic Claude ë””ë²„ê¹… ê°€ì´ë“œ](https://docs.anthropic.com/claude/docs/debugging)
- [MoAI-ADK í’ˆì§ˆ ê´€ë¦¬](../core-concepts/workflow)
- [ì„±ëŠ¥ ìµœì í™”](../skills/moai-workflow-docs)