# coding: utf-8

"""
    Lakewatch public API

    Interact with the Lakewatch API.

    The version of the OpenAPI document: 0.1.30
    Contact: support@antimatter.io
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import BaseModel, ConfigDict, Field, StrictStr
from typing import Any, ClassVar, Dict, List, Optional
from typing import Optional, Set
from typing_extensions import Self

class DbuiV1DataSourceInferenceResult(BaseModel):
    """
    DbuiV1DataSourceInferenceResult
    """ # noqa: E501
    format: Optional[StrictStr] = Field(default=None, description="The inferred format of the data in the external location (e.g., parquet, json, jsonl, csv, etc.)")
    source: Optional[StrictStr] = Field(default=None, description="The detected data provider or origin vendor (e.g., \"aws\", \"onelogin\", \"akamai\")")
    source_type: Optional[StrictStr] = Field(default=None, description="The detected product, service, or log type from the vendor (e.g., \"route53\", \"cloudtrail\", \"waf\")", alias="sourceType")
    bronze_table_name: Optional[StrictStr] = Field(default=None, description="The inferred bronze table name. Typically this is equal to source_sourceType", alias="bronzeTableName")
    time_column: Optional[StrictStr] = Field(default=None, description="The column or nested field path containing timestamp information for the records (e.g., \"event_time\",\"metadata.timestamp\")", alias="timeColumn")
    primary_keys: Optional[List[StrictStr]] = Field(default=None, description="A set of column names or field paths that together uniquely identify each record (e.g., [\"id\", \"account_id\", \"region\"]). ", alias="primaryKeys")
    presets: Optional[List[StrictStr]] = Field(default=None, description="Array of potential preset names ordered by relevance / confidence in descending order. ")
    pre_transforms: Optional[List[List[StrictStr]]] = Field(default=None, description="List of potential pretransforms (SQL expressions) that will be used as bronze.preTransforms. For specific details refer to the bronze.preTransforms description of Datasource resource. ", alias="preTransforms")
    __properties: ClassVar[List[str]] = ["format", "source", "sourceType", "bronzeTableName", "timeColumn", "primaryKeys", "presets", "preTransforms"]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )


    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of DbuiV1DataSourceInferenceResult from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([
        ])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # set to None if format (nullable) is None
        # and model_fields_set contains the field
        if self.format is None and "format" in self.model_fields_set:
            _dict['format'] = None

        # set to None if source (nullable) is None
        # and model_fields_set contains the field
        if self.source is None and "source" in self.model_fields_set:
            _dict['source'] = None

        # set to None if source_type (nullable) is None
        # and model_fields_set contains the field
        if self.source_type is None and "source_type" in self.model_fields_set:
            _dict['sourceType'] = None

        # set to None if bronze_table_name (nullable) is None
        # and model_fields_set contains the field
        if self.bronze_table_name is None and "bronze_table_name" in self.model_fields_set:
            _dict['bronzeTableName'] = None

        # set to None if time_column (nullable) is None
        # and model_fields_set contains the field
        if self.time_column is None and "time_column" in self.model_fields_set:
            _dict['timeColumn'] = None

        # set to None if primary_keys (nullable) is None
        # and model_fields_set contains the field
        if self.primary_keys is None and "primary_keys" in self.model_fields_set:
            _dict['primaryKeys'] = None

        # set to None if presets (nullable) is None
        # and model_fields_set contains the field
        if self.presets is None and "presets" in self.model_fields_set:
            _dict['presets'] = None

        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of DbuiV1DataSourceInferenceResult from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate({
            "format": obj.get("format"),
            "source": obj.get("source"),
            "sourceType": obj.get("sourceType"),
            "bronzeTableName": obj.get("bronzeTableName"),
            "timeColumn": obj.get("timeColumn"),
            "primaryKeys": obj.get("primaryKeys"),
            "presets": obj.get("presets"),
            "preTransforms": obj.get("preTransforms")
        })
        return _obj


