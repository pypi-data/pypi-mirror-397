{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ **AIE4ML Tutorial: From QKeras ‚Üí hls4ml ‚Üí AMD AI Engine**\n",
    "\n",
    "This tutorial shows how to:\n",
    "\n",
    "* Build a small **quantized QKeras** model\n",
    "* Convert to **hls4ml (bit-exact)**\n",
    "* Convert to **AIE4ML (bit-exact)**\n",
    "* Compare **x86 simulation output**\n",
    "* Apply **simple AIE tuning overrides** (parallelism, tiling, placement)\n",
    "* Inspect **AIE simulation reports**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1Ô∏è‚É£ Setup & Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from qkeras import QDense, QActivation, quantized_bits, quantized_relu\n",
    "\n",
    "import hls4ml\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ Build a Small QKeras Model\n",
    "\n",
    "Currently supports MLP-style architectures, like Dense ‚Üí ReLU ‚Üí Dense ‚Üí ‚Ä¶."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_FEATURES = 128\n",
    "HIDDEN = 256\n",
    "OUT_FEATURES = 64\n",
    "\n",
    "\n",
    "def build_qkeras_model(in_features=128, hidden=256, out_features=64):\n",
    "    model = Sequential([\n",
    "        QActivation(quantized_bits(8, 2), name=\"input_quant\", input_shape=(in_features,)),\n",
    "        QDense(hidden,\n",
    "               name=\"qfc1\",\n",
    "               kernel_quantizer=quantized_bits(8,0,alpha=1),\n",
    "               bias_quantizer=quantized_bits(8,2,alpha=1)),\n",
    "        QActivation(quantized_relu(8,0), name=\"qrelu1\"),\n",
    "        QDense(out_features,\n",
    "               name=\"qfc2\",\n",
    "               kernel_quantizer=quantized_bits(8,0,alpha=1),\n",
    "               bias_quantizer=quantized_bits(8,2,alpha=1)),\n",
    "        QActivation(quantized_bits(8,2), name=\"output_quant\"),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_qkeras_model()\n",
    "\n",
    "model.compile(optimizer=Adam(1e-3), loss=\"mse\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3Ô∏è‚É£ Generate hls4ml config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HLS config from model\n",
    "cfg = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "\n",
    "# Explicitly set output precision for last layer when activation is linear.\n",
    "# Needed because hls4ml may omit linear activation nodes in the graph.\n",
    "cfg['LayerName']['qfc2']['Precision'] = 'fixed<8,3,TRN,WRAP,0>'\n",
    "cfg['LayerName']['qfc2_linear']['Precision'] = 'fixed<8,3,TRN,WRAP,0>'\n",
    "\n",
    "print('Layer precision summary:')\n",
    "for name, layer_cfg in cfg.get('LayerName', {}).items():\n",
    "    print(f\"  {name}: {layer_cfg.get('Precision', {})}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4Ô∏è‚É£  Convert: Baseline hls4ml + AIE models\n",
    "\n",
    "We create two compiled projects:\n",
    "\n",
    " üîπ `proj_hls/` ‚Äì reference bit-exact model\n",
    " \n",
    " üîπ `proj_aie/` ‚Äì AIE-backend model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model,\n",
    "    hls_config=cfg,\n",
    "    output_dir='proj_hls',\n",
    "    project_name='proj_hls',\n",
    "    bit_exact=True,\n",
    ")\n",
    "\n",
    "# You can specify the batch size and number of graph iterations for AIE backend\n",
    "BATCH = 8\n",
    "ITERS = 10\n",
    "PLATFORM = 'xilinx_vek280_base_202520_1'\n",
    "\n",
    "aie_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model,\n",
    "    hls_config=cfg,\n",
    "    output_dir='proj_aie',\n",
    "    backend='aie',\n",
    "    project_name='proj_aie',\n",
    "    batch_size=BATCH,\n",
    "    iterations=ITERS,\n",
    "    part = PLATFORM\n",
    ")\n",
    "\n",
    "hls_model.compile()\n",
    "aie_model.compile()\n",
    "\n",
    "print(\"Models compiled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5Ô∏è‚É£ Bit-Exact Check (HLS vs AIE x86)\n",
    "\n",
    "We test the first output batch.\n",
    "\n",
    "The AIE simulator may emit more samples if the graph has multiple iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_bit_exact(hls4ml_model, aie4ml_model, sim_mode = 'x86'):\n",
    "    x = np.random.random((BATCH, IN_FEATURES)).astype(np.float32)\n",
    "    y_hls = hls4ml_model.predict(x)\n",
    "    y_aie = aie4ml_model.predict(x, simulator=sim_mode)[:BATCH]\n",
    "\n",
    "    mse = np.mean((y_hls - y_aie)**2)\n",
    "    mae = np.mean(np.abs(y_hls - y_aie))\n",
    "    max_diff = np.max(np.abs(y_hls - y_aie))\n",
    "\n",
    "    print(\"MSE       :\", mse)\n",
    "    print(\"MAE       :\", mae)\n",
    "    print(\"Max |diff|:\", max_diff)\n",
    "\n",
    "# compare bit-exactness on the AIE x86 simulator output\n",
    "compare_bit_exact(hls_model, aie_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6Ô∏è‚É£ Build the model\n",
    "\n",
    "Compile the aie_model in `aie` mode to generate the AIE hardware design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building AIE project...\")\n",
    "\n",
    "aie_model.build()\n",
    "\n",
    "print(\"AIE build & compile completed.\")\n",
    "\n",
    "# compare bit-exactness on the AIE HW simulator output\n",
    "compare_bit_exact(hls_model, aie_model, sim_mode = 'aie')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7Ô∏è‚É£ View AIE Simulation Report\n",
    "\n",
    "The report includes:\n",
    "\n",
    "* reports on output interval and throughput (across all out ports)\n",
    "* ports, memory, AIE core usage, and others\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aie4ml.simulation import read_aie_report\n",
    "\n",
    "report = read_aie_report(aie_model)\n",
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8Ô∏è‚É£ Apply Tuning Overrides (Parallelism, Tiling, Placement)\n",
    "\n",
    "AIE4ML lets users **override hardware choices** per layer.\n",
    "\n",
    "### Example knobs:\n",
    "* Number of parallel cascade chains (`cas_num`)\n",
    "* Length of each cascade (`cas_length`)\n",
    "* Tiling sizes (`tile_m`, `tile_n`, `tile_k`)\n",
    "* AIE tile placement (`row`, `col`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_first_dense(cfg):\n",
    "    layers = list(cfg['LayerName'].keys())\n",
    "    dense_like = [l for l in layers if 'dense' in l.lower() or 'fc' in l.lower()]\n",
    "    target = dense_like[0]\n",
    "\n",
    "    cfg['LayerName'][target].update({\n",
    "\n",
    "        # ‚öôÔ∏è Parallelism:\n",
    "        #   - cas_num splits the *output features* (N dimension)\n",
    "        #   - cas_length splits the *input features*  (K dimension)\n",
    "        # Higher parallelism = more AIE tiles used = higher throughput. Try to keep both <= 8.\n",
    "        'parallelism': {'cas_num': 2, 'cas_length': 2},\n",
    "\n",
    "        # üß© Tiling: how the GEMM is partitioned inside the AIE. Default is usually optimal\n",
    "        # Controls tile sizes along M (batch), K (input features), N (output features).\n",
    "        'tiling': {'tile_m': 4, 'tile_k': 8, 'tile_n': 8},\n",
    "\n",
    "        # üìç Placement: hard-pin the AIE layer graph to start from a specific tile (row, col).\n",
    "        'placement': {'row': 0, 'col': 10},\n",
    "    })\n",
    "\n",
    "    print(\"Tuned:\", target)\n",
    "    return target\n",
    "\n",
    "tuned_layer = tune_first_dense(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9Ô∏è‚É£ Convert Tuned AIE Model\n",
    "\n",
    "We keep the same model but pass **the updated hls_config**.\n",
    "\n",
    "‚û°Ô∏è *Tip:* You can now test impact on bit-exactness and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aie_model_tuned = hls4ml.converters.convert_from_keras_model(\n",
    "    model,\n",
    "    hls_config=cfg,\n",
    "    output_dir='proj_aie_tuned',\n",
    "    backend='aie',\n",
    "    project_name='proj_aie_tuned',\n",
    "    batch_size=BATCH,\n",
    "    iterations=ITERS,\n",
    "    part=PLATFORM\n",
    ")\n",
    "\n",
    "aie_model_tuned.write()\n",
    "aie_model_tuned.build()\n",
    "\n",
    "compare_bit_exact(hls_model, aie_model_tuned, sim_mode = 'aie')\n",
    "\n",
    "read_aie_report(aie_model_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéâ Tutorial Complete!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hls4ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
