"""
AI generation indicators analysis.

Detects indicators that an image may have been generated by AI rather than
captured by a camera. Uses non-ML techniques based on statistical and
frequency analysis.

Key techniques:
1. Frequency spectrum analysis - AI images often have unusual frequency patterns
2. GAN upsampling artifacts - Checkerboard patterns from transposed convolutions
3. Noise pattern analysis - AI images lack natural sensor noise characteristics
4. Colour distribution analysis - Unusual histogram patterns
5. Texture repetition detection - AI may produce subtle repeating patterns

These techniques complement metadata-based AI signature detection. They are
most effective when multiple indicators converge, as no single technique is
definitive.

Note: This is NOT an ML-based classifier. It detects statistical anomalies
consistent with AI generation, not a binary "real/fake" determination.

References:
    Marra, F. et al. (2019). "Do GANs Leave Artificial Fingerprints?"
    Wang, S.Y. et al. (2020). "CNN-generated images are surprisingly easy
        to spot... for now"
    Durall, R. et al. (2020). "Watch your Up-Convolution: CNN Based
        Generative Deep Neural Networks are Failing to Reproduce
        Spectral Distributions"
"""

from dataclasses import dataclass, field
from typing import List, Optional, Tuple, Dict, Any

from ..state import DimensionResult, DimensionState, Confidence, Evidence

# Optional dependencies
try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False

try:
    from PIL import Image
    HAS_PIL = True
except ImportError:
    HAS_PIL = False

try:
    from scipy import fftpack, stats, ndimage
    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False


# =============================================================================
# CONFIGURATION
# =============================================================================

# Frequency analysis thresholds
# AI images often have attenuated high frequencies
HIGH_FREQ_RATIO_THRESHOLD = 0.15  # Ratio of high to low frequency energy
SPECTRAL_ROLLOFF_THRESHOLD = 0.7  # Where 85% of energy is concentrated

# GAN checkerboard detection
# Typical checkerboard periods from 2x upsampling
CHECKERBOARD_PERIODS = [2, 4, 8, 16]
CHECKERBOARD_PEAK_THRESHOLD = 2.5  # Standard deviations above mean

# Noise analysis
# Natural images have characteristic noise patterns
NOISE_VARIANCE_MIN = 0.5  # Minimum expected noise variance
NOISE_VARIANCE_MAX = 50.0  # Maximum expected noise variance
NOISE_UNIFORMITY_THRESHOLD = 0.3  # How uniform noise should be across regions

# Colour analysis
HISTOGRAM_SMOOTHNESS_THRESHOLD = 0.8  # AI images often have smoother histograms
COLOR_MODE_THRESHOLD = 5  # Minimum modes in colour histogram


@dataclass
class FrequencyAnalysis:
    """Results of frequency spectrum analysis."""
    high_freq_ratio: float  # Ratio of high to total frequency energy
    spectral_rolloff: float  # Frequency below which 85% of energy lies
    spectral_centroid: float  # Centre of mass of spectrum
    has_unusual_spectrum: bool
    details: str


@dataclass
class CheckerboardAnalysis:
    """Results of GAN checkerboard artifact detection."""
    detected_periods: List[int]  # Periods where peaks were found
    peak_strengths: Dict[int, float]  # Period -> strength
    has_checkerboard: bool
    details: str


@dataclass
class NoiseAnalysis:
    """Results of noise pattern analysis."""
    estimated_variance: float
    uniformity_score: float  # How uniform noise is across image
    has_natural_noise: bool
    details: str


@dataclass
class ColorAnalysis:
    """Results of colour distribution analysis."""
    histogram_smoothness: float  # How smooth the colour histogram is
    num_color_modes: int  # Number of peaks in histogram
    saturation_distribution: str  # Description of saturation pattern
    has_natural_colors: bool
    details: str


@dataclass
class AIGenAnalysis:
    """Combined AI generation indicator analysis."""
    frequency: Optional[FrequencyAnalysis] = None
    checkerboard: Optional[CheckerboardAnalysis] = None
    noise: Optional[NoiseAnalysis] = None
    color: Optional[ColorAnalysis] = None
    num_indicators: int = 0
    indicator_summary: List[str] = field(default_factory=list)


class AIGenerationAnalyzer:
    """
    Analyzes images for indicators of AI generation.

    Uses multiple non-ML techniques to detect statistical anomalies
    consistent with AI-generated images. No single technique is
    definitive; the analysis looks for convergent indicators.

    Usage:
        analyzer = AIGenerationAnalyzer()
        result = analyzer.analyze("image.jpg")
    """

    def __init__(
        self,
        enable_frequency: bool = True,
        enable_checkerboard: bool = True,
        enable_noise: bool = True,
        enable_color: bool = True,
    ):
        """
        Initialize analyzer.

        Args:
            enable_frequency: Enable frequency spectrum analysis
            enable_checkerboard: Enable GAN checkerboard detection
            enable_noise: Enable noise pattern analysis
            enable_color: Enable colour distribution analysis
        """
        self.enable_frequency = enable_frequency
        self.enable_checkerboard = enable_checkerboard
        self.enable_noise = enable_noise
        self.enable_color = enable_color

    def analyze(self, file_path: str) -> DimensionResult:
        """
        Analyze an image for AI generation indicators.

        Args:
            file_path: Path to image file

        Returns:
            DimensionResult with AI generation indicator findings
        """
        if not HAS_NUMPY or not HAS_PIL:
            return DimensionResult(
                dimension="aigen",
                state=DimensionState.UNCERTAIN,
                confidence=Confidence.NA,
                evidence=[
                    Evidence(
                        finding="Dependencies unavailable",
                        explanation="numpy and PIL required for AI generation analysis",
                    )
                ],
                methodology="AI generation indicator analysis (unavailable)",
            )

        # Load image
        try:
            img = Image.open(file_path)
            if img.mode not in ('RGB', 'L'):
                img = img.convert('RGB')
            img_array = np.array(img, dtype=np.float32)
        except Exception as e:
            return DimensionResult(
                dimension="aigen",
                state=DimensionState.UNCERTAIN,
                confidence=Confidence.NA,
                evidence=[
                    Evidence(
                        finding="Image load failed",
                        explanation=str(e),
                    )
                ],
                methodology="AI generation indicator analysis",
            )

        # Check minimum size
        if img_array.shape[0] < 64 or img_array.shape[1] < 64:
            return DimensionResult(
                dimension="aigen",
                state=DimensionState.UNCERTAIN,
                confidence=Confidence.NA,
                evidence=[
                    Evidence(
                        finding="Image too small",
                        explanation="Minimum 64x64 pixels required for AI analysis",
                    )
                ],
                methodology="AI generation indicator analysis",
            )

        # Run analyses
        analysis = AIGenAnalysis()
        evidence = []

        if self.enable_frequency and HAS_SCIPY:
            analysis.frequency = self._analyze_frequency(img_array)
            if analysis.frequency.has_unusual_spectrum:
                analysis.num_indicators += 1
                analysis.indicator_summary.append("unusual frequency spectrum")
                evidence.append(Evidence(
                    finding="Unusual frequency spectrum detected",
                    explanation=analysis.frequency.details,
                    citation="Durall et al. (2020) - spectral analysis of GAN images",
                ))

        if self.enable_checkerboard and HAS_SCIPY:
            analysis.checkerboard = self._analyze_checkerboard(img_array)
            if analysis.checkerboard.has_checkerboard:
                analysis.num_indicators += 1
                analysis.indicator_summary.append("GAN checkerboard artifacts")
                evidence.append(Evidence(
                    finding="GAN upsampling artifacts detected",
                    explanation=analysis.checkerboard.details,
                    citation="Odena et al. (2016) - checkerboard artifacts in GANs",
                ))

        if self.enable_noise:
            analysis.noise = self._analyze_noise(img_array)
            if not analysis.noise.has_natural_noise:
                analysis.num_indicators += 1
                analysis.indicator_summary.append("unnatural noise pattern")
                evidence.append(Evidence(
                    finding="Unnatural noise characteristics",
                    explanation=analysis.noise.details,
                    citation="Camera sensor noise vs synthetic noise patterns",
                ))

        if self.enable_color:
            analysis.color = self._analyze_color(img_array)
            if not analysis.color.has_natural_colors:
                analysis.num_indicators += 1
                analysis.indicator_summary.append("unusual colour distribution")
                evidence.append(Evidence(
                    finding="Unusual colour distribution",
                    explanation=analysis.color.details,
                ))

        # Determine state based on number of indicators
        if analysis.num_indicators >= 3:
            state = DimensionState.SUSPICIOUS
            confidence = Confidence.MEDIUM
            evidence.insert(0, Evidence(
                finding=f"Multiple AI generation indicators ({analysis.num_indicators}/4)",
                explanation=(
                    f"Detected: {', '.join(analysis.indicator_summary)}. "
                    "Multiple converging indicators suggest possible AI generation."
                ),
            ))
        elif analysis.num_indicators >= 2:
            state = DimensionState.SUSPICIOUS
            confidence = Confidence.LOW
            evidence.insert(0, Evidence(
                finding=f"Some AI generation indicators ({analysis.num_indicators}/4)",
                explanation=(
                    f"Detected: {', '.join(analysis.indicator_summary)}. "
                    "Some indicators present but not conclusive."
                ),
            ))
        elif analysis.num_indicators == 1:
            state = DimensionState.CONSISTENT
            confidence = Confidence.LOW
            evidence.append(Evidence(
                finding="Single weak indicator",
                explanation=(
                    f"Only one indicator detected ({analysis.indicator_summary[0]}). "
                    "Insufficient for suspicion."
                ),
            ))
        else:
            state = DimensionState.CONSISTENT
            confidence = Confidence.MEDIUM
            evidence.append(Evidence(
                finding="No AI generation indicators detected",
                explanation=(
                    "Frequency spectrum, noise patterns, and colour distribution "
                    "are consistent with camera-captured imagery."
                ),
            ))

        return DimensionResult(
            dimension="aigen",
            state=state,
            confidence=confidence,
            evidence=evidence,
            methodology=(
                "AI generation indicator analysis: frequency spectrum, "
                "GAN artifact detection, noise pattern analysis, colour distribution"
            ),
            raw_data={
                "num_indicators": analysis.num_indicators,
                "indicators": analysis.indicator_summary,
                "frequency_ratio": (
                    analysis.frequency.high_freq_ratio
                    if analysis.frequency else None
                ),
                "checkerboard_periods": (
                    analysis.checkerboard.detected_periods
                    if analysis.checkerboard else None
                ),
                "noise_variance": (
                    analysis.noise.estimated_variance
                    if analysis.noise else None
                ),
            },
        )

    def _analyze_frequency(self, img_array: np.ndarray) -> FrequencyAnalysis:
        """
        Analyze frequency spectrum for AI generation indicators.

        AI-generated images, especially from GANs, often have attenuated
        high-frequency content compared to natural images.
        """
        # Convert to grayscale if colour
        if len(img_array.shape) == 3:
            gray = np.mean(img_array, axis=2)
        else:
            gray = img_array

        # Compute 2D FFT
        fft = fftpack.fft2(gray)
        fft_shifted = fftpack.fftshift(fft)
        magnitude = np.abs(fft_shifted)

        # Create radial frequency bins
        h, w = magnitude.shape
        cy, cx = h // 2, w // 2
        y, x = np.ogrid[:h, :w]
        r = np.sqrt((x - cx)**2 + (y - cy)**2)

        # Normalise radius to [0, 1]
        max_r = np.sqrt(cx**2 + cy**2)
        r_norm = r / max_r

        # Calculate energy in frequency bands
        low_mask = r_norm < 0.2
        mid_mask = (r_norm >= 0.2) & (r_norm < 0.5)
        high_mask = r_norm >= 0.5

        low_energy = np.sum(magnitude[low_mask]**2)
        mid_energy = np.sum(magnitude[mid_mask]**2)
        high_energy = np.sum(magnitude[high_mask]**2)
        total_energy = low_energy + mid_energy + high_energy

        if total_energy == 0:
            return FrequencyAnalysis(
                high_freq_ratio=0.0,
                spectral_rolloff=0.0,
                spectral_centroid=0.0,
                has_unusual_spectrum=False,
                details="Could not compute frequency spectrum",
            )

        high_freq_ratio = high_energy / total_energy

        # Spectral rolloff (frequency below which 85% of energy lies)
        # Compute radial power distribution
        num_bins = 50
        bin_edges = np.linspace(0, 1, num_bins + 1)
        radial_power = np.zeros(num_bins)

        for i in range(num_bins):
            mask = (r_norm >= bin_edges[i]) & (r_norm < bin_edges[i + 1])
            radial_power[i] = np.sum(magnitude[mask]**2)

        cumulative_power = np.cumsum(radial_power) / np.sum(radial_power)
        rolloff_idx = np.searchsorted(cumulative_power, 0.85)
        spectral_rolloff = bin_edges[min(rolloff_idx, num_bins - 1)]

        # Spectral centroid
        bin_centres = (bin_edges[:-1] + bin_edges[1:]) / 2
        if np.sum(radial_power) > 0:
            spectral_centroid = np.sum(bin_centres * radial_power) / np.sum(radial_power)
        else:
            spectral_centroid = 0.0

        # Determine if spectrum is unusual
        # AI images typically have lower high_freq_ratio and lower rolloff
        has_unusual = (
            high_freq_ratio < HIGH_FREQ_RATIO_THRESHOLD or
            spectral_rolloff < SPECTRAL_ROLLOFF_THRESHOLD
        )

        if has_unusual:
            details = (
                f"High frequency energy ratio ({high_freq_ratio:.3f}) "
                f"{'is below' if high_freq_ratio < HIGH_FREQ_RATIO_THRESHOLD else 'and'} "
                f"spectral rolloff ({spectral_rolloff:.3f}) "
                f"{'is below' if spectral_rolloff < SPECTRAL_ROLLOFF_THRESHOLD else 'within'} "
                "expected range for camera-captured images."
            )
        else:
            details = (
                f"Frequency distribution (high freq ratio: {high_freq_ratio:.3f}, "
                f"rolloff: {spectral_rolloff:.3f}) is consistent with natural imagery."
            )

        return FrequencyAnalysis(
            high_freq_ratio=high_freq_ratio,
            spectral_rolloff=spectral_rolloff,
            spectral_centroid=spectral_centroid,
            has_unusual_spectrum=has_unusual,
            details=details,
        )

    def _analyze_checkerboard(self, img_array: np.ndarray) -> CheckerboardAnalysis:
        """
        Detect GAN checkerboard artifacts from upsampling layers.

        GANs using transposed convolutions often produce checkerboard
        patterns at specific frequencies corresponding to the upsampling
        factors used.
        """
        # Convert to grayscale
        if len(img_array.shape) == 3:
            gray = np.mean(img_array, axis=2)
        else:
            gray = img_array

        # Compute autocorrelation via FFT
        fft = fftpack.fft2(gray)
        power_spectrum = np.abs(fft)**2
        autocorr = np.real(fftpack.ifft2(power_spectrum))
        autocorr = fftpack.fftshift(autocorr)

        # Normalise
        autocorr = autocorr / autocorr.max() if autocorr.max() > 0 else autocorr

        h, w = autocorr.shape
        cy, cx = h // 2, w // 2

        # Look for peaks at checkerboard periods
        detected_periods = []
        peak_strengths = {}

        # Get baseline (mean of autocorrelation excluding centre)
        mask = np.ones_like(autocorr, dtype=bool)
        mask[cy-5:cy+5, cx-5:cx+5] = False
        baseline_mean = np.mean(autocorr[mask])
        baseline_std = np.std(autocorr[mask])

        for period in CHECKERBOARD_PERIODS:
            if period >= min(h, w) // 4:
                continue

            # Sample at checkerboard positions
            positions = [
                (cy + period, cx),
                (cy - period, cx),
                (cy, cx + period),
                (cy, cx - period),
                (cy + period, cx + period),
                (cy - period, cx - period),
            ]

            values = []
            for py, px in positions:
                if 0 <= py < h and 0 <= px < w:
                    values.append(autocorr[py, px])

            if values:
                peak_value = np.mean(values)
                # Check if significantly above baseline
                if baseline_std > 0:
                    z_score = (peak_value - baseline_mean) / baseline_std
                    if z_score > CHECKERBOARD_PEAK_THRESHOLD:
                        detected_periods.append(period)
                        peak_strengths[period] = float(z_score)

        has_checkerboard = len(detected_periods) > 0

        if has_checkerboard:
            details = (
                f"Detected periodic patterns at intervals: {detected_periods}. "
                "These are consistent with GAN upsampling artifacts."
            )
        else:
            details = "No checkerboard artifacts detected at common GAN upsampling intervals."

        return CheckerboardAnalysis(
            detected_periods=detected_periods,
            peak_strengths=peak_strengths,
            has_checkerboard=has_checkerboard,
            details=details,
        )

    def _analyze_noise(self, img_array: np.ndarray) -> NoiseAnalysis:
        """
        Analyze noise patterns for natural vs synthetic characteristics.

        Camera sensors produce characteristic noise patterns. AI-generated
        images typically have either no noise, uniform synthetic noise, or
        noise that doesn't match sensor characteristics.
        """
        # Convert to grayscale
        if len(img_array.shape) == 3:
            gray = np.mean(img_array, axis=2)
        else:
            gray = img_array.copy()

        # Estimate noise using Laplacian
        # High-pass filter to isolate noise
        if HAS_SCIPY:
            laplacian = ndimage.laplace(gray)
        else:
            # Simple approximation without scipy
            kernel = np.array([[0, 1, 0], [1, -4, 1], [0, 1, 0]])
            laplacian = ndimage.convolve(gray, kernel) if HAS_SCIPY else gray

        # Estimate noise variance (robust median estimator)
        noise_estimate = np.median(np.abs(laplacian)) / 0.6745
        noise_variance = noise_estimate ** 2

        # Check noise uniformity across regions
        h, w = gray.shape
        block_size = min(64, h // 4, w // 4)
        if block_size < 16:
            block_size = min(h, w) // 2

        regional_variances = []
        for y in range(0, h - block_size, block_size):
            for x in range(0, w - block_size, block_size):
                block = laplacian[y:y+block_size, x:x+block_size]
                block_var = np.var(block)
                if block_var > 0:
                    regional_variances.append(block_var)

        if regional_variances:
            variance_of_variances = np.std(regional_variances) / np.mean(regional_variances)
            uniformity_score = 1.0 - min(1.0, variance_of_variances)
        else:
            uniformity_score = 0.5

        # Determine if noise is natural
        # Natural images have:
        # - Non-zero noise variance
        # - Some variation in noise across regions (not perfectly uniform)
        has_natural = (
            NOISE_VARIANCE_MIN < noise_variance < NOISE_VARIANCE_MAX and
            uniformity_score < NOISE_UNIFORMITY_THRESHOLD + 0.5  # Allow some uniformity
        )

        # Very uniform noise is suspicious (synthetic)
        if uniformity_score > 0.9:
            has_natural = False

        # Very low noise is suspicious (AI often produces "too clean" images)
        if noise_variance < NOISE_VARIANCE_MIN:
            has_natural = False

        if not has_natural:
            if noise_variance < NOISE_VARIANCE_MIN:
                details = (
                    f"Noise variance ({noise_variance:.2f}) is unusually low, "
                    "suggesting synthetic or heavily processed image."
                )
            elif uniformity_score > 0.9:
                details = (
                    f"Noise is unusually uniform (score: {uniformity_score:.2f}), "
                    "which is atypical of camera sensor noise."
                )
            else:
                details = (
                    f"Noise characteristics (variance: {noise_variance:.2f}, "
                    f"uniformity: {uniformity_score:.2f}) are atypical of natural imagery."
                )
        else:
            details = (
                f"Noise pattern (variance: {noise_variance:.2f}, "
                f"uniformity: {uniformity_score:.2f}) is consistent with camera capture."
            )

        return NoiseAnalysis(
            estimated_variance=float(noise_variance),
            uniformity_score=float(uniformity_score),
            has_natural_noise=has_natural,
            details=details,
        )

    def _analyze_color(self, img_array: np.ndarray) -> ColorAnalysis:
        """
        Analyze colour distribution for AI generation indicators.

        AI-generated images sometimes have unusual colour distributions,
        overly smooth histograms, or unnatural saturation patterns.
        """
        if len(img_array.shape) != 3:
            return ColorAnalysis(
                histogram_smoothness=0.5,
                num_color_modes=0,
                saturation_distribution="grayscale",
                has_natural_colors=True,
                details="Grayscale image, colour analysis not applicable.",
            )

        # Compute colour histogram smoothness
        # Very smooth histograms can indicate AI generation
        hist_r, _ = np.histogram(img_array[:, :, 0].flatten(), bins=64, range=(0, 255))
        hist_g, _ = np.histogram(img_array[:, :, 1].flatten(), bins=64, range=(0, 255))
        hist_b, _ = np.histogram(img_array[:, :, 2].flatten(), bins=64, range=(0, 255))

        # Normalise histograms
        hist_r = hist_r / hist_r.sum() if hist_r.sum() > 0 else hist_r
        hist_g = hist_g / hist_g.sum() if hist_g.sum() > 0 else hist_g
        hist_b = hist_b / hist_b.sum() if hist_b.sum() > 0 else hist_b

        # Compute smoothness (inverse of variation)
        def compute_smoothness(hist):
            if len(hist) < 2:
                return 1.0
            diff = np.abs(np.diff(hist))
            return 1.0 - np.mean(diff) / (np.max(hist) + 1e-10)

        smoothness = (
            compute_smoothness(hist_r) +
            compute_smoothness(hist_g) +
            compute_smoothness(hist_b)
        ) / 3

        # Count color modes (peaks in histogram)
        def count_modes(hist, threshold=0.02):
            # Find local maxima above threshold
            modes = 0
            for i in range(1, len(hist) - 1):
                if hist[i] > threshold and hist[i] > hist[i-1] and hist[i] > hist[i+1]:
                    modes += 1
            return modes

        num_modes = (
            count_modes(hist_r) +
            count_modes(hist_g) +
            count_modes(hist_b)
        )

        # Analyze saturation
        # Convert to HSV-like saturation
        max_rgb = np.max(img_array, axis=2)
        min_rgb = np.min(img_array, axis=2)
        saturation = np.zeros_like(max_rgb)
        nonzero = max_rgb > 0
        saturation[nonzero] = (max_rgb[nonzero] - min_rgb[nonzero]) / max_rgb[nonzero]

        mean_sat = np.mean(saturation)
        std_sat = np.std(saturation)

        if mean_sat < 0.1:
            sat_description = "very low (near grayscale)"
        elif mean_sat < 0.3:
            sat_description = "low"
        elif mean_sat < 0.6:
            sat_description = "moderate"
        else:
            sat_description = "high"

        # Determine if colors are natural
        # AI images may have: overly smooth histograms, few modes, unusual saturation
        has_natural = (
            smoothness < HISTOGRAM_SMOOTHNESS_THRESHOLD and
            num_modes >= COLOR_MODE_THRESHOLD
        )

        if not has_natural:
            if smoothness >= HISTOGRAM_SMOOTHNESS_THRESHOLD:
                details = (
                    f"Colour histogram is unusually smooth ({smoothness:.2f}), "
                    "which may indicate synthetic generation."
                )
            else:
                details = (
                    f"Colour distribution has few distinct modes ({num_modes}), "
                    "which is atypical of natural photographs."
                )
        else:
            details = (
                f"Colour distribution (smoothness: {smoothness:.2f}, "
                f"modes: {num_modes}, saturation: {sat_description}) "
                "is consistent with natural imagery."
            )

        return ColorAnalysis(
            histogram_smoothness=float(smoothness),
            num_color_modes=num_modes,
            saturation_distribution=sat_description,
            has_natural_colors=has_natural,
            details=details,
        )
