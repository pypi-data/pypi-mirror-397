#!/usr/bin/env python3
"""
gitrun Utilities & Cache Manager
===============================

Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© ÙˆÙ…Ø¯ÙŠØ± ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ø°ÙƒÙŠ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
Helper utilities and smart cache manager to improve performance by avoiding repeated downloads
"""

import hashlib
import json
import os
import time
from pathlib import Path
from typing import Optional, Dict, Any


class CacheManager:
    """
    Ù…Ø¯ÙŠØ± ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ø°ÙƒÙŠ Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¬Ù„ÙˆØ¨Ø© Ù…Ù† GitHub/GitLab
    Smart cache manager for files fetched from remote repositories

    ÙŠØ®Ø²Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ø­Ù„ÙŠÙ‹Ø§ ÙÙŠ ~/.gitrun/cache Ù…Ø¹ ØµÙ„Ø§Ø­ÙŠØ© Ø²Ù…Ù†ÙŠØ© (TTL)
    Caches files locally with time-to-live (TTL) to reduce network requests
    """

    def __init__(self, ttl: int = 3600):  # 1 hour default TTL
        """
        ØªÙ‡ÙŠØ¦Ø© Ù…Ø¯ÙŠØ± Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª

        Args:
            ttl (int): Ù…Ø¯Ø© Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ© Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ (Ø§ÙØªØ±Ø§Ø¶ÙŠ: Ø³Ø§Ø¹Ø©) / Cache validity in seconds (default: 1 hour)
        """
        self.cache_dir = Path.home() / '.gitrun' / 'cache'
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.metadata_file = self.cache_dir / 'metadata.json'
        self.ttl = ttl
        self.metadata: Dict[str, Any] = self._load_metadata()

    def _load_metadata(self) -> Dict[str, Any]:
        """ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØµÙÙŠØ© Ù…Ù† Ù…Ù„Ù JSON / Load metadata from JSON file"""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except (json.JSONDecodeError, IOError) as e:
                print(f"âš ï¸ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª (Ø³ÙŠØªÙ… Ø¥Ø¹Ø§Ø¯Ø© Ø¥Ù†Ø´Ø§Ø¦Ù‡Ø§)\n"
                      f"    Warning: Failed to load cache metadata (will be recreated): {e}")
                return {}
        return {}

    def _save_metadata(self):
        """Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© Ø¥Ù„Ù‰ Ù…Ù„Ù JSON / Save metadata to JSON file"""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, indent=2, ensure_ascii=False)
        except IOError as e:
            print(f"âš ï¸ ÙØ´Ù„ Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª\n"
                  f"    Warning: Failed to save cache metadata: {e}")

    def get_cache_key(self, owner: str, repo: str, branch: str, filename: str) -> str:
        """
        Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØªØ§Ø­ ØªØ®Ø²ÙŠÙ† ÙØ±ÙŠØ¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ù„Ù
        Generate a unique cache key based on repository and file details
        """
        key_str = f"{owner.lower()}/{repo.lower()}/{branch}/{filename}"
        return hashlib.sha256(key_str.encode('utf-8')).hexdigest()[:16]

    def is_valid(self, key: str) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµÙ„Ø§Ø­ÙŠØ© Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ù…Ø®Ø²Ù† / Check if cached item is still valid"""
        if key not in self.metadata:
            return False
        cache_time = self.metadata[key].get('timestamp', 0)
        return (time.time() - cache_time) < self.ttl

    def get_cached(self, key: str) -> Optional[str]:
        """
        Ø¬Ù„Ø¨ Ù…Ø­ØªÙˆÙ‰ Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø¥Ø°Ø§ ÙƒØ§Ù† ØµØ§Ù„Ø­Ù‹Ø§
        Retrieve cached content if valid and exists
        """
        if not self.is_valid(key):
            return None

        cache_file = self.cache_dir / f"{key}.cache"
        if cache_file.exists():
            try:
                return cache_file.read_text(encoding='utf-8')
            except IOError as e:
                print(f"âš ï¸ ÙØ´Ù„ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø®Ø²Ù† Ù…Ø¤Ù‚ØªÙ‹Ø§\n"
                      f"    Warning: Failed to read cached file: {e}")
        return None

    def set_cache(self, key: str, content: str):
        """
        Ø­ÙØ¸ Ù…Ø­ØªÙˆÙ‰ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù…Ø¹ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
        Cache content and update metadata
        """
        cache_file = self.cache_dir / f"{key}.cache"
        try:
            cache_file.write_text(content, encoding='utf-8')

            self.metadata[key] = {
                'timestamp': time.time(),
                'size': len(content),
                'display_size': display_size(len(content))
            }
            self._save_metadata()

            if os.getenv('GITRUN_VERBOSE'):  # ÙÙ‚Ø· ÙÙŠ ÙˆØ¶Ø¹ verbose
                print(f"ðŸ’¾ ØªÙ… ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ù„Ù Ù…Ø¤Ù‚ØªÙ‹Ø§ ({display_size(len(content))})\n"
                      f"    Cached file ({display_size(len(content))})")
        except IOError as e:
            print(f"âš ï¸ ÙØ´Ù„ Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª\n"
                  f"    Warning: Failed to cache file: {e}")

    def clear_cache(self):
        """Ù…Ø³Ø­ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ / Clear all cached files and metadata"""
        try:
            for file in self.cache_dir.glob("*.cache"):
                file.unlink()
            if self.metadata_file.exists():
                self.metadata_file.unlink()
            self.metadata.clear()
            print("ðŸ§¹ ØªÙ… Ù…Ø³Ø­ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ / Cache cleared successfully")
        except Exception as e:
            print(f"âš ï¸ ÙØ´Ù„ Ù…Ø³Ø­ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª\n"
                  f"    Warning: Failed to clear cache: {e}")


def get_file_hash(content: str) -> str:
    """
    Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø§Ø´ MD5 Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù (Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø£Ùˆ Ø§Ù„ØªØ­Ù‚Ù‚)
    Generate MD5 hash of file content
    """
    return hashlib.md5(content.encode('utf-8')).hexdigest()


def display_size(bytes_size: int) -> str:
    """
    ØªØ­ÙˆÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ø¨Ø§ÙŠØª Ø¥Ù„Ù‰ ÙˆØ­Ø¯Ø© Ù…Ù‚Ø±ÙˆØ¡Ø© (KB, MB, ...)
    Convert bytes to human-readable format
    """
    if bytes_size == 0:
        return "0 B"
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if bytes_size < 1024.0:
            return f"{bytes_size:.1f} {unit}"
        bytes_size /= 1024.0
    return f"{bytes_size:.1f} PB"
    for unit in ['B', 'KB', 'MB', 'GB']:
        if bytes_size < 1024.0:
            return f"{bytes_size:.1f} {unit}"
        bytes_size /= 1024.0
    return f"{bytes_size:.1f} TB"
