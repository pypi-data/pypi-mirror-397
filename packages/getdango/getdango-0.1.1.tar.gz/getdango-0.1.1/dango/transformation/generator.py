"""
dbt Model Auto-Generation

Automatically generates dbt staging models from configured data sources.
Supports 4 deduplication strategies from Phase 1 CSV design:
  - none: No deduplication (events, logs)
  - latest_only: Keep most recent by timestamp (snapshots)
  - append_only: No dedup in staging (already handled per-file)
  - scd_type2: Slowly Changing Dimension Type 2 with history tracking

Foundation created in Phase 2 Day 10.
Completed in Phase 2 Day 11.
CSV dedup â†’ dbt template mapping fixed in MVP Week 1 Day 1 (Oct 27, 2025).
"""

from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
import jinja2
import duckdb

from dango.config.models import DataSource, SourceType, DeduplicationStrategy
from dango.ingestion.sources.registry import get_source_metadata


class DbtModelGenerator:
    """
    Generates dbt staging models from data sources configuration.

    Features:
    - Auto-generates staging models for each data source
    - Supports 4 deduplication strategies (from Phase 1 CSV design)
    - Creates sources.yml documenting raw.* tables
    - Supports custom dbt configurations per source
    """

    # dbt template deduplication strategies
    DBT_TEMPLATE_STRATEGIES = {
        "last_modified": "Keep most recent record based on timestamp",
        "first_seen": "Keep oldest record (first occurrence)",
        "composite_key": "Deduplicate using multiple columns as composite key",
        "row_number": "Keep first row when sorted by specified columns",
    }

    # Map CSV dedup strategies to dbt template strategies
    # Based on CSV_LOADING_DESIGN_SUMMARY.md Phase 1 design
    CSV_TO_DBT_STRATEGY_MAP = {
        DeduplicationStrategy.NONE: None,  # No dedup - keep all rows
        DeduplicationStrategy.LATEST_ONLY: "last_modified",  # Keep latest by timestamp (PARTITION BY pk, ORDER BY ts DESC)
        DeduplicationStrategy.APPEND_ONLY: None,  # No dedup in staging (already deduped per-file during load)
        DeduplicationStrategy.SCD_TYPE2: "scd_type2",  # SCD Type 2 with valid_from/valid_to/is_current
    }

    def __init__(self, project_root: Path):
        """
        Initialize dbt model generator

        Args:
            project_root: Path to dango project root
        """
        self.project_root = project_root
        self.dbt_models_dir = project_root / "dbt" / "models"
        self.staging_dir = self.dbt_models_dir / "staging"
        self.templates_dir = Path(__file__).parent.parent / "templates" / "dbt"
        self.duckdb_path = project_root / "data" / "warehouse.duckdb"

        # Initialize Jinja2 environment
        self.jinja_env = jinja2.Environment(
            loader=jinja2.FileSystemLoader(str(self.templates_dir)),
            trim_blocks=True,
            lstrip_blocks=True,
        )

    def is_auto_generated(self, model_path: Path) -> bool:
        """
        Check if a model file is auto-generated (has marker comment)

        Args:
            model_path: Path to the model file

        Returns:
            True if file has auto-generated marker, False otherwise
        """
        if not model_path.exists():
            return True  # Non-existent file can be generated

        try:
            content = model_path.read_text()
            # Check for marker comment in first 500 characters
            return "-- Auto-generated by Dango" in content[:500]
        except Exception:
            # If we can't read the file, assume it's customized
            return False

    def get_table_schema(self, table_name: str, schema: str = "main") -> List[Dict[str, Any]]:
        """
        Get schema information from DuckDB table

        Args:
            table_name: Name of the table
            schema: Schema name (default: main)

        Returns:
            List of column definitions with name, type, nullable info
        """
        if not self.duckdb_path.exists():
            return []

        try:
            conn = duckdb.connect(str(self.duckdb_path), read_only=True)

            # Query table schema
            result = conn.execute(f"""
                SELECT
                    column_name,
                    data_type,
                    is_nullable
                FROM information_schema.columns
                WHERE table_schema = '{schema}'
                  AND table_name = '{table_name}'
                ORDER BY ordinal_position
            """).fetchall()

            columns = []
            for row in result:
                column_name, data_type, is_nullable = row

                # Generate tests based on column properties
                tests = []
                if is_nullable == 'NO':
                    tests.append('not_null')

                # Common patterns for unique columns
                if column_name.lower() in ['id', 'uuid', 'key']:
                    tests.append('unique')

                columns.append({
                    'name': column_name,
                    'type': data_type,
                    'nullable': is_nullable == 'YES',
                    'tests': tests,
                    'description': f'{column_name} column'
                })

            conn.close()
            return columns

        except Exception as e:
            # Table might not exist yet - return empty
            return []

    def infer_dedup_strategy(
        self,
        source: DataSource,
        columns: List[Dict[str, Any]]
    ) -> Tuple[Optional[str], Optional[List[str]]]:
        """
        Infer deduplication strategy based on source type and columns

        Args:
            source: Data source configuration
            columns: List of column definitions

        Returns:
            Tuple of (dbt_strategy, dedup_columns)
            where dbt_strategy is one of DBT_TEMPLATE_STRATEGIES keys or None
        """
        column_names = [col['name'].lower() for col in columns]

        # CSV sources: Map from CSV config strategy to dbt template strategy
        if source.type == SourceType.CSV and source.csv:
            csv_strategy = source.csv.deduplication_strategy

            # Map CSV strategy to dbt template strategy
            dbt_strategy = self.CSV_TO_DBT_STRATEGY_MAP.get(csv_strategy)

            # Build dedup_columns based on strategy
            if dbt_strategy == "last_modified":
                # latest_only: Use primary_key + timestamp_column
                if source.csv.primary_key and source.csv.timestamp_column:
                    return (dbt_strategy, [source.csv.primary_key, source.csv.timestamp_column])
                else:
                    # Fallback: Try to infer from column names
                    id_cols = [col for col in column_names
                              if any(id_field in col for id_field in ['id', 'uuid', 'key'])]
                    timestamp_cols = [col for col in column_names
                                    if any(ts in col for ts in ['updated_at', 'modified_at', 'timestamp'])]
                    if id_cols and timestamp_cols:
                        return (dbt_strategy, [id_cols[0], timestamp_cols[0]])

            elif dbt_strategy == "scd_type2":
                # SCD Type 2: Use primary_key + timestamp_column
                if source.csv.primary_key and source.csv.timestamp_column:
                    return (dbt_strategy, [source.csv.primary_key, source.csv.timestamp_column])

            elif dbt_strategy is None:
                # none or append_only: No deduplication
                return (None, None)

        # API sources: Usually have updated_at + id
        if any(col in column_names for col in ['updated_at', 'modified_at']):
            id_col = next((col for col in column_names if 'id' in col), None)
            updated_col = next((col for col in column_names
                               if col in ['updated_at', 'modified_at']), None)
            if id_col and updated_col:
                return ('last_modified', [id_col, updated_col])

        # No deduplication
        return (None, None)

    def _get_source_endpoints(self, source: DataSource) -> List[str]:
        """
        Extract configured endpoints/resources from source config

        Args:
            source: Data source configuration

        Returns:
            List of endpoint/resource names, or empty list if none configured
        """
        source_config = getattr(source, source.type.value, None)
        if source_config:
            try:
                source_dict = source_config.model_dump() if hasattr(source_config, 'model_dump') else {}

                # Check for endpoints/resources/tables/range_names parameter
                resources = (
                    source_dict.get('endpoints') or
                    source_dict.get('resources') or
                    source_dict.get('tables') or
                    source_dict.get('objects') or
                    source_dict.get('range_names') or  # Google Sheets uses range_names
                    []
                )

                if isinstance(resources, list) and resources:
                    return resources

                # Handle GA4 queries format - each query has a resource_name
                queries = source_dict.get('queries', [])
                if isinstance(queries, list) and queries:
                    return [q.get('resource_name') for q in queries if q.get('resource_name')]
            except Exception:
                pass

        return []

    def _discover_tables_from_db(self, schema_name: str) -> List[str]:
        """
        Discover tables from database for a given schema.
        Filters out dlt internal tables and metadata tables.

        Args:
            schema_name: The schema to query

        Returns:
            List of user data table names
        """
        try:
            conn = duckdb.connect(str(self.duckdb_path), read_only=True)
            result = conn.execute("""
                SELECT table_name
                FROM information_schema.tables
                WHERE table_schema = ?
                ORDER BY table_name
            """, [schema_name]).fetchall()
            conn.close()

            # Filter out dlt internal tables and metadata tables
            skip_prefixes = ('_dlt_', 'dimensions', 'metrics')
            skip_suffixes = ('__deprecated_api_names',)
            skip_exact = ('spreadsheet', 'spreadsheet_info')  # Google Sheets metadata

            tables = []
            for (table_name,) in result:
                if table_name.startswith(skip_prefixes):
                    continue
                if table_name.endswith(skip_suffixes):
                    continue
                if table_name in skip_exact:
                    continue
                tables.append(table_name)

            return tables
        except Exception:
            return []

    def generate_staging_model(
        self,
        source: DataSource,
        table_name: str,
        schema_name: str = "raw",
        dedup_strategy: Optional[str] = None,
        dedup_columns: Optional[List[str]] = None,
    ) -> str:
        """
        Generate staging model SQL for a data source

        Args:
            source: Data source configuration
            table_name: Name of the raw table in DuckDB
            schema_name: Schema name in DuckDB (raw or raw_{source_name})
            dedup_strategy: Deduplication strategy (one of DEDUP_STRATEGIES keys)
            dedup_columns: Columns to use for deduplication

        Returns:
            Generated SQL content
        """
        # Validate dedup strategy
        valid_strategies = list(self.DBT_TEMPLATE_STRATEGIES.keys()) + ["scd_type2"]
        if dedup_strategy and dedup_strategy not in valid_strategies:
            raise ValueError(
                f"Invalid dedup_strategy '{dedup_strategy}'. "
                f"Must be one of: {valid_strategies}"
            )

        # Template context
        context = {
            "source_name": source.name,
            "source_type": source.type.value,
            "schema_name": schema_name,
            "table_name": table_name,
            "dedup_strategy": dedup_strategy,
            "dedup_columns": dedup_columns or [],
            "generated_at": datetime.now().isoformat(),
        }

        # Render template
        template = self.jinja_env.get_template("staging_model.sql.j2")
        return template.render(**context)

    def generate_sources_yml(
        self,
        source: DataSource,
        schema_name: str,
        tables: List[Dict[str, Any]],
    ) -> str:
        """
        Generate sources.yml documenting raw tables with dbt sources syntax

        This follows dbt sources convention:
        - One sources.yml per source system (not per table)
        - Documents raw.* tables (operational, as-loaded data)
        - Lives in dbt/models/staging/sources_{source_name}.yml

        Args:
            source: Data source configuration
            schema_name: Schema name in DuckDB (raw or raw_{source_name})
            tables: List of table definitions with columns
                    For single-resource: [{"name": table_name, "columns": [...]}]
                    For multi-resource: [{"name": "charge", "columns": [...]}, {"name": "customer", "columns": [...]}]

        Returns:
            Generated YAML content
        """
        context = {
            "source_name": source.name,
            "source_type": source.type.value,
            "schema_name": schema_name,
            "tables": tables,
            "generated_at": datetime.now().isoformat(),
        }

        template = self.jinja_env.get_template("sources.yml.j2")
        return template.render(**context)

    def generate_staging_schema_yml(
        self,
        source: DataSource,
        models: List[Dict[str, Any]],
    ) -> str:
        """
        Generate schema.yml documenting staging models

        This documents the staging models themselves (not the raw sources).

        Args:
            source: Data source configuration
            models: List of model definitions with columns
                    [{"name": "stg_source__table", "table_name": "table", "schema_name": "raw_source", "columns": [...]}]

        Returns:
            Generated YAML content
        """
        context = {
            "source_name": source.name,
            "source_type": source.type.value,
            "models": models,
            "generated_at": datetime.now().isoformat(),
        }

        template = self.jinja_env.get_template("staging_schema.yml.j2")
        return template.render(**context)

    def generate_all_models(
        self,
        sources: List[DataSource],
        generate_schema_yml: bool = True,
        skip_customized: bool = False
    ) -> Dict[str, Any]:
        """
        Generate staging models for all configured sources

        Supports both single-resource (CSV, single table) and multi-resource sources (Stripe, Shopify, etc.)

        Args:
            sources: List of data source configurations
            generate_schema_yml: Whether to generate sources.yml files (documenting raw tables)
            skip_customized: If True, skip models that have been customized by user

        Returns:
            Summary of generated models
        """
        summary = {
            "generated": [],
            "skipped": [],
            "errors": [],
        }

        for source in sources:
            try:
                # Ensure staging directory exists
                self.staging_dir.mkdir(parents=True, exist_ok=True)

                # Always use raw_{source_name} schema (industry best practice)
                schema_name = f"raw_{source.name}"

                # Try to get endpoints from config, fall back to DB discovery
                endpoints = self._get_source_endpoints(source)
                if not endpoints:
                    endpoints = self._discover_tables_from_db(schema_name)

                if not endpoints:
                    summary["skipped"].append({
                        "source": source.name,
                        "reason": "No tables found in database (run dango sync first)"
                    })
                    continue

                # Collect tables for sources.yml
                tables_for_yml = []
                generated_models = []

                for endpoint in endpoints:
                    # Normalize table name the same way dlt does:
                    # - Convert to lowercase
                    # - Replace spaces and special chars with underscores
                    # - Remove consecutive underscores
                    table_name = endpoint.lower()
                    table_name = ''.join(c if c.isalnum() else '_' for c in table_name)
                    table_name = '_'.join(filter(None, table_name.split('_')))  # Remove consecutive underscores

                    # Get schema from DuckDB
                    columns = self.get_table_schema(table_name, schema=schema_name)

                    if not columns:
                        # Skip this endpoint if table doesn't exist yet
                        summary["skipped"].append({
                            "source": source.name,
                            "endpoint": endpoint,
                            "reason": f"Table {schema_name}.{table_name} not found (run dango sync first)"
                        })
                        continue

                    # Check if model exists and is customized
                    # Naming: stg_{source_name}__{table_name}.sql
                    model_file = self.staging_dir / f"stg_{source.name}__{table_name}.sql"

                    if skip_customized and not self.is_auto_generated(model_file):
                        summary["skipped"].append({
                            "source": source.name,
                            "endpoint": endpoint,
                            "reason": "User-customized (marker comment removed)"
                        })
                        continue

                    # Infer deduplication strategy
                    dedup_strategy, dedup_columns = self.infer_dedup_strategy(source, columns)

                    # Generate staging model SQL
                    model_sql = self.generate_staging_model(
                        source=source,
                        table_name=table_name,
                        schema_name=schema_name,
                        dedup_strategy=dedup_strategy,
                        dedup_columns=dedup_columns,
                    )

                    # Write model file
                    with open(model_file, "w") as f:
                        f.write(model_sql)

                    generated_models.append({
                        "endpoint": endpoint,
                        "model": str(model_file),
                        "columns": len(columns),
                        "dedup_strategy": dedup_strategy or "none",
                    })

                    # Add to sources.yml tables list
                    tables_for_yml.append({
                        "name": table_name,
                        "columns": columns
                    })

                # Generate sources.yml for all tables (documents raw tables)
                sources_file = None
                staging_schema_file = None
                if generate_schema_yml and tables_for_yml:
                    sources_yml = self.generate_sources_yml(source, schema_name, tables_for_yml)
                    sources_file = self.staging_dir / f"sources_{source.name}.yml"
                    with open(sources_file, "w") as f:
                        f.write(sources_yml)

                    # Generate staging schema.yml (documents staging models)
                    staging_models_for_yml = []
                    for table in tables_for_yml:
                        staging_models_for_yml.append({
                            "name": f"stg_{source.name}__{table['name']}",
                            "table_name": table["name"],
                            "schema_name": schema_name,
                            "columns": table["columns"],
                        })

                    staging_schema_yml = self.generate_staging_schema_yml(source, staging_models_for_yml)
                    staging_schema_file = self.staging_dir / f"stg_{source.name}.yml"
                    # Only write if file doesn't exist (don't overwrite user customizations)
                    if not staging_schema_file.exists():
                        with open(staging_schema_file, "w") as f:
                            f.write(staging_schema_yml)

                if generated_models:
                    summary["generated"].append({
                        "source": source.name,
                        "schema": schema_name,
                        "models": generated_models,
                        "sources": str(sources_file) if sources_file else None,
                    })

            except Exception as e:
                summary["errors"].append({
                    "source": source.name,
                    "error": str(e),
                })

        return summary


def generate_staging_models(project_root: Path, sources: List[DataSource]) -> Dict[str, Any]:
    """
    Convenience function to generate staging models

    Args:
        project_root: Path to dango project root
        sources: List of data source configurations

    Returns:
        Summary of generated models
    """
    generator = DbtModelGenerator(project_root)
    return generator.generate_all_models(sources)
