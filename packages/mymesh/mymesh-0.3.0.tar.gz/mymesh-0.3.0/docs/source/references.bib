@article{Cavendish1974,
abstract = {The object of this paper is to describe a new algorithm for the semi‐automatic triangulation of arbitrary, multiply connected planar domains. The strategy is based upon a modification of a finite element mesh genration algorithm recently developed. 1 The scheme is designed for maximum flexibility and is capable of generating meshes of triangular elements for the decomposition of virtually any multiply connected planar domain. Moreover, the desired density of elements in various regions of the problem domain is specified by the user, thus allowing him to obtain a mesh decomposition appropriate to the physical loading and/or boundary conditions of the particular problem at hand. Several examples are presented to illustrate the applicability of the algorithm. An extension of the algorithm to the triangulation of shell structures is indicated. Copyright {\textcopyright} 1974 John Wiley {\&} Sons, Ltd},
author = {Cavendish, James C.},
doi = {10.1002/nme.1620080402},
file = {:home/tim/Nextcloud/Documents/Papers/1974 - Cavendish - Automatic triangulation of arbitrary planar domains for the finite element method.pdf:pdf},
issn = {10970207},
journal = {International Journal for Numerical Methods in Engineering},
number = {4},
pages = {679--696},
title = {{Automatic triangulation of arbitrary planar domains for the finite element method}},
volume = {8},
year = {1974}
}
@article{Carr2001,
abstract = {We use polyharmonic Radial Basis Functions (RBFs) to reconstruct smooth, manifold surfaces from point-cloud data and to repair incomplete meshes. An object's surface is defined implicitly as the zero set of an RBF fitted to the given surface data. Fast methods for fitting and evaluating RBFs allow us to model large data sets, consisting of millions of surface points, by a single RBF - previously an impossible task. A greedy algorithm in the fitting process reduces the number of RBF centers required to represent a surface and results in significant compression and further computational advantages. The energy-minimisation characterisation of polyharmonic splines result in a "smoothest" interpolant. This scale-independent characterisation is well-suited to reconstructing surfaces from non-uniformly sampled data. Holes are smoothly filled and surfaces smoothly extrapolated. We use a non-interpolating approximation when the data is noisy. The functional representation is in effect a solid model, which means that gradients and surface normals can be determined analytically. This helps generate uniform meshes and we show that the RBF representation has advantages for mesh simplification and remeshing applications. Results are presented for real-world rangefinder data. {\textcopyright} 2001 ACM.},
author = {Carr, J. C. and Beatson, R. K. and Cherrie, J. B. and Mitchell, T. J. and Fright, W. R. and McCallum, B. C. and Evans, T. R.},
doi = {10.1145/383259.383266},
file = {:home/tim/Nextcloud/Documents/Papers/2001 - Carr et al. - Reconstruction and representation of 3D objects with radial basis functions.pdf:pdf},
isbn = {158113374X},
journal = {Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2001},
keywords = {RBF,Radial Basis Function,geometry compression,mesh repair,point-cloud surfacing,solid modeling,surface reconstruction,variational implicit surfaces},
pages = {67--76},
title = {{Reconstruction and representation of 3D objects with radial basis functions}},
year = {2001}
}
@article{Liu1994,
abstract = {Tetrahedron shape measures are used for evaluating the quality of tetrahedra in finite element meshes. Three shape measures, the minimum solid angle $\theta${\{}symbol{\}}min the radius ratio $\rho$, and the mean ratio $\eta$, are discussed in this paper. A new formula for the computation of a solid angle of tetrahedron is derived. For different shape measures $\mu$ and v (with values ≤1), we establish a relationship between $\mu$ and v of the form {\{}Mathematical expression{\}} where c0, c1, e0, and e1 are positive constants. This means that if one measure approaches zero for a poorly-shaped tetrahedron, so does the other. Combined with the property that each measure attains a maximum value only for the regular tetrahedron, this means that the shape measures are "equivalent." {\textcopyright} 1994 the BIT Foundation.},
author = {Liu, A. and Joe, B.},
doi = {10.1007/BF01955874},
file = {:home/tim/Nextcloud/Documents/Papers/1994 - Liu, Joe - Relationship between tetrahedron shape measures.pdf:pdf},
issn = {00063835},
journal = {Bit},
keywords = {AMS subject classifications: 51M25, 51M16, 52B10,,finite element analysis,mesh generation,tetrahedron shape measure},
number = {2},
pages = {268--287},
title = {{Relationship between tetrahedron shape measures}},
volume = {34},
year = {1994}
}
@article{Faraj2016,
abstract = {We propose a practical iterative remeshing algorithm for multi-material tetrahedral meshes which is solely based on simple local topological operations, such as edge collapse, flip, split and vertex smoothing. To do so, we exploit an intermediate implicit feature complex which reconstructs piecewise smooth multi-material boundaries made of surface patches, feature edges and corner vertices. Furthermore, we design specific feature-aware local remeshing rules which, combined with a moving least square projection, result in high quality isotropic meshes representing the input mesh at a user defined resolution while preserving important features. Our algorithm uses only topology-aware local operations, which allows us to process difficult input meshes such as self-intersecting ones. We evaluate our approach on a collection of examples and experimentally show that it is fast and scales well.},
author = {Faraj, Noura and Thiery, Jean Marc and Boubekeur, Tamy},
doi = {10.1016/j.cag.2016.05.019},
file = {:home/tim/Nextcloud/Documents/Papers/2016 - Faraj, Thiery, Boubekeur - Multi-material adaptive volume remesher.pdf:pdf},
issn = {00978493},
journal = {Computers and Graphics (Pergamon)},
keywords = {Feature preservation,Multi-material tetrahedral mesh,Volume remeshing},
pages = {150--160},
title = {{Multi-material adaptive volume remesher}},
url = {http://dx.doi.org/10.1016/j.cag.2016.05.019},
volume = {58},
year = {2016}
}
@article{Bloomenthal1994,
abstract = {An algorithm for the polygonization of implicit surfaces is described and an implementation in C is provided. The discussion reviews implicit surface polygonization, and compares various methods.},
author = {Bloomenthal, Jules},
doi = {10.1016/b978-0-12-336156-1.50040-9},
file = {:home/tim/Nextcloud/Documents/Papers/1994 - Bloomenthal - An Implicit Surface Polygonizer.pdf:pdf},
journal = {Graphics Gems},
pages = {324--349},
title = {{An Implicit Surface Polygonizer}},
year = {1994}
}
@article{Akenine-Moller2005,
abstract = {A fast routine for testing whether a triangle and a box are overlapping in three dimensions is presented. The test is derived using the separating axis theorem, whereafter the test is simplified and the code is optimized for speed. We show that this approach is 2:3 vs. 3:8 (PC vs. Sun) times faster than previous routines for this. It can be used for faster collision detection and faster voxelization in interactive ray tracers. The code is available online.},
author = {Akenine-M{\"{o}}ller, Tomas},
doi = {10.1145/1198555.1198747},
file = {:home/tim/Nextcloud/Documents/Papers/2005 - Akenine-M{\"{o}}ller - Fast 3D triangle-box overlap testing.pdf:pdf},
journal = {ACM SIGGRAPH 2005 Courses, SIGGRAPH 2005},
title = {{Fast 3D triangle-box overlap testing}},
url = {https://www.doi.org/10.1145/1198555.1198747},
year = {2005}
}
@article{Klingner2008,
abstract = {We present a tetrahedral mesh improvement schedule that usually creates meshes whose worst tetrahedra have a level of quality substantially better than those produced by any previous method for tetrahedral mesh generation or "mesh clean-up." Our goal is to aggressively optimize the worst tetrahedra, with speed a secondary consideration. Mesh optimization methods often get stuck in bad local optima (poor-quality meshes) because their repertoire of mesh transformations is weak. We employ a broader palette of operations than any previous mesh improvement software. Alongside the best traditional topological and smoothing operations, we introduce a topological transformation that inserts a new vertex (sometimes deleting others at the same time). We describe a schedule for applying and composing these operations that rarely gets stuck in a bad optimum. We demonstrate that all three techniques - smoothing, vertex insertion, and traditional transformations - are substantially more effective than any two alone. Our implementation usually improves meshes so that all dihedral angles are between 31° and 149°, or (with a different objective function) between 23° and 136°.},
author = {Klingner, Bryan Matthew and Shewchuk, Jonathan Richard},
doi = {10.1007/978-3-540-75103-8_1},
file = {:home/tim/Nextcloud/Documents/Papers/2008 - Klingner, Shewchuk - Aggressive tetrahedral mesh improvement.pdf:pdf},
isbn = {9783540751021},
journal = {Proceedings of the 16th International Meshing Roundtable, IMR 2007},
keywords = {()},
pages = {3--23},
title = {{Aggressive tetrahedral mesh improvement}},
year = {2008}
}
@article{Moller1997,
abstract = {This paper presents a method, along with some optimizations, for computing whether or not two triangles intersect. The code, which is shown to be fast, can be used in, for example, collision detection algorithms.},
author = {M{\"{o}}ller, Tomas},
doi = {10.1080/10867651.1997.10487472},
file = {:home/tim/Nextcloud/Documents/Papers/1997 - M{\"{o}}ller - Fast triangle-triangle intersection test.pdf:pdf},
issn = {0346718X},
journal = {Journal of Graphics Tools},
number = {2},
pages = {25--30},
publisher = {Taylor {\&} Francis Group},
title = {{Fast triangle-triangle intersection test}},
url = {https://www.doi.org/10.1080/10867651.1997.10487472},
volume = {2},
year = {1997}
}
@article{Freitag1997,
abstract = {Automatic mesh generation and adaptive refinement methods for complex three-dimensional domains have proven to be very successful tools for the efficient solution of complex applications problems. These methods can, however, produce poorly shaped elements that cause the numerical solution to be less accurate and more difficult to compute. Fortunately, the shape of the elements can be improved through several mechanisms, including face- and edge-swapping techniques, which change local connectivity, and optimization-based mesh smoothing methods, which adjust mesh point location. We consider several criteria for each of these two methods and compare the quality of several meshes obtained by using different combinations of swapping and smoothing. Computational experiments show that swapping is critical to the improvement of general mesh quality and that optimization-based smoothing is highly effective in eliminating very small and very large angles. High-quality meshes are obtained in a computationally efficient manner by using optimization-based smoothing to improve only the worst elements and a smart variant of Laplacian smoothing on the remaining elements. Based on our experiments, we offer several recommendations for the improvement of tetrahedral meshes. {\textcopyright} 1997 John Wiley {\&} Sons, Ltd.},
author = {Freitag, Lori A. and Ollivier-Gooch, Carl},
doi = {10.1002/(SICI)1097-0207(19971115)40:21<3979::AID-NME251>3.0.CO;2-9},
file = {:home/tim/Nextcloud/Documents/Papers/1997 - Freitag, Ollivier-Gooch - Tetrahedral mesh improvement using swapping and smoothing.pdf:pdf},
issn = {00295981},
journal = {International Journal for Numerical Methods in Engineering},
keywords = {Local reconnection,Mesh improvement,Mesh smoothing,Optimal smoothing},
number = {21},
pages = {3979--4002},
publisher = {John Wiley {\&} Sons},
title = {{Tetrahedral mesh improvement using swapping and smoothing}},
url = {https://onlinelibrary.wiley.com/terms-and-conditions},
volume = {40},
year = {1997}
}
@article{Vartziotis2008,
abstract = {A novel method for smoothing triangular surface meshes is presented. It is based on the Geometric Element Transformation Method (GETMe) representing a simple geometric operation. In contrast to existing smoothing techniques, GETMe transforms single elements thus resulting in a simultaneous movement of all nodes of an element. The effect of this transformation, if applied iteratively, is the asymptotical but rapid regularization of the initial element. Mesh smoothing is achieved by applying this transformation to every poor quality element until overall mesh quality reaches a user defined level. Thereby, a lower bound for the minimal element angle and an upper bound for the maximal element angle serve as natural control variables in order to freely adjust mesh quality with respect to subsequent applications. Descriptions of the basic smoothing algorithm as well as modifications to preserve shape and to incorporate constraints on nodes are given. Numerical experiments on both artificial and real world meshes demonstrate the method's potential and flexibility. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Vartziotis, Dimitris and Athanasiadis, Theodoros and Goudas, Iraklis and Wipper, Joachim},
doi = {10.1016/j.cma.2008.02.028},
file = {:home/tim/Nextcloud/Documents/Papers/2008 - Vartziotis et al. - Mesh smoothing using the Geometric Element Transformation Method.pdf:pdf},
issn = {00457825},
journal = {Computer Methods in Applied Mechanics and Engineering},
keywords = {Finite element mesh,Iterative element optimization,Mesh smoothing,Surface mesh,Triangulation},
number = {45-48},
pages = {3760--3767},
title = {{Mesh smoothing using the Geometric Element Transformation Method}},
volume = {197},
year = {2008}
}
@article{Aubry2008a,
abstract = {Given a set of normals R3 in two algorithms are presented to compute the 'most normal' normal. The 'most normal' normal is the normal that minimizes the maximal angle with the given set of normals. A direct application is provided supposing a surface triangulation is available. The set of normals may represent either the face normals of the faces surrounding a point or the point normals of the points surrounding a point. The first algorithm is iterative and straightforward, and is inspired by the one proposed by Pirzadeh (AJAA Paper 94-0417, 1994). The second gives more insight into the complete problem as it provides the unique solution explicitly. It would correspond to the general extension of the algorithm presented by Kallinderis (AIAA-92-2721, 1992). Copyright {\textcopyright} 2007 John Wiley {\&} Sons, Ltd.},
author = {Aubry, Romain and L{\"{o}}hner, Rainald},
doi = {10.1002/cnm.1056},
file = {:home/tim/Nextcloud/Documents/Papers/2008 - Aubry, L{\"{o}}hner - On the 'most normal' normal.pdf:pdf},
issn = {10698299},
journal = {Communications in Numerical Methods in Engineering},
keywords = {Normal smoothing,Point normal,Smallest circumscribed circle,Surface triangulation},
month = {dec},
number = {12},
pages = {1641--1652},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{On the 'most normal' normal}},
url = {https://onlinelibrary.wiley.com/doi/full/10.1002/cnm.1056 https://onlinelibrary.wiley.com/doi/abs/10.1002/cnm.1056 https://onlinelibrary.wiley.com/doi/10.1002/cnm.1056},
volume = {24},
year = {2008}
}
@article{Raman2008,
abstract = {The Marching Cubes Algorithm may return degenerate, zero area isosurface triangles, and often returns isosurface triangles with small areas, edges or angles. We show how to avoid both problems using an extended Marching Cubes lookup table. As opposed to the conventional Marching Cubes lookup table, the extended lookup table differentiates scalar values equal to the isovalue from scalar values greater than the isovalue. The lookup table has 38 = 6561 entries, based on three possible labels, '-' or '=' or '+', of each cube vertex. We present an algorithm based on this lookup table which returns an isosurface close to the Marching Cubes isosurface, but without any degenerate triangles or any small areas, edges or angles. {\textcopyright} 2008 The Eurographics Association and Blackwell Publishing Ltd.},
author = {Raman, Sundaresan and Wenger, Rephael},
doi = {10.1111/j.1467-8659.2008.01209.x},
file = {:home/tim/Nextcloud/Documents/Papers/2008 - Raman, Wenger - Quality isosurface mesh generation using an extended marching cubes lookup table.pdf:pdf},
issn = {14678659},
journal = {Computer Graphics Forum},
month = {may},
number = {3},
pages = {791--798},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Quality isosurface mesh generation using an extended marching cubes lookup table}},
url = {https://onlinelibrary.wiley.com/doi/full/10.1111/j.1467-8659.2008.01209.x https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2008.01209.x https://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2008.01209.x},
volume = {27},
year = {2008}
}
@article{Shapiro1999,
abstract = {Theory of R-functions provides the methodology for constructing exact implicit functions for any semi-analytic set. This paper systematically explores and compares the known constructions in terms of their differential properties and explains how such functions may be constructed automatically from CSG and boundary representations of solids. The constructed functions may be automatically differentiated and integrated and have many important applications in mesh-free engineering analysis, motion planning, and scientific visualization.},
author = {Shapiro, Vadim and Tsukanov, Igor},
doi = {10.1145/304012.304038},
file = {:home/tim/Nextcloud/Documents/Papers/1999 - Shapiro, Tsukanov - Implicit functions with guaranteed differential properties.pdf:pdf},
journal = {Proceedings of the Symposium on Solid Modeling and Applications},
pages = {258--269},
publisher = {ACM},
title = {{Implicit functions with guaranteed differential properties}},
url = {https://doi.org/10.1145/304012.304038},
year = {1999}
}
@article{Goldman2005,
abstract = {Curvature formulas for implicit curves and surfaces are derived from the classical curvature formulas in Differential Geometry for parametric curves and surfaces. These closed formulas include curvature for implicit planar curves, curvature and torsion for implicit space curves, and mean and Gaussian curvature for implicit surfaces. Some extensions of these curvature formulas to higher dimensions are also provided. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Goldman, Ron},
doi = {10.1016/j.cagd.2005.06.005},
file = {:home/tim/Nextcloud/Documents/Papers/2005 - Goldman - Curvature formulas for implicit curves and surfaces.pdf:pdf},
issn = {01678396},
journal = {Computer Aided Geometric Design},
keywords = {Curvature,Gaussian curvature,Gradient,Hessian,Implicit curves,Implicit surface,Mean curvature,Torsion},
number = {7 SPEC. ISS.},
pages = {632--658},
publisher = {Elsevier},
title = {{Curvature formulas for implicit curves and surfaces}},
volume = {22},
year = {2005}
}
@article{Chernyaev1995,
abstract = {An algorithm implemented in the HIGZ graphics package for the construction of isosurfaces from volumetric datasets is discussed. This algorithm is an improved version of the Marching Cubes method. For each cell considered independently, the algorithm permits the construction of a triangle model, the topology of which coincides exactly with the topology of the isosurface of the trilinear function. It is shown that there are 33 topologically different configurations, instead of 15 as with the MC-method.},
author = {Chernyaev, Ev},
file = {:home/tim/Nextcloud/Documents/Papers/1995 - Chernyaev - Marching cubes 33 Construction of topologically correct isosurfaces.pdf:pdf},
journal = {Institute for High Energy Physics, Moscow, Russia,},
pages = {1--8},
title = {{Marching cubes 33: Construction of topologically correct isosurfaces}},
year = {1995}
}
@article{Josephson2024b,
abstract = {Synthetic bone graft scaffolds aim to generate new bone tissue and alleviate the limitations of autografts and allografts. To meet that aim, it is essential to have a design approach able to generate scaffold architectures that will promote bone formation. Here, we present a topology-varying design optimization method, the “mixed-topology” approach, that generates new designs from a set of starting structures. This approach was used with objective functions focusing on improving the scaffold's local mechanical microenvironments to mechanobiologically promote bone formation within the scaffold and constraints to ensure manufacturability and achieve desired macroscale properties. The results demonstrate that this approach can successfully generate scaffold designs with improved microenvironments, taking into account different combinations of relevant stimuli and constraints.},
author = {Josephson, Timothy O. and Morgan, Elise F.},
doi = {10.1007/S10237-024-01880-0},
file = {:home/tim/Nextcloud/Documents/Papers/2024 - Josephson, Morgan - Mechanobiological optimization of scaffolds for bone tissue engineering.pdf:pdf},
isbn = {0123456789},
issn = {1617-7940},
journal = {Biomechanics and Modeling in Mechanobiology},
keywords = {Biological and Medical Physics,Biomedical Engineering and Bioengineering,Biophysics,Theoretical and Applied Mechanics},
month = {jul},
pages = {1--18},
publisher = {Springer},
title = {{Mechanobiological optimization of scaffolds for bone tissue engineering}},
url = {https://link.springer.com/article/10.1007/s10237-024-01880-0},
year = {2024}
}
@article{Hilton1996,
abstract = {A new surface based approach to implicit surface polygonization is introduced in this paper. This is applied to the reconstruction of 3D surface models of complex objects from multiple range images. Geometric fusion of multiple range images into an implicit surface representation was presented to previous work. This paper introduces an efficient algorithm to reconstruct a triangulated model of a manifold implicit surface. A local 3D constraint is derived which defines the Delaunay surface triangulation of a set of points on a manifold surface in 3D space. The `Marching Triangles' algorithm uses the local 3D constraint to reconstruct a Delaunay triangulation of an arbitrary topology manifold surface. Computational and representational costs are both a factor of 3-5 lower than previous volumetric approaches such as marching cubes.},
author = {Hilton, A. and Stoddart, A. J. and Illingworth, J. and Windeatt, T.},
doi = {10.1109/icip.1996.560840},
file = {:home/tim/Nextcloud/Documents/Papers/1996 - Hilton et al. - Marching triangles Range image fusion for complex object modelling.pdf:pdf},
journal = {IEEE International Conference on Image Processing},
pages = {381--384},
publisher = {IEEE},
title = {{Marching triangles: Range image fusion for complex object modelling}},
url = {https://doi.org/10.1109/ICIP.1996.560840},
volume = {2},
year = {1996}
}
@article{Moller2005,
abstract = {We present a clean algorithm for determining whether a ray intersects a triangle. The algorithm translates the origin of the ray and then changes the base of that vector which yields a vector (t u v)T, where t is the distance to the plane in which the triangle lies and (u, v) represents the coordinates inside the triangle. One advantage of this method is that the plane equation need not be computed on the fly nor be stored, which can amount to significant memory savings for triangle meshes. As we found our method to be comparable in speed to previous methods, we believe it is the fastest ray/triangle intersection routine for triangles which do not have precomputed plane equations.},
author = {M{\"{o}}ller, Tomas and Trumbore, Ben},
doi = {10.1145/1198555.1198746},
file = {:home/tim/Nextcloud/Documents/Papers/2005 - M{\"{o}}ller, Trumbore - Fast, minimum storage raytriangle intersection.pdf:pdf},
journal = {ACM SIGGRAPH 2005 Courses, SIGGRAPH 2005},
keywords = {Base transformation,Intersection,Ray tracing,Ray/triangle-intersection},
pages = {7823--7830},
title = {{Fast, minimum storage ray/triangle intersection}},
url = {https://www.doi.org/10.1145/1198555.1198746},
year = {2005}
}
@article{Blom2000,
abstract = {This paper presents an investigation on the spring analogy. The spring analogy serves for deformation in a moving boundary problem. First, two different kinds of springs are discussed: the vertex springs and the segment springs. The vertex spring analogy is originally used for smoothing a mesh after mesh generation or refinement. The segment spring analogy is used for deformation of the mesh in a moving boundary problem. The difference between the two methods lies in the equilibrium length of the springs. By means of an analogy to molecular theory, the two theories are generalized into a single theory that covers both. The usual choice of the stiffness of the spring is clarified by the mathematical analysis of a representative one-dimensional configuration. The analysis shows that node collision is prevented when the stiffness is chosen as the inverse of the segment length. The observed similarity between elliptic grid generation and the spring analogy is also investigated. This investigation shows that both methods update the grid point position by a weighted average of the surrounding points in an iterative manner. The weighting functions enforce regularity of the mesh. Based on these considerations, several improvements on the spring analogy are developed. The principle of Saint Venant is circumvented by a boundary correction. The prevention of inversion of triangular elements is improved by semi-torsional springs. The numerical results show the superiority of the segment spring analogy over the vertex one for a small rotation of an NACA0012 mesh. The boundary correction allows for large deformation of the mesh, where the standard spring analogy fails. The final test is performed on a Navier-Stokes mesh. This mesh contains high aspect ratio mesh cells near the boundary. Large deformation of this mesh shows that the semi-torsional spring improvement is imperative to retain the validity of this mesh. Copyright (C) 2000 John Wiley and Sons, Ltd.},
author = {Blom, Frederic J.},
doi = {10.1002/(SICI)1097-0363(20000330)32:6<647::AID-FLD979>3.0.CO;2-K},
file = {:home/tim/Nextcloud/Documents/Papers/2000 - Blom - Considerations on the spring analogy.pdf:pdf},
issn = {02712091},
journal = {International Journal for Numerical Methods in Fluids},
keywords = {Elliptic grid generation,Mesh deformation,Moving boundaries,Unstructured meshes},
number = {6},
pages = {647--668},
title = {{Considerations on the spring analogy}},
url = {https://doi.org/10.1002/(SICI)1097-0363(20000330)32:6{\%}3C647::AID-FLD979{\%}3E3.0.CO;2-K},
volume = {32},
year = {2000}
}
@article{Zhang2003,
abstract = {This paper presents an algorithm to extract adaptive and quality 3D meshes directly from volumetric imaging data primarily Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). The extracted tetrahedral and hexahedral meshes are extensively used in finite element simulations. Our comprehensive approach combines bilateral and anisotropic (feature specific) diffusion filtering, with contour spectrum based, isosurface and interval volume selection. Next, a top-down octree subdivision coupled with the dual contouring method is used to rapidly extract adaptive 3D finite element meshes from volumetric imaging data. The main contributions are extending the dual contouring method to crack free interval volume tetrahedralization and hexahedralization with feature sensitive adaptation. Compared to other tetrahedral extraction methods from imaging data, our method generates better quality adaptive 3D meshes without hanging nodes. Our method has the properties of crack prevention and feature sensitivity.},
author = {Zhang, Yongjie and Bajaj, Chandrajit and Sohn, Bong Soo},
doi = {10.1145/781606.781653},
file = {:home/tim/Nextcloud/Documents/Papers/2003 - Zhang, Bajaj, Sohn - Adaptive and quality 3D meshing from imaging data.pdf:pdf},
journal = {Proceedings of the Symposium on Solid Modeling and Applications},
keywords = {3D meshes,Adaptive,Feature sensitive,Hanging nodes,Quality},
pages = {286--291},
title = {{Adaptive and quality 3D meshing from imaging data}},
year = {2003}
}
@article{Ohtake2003,
abstract = {The paper presents a novel approach for accurate polygonization of implicit surfaces with sharp features. The approach is based on mesh evolution toward a given implicit surface with simultaneous control of the mesh vertex positions and mesh normals. Given an initial polygonization of an implicit surface, a mesh evolution process initialized by the polygonization is used. The evolving mesh converges to a limit mesh which delivers a high-quality approximation of the implicit surface. For analyzing how close the evolving mesh approaches the implicit surface, we use two error metrics: the metrics measure deviations of the mesh vertices from the implicit surface and deviations of mesh normals from the normals of the implicit surface.},
author = {Ohtake, Yutaka and Belyaev, Alexander and Pasko, Alexander},
doi = {10.1007/s00371-002-0181-z},
file = {:home/tim/Nextcloud/Documents/Papers/2003 - Ohtake, Belyaev, Pasko - Dynamic mesh optimization for polygonized implicit surfaces with sharp features.pdf:pdf},
issn = {01782789},
journal = {Visual Computer},
keywords = {Implicit surfaces,Mesh evolution},
number = {2-3},
pages = {115--126},
title = {{Dynamic mesh optimization for polygonized implicit surfaces with sharp features}},
volume = {19},
year = {2003}
}
@article{Taubin1993,
abstract = {In this paper we introduce a new algorithm for rasterizing algebraic curves, and we discuss applications to surface and surface-surface intersection rendering and visualization. By rasterizing an algebraic curve we mean to determine which cells, or pixels, from a square mesh of cells in the plane, are cut by a curve represented as the set of zeros of a polynomial in two variables. By using a recursive space subdivision scheme, the problem is be reduced to testing whether the curve cuts a square or not. Other researchers have followed this approach, but their tests are either computationally expensive, or apply just to special cases. Curves with singularities are particularly difficult to deal with, and most know algorithms fail to render these curves correctly. Our contribution in this paper is the introduction of a computationally efficient and asymptotically correct test, which applies not only to dimension two, but also to dimension three and above. We prove that the recursive space subdivision algorithm based on this new test renders a curve of constant width, even in neighborhoods of singular points, and with no missing parts. For example, if f is a polynomial, it produces the same results whether the coefficients of f or f2 are given as input to the algorithm. Not many algorithms satisfy this property. Finally, the same methodology can be applied to compute a set of voxels containing an algebraic surface, and by representing it as a singular surface, space algebraic curves (surface-surface intersections) can be approximated as well. We show examples of these applications. This sets of voxels can be used for tolerance analysis and to compute polyhedral or piecewise linear approximations of the curves or surfaces for interactive rendering purposes as well.},
author = {Taubin, Gabriel},
doi = {10.1145/164360.164427},
file = {:home/tim/Nextcloud/Documents/Papers/1993 - Taubin - Accurate algorithm for rasterizing algebraic curves.pdf:pdf},
isbn = {0897915844},
journal = {Proceedings on the second ACM symposium on Solid modeling and applications},
pages = {221--230},
title = {{Accurate algorithm for rasterizing algebraic curves}},
year = {1993}
}
@article{Garland1997,
abstract = {Many applications in computer graphics require complex, highly detailed models. However, the level of detail actually necessary may vary considerably. To control processing time, it is often desirable to use approximations in place of excessively detailed models. We have developed a surface simplification algorithm which can rapidly produce high quality approximations of polygonal models. The algorithm uses iterative contractions of vertex pairs to simplify models and maintains surface error approximations using quadric matrices. By contracting arbitrary vertex pairs (not just edges), our algorithm is able to join unconnected regions of models. This can facilitate much better approximations, both visually and with respect to geometric error. In order to allow topological joining, our system also supports non-manifold surface models.},
author = {Garland, Michael and Heckbert, Paul S.},
doi = {10.1145/258734.258849},
file = {:home/tim/Nextcloud/Documents/Papers/1997 - Garland, Heckbert - Surface simplification using quadric error metrics.pdf:pdf},
isbn = {0897918967},
journal = {Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 1997},
keywords = {level of detail,mutiresolution modeling,non-manifold,pair contraction,surface simplification},
pages = {209--216},
title = {{Surface simplification using quadric error metrics}},
url = {https://www.doi.org/10.1145/258734.258849},
year = {1997}
}
@article{Freitag1997a,
abstract = {Local mesh smoothing algorithms have been shown to be effective in repairing distorted elements in automatically generated meshes. The simplest such algorithm is Laplacian smoothing, which moves grid points to the geometric center of incident vertices. Unfortunately, this method operates heuristically and can create invalid meshes or elements of worse quality than those contained in the original mesh. In contrast, optimization-based methods are designed to maximize some measure of mesh quality and are very effective at eliminating extremal angles in the mesh. These improvements come at a higher computational cost, however. In this article we propose four smoothing techniques that combine a smart variant of Laplacian smoothing with an optimization-based approach. Several numerical experiments are performed that compare the mesh quality and computational cost for each of the methods in two and three dimensions. We find that the combined approaches are very cost effective and yield high-quality meshes.},
author = {Freitag, Lori A.},
file = {:home/tim/Nextcloud/Documents/Papers/1997 - Freitag - On combining laplacian and optimization-based mesh smoothing techniques.pdf:pdf},
issn = {01608835},
journal = {American Society of Mechanical Engineers, Applied Mechanics Division, AMD},
pages = {37--43},
title = {{On combining laplacian and optimization-based mesh smoothing techniques}},
volume = {220},
year = {1997}
}
@article{Morales2007,
abstract = {Goals: To review the literature on chondrocyte movements and to develop plausible hypothesis for further work. Design: Chondrocyte movements are herein defined as translocations of the cell body. A brief overview of cell migration in other cell types is presented to set the stage for a discussion of chondrocyte moves; this includes a discussion of the challenges that cells find when moving within tissues. Reports of isolated chondrocyte migration in vitro (isolated cell systems) and ex vivo (cartilage organ cultures) are then summarized, followed by a discussion of recent studies that infer chondrocyte movements in vivo. Results: Investigators from different laboratories have observed chondrocyte motility in vitro. I became interested in the question of whether articular chondrocytes retained their phenotype during their migratory excursions. We devised a simple method to separate migratory and stationary chondrocytes and then showed that migratory chondrocytes synthesized collagen II but not I - consistent with a differentiated phenotype. Our time-lapse video microscopy studies showed that the cells displayed appropriate movement kinetics, albeit with low speed and directionality. Similarly, others have presented data consistent with slow movement of chondrocytes out of cartilage explants. It is important to decipher whether these in vitro movements reflect physiological states and if so, which events are simulated. Examples of in vivo studies that have inferred chondrocyte movements include those describing rotational or gliding movements of chondrocytes in the proliferative zone of the growth plate and its importance in the growth process; and the notion that chondrocytes move from the cartilage endplates to the nucleus pulposus (NP) in the spine of rabbits and rats during development. Such studies are consistent with the hypothesis that chondrocytes exhibit highly controlled and specialized movements during tissue growth and remodeling in vivo. On the other hand, the cartilage explant studies elicit interest in the possibility that matrix injuries resulting in disruption of the collagen network of adult cartilages provide a permissive environment for chondrocyte motility. Conclusions: The case for in vivo chondrocyte motility remains to be proven. However, the in vitro and in vivo data on chondrocyte movements present an argument for further thought and studies in this area. {\textcopyright} 2007 Osteoarthritis Research Society International.},
author = {Morales, T. I.},
doi = {10.1016/j.joca.2007.02.022},
file = {:home/tim/Nextcloud/Documents/Papers/2007 - Morales - Chondrocyte moves clever strategies.pdf:pdf},
issn = {10634584},
journal = {Osteoarthritis and Cartilage},
keywords = {Cartilage,Cell process,Chondrocyte,Migration,Movement},
month = {aug},
number = {8},
pages = {861--871},
pmid = {17467303},
publisher = {W.B. Saunders},
title = {{Chondrocyte moves: clever strategies?}},
volume = {15},
year = {2007}
}
@article{Marot2019,
abstract = {This paper presents a new scalable parallelization scheme to generate the 3D Delaunay triangulation of a given set of points. Our first contribution is an efficient serial implementation of the incremental Delaunay insertion algorithm. A simple dedicated data structure, an efficient sorting of the points, and the optimization of the insertion algorithm have permitted to accelerate reference implementations by a factor three. Our second contribution is a multithreaded version of the Delaunay kernel that is able to concurrently insert vertices. Moore curve coordinates are used to partition the point set, avoiding heavy synchronization overheads. Conflicts are managed by modifying the partitions with a simple rescaling of the space-filling curve. The performances of our implementation have been measured on three different processors: an Intel core-i7, an Intel Xeon Phi, and an AMD EPYC, on which we have been able to compute three billion tetrahedra in 53 seconds. This corresponds to a generation rate of over 55 million tetrahedra per second. We finally show how this very efficient parallel Delaunay triangulation can be integrated in a Delaunay refinement mesh generator, which takes as input the triangulated surface boundary of the volume to mesh.},
archivePrefix = {arXiv},
arxivId = {1805.08831},
author = {Marot, C{\'{e}}lestin and Pellerin, Jeanne and Remacle, Jean Fran{\c{c}}ois},
doi = {10.1002/nme.5987},
eprint = {1805.08831},
file = {:home/tim/Nextcloud/Documents/Papers/2019 - Marot, Pellerin, Remacle - One machine, one minute, three billion tetrahedra.pdf:pdf},
issn = {10970207},
journal = {International Journal for Numerical Methods in Engineering},
keywords = {3D Delaunay triangulation,SFC partitioning,parallel delaunay,radix sort,tetrahedral mesh generation},
number = {9},
pages = {967--990},
title = {{One machine, one minute, three billion tetrahedra}},
url = {http://gmsh.info.},
volume = {117},
year = {2019}
}
@article{VonSchnering1991,
abstract = {Periodic Nodal Surfaces (PNS) of Fourier series are derived and classified as fundamental invariants of structured matter. Relationships to periodic minimal surfaces PMS and to periodic zero potential surfaces (POPS) are given. A basic set of cubic PNS is represented in arithmetic form. The special importance of the invariance of the zeros to the type of the potential is stressed. {\textcopyright} 1991 Springer-Verlag.},
author = {von Schnering, H. G. and Nesper, R.},
doi = {10.1007/BF01313411},
file = {:home/tim/Nextcloud/Documents/Papers/1991 - von Schnering, Nesper - Nodal surfaces of Fourier series Fundamental invariants of structured matter.pdf:pdf},
issn = {07223277},
journal = {Zeitschrift f{\"{u}}r Physik B Condensed Matter},
number = {3},
pages = {407--412},
title = {{Nodal surfaces of Fourier series: Fundamental invariants of structured matter}},
url = {https://www.doi.org/10.1007/BF01313411},
volume = {83},
year = {1991}
}
@article{Thurrner1998,
abstract = {The method most commonly used to estimate the normal vector at a vertex of a polygonal surface averages the normal vectors of the facets incident to the vertex considered. The vertex normal obtaine...},
author = {Th{\"{u}}rrner, Grit and W{\"{u}}thrich, Charles A.},
doi = {10.1080/10867651.1998.10487487},
file = {:home/tim/Nextcloud/Documents/Papers/1998 - Th{\"{u}}rrner, W{\"{u}}thrich - Computing Vertex Normals from Polygonal Facets.pdf:pdf},
issn = {1086-7651},
journal = {Journal of Graphics Tools},
month = {jan},
number = {1},
pages = {43--46},
publisher = {Taylor {\&} Francis Group},
title = {{Computing Vertex Normals from Polygonal Facets}},
url = {https://www.tandfonline.com/doi/abs/10.1080/10867651.1998.10487487},
volume = {3},
year = {1998}
}
@article{Escobar2003,
abstract = {The quality improvement in mesh optimisation techniques that preserve its connectivity are obtained by an iterative process in which each node of the mesh is moved to a new position that minimises a certain objective function. The objective function is derived from some quality measure of the local submesh , that is, the set of tetrahedra connected to the adjustable or free node . Although these objective functions are suitable to improve the quality of a mesh in which there are non- inverted elements, they are not when the mesh is tangled. This is due to the fact that usual objective functions are not defined on all R3 and they present several discontinuities and local minima that prevent the use of conventional optimisation procedures. Otherwise, when the mesh is tangled, there are local submeshes for which the free node is out of the feasible region , or this does not exist. In this paper we propose the substitution of objective functions having barriers by modified versions that are defined and regular on all R3. With these modifications, the optimisation process is also directly applicable to meshes with inverted elements, making a previous untangling procedure unnecessary. This simultaneous procedure allows to reduce the number of iterations for reaching a prescribed quality. To illustrate the effectiveness of our approach, we present several applications where it can be seen that our results clearly improve those obtained by other authors. {\textcopyright} 2003 Published by Elsevier Science B.V.},
author = {Escobar, J. M. and Rodr{\'{i}}guez, E. and Montenegro, R. and Montero, G. and Gonz{\'{a}}lez-Yuste, J. M.},
doi = {10.1016/S0045-7825(03)00299-8},
file = {:home/tim/Nextcloud/Documents/Papers/2003 - Escobar et al. - Simultaneous untangling and smoothing of tetrahedral meshes.pdf:pdf},
issn = {00457825},
journal = {Computer Methods in Applied Mechanics and Engineering},
keywords = {Adaptive meshes,Finite elements,Mesh smoothing,Mesh untangling,Tetrahedral mesh generation},
month = {jun},
number = {25},
pages = {2775--2787},
publisher = {Elsevier},
title = {{Simultaneous untangling and smoothing of tetrahedral meshes}},
url = {https://www.doi.org/10.1016/S0045-7825(03)00299-8},
volume = {192},
year = {2003}
}
@article{Morse2005,
abstract = {We describe algebraic methods for creating implicit surfaces using linear combinations of radial basis interpolants to form complex models from scattered surface points. Shapes with arbitrary topology are easily represented without the usual interpolation or aliasing errors arising from discrete sampling. These methods were first applied to implicit surfaces by Savchenko, et al. and later developed independently by Turk and O'Brien as a means of performing shape interpolation. Earlier approaches were limited as a modeling mechanism because of the order of the computational complexity involved. We explore and extend these implicit interpolating methods to make them suitable for systems of large numbers of scattered surface points by using compactly supported radial basis interpolants. The use of compactly supported elements generates a sparse solution space, reducing the computational complexity and making the technique practical for large models. The local nature of compactly supported radial basis functions permits the use of computational techniques and data structures such as k-d trees for spatial subdivision, promoting fast solvers and methods to divide and conquer many of the subproblems associated with these methods. Moreover, the representation of complex models permits the exploration of diverse surface geometry. This reduction in computational complexity enables the application of these methods to the study of shape properties of large complex shapes.},
author = {Morse, Bryan S. and Yoo, Terry S. and Rheingans, Penny and Chen, David T. and Subramanian, K. R.},
doi = {10.1145/1198555.1198645},
file = {:home/tim/Nextcloud/Documents/Papers/2005 - Morse et al. - Interpolating implicit surfaces from scattered surface data using compactly supported radial basis functions.pdf:pdf},
journal = {ACM SIGGRAPH 2005 Courses, SIGGRAPH 2005},
title = {{Interpolating implicit surfaces from scattered surface data using compactly supported radial basis functions}},
year = {2005}
}
@article{Vartziotis2009,
abstract = {The geometric element transformation method (GETMe) has been introduced as a new element driven approach to mesh smoothing. It is based on simple geometric transformations, which, if applied iteratively, lead to the regularization of mesh elements. Global mesh smoothing can be accomplished by successively improving the worst elements or by averaging node positions obtained by the simultaneous transformation of all elements. GETMe smoothing has been successfully applied in the case of surface meshes. As shown in this paper, this approach also naturally extends to tetrahedral mesh smoothing without major conceptual modifications. A regularizing transformation for tetrahedra is presented and a combined approach of simultaneous and sequential GETMe smoothing is described. First numerical examples yield high quality meshes superior to those obtained by other geometry-based methods. In fact, the presented results are in a majority of cases at least comparable to those obtained by a state of the art global optimization-based method. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {Vartziotis, Dimitris and Wipper, Joachim and Schwald, Bernd},
doi = {10.1016/j.cma.2009.09.027},
file = {:home/tim/Nextcloud/Documents/Papers/2009 - Vartziotis, Wipper, Schwald - The geometric element transformation method for tetrahedral mesh smoothing.pdf:pdf},
issn = {00457825},
journal = {Computer Methods in Applied Mechanics and Engineering},
keywords = {Finite element mesh,Iterative element regularization,Mesh quality,Mesh smoothing,Tetrahedral mesh},
number = {1-4},
pages = {169--182},
title = {{The geometric element transformation method for tetrahedral mesh smoothing}},
url = {http://www.twt-gmbh.de},
volume = {199},
year = {2009}
}
@article{Shewchuk1996,
abstract = {Triangle is a robust implementation of two-dimensional constrained Delaunay triangulation and Ruppert's Delaunay refinement algorithm for quality mesh generation. Several implementation issues are discussed, including the choice of triangulation algorithms and data structures, the effect of several variants of the Delaunay refinement algorithm on mesh quality, and the use of adaptive exact arithmetic to ensure robustness with minimal sacrifice of speed. The problem of triangulating a planar straight line graph (PSLG) without introducing new small angles is shown to be impossible for some PSLGs, contradicting the claim that a variant of the Delaunay refinement algorithm solves this problem.},
author = {Shewchuk, Jonathan Richard},
doi = {10.1007/bfb0014497},
file = {:home/tim/Nextcloud/Documents/Papers/1996 - Shewchuk - Triangle Engineering a 2D quality mesh generator and delaunay triangulator.pdf:pdf},
isbn = {354061785X},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {203--222},
title = {{Triangle: Engineering a 2D quality mesh generator and delaunay triangulator}},
url = {http://www.cs.cmu.edu/quake/triangle.html},
volume = {1148},
year = {1996}
}
@article{Koenderink1992a,
abstract = {The classical surface curvature measures, such as the Gaussian and the mean curvature at a point of a surface, are not very indicative of local shape. The two principal curvatures (taken as a pair) are more informative, but one would prefer a single shape indicator rather than a pair of numbers. Moreover, the shape indicator should preferably be independent of the size i.e. the amount of curvature, as distinct from the type of curvature. We propose two novel measures of local shape, the 'curvedness' and the 'shape index'. The curvedness is a positive number that specifies the amount of curvature, whereas the shape index is a number in the range [-1, +1] and is scale invariant. The shape index captures the intuitive notion of 'local shape' particularly well. The shape index can be mapped upon an intuitively natural colour scale. Two complementary shapes (like stamp and mould) map to complementary hues. The symmetrical saddle (which is very special because it is self-complementary) maps to white. When a surface is tinted according to this colour scheme, this induces an immediate perceptual segmentation of convex, concave, and hyperbolic areas. We propose it as a useful tool in graphics representation of 3D shape. {\textcopyright} 1992.},
author = {Koenderink, Jan J. and van Doorn, Andrea J.},
doi = {10.1016/0262-8856(92)90076-F},
file = {:home/tim/Nextcloud/Documents/Papers/1992 - Koenderink, van Doorn - Surface shape and curvature scales.pdf:pdf},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {curvature scales,surface shape},
number = {8},
pages = {557--564},
title = {{Surface shape and curvature scales}},
url = {https://www.doi.org/10.1016/0262-8856(92)90076-F},
volume = {10},
year = {1992}
}
@article{Goldfeather2004,
abstract = {There are a number of applications in computer graphics that require as a first step the accurate estimation of principal direction vectors at arbitrary vertices on a triangulated surface. Although several methods for calculating principal directions over such models have been previously proposed, we have found in practice that all exhibit unexplained large errors in some cases. In this article, we describe our theoretical and experimental investigations into possible sources of errors in the approximation of principal direction vectors from triangular meshes, and suggest a new method for estimating principal directions that can yield better results under some circumstances. {\textcopyright} 2004 ACM.},
author = {Goldfeather, Jack and Interrante, Victoria},
doi = {10.1145/966131.966134},
file = {:home/tim/Nextcloud/Documents/Papers/2004 - Goldfeather, Interrante - A novel cubic-order algorithm for approximating principal direction vectors.pdf:pdf},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {3D shape,Principal directions},
number = {1},
pages = {45--63},
title = {{A novel cubic-order algorithm for approximating principal direction vectors}},
volume = {23},
year = {2004}
}
@article{Lorensen1987,
abstract = {We present a new algorithm, called marching cubes, that creates triangle models of constant density surfaces from 3D medical data. Using a divide-and-conquer approach to generate inter-slice connectivity, we create a case table that defines triangle topology. The algorithm processes the 3D medical data in scan-line order and calculates triangle vertices using linear interpolation. We find the gradient of the original data, normalize it, and use it as a basis for shading the models. The detail in images produced from the generated surface models is the result of maintaining the inter-slice connectivity, surface data, and gradient information present in the original 3D data. Results from computed tomography (CT), magnetic resonance (MR), and single-photon emission computed tomography (SPECT) illustrate the quality and functionality of marching cubes. We also discuss improvements that decrease processing time and add solid modeling capabilities.},
author = {Lorensen, William E. and Cline, Harvey E.},
doi = {10.1145/37401.37422},
file = {:home/tim/Nextcloud/Documents/Papers/1987 - Lorensen, Cline - Marching cubes A high resolution 3D surface construction algorithm.pdf:pdf},
isbn = {0897912276},
journal = {Proceedings of the 14th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 1987},
keywords = {Computer graphics,Medical imaging,Surface reconstruction},
number = {4},
pages = {163--169},
title = {{Marching cubes: A high resolution 3D surface construction algorithm}},
volume = {21},
year = {1987}
}
@article{Labelle2007,
abstract = {Figure 1: A 134,400-tetrahedron mesh produced by isosurface stuffing, with cutaway views. At the lower right is a histogram of tetrahedron dihedral angles in 2 • intervals; multiply the heights of the red bars by 20. (Angles of 45 • , 60 • , and 90 • occur with high frequency.) The extreme dihedral angles are 15.2 • and 158.2 •. This mesh took 55 seconds to generate on a Mac Pro with a 2.66 GHz Intel Xeon processor, but the mesh generation time was only 642 milliseconds; nearly all the time was spent in the isosurface evaluation code. Abstract The isosurface stuffing algorithm fills an isosurface with a uniformly sized tetrahedral mesh whose dihedral angles are bounded between 10.7 • and 164.8 • , or (with a change in parameters) between 8.9 • and 158.8 •. The algorithm is whip fast, numerically robust , and easy to implement because, like Marching Cubes, it generates tetrahedra from a small set of precomputed stencils. A variant of the algorithm creates a mesh with internal grading: on the boundary , where high resolution is generally desired, the elements are fine and uniformly sized, and in the interior they may be coarser and vary in size. This combination of features makes isosurface stuffing a powerful tool for dynamic fluid simulation, large-deformation mechanics, and applications that require interactive remeshing or use objects defined by smooth implicit surfaces. It is the first algorithm that rigorously guarantees the suitability of tetrahedra for finite element methods in domains whose shapes are substantially more challenging than boxes. Our angle bounds are guaranteed by a computer-assisted proof. If the isosurface is a smooth 2-manifold with bounded curvature, and the tetrahedra are sufficiently small, then the boundary of the mesh is guaranteed to be a geometrically and topologically accurate approximation of the isosurface.},
author = {Labelle, Fran{\c{c}}ois and Shewchuk, Jonathan Richard},
doi = {10.1145/1239451.1239508},
file = {:home/tim/Nextcloud/Documents/Papers/2007 - Labelle, Shewchuk - Isosurface stuffing.pdf:pdf},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {CR Categories: I35 [Computer Graphics]: Computatio,dihedral angle,tetrahedral mesh generation},
number = {99},
pages = {57},
title = {{Isosurface stuffing}},
url = {http://doi.acm.org/10.1145/1239451.1239508},
volume = {26},
year = {2007}
}
@article{Sullivan2019,
abstract = {Sullivan et al., (2019). PyVista: 3D plotting and mesh analysis through a streamlined interface for the Visualization Toolkit (VTK). Journal of Open Source Software, 4(37), 1450, https://doi.org/10.21105/joss.01450},
author = {Sullivan, C. and Kaszynski, Alexander},
doi = {10.21105/joss.01450},
file = {:home/tim/Nextcloud/Documents/Papers/2019 - Sullivan, Kaszynski - PyVista 3D plotting and mesh analysis through a streamlined interface for the Visualization Toolkit (VTK).pdf:pdf},
journal = {Journal of Open Source Software},
month = {may},
number = {37},
pages = {1450},
publisher = {The Open Journal},
title = {{PyVista: 3D plotting and mesh analysis through a streamlined interface for the Visualization Toolkit (VTK)}},
url = {https://www.doi.org/10.21105/joss.01450},
volume = {4},
year = {2019}
}
@article{Lindstrom2000,
abstract = {We present an algorithm for out-of-core simplification of large polygonal datasets that are too complex to fit in main memory. The algorithm extends the vertex clustering scheme of Rossignac and Borrel [13] by using error quadric information for the placement of each cluster's representative vertex, which better preserves fine details and results in a low mean geometric error. The use of quadrics instead of the vertex grading approach in [13] has the additional benefits of requiring less disk space and only a single pass over the model rather than two. The resulting linear time algorithm allows simplification of datasets of arbitrary complexity. In order to handle degenerate quadrics associated with (near) flat regions and regions with zero Gaussian curvature, we present a robust method for solving the corresponding underconstrained least-squares problem. The algorithm is able to detect these degeneracies and handle them gracefully. Key features of the simplification method include a bounded Hausdorff error, low mean geometric error, high simplification speed (up to 100,000 triangles/second reduction), output (but not input) sensitive memory requirements, no disk space overhead, and a running time that is independent of the order in which vertices and triangles occur in the mesh.},
author = {Lindstrom, Peter},
doi = {10.1145/344779.344912},
file = {:home/tim/Nextcloud/Documents/Papers/2000 - Lindstrom - Out-of-Core Simplification of Large Polygonal Models.pdf:pdf},
isbn = {1581132085},
journal = {SIGGRAPH 2000 - Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques},
month = {jul},
pages = {259--262},
publisher = {Association for Computing Machinery, Inc},
title = {{Out-of-Core Simplification of Large Polygonal Models}},
url = {www.cc.gatech.edu/},
year = {2000}
}
@article{Dice1945,
abstract = {The coefficient of association of Forbes indicates the amount of association be- tween two given species compared to the amount of association between them expected by chance. In order to provide a simple direct measure of the amount of association of one species with another the association index is proposed. If a is the number of random samples of a given series in which species A occurs and h is the number of samples in which another species B occurs together with A, then the association index B/A = h/a. Similarly, if b is the number of samples in which species B occurs, then the associa- tion index A/B = h/b. There is also proposed a coincidence index, 2h/(a + b), whose value is intermediate between the two reciprocal association indices. As a measure of the statistical reliability of the deviation shown by the samples of a given series from the amount of associa- tion expected by chance, the chi-square test may be used.},
author = {Dice, Lee R.},
doi = {10.2307/1932409},
file = {:home/tim/Nextcloud/Documents/Papers/1945 - Dice - Measures of the Amount of Ecologic Association Between Species.pdf:pdf},
issn = {0012-9658},
journal = {Ecology},
number = {3},
pages = {297--302},
title = {{Measures of the Amount of Ecologic Association Between Species}},
url = {https://about.jstor.org/terms},
volume = {26},
year = {1945}
}
@article{Si2015,
abstract = {TetGen is a C++ program for generating good quality tetrahedral meshes aimed to support numerical methods and scientific computing. The problem of quality tetrahedralmesh generation is challenged by many theoretical and practical issues. TetGen uses Delaunay-based algorithms which have theoretical guarantee of correctness. It can robustly handle arbitrary complex 3D geometries and is fast in practice. The source code of TetGen is freely available. This article presents the essential algorithms and techniques used to develop TetGen. The intended audience are researchers or developers in mesh generation or other related areas. It describes the key software components of TetGen, including an efficient tetrahedral mesh data structure, a set of enhanced local mesh operations (combination of flips and edge removal), and filtered exact geometric predicates. The essential algorithms include incremental Delaunay algorithms for inserting vertices, constrained Delaunay algorithms for inserting constraints (edges and triangles), a new edge recovery algorithm for recovering constraints, and a new constrained Delaunay refinement algorithm for adaptive quality tetrahedral mesh generation. Experimental examples as well as comparisons with other softwares are presented.},
author = {Si, Hang},
doi = {10.1145/2629697},
file = {:home/tim/Nextcloud/Documents/Papers/2015 - Si - TetGen, a delaunay-based quality tetrahedral mesh generator.pdf:pdf},
issn = {15577295},
journal = {ACM Transactions on Mathematical Software},
keywords = {Algorithms,Delaunay,Design,G4 [Mathematical Software]: Algorithm design and a,I35 [Computational Geometry and Object Modeling] G,Performance,Performance Additional Key Words and Phrases: Tetr,Steiner points,boundary recovery,constrained Delaunay,edge removal,effi-ciency,flips,mesh improvement,mesh quality,mesh refinement,reliability and robustness},
month = {feb},
number = {2},
publisher = {
		ACM
		PUB27
		New York, NY, USA
	},
title = {{TetGen, a delaunay-based quality tetrahedral mesh generator}},
url = {http://dx.doi.org/10.1145/2629697 https://dl.acm.org/doi/10.1145/2629697},
volume = {41},
year = {2015}
}
@article{Hu2018,
abstract = {We propose a novel tetrahedral meshing technique that is unconditionally robust, requires no user interaction, and can directly convert a triangle soup into an analysis-ready volumetric mesh. The approach is based on several core principles: (1) initial mesh construction based on a fully robust, yet efficient, filtered exact computation (2) explicit (automatic or user-defined) tolerancing of the mesh relative to the surface input (3) iterative mesh improvement with guarantees, at every step, of the output validity. The quality of the resulting mesh is a direct function of the target mesh size and allowed tolerance: Increasing allowed deviation from the initial mesh and decreasing the target edge length both lead to higher mesh quality. Our approach enables "black-box" analysis, i.e. it allows to automatically solve partial differential equations on geometrical models available in the wild, offering a robustness and reliability comparable to, e.g., image processing algorithms, opening the door to automatic, large scale processing of real-world geometric data.},
author = {Hu, Yixin and Zhou, Qingnan and Gao, Xifeng and Jacobson, Alec and Zorin, Denis and Panozzo, Daniele},
doi = {10.1145/3197517.3201353},
file = {:home/tim/Nextcloud/Documents/Papers/2018 - Hu et al. - Tetrahedral meshing in the wild.pdf:pdf},
issn = {15577368},
journal = {ACM Transactions on Graphics},
keywords = {Mesh Generation,Robust Geometry Processing,Tetrahedral Meshing},
number = {4},
publisher = {Association for Computing Machinery},
title = {{Tetrahedral meshing in the wild}},
volume = {37},
year = {2018}
}
@article{Williams2005,
abstract = {The computational bottleneck in a ray tracer using bounding volume hierarchies is often the ray intersection routine with axis-aligned bounding boxes. We describe a version of this routine that uses IEEE numerical properties to ensure that those tests are both robust and efficient. Sample source code is available online.},
author = {Williams, Amy and Barrus, Steve and Morley, R. Keith and Shirley, Peter},
doi = {10.1145/1198555.1198748},
file = {:home/tim/Nextcloud/Documents/Papers/2005 - Williams et al. - An efficient and robust ray-box intersection algorithm.pdf:pdf},
journal = {ACM SIGGRAPH 2005 Courses, SIGGRAPH 2005},
number = {1},
pages = {55--60},
title = {{An efficient and robust ray-box intersection algorithm}},
url = {https://www.doi.org/10.1145/1198555.1198748},
volume = {10},
year = {2005}
}
@article{Hu2020,
abstract = {We propose a new tetrahedral meshing method, fTetWild, to convert triangle soups into high-quality tetrahedral meshes. Our method builds on the TetWild algorithm, replacing the rational triangle insertion with a new incremental approach to construct and optimize the output mesh, interleaving triangle insertion and mesh optimization. Our approach makes it possible to maintain a valid floating-point tetrahedral mesh at all algorithmic stages, eliminating the need for costly constructions with rational numbers used by TetWild, while maintaining full robustness and similar output quality. This allows us to improve on TetWild in two ways. First, our algorithm is significantly faster, with running time comparable to less robust Delaunay-based tetrahedralization algorithms. Second, our algorithm is guaranteed to produce a valid tetrahedral mesh with floating-point vertex coordinates, while TetWild produces a valid mesh with rational coordinates which is not guaranteed to be valid after floating-point conversion. As a trade-off, our algorithm no longer guarantees that all input triangles are present in the output mesh, but in practice, as confirmed by our tests on the Thingi10k dataset, the algorithm always succeeds in inserting all input triangles.},
archivePrefix = {arXiv},
arxivId = {1908.03581},
author = {Hu, Yixin and Schneider, Teseo and Wang, Bolun and Zorin, Denis and Panozzo, Daniele},
doi = {10.1145/3386569.3392385},
eprint = {1908.03581},
file = {:home/tim/Nextcloud/Documents/Papers/2020 - Hu et al. - Fast tetrahedral meshing in the wild.pdf:pdf},
issn = {15577368},
journal = {ACM Transactions on Graphics},
keywords = {mesh generation,robust geometry processing,tetrahedral meshing},
number = {4},
pages = {18},
title = {{Fast tetrahedral meshing in the wild}},
url = {https://www.doi.org/10.1145/3386569.3392385},
volume = {39},
year = {2020}
}
@article{Bowyer1981,
abstract = {An efficient algorithm is proposed for computing the Dirichlet tessellation and Delaunay triangulation in a k dimensional Euclidean space (k greater than equivalent to 2). The algorithm is designed in a way that should allow it to be extended to some of the simpler non-Euclidean metric spaces as well. The algorithm has been implemented in ISO FORTRAN by the author and execution times and steroscopic pictures of the tessellation and triangulation are presented.},
author = {Bowyer, Adrian},
doi = {10.1093/comjnl/24.2.162},
file = {:home/tim/Nextcloud/Documents/Papers/1981 - Bowyer - Computing Dirichlet Tessellations.pdf:pdf},
issn = {00104620},
journal = {Computer Journal},
number = {2},
pages = {162--166},
title = {{Computing Dirichlet Tessellations.}},
url = {https://academic.oup.com/comjnl/article/24/2/162/338193},
volume = {24},
year = {1981}
}
@article{Laidlaw1986,
abstract = {Constructive Solid Geometry (CSG) is a powerful way of describing solid objects for computer graphics and modeling. The surfaces of any primitive object (such as a cube, sphere or cylinder) can be approximated by polygons. Being abile to find the union, intersection or difference of these objects allows more interesting and complicated polygonal objects to be created. The algorithm presented here performs these set operations on objects constructed from convex polygons. These objects must bound a finite volume, but need not be convex. An object that results from one of these operations also contains only convex polygons, and bounds a finite volume; thus, it can be used in later combinations, allowing the generation of quite complicated objects. Our algorithm is robust and is presented in enough detail to be implemented.},
author = {Laidlaw, David H. and Trumbore, W. Benjamin and Hughes, John F.},
doi = {10.1145/15922.15904},
file = {:home/tim/Nextcloud/Documents/Papers/1986 - Laidlaw, Trumbore, Hughes - Constructive solid geometry for polyhedral objects.pdf:pdf},
isbn = {0897911962},
journal = {Proceedings of the 13th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 1986},
month = {aug},
pages = {161--170},
publisher = {Association for Computing Machinery, Inc},
title = {{Constructive solid geometry for polyhedral objects}},
url = {https://www.doi.org/10.1145/15922.15904},
year = {1986}
}
@article{Schaefer2005,
abstract = {We present a method for contouring an implicit function using a grid topologically dual to structured grids such as octrees. By aligning the vertices of the dual grid with the features of the implicit function, we are able to reproduce thin features of the extracted surface without excessive subdivision required by methods such as Marching Cubes or Dual Contouring. Dual Marching Cubes produces a crack-free, adaptive polygonalization of the surface that reproduces sharp features. Our approach maintains the advantage of using structured grids for operations such as CSG while being able to conform to the relevant features of the implicit function yielding much sparser polygonalizations than has been possible using structured grids. {\textcopyright} IEEE Proceedings of Pacific Graphics 2004.},
author = {Schaefer, Scott and Warren, Joe},
doi = {10.1111/j.1467-8659.2005.00843.x},
file = {:home/tim/Nextcloud/Documents/Papers/2005 - Schaefer, Warren - Dual marching cubes Primal contouring of dual grids.pdf:pdf},
issn = {14678659},
journal = {Computer Graphics Forum},
keywords = {CSG,Contouring,Implicit functions,Marching cubes,Octree,QEF},
number = {2},
pages = {195--201},
title = {{Dual marching cubes: Primal contouring of dual grids}},
volume = {24},
year = {2005}
}
@article{Ohtake2001a,
abstract = {In this paper, we propose a method for improvement of isosurface polygonizations. Given an initial polygonization of an isosurface, we introduce a mesh evolution process initialized by the polygonization. The evolving mesh converges quickly to its limit mesh which provides with a high quality approximation of the isosurface even if the isosurface has sharp features, boundary, complex topology. To analyze how close the evolving mesh approaches its destined isosurface, we introduce error estimators measuring the deviations of the mesh vertices from the isosurface and mesh normals from the isosurface normals. A new technique for mesh editing with isosurfaces is also proposed. In particular, it can be used for creating carving effects.},
author = {Ohtake, Y. and Belyaev, A. G.},
doi = {10.1111/1467-8659.00529},
file = {:home/tim/Nextcloud/Documents/Papers/2001 - Ohtake, Belyaev - Mesh optimization for polygonized isosurfaces.pdf:pdf},
issn = {01677055},
journal = {Computer Graphics Forum},
keywords = {agents communication,behavioural animation and planning,inter,virtual humans animation},
month = {sep},
number = {3},
pages = {368--376},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Mesh optimization for polygonized isosurfaces}},
url = {https://onlinelibrary-wiley-com.ezproxy.bu.edu/doi/10.1111/1467-8659.00529},
volume = {20},
year = {2001}
}
@article{Jarvis1973,
abstract = {This paper presents an extremely simple algorithm fat idcntlfying the convex hull of a finite aet of points in the plane in essentially, at most n(ni + 1) operations fen n pints in the set and m Q; n points on the convex L In most cases far less than n(m + 1) operations are necessary because of a powerful point deletion mechanism that can easily be included. The operations are themselves trivial (computationahy inexpensive) and consist of angle comparisons only. Even these angle comparisons need not be actuahy carried out if an improvement suggested in *a late: section is implemented. Al?hough Graham's algorif hm [ 1 ] requires IE0 more '{\&}an (n lof+log2 Ctr operations*, the operations are themselves more compie;r than those 01 the method presented here; in partlcukr, Graham's method IMould not be as efficient for low m.},
author = {Jarvis, R. A.},
doi = {10.1016/0020-0190(73)90020-3},
file = {:home/tim/Nextcloud/Documents/Papers/1973 - Jarvis - On the identification of the convex hull of a finite set of points in the plane.pdf:pdf;:home/tim/Nextcloud/Documents/Papers/1973 - Jarvis - On the identification of the convex hull of a finite set of points in the plane(2).pdf:pdf},
issn = {00200190},
journal = {Information Processing Letters},
keywords = {algorithm,convex hull},
number = {1},
pages = {18--21},
title = {{On the identification of the convex hull of a finite set of points in the plane}},
volume = {2},
year = {1973}
}
@article{Hormann2001,
abstract = {A detailed discussion of the point in polygon problem for arbitrary polygons is given. Two concepts for solving this problem are known in literature: the even-odd rule and the winding number, the former leading to ray-crossing, the latter to angle summation algorithms. First we show by mathematical means that both concepts are very closely related, thereby developing a first version of an algorithm for determining the winding number. Then we examine how to accelerate this algorithm and how to handle special cases. Furthermore we compare these algorithms with those found in literature and discuss the results. {\textcopyright} 2001 Elsevier Science B.V.},
author = {Hormann, Kai and Agathos, Alexander},
doi = {10.1016/S0925-7721(01)00012-8},
file = {:home/tim/Nextcloud/Documents/Papers/2001 - Hormann, Agathos - The point in polygon problem for arbitrary polygons.pdf:pdf},
issn = {09257721},
journal = {Computational Geometry: Theory and Applications},
keywords = {Computational geometry,Integer algorithms,Point containment,Polygons,Winding number},
number = {3},
pages = {131--144},
title = {{The point in polygon problem for arbitrary polygons}},
url = {https://www.doi.org/10.1016/S0925-7721(01)00012-8},
volume = {20},
year = {2001}
}
@article{Holmlid2010,
abstract = {Volumetric data is a convenient representation of shape on many occasions. One application area is remeshing, where a poorly trian- gulated model is converted to a volumetric representation and then transformed back into a model of better triangle quality. In certain areas, for example medical scans, volumetric data arise naturally. To render the shapes captured by such a scan, a common approach is to convert the volumetric data into a triangle mesh. Since both types of representations are valuable, it is interesting to find reliable and effi- cient ways of converting between them. Here, we will exclusively look at the conversion from volume data to triangle mesh. Many methods exist for performing such an operation, where one of the most popu- lar is Marching Cubes. The meshes resulting from this algorithm will however have properties often undesirable. Furthermore, the method is not applicable when storing the volumetric data in an adaptive structure, such as an octree. In this report, we will describe how meshes without these undesirable properties can be generated from an octree in a straightforward manner.},
author = {Holmlid, Elias},
file = {:home/tim/Nextcloud/Documents/Papers//2010 - Holmlid - Manifold Contouring of an Adaptively Sampled Distance Field.pdf:pdf;:home/tim/Nextcloud/Documents/Papers//2010 - Holmlid - Manifold Contouring of an Adaptively Sampled Distance Field.pdf:pdf},
journal = {Chalmers Publication Library},
number = {June},
pages = {37},
title = {{Manifold Contouring of an Adaptively Sampled Distance Field}},
url = {http://publications.lib.chalmers.se/records/fulltext/123811.pdf},
year = {2010}
}
@article{Bottasso2002,
abstract = {We develop a methodology for introducing regions of high anisotropy in existing isotropic unstructured grids in complex, curved, three-dimensional domains. The new procedures are here applied to the capturing of solution features in the proximity of model boundaries (e.g. boundary layers). Suitable voids are created in an existing grid in the regions of localization using a mesh motion algorithm that solves a fictitious elasticity problem. The voids are then filled with stacks of prisms that are subsequently tetrahedronized to yield a simplicial mesh. The mesh motion algorithm allows us to deal in a simple and effective manner with the problem of self-intersection of elements in concave regions of the model boundaries, and in the case of closely spaced model faces, avoiding the need for cross-over checks and complex grid correction procedures. The capabilities and performance of the proposed methodology are illustrated with the help of practical examples.},
author = {Bottasso, C. L. and Detomi, D.},
doi = {10.1007/s003660200006},
file = {:home/tim/Nextcloud/Documents/Papers/2002 - Bottasso, Detomi - A procedure for tetrahedral boundary layer mesh generation.pdf:pdf},
issn = {01770667},
journal = {Engineering with Computers},
keywords = {Anisotropic grids,Boundary layers,Directional refinement,Spring analogy,Tetrahedral mesh,Unstructured grids},
number = {1},
pages = {66--79},
title = {{A procedure for tetrahedral boundary layer mesh generation}},
volume = {18},
year = {2002}
}
@article{Sorensen1948,
abstract = {First publication of S{\{}/"o{\}}rensen index},
author = {Sorensen, T.},
issn = {0001-6926},
journal = {Det. Kong. Danske Vidensk, Selesk Biology Skr},
number = {1},
pages = {1--34},
title = {{A Method of Establishing Groups of Equal Amplitude in Plant Sociology Based on Similarity of Species Content}},
url = {https://cir.nii.ac.jp/crid/1571135649789292416},
volume = {5},
year = {1948}
}
@article{Watson1981,
abstract = {The Delaunay tessellation in n-dimensional space is a space-filling aggregate of n-simplices. These n-simplices are the dual forms of the vertices in the commonly used Voronoi tessellation. Several efforts have been made to simulate the 2-dimensional Voronoi tessellation on the computer. Additional problems occur for the 3 and higher dimensional implementations but some of these can be avoided by alternatively computing the dual Delaunay tessellation. An algorithm that finds the topological relationships in these tessellations is given.},
author = {Watson, David F.},
doi = {10.1093/comjnl/24.2.167},
file = {:home/tim/Nextcloud/Documents/Papers/1981 - Watson - Computing the n-dimensional Delaunau tessellation with application to Voronoi polytopes.pdf:pdf},
issn = {00104620},
journal = {Computer Journal},
number = {2},
pages = {167--172},
title = {{Computing the n-dimensional Delaunau tessellation with application to Voronoi polytopes}},
url = {https://academic.oup.com/comjnl/article/24/2/167/338200},
volume = {24},
year = {1981}
}
@article{Arun1987,
abstract = {Two point sets {\{} p{\textless}inf{\textgreater}i{\textless}/inf{\textgreater} {\}} and {\{} p'{\textless}inf{\textgreater}i{\textless}/inf{\textgreater} {\}}; i = 1, 2, {\ldots} {\ldots} {\ldots}, N are related by p'{\textless}inf{\textgreater}i{\textless}/inf{\textgreater}= Rp{\textless}inf{\textgreater}i{\textless}/inf{\textgreater}+ T + N{\textless}inf{\textgreater}i{\textless}/inf{\textgreater}, where R is a rotation matrix, T a translation vector, and N{\textless}inf{\textgreater}i{\textless}/inf{\textgreater} a noise vector. Given {\{}p{\textless}inf{\textgreater}i{\textless}/inf{\textgreater}{\}} and {\{} p'{\textless}inf{\textgreater}i{\textless}/inf{\textgreater} {\}}, we present an algorithm for finding the least-squares solution of R and T, which is based on the singular value decomposition (SVD) of a 3 × 3 matrix. This new algorithm is compared to two earlier algorithms with respect to computer time requirements. Copyright {\textcopyright} 1987 by The Institute of Electrical and Electronics Engineers. Inc.},
author = {Arun, K. S. and Huang, T. S. and Blostein, S. D.},
doi = {10.1109/TPAMI.1987.4767965},
file = {:home/tim/Nextcloud/Documents/Papers//1987 - Arun, Huang, Blostein - Least-Squares Fitting of Two 3-D Point Sets.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {,Computer vision,least-squares,motion estimation,quaternion,singular value decomposition},
number = {5},
pages = {698--700},
title = {{Least-Squares Fitting of Two 3-D Point Sets}},
volume = {PAMI-9},
year = {1987}
}
@article{Taubin1995,
abstract = {For a number of computational purposes, including visualization of scientific data and registration of multimodal medical data, smooth curves must be approximated by polygonal curves, and surfaces by polyhedral surfaces. An inherent problem of these approximation algorithms is that the resulting curves and surfaces appear faceted. Boundary-following and iso-surface construction algorithms are typical examples. To reduce the apparent faceting, smoothing methods are used. In this paper we introduce a new method for smoothing piece-wise linear shapes of arbitrary dimension and topology. This new method is in fact a linear low-pass filter that removes high curvature variations, and does not produce shrinkage. Its computational complexity is linear in the number of edges or faces of the shape, and the required storage is linear in the number of vertices.},
author = {Taubin, Gabriel},
doi = {10.1109/iccv.1995.466848},
file = {:home/tim/Nextcloud/Documents/Papers/1995 - Taubin - Curve and surface smoothing without shrinkage.pdf:pdf;:home/tim/Nextcloud/Documents/Papers/1995 - Taubin - Curve and surface smoothing without shrinkage.pdf:pdf},
isbn = {0818670428},
journal = {IEEE International Conference on Computer Vision},
keywords = {low-level processing,medical computer vision,sentation,shape and object repre-},
pages = {852--857},
title = {{Curve and surface smoothing without shrinkage}},
year = {1995}
}
