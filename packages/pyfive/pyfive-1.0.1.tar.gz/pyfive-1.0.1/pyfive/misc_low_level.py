""" Misc low-level representation of HDF5 objects. """

import struct
from math import log2
from collections import OrderedDict

from .core import _padded_size
from .core import _structure_size
from .core import _unpack_struct_from
from .core import _unpack_struct_from_file
from .core import _unpack_integer
from .core import InvalidHDF5File
from .core import UNDEFINED_ADDRESS
from .core import Reference
from math import prod
import numpy as np

# uncomment this and use as shown in the FractalHeap if I/O diagnostic is needed
#from .utilities import Interceptor


class SuperBlock(object):
    """
    HDF5 Superblock.
    """

    def __init__(self, fh, offset):
        """ initalize. """

        fh.seek(offset)
        version_hint = struct.unpack_from('<B', fh.read(9), 8)[0]
        fh.seek(offset)
        if version_hint == 0:
            contents = _unpack_struct_from_file(SUPERBLOCK_V0, fh)
            self._end_of_sblock = offset + SUPERBLOCK_V0_SIZE
        elif version_hint == 2 or version_hint == 3:
            contents = _unpack_struct_from_file(SUPERBLOCK_V2_V3, fh)
            self._end_of_sblock = offset + SUPERBLOCK_V2_V3_SIZE
        else:
            raise NotImplementedError(
                "unsupported superblock version: %i" % (version_hint))

        # verify contents
        if contents['format_signature'] != FORMAT_SIGNATURE:
            raise InvalidHDF5File('Incorrect file signature')
        if contents['offset_size'] != 8 or contents['length_size'] != 8:
            raise NotImplementedError('File uses none 64-bit addressing')
        self.version = contents['superblock_version']
        self._contents = contents
        self._root_symbol_table = None
        self._fh = fh

    @property
    def offset_to_dataobjects(self):
        """ The offset to the data objects collection for the superblock. """
        if self.version == 0:
            sym_table = SymbolTable(self._fh, self._end_of_sblock, root=True)
            self._root_symbol_table = sym_table
            return sym_table.group_offset
        elif self.version == 2 or self.version == 3:
            return self._contents['root_group_address']
        else:
            raise NotImplementedError


class Heap(object):
    """
    HDF5 local heap.
    """

    def __init__(self, fh, offset):
        """ initalize. """

        fh.seek(offset)
        local_heap = _unpack_struct_from_file(LOCAL_HEAP, fh)
        assert local_heap['signature'] == b'HEAP'
        assert local_heap['version'] == 0
        fh.seek(local_heap['address_of_data_segment'])
        heap_data = fh.read(local_heap['data_segment_size'])
        local_heap['heap_data'] = heap_data
        self._contents = local_heap
        self.data = heap_data

    def get_object_name(self, offset):
        """ Return the name of the object indicated by the given offset. """
        end = self.data.index(b'\x00', offset)
        return self.data[offset:end]


class SymbolTable(object):
    """
    HDF5 Symbol Table.
    """

    def __init__(self, fh, offset, root=False):
        """ initialize, root=True for the root group, False otherwise. """

        fh.seek(offset)
        if root:
            # The root symbol table has no Symbol table node header
            # and contains only a single entry
            node = OrderedDict([('symbols', 1)])
        else:
            node = _unpack_struct_from_file(SYMBOL_TABLE_NODE, fh)
            assert node['signature'] == b'SNOD'
        entries = [_unpack_struct_from_file(SYMBOL_TABLE_ENTRY, fh) for i in
                   range(node['symbols'])]
        if root:
            self.group_offset = entries[0]['object_header_address']
        self.entries = entries
        self._contents = node

    def assign_name(self, heap):
        """ Assign link names to all entries in the symbol table. """
        for entry in self.entries:
            offset = entry['link_name_offset']
            link_name = heap.get_object_name(offset).decode('utf-8')
            entry['link_name'] = link_name
        return

    def get_links(self, heap):
        """ Return a dictionary of links (dataset/group) and offsets. """
        links = {}
        for e in self.entries:
            if e['cache_type'] in [0,1]:
                links[e['link_name']] = e['object_header_address']
            elif e['cache_type'] == 2:
                offset = struct.unpack('<4I', e['scratch'])[0]
                links[e['link_name']] = heap.get_object_name(offset).decode('utf-8')
        return links


class GlobalHeap(object):
    """
    HDF5 Global Heap collection.
    """

    def __init__(self, fh, offset):

        fh.seek(offset)
        header = _unpack_struct_from_file(GLOBAL_HEAP_HEADER, fh)
        assert header['signature'] == b'GCOL'
        assert header['version'] == 1
        heap_data_size = header['collection_size'] - GLOBAL_HEAP_HEADER_SIZE
        heap_data = fh.read(heap_data_size)
        assert len(heap_data) == heap_data_size  # check for early end of file

        self.heap_data = heap_data
        self._header = header
        self._objects = None

    @property
    def objects(self):
        """ Dictionary of objects in the heap. """
        if self._objects is None:
            self._objects = OrderedDict()
            offset = 0
            while offset < len(self.heap_data):
                info = _unpack_struct_from(
                    GLOBAL_HEAP_OBJECT, self.heap_data, offset)
                if info['object_index'] == 0:
                    break
                offset += GLOBAL_HEAP_OBJECT_SIZE
                fmt = '<' + str(info['object_size']) + 's'
                obj_data = struct.unpack_from(fmt, self.heap_data, offset)[0]
                self._objects[info['object_index']] = obj_data
                offset += _padded_size(info['object_size'])
        return self._objects


class FractalHeap(object):
    """
    HDF5 Fractal Heap

    The fractal heap implements the doubling table structure with indirect and direct blocks. 
    Indirect blocks in the heap do not actually contain data for objects in the heap, 
    their “size” is abstract - they represent the indexing structure for locating the direct blocks 
    in the doubling table. Direct blocks contain the actual data for objects stored in the heap.
    They could be scattered all over the file unless the metadata is stored at the front by 
    carerful use of the HDF5 file creation properties.
    
    The fractal heap ID can refer to a “tiny”, “huge”, or “managed” object. 
    If it's tiny, the ID contains the actual data and the heap itself does not need to be read from. 
    If it's huge, the ID contains the address on disk of the data or a b-tree key that can be used to find this address. 
    If it's managed, then it contains the offset and length within the virtual fractal heap address space 
    (i.e. inside a direct block, possibly indexed by one or more indirect blocks). 

    Which direct and indirect blocks contains the data, and the offset within the direct 
    block can be calculated by using the various parameters and algorithms described 
    at the start of the fractal heap section. It is an array of blocks of increasing size 
    within a linear address space.

    Documentation lifted from the HDF5 file format documentation:

    The number of rows of blocks, nrows, in an indirect block is calculated by the following expression:

        nrows = (log2(iblock_size) - log2(<Starting Block Size>)) + 1 
        
    where block_size is the size of the block that the indirect block represents in the doubling table. 
    For example, to represent a block with block_size equals to 1024, and Starting Block Size equals to 256, 
    three rows are needed.

    The maximum number of rows of direct blocks, max_dblock_rows, in any indirect block of a fractal heap
    is given by the following expression:

        max_dblock_rows = (log2(<Maximum Direct Block Size>) - log2(<Starting Block Size>)) + 2

    Using the computed values for nrows and max_dblock_rows, along with the Width of the doubling table, 
    the number of direct and indirect block entries (K and N in the indirect block description, below) in 
    an indirect block can be computed:

        K = MIN(nrows, max_dblock_rows) * Table Width

    If nrows is less than or equal to max_dblock_rows, N is 0. Otherwise, N is simply computed:

        N = K - (max_dblock_rows * Table Width)

    The size of indirect blocks on disk is determined by the number of rows in the indirect block (computed above). 
    The size of direct blocks on disk is exactly the size of the block in the doubling table.    
    
    """

    def __init__(self, fh, offset):
        """ 
        Read the heap header and construct the linear block mapping 
        """
        #fh = Interceptor(fh)
        fh.seek(offset)
        header = _unpack_struct_from_file(FRACTAL_HEAP_HEADER, fh)
        assert header['signature'] == b'FRHP'
        assert header['version'] == 0

        if header['filter_info_size']:
            raise NotImplementedError

        if header["btree_address_huge_objects"] == UNDEFINED_ADDRESS:
            header["btree_address_huge_objects"] = None
        else:
            raise NotImplementedError

        if header["root_block_address"] == UNDEFINED_ADDRESS:
            header["root_block_address"] = None

        nbits = header["log2_maximum_heap_size"]
        block_offset_size = self._min_size_nbits(nbits)
        h = OrderedDict((
            ('signature', '4s'),
            ('version', 'B'),
            ('heap_header_adddress', 'Q'),
            ('block_offset', '{}s'.format(block_offset_size))
        ))
        self.indirect_block_header = h.copy()
        self.indirect_block_header_size = _structure_size(h)
        if (header["flags"] & 2) == 2:
            h['checksum'] = 'I'
        self.direct_block_header = h
        self.direct_block_header_size = _structure_size(h)

        maximum_dblock_size = header['maximum_direct_block_size']
        nbits = header['log2_maximum_heap_size']
        self._managed_object_offset_size = self._min_size_nbits(nbits)
        value = min(maximum_dblock_size, header['max_managed_object_size'])
        self._managed_object_length_size = self._min_size_integer(value)

        start_block_size = header['starting_block_size']
        table_width = header['table_width']
        if not start_block_size:
            raise NotImplementedError

        log2_maximum_dblock_size = int(log2(maximum_dblock_size))
        assert 2**log2_maximum_dblock_size == maximum_dblock_size
        log2_start_block_size = int(log2(start_block_size))
        assert 2**log2_start_block_size == start_block_size
        self._max_direct_nrows = log2_maximum_dblock_size - log2_start_block_size + 2

        log2_table_width = int(log2(table_width))
        assert 2**log2_table_width == table_width

        # TODO: double check this calculation, the HDF5 docs say the 
        # number of nblocks, nrows, in an indirect block is calculated by the following expression
        # nrows = (log2(iblock_size) - log2(<Starting Block Size>)) + 1
        # the question is, how is this used?

        self._indirect_nrows_sub = log2_table_width + log2_start_block_size - 1

        self.header = header
        self.nobjects = header["managed_object_count"] + header["huge_object_count"] + header["tiny_object_count"]

        managed = []
        # while iterating over direct and indirect blocks we keep track of the heap_offset
        # thus, we are able to map this later back to an offset into our managed heap buffer
        blocks = []
        buffer_offset = 0
        root_address = header["root_block_address"]
        if root_address:
            nrows = header["indirect_current_rows_count"]
            if nrows:
                # Address of root block points to an indirect block
                for data, heap_offset, block_size in self._iter_indirect_block(fh, root_address, nrows):
                    managed.append(data)
                    blocks.append((heap_offset, buffer_offset, block_size))
                    buffer_offset += len(data)
            else:
                # Address of root block points to a direct block
                data, heap_offset = self._read_direct_block(fh, root_address, start_block_size)
                managed.append(data)
                blocks.append((heap_offset, buffer_offset, start_block_size))
                buffer_offset += len(data)

        self.managed = b"".join(managed)
        self.blocks = blocks

    def _read_direct_block(self, fh, offset, block_size):
        """
        Read FHDB - direct block - from heap and return data and heap offset
        """
        fh.seek(offset)
        data = fh.read(block_size)
        header = _unpack_struct_from(self.direct_block_header, data)
        assert header["signature"] == b"FHDB"
        return data, int.from_bytes(header["block_offset"],
                                                byteorder="little", signed=False)

    def _heapid_to_buffer_offset(self, heapid_offset):
        """
        Get offset into flat managed buffer from heapid offset
        """
        for heap_offset, buffer_offset, block_size in self.blocks:
            if heap_offset <= heapid_offset < heap_offset + block_size:
                relative = heapid_offset - heap_offset
                return buffer_offset + relative

        raise KeyError("HeapID offset not inside any heap block")

    def get_data(self, heapid):
        firstbyte = heapid[0]
        reserved = firstbyte & 15  # bit 0-3
        idtype = (firstbyte >> 4) & 3  # bit 4-5
        version = firstbyte >> 6  # bit 6-7
        data_offset = 1
        # throws a flake8 wobbly for Python<3.10; match is Py3.10+ syntax
        match idtype:  # noqa
            case 0: # managed
                assert version == 0
                nbytes = self._managed_object_offset_size
                offset = _unpack_integer(nbytes, heapid, data_offset)
                data_offset += nbytes
                nbytes = self._managed_object_length_size
                size = _unpack_integer(nbytes, heapid, data_offset)

                # map heap_id offset to flat buffer offset
                offset = self._heapid_to_buffer_offset(offset)
                if offset < len(self.managed):
                    return self.managed[offset:offset + size]

                return None

            case 1: # tiny
                raise NotImplementedError
            case 2: # huge
                raise NotImplementedError
            case _:
                raise NotImplementedError


    def _min_size_integer(self, integer):
        """ Calculate the minimal required bytes to contain an integer. """
        return self._min_size_nbits(integer.bit_length())

    @staticmethod
    def _min_size_nbits(nbits):
        """ Calculate the minimal required bytes to contain a number of bits. """
        return nbits // 8 + min(nbits % 8, 1)

    def _read_integral(self, fh, nbytes):
        num = fh.read(nbytes)
        num = struct.unpack("{}s".format(nbytes))[0]
        return int.from_bytes(num, byteorder="little", signed=False)

    def _iter_indirect_block(self, fh, offset, nrows):
        fh.seek(offset)
        header = _unpack_struct_from_file(self.indirect_block_header, fh)
        assert header["signature"] == b"FHIB"
        header["block_offset"] = int.from_bytes(header["block_offset"], byteorder="little", signed=False)
        # todo: this isn't really clear how the number of ndirect is deduced
        # at least, we need to derive the correct number by iterating over below
        ndirect, nindirect = self._indirect_info(nrows)

        direct_blocks = list()
        for i in range(ndirect):
            address = struct.unpack('<Q', fh.read(8))[0]
            if address == UNDEFINED_ADDRESS:
                # if there is no valid address, we move on to the next
                continue
            block_size = self._calc_block_size(i)
            direct_blocks.append((address, block_size))

        indirect_blocks = list()
        for i in range(ndirect, ndirect+nindirect):
            address = struct.unpack('<Q', fh.read(8))[0]
            if address == UNDEFINED_ADDRESS:
                # same here, move on to the next address
                continue
            block_size = self._calc_block_size(i)
            nrows = self._iblock_nrows_from_block_size(block_size)
            indirect_blocks.append((address, block_size, nrows))

        for address, block_size in direct_blocks:
            obj, heap_offset = self._read_direct_block(fh, address, block_size)
            yield obj, heap_offset, block_size

        for address, block_size, nrows in indirect_blocks:
            for obj, heap_offset, _block_size in self._iter_indirect_block(fh, address, nrows):
                yield obj, heap_offset, _block_size

    def _calc_block_size(self, iblock):
        row = iblock//self.header["table_width"]
        return 2**max(row-1, 0) * self.header['starting_block_size']

    def _iblock_nrows_from_block_size(self, block_size):
        log2_block_size = int(log2(block_size))
        assert 2**log2_block_size == block_size
        return log2_block_size - self._indirect_nrows_sub

    def _indirect_info(self, nrows):
        table_width = self.header['table_width']
        nobjects = nrows * table_width
        ndirect_max = self._max_direct_nrows * table_width
        # this info cannot tell the precise amount of blocks
        # it can just tell us the maximum possible amount we should parse
        if nobjects <= ndirect_max:
            ndirect = nobjects
            nindirect = 0
        else:
            ndirect = ndirect_max
            nindirect = nobjects - ndirect_max
        return ndirect, nindirect

def get_vlen_string_data_contiguous(
        fh, data_offset, global_heaps, shape, dtype, fillvalue
):
    """ Return the data for a variable which is made up of variable length string data """
    # we need to import this from DatasetID, and that's imported from Dataobjects hence
    # hiding it here in misc_low_level.
    if fillvalue in [0, None]:
        fillvalue = b""

    fh.seek(data_offset)
    count = prod(shape)

    # create with fillvalue
    value = np.full(count, fillvalue, dtype=object)
    offset = 0
    buf = fh.read(16*count)
    for i in range(count):
        # vlen_size, = struct.unpack_from('<I', buf, offset=offset)
        gheap_id = _unpack_struct_from(GLOBAL_HEAP_ID, buf, offset+4)
        gheap_address = gheap_id['collection_address']
        # only work on valid global heap addresses
        if gheap_address != 0:
            #print('Collection address for data', gheap_address)
            if gheap_address not in global_heaps:
                # load the global heap and cache the instance
                gheap = GlobalHeap(fh, gheap_address)
                global_heaps[gheap_address] = gheap
            gheap = global_heaps[gheap_address]

            # skip if NULL vlen entry
            if (obj_index:=gheap_id['object_index']) != 0:
                value[i] = gheap.objects[obj_index]

        offset +=16

    # If character_set == 0 ascii character set, return as
    # bytes. Otherwise return as UTF-8.
    if dtype.character_set:
        value = _convert_to_utf8_string_objects(value)

    return value

def get_vlen_string_data_from_chunk(
        fh, data_offset, global_heaps, shape, dtype
):
    """Return the data for a data chunk which is made up of variable
length string data.

    """
    # we need to import this from DatasetID, and that's imported from
    # Dataobjects hence hiding it here in misc_low_level.
    fh.seek(data_offset)
    count = prod(shape)

    value = np.empty(count, dtype=object)
    offset = 0
    buf = fh.read(16*count)
    for i in range(count):
        gheap_id = _unpack_struct_from(GLOBAL_HEAP_ID, buf, offset + 4)
        gheap_address = gheap_id['collection_address']
        if gheap_address not in global_heaps:
            gheap = GlobalHeap(fh, gheap_address)
            global_heaps[gheap_address] = gheap

        gheap = global_heaps[gheap_address]
        value[i] = gheap.objects[gheap_id['object_index']]
        offset +=16

    # If character_set == 0 ascii character set, return as
    # bytes. Otherwise return as UTF-8.
    if dtype.character_set:
        value = _convert_to_utf8_string_objects(value)

    return value


def _convert_to_utf8_string_objects(array):
    """Convert an numpy array of byte string objects to an array of UTF-8
    string objects.

    """
    decode = np.vectorize(lambda x: x.decode('utf-8'))
    array = decode(array)
    array = array.astype('O')
    return array


def dtype_replace_refs_with_object(dtype):
    """
    Recursively build a new dtype from `dtype` where all REFERENCE fields
    (metadata['h5_class']=='REFERENCE') are replaced with object dtype.
    """
    # atomic type
    if dtype.fields is None:
        meta = dtype.metadata or {}
        if meta.get("h5py_class") == "REFERENCE":
            return np.dtype(object, metadata=dict(meta))
        return dtype

    # compound type (recursive)
    fields = []
    for name, (subdtype, offset) in dtype.fields.items():
        new_subdtype = dtype_replace_refs_with_object(subdtype)
        fields.append((name, new_subdtype))
    return np.dtype(fields)


def _decode_array(arr, decoded):
    # todo: check for other types
    # currently only compound
    for name, (subdtype, offset) in arr.dtype.fields.items():
        field_data = arr[name]
        meta = subdtype.metadata or {}

        if subdtype.fields is not None:
            decoded[name] = _decode_array(field_data, decoded[name])
            continue

        if meta.get("h5py_class") == "REFERENCE":
            ids = field_data.view("<u8")
            decoded[name] = np.frompyfunc(Reference, 1, 1)(ids)
            continue

    return decoded


FORMAT_SIGNATURE = b'\211HDF\r\n\032\n'

# Version 0 SUPERBLOCK
SUPERBLOCK_V0 = OrderedDict((
    ('format_signature', '8s'),

    ('superblock_version', 'B'),
    ('free_storage_version', 'B'),
    ('root_group_version', 'B'),
    ('reserved_0', 'B'),

    ('shared_header_version', 'B'),
    ('offset_size', 'B'),            # assume 8
    ('length_size', 'B'),            # assume 8
    ('reserved_1', 'B'),

    ('group_leaf_node_k', 'H'),
    ('group_internal_node_k', 'H'),

    ('file_consistency_flags', 'L'),

    ('base_address', 'Q'),                  # assume 8 byte addressing
    ('free_space_address', 'Q'),            # assume 8 byte addressing
    ('end_of_file_address', 'Q'),           # assume 8 byte addressing
    ('driver_information_address', 'Q'),    # assume 8 byte addressing

))
SUPERBLOCK_V0_SIZE = _structure_size(SUPERBLOCK_V0)

# Version 2 and 3 SUPERBLOCK
SUPERBLOCK_V2_V3 = OrderedDict((
    ('format_signature', '8s'),

    ('superblock_version', 'B'),
    ('offset_size', 'B'),
    ('length_size', 'B'),
    ('file_consistency_flags', 'B'),

    ('base_address', 'Q'),                  # assume 8 byte addressing
    ('superblock_extension_address', 'Q'),  # assume 8 byte addressing
    ('end_of_file_address', 'Q'),           # assume 8 byte addressing
    ('root_group_address', 'Q'),            # assume 8 byte addressing

    ('superblock_checksum', 'I'),

))
SUPERBLOCK_V2_V3_SIZE = _structure_size(SUPERBLOCK_V2_V3)


SYMBOL_TABLE_NODE = OrderedDict((
    ('signature', '4s'),
    ('version', 'B'),
    ('reserved_0', 'B'),
    ('symbols', 'H'),
))

SYMBOL_TABLE_ENTRY = OrderedDict((
    ('link_name_offset', 'Q'),     # 8 byte address
    ('object_header_address', 'Q'),
    ('cache_type', 'I'),
    ('reserved', 'I'),
    ('scratch', '16s'),
))

GLOBAL_HEAP_ID = OrderedDict((
    ('collection_address', 'Q'),  # 8 byte addressing
    ('object_index', 'I'),
))

# III.D Disk Format: Level 1D - Local Heaps
LOCAL_HEAP = OrderedDict((
    ('signature', '4s'),
    ('version', 'B'),
    ('reserved', '3s'),
    ('data_segment_size', 'Q'),         # 8 byte size of lengths
    ('offset_to_free_list', 'Q'),       # 8 bytes size of lengths
    ('address_of_data_segment', 'Q'),   # 8 byte addressing
))

# III.E Disk Format: Level 1E - Global Heap
GLOBAL_HEAP_HEADER = OrderedDict((
    ('signature', '4s'),
    ('version', 'B'),
    ('reserved', '3s'),
    ('collection_size', 'Q'),
))
GLOBAL_HEAP_HEADER_SIZE = _structure_size(GLOBAL_HEAP_HEADER)

GLOBAL_HEAP_OBJECT = OrderedDict((
    ('object_index', 'H'),
    ('reference_count', 'H'),
    ('reserved', 'I'),
    ('object_size', 'Q')    # 8 byte addressing
))
GLOBAL_HEAP_OBJECT_SIZE = _structure_size(GLOBAL_HEAP_OBJECT)

# III.G. Disk Format: Level 1G - Fractal Heap
FRACTAL_HEAP_HEADER = OrderedDict((
    ('signature', '4s'),
    ('version', 'B'),

    ('object_index_size', 'H'),
    ('filter_info_size', 'H'),
    ('flags', 'B'),

    ('max_managed_object_size', 'I'),
    ('next_huge_object_index', 'Q'),       # 8 byte addressing
    ('btree_address_huge_objects', 'Q'),   # 8 byte addressing

    ('managed_freespace_size', 'Q'),       # 8 byte addressing
    ('freespace_manager_address', 'Q'),    # 8 byte addressing
    ('managed_space_size', 'Q'),           # 8 byte addressing; this is the upper bound in the heaps linear address space
    ('managed_alloc_size', 'Q'),           # 8 byte addressing; this is how much of that that is currently allocated to the heap.
    ('next_directblock_iterator_address', 'Q'), # 8 byte addressing

    ('managed_object_count', 'Q'),         # 8 byte addressing
    ('huge_objects_total_size', 'Q'),      # 8 byte addressing
    ('huge_object_count', 'Q'),            # 8 byte addressing
    ('tiny_objects_total_size', 'Q'),      # 8 byte addressing
    ('tiny_object_count', 'Q'),            # 8 byte addressing

    ('table_width', 'H'),
    ('starting_block_size', 'Q'),          # 8 byte addressing
    ('maximum_direct_block_size', 'Q'),    # 8 byte addressing
    ('log2_maximum_heap_size', 'H'),
    ('indirect_starting_rows_count', 'H'),
    ('root_block_address', 'Q'),           # 8 byte addressing
    ('indirect_current_rows_count', 'H'),
))
