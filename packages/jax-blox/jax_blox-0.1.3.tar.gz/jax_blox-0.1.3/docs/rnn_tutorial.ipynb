{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a619b039",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "# Recurrent Neural Networks: From Sine Waves to Shakespeare\n",
    "\n",
    "This tutorial explores Recurrent Neural Networks (RNNs) using **blox** and **Distrax**.\n",
    "\n",
    "We will cover two probabilistic modeling tasks:\n",
    "1.  **Regression**: Modeling a noisy sine wave using a Gaussian distribution.\n",
    "2.  **Generation**: Modeling character sequences (Shakespeare) using a Categorical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47813dd5",
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "!pip install -q jax-blox optax matplotlib distrax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843897df",
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import blox as bx\n",
    "import distrax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c840ea0d",
   "metadata": {},
   "source": [
    "## Part 1: Modeling a Sine Wave (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c4bb07",
   "metadata": {
    "id": "sine-data"
   },
   "outputs": [],
   "source": [
    "def generate_sine_data(batch_size, seq_len):\n",
    "  \"\"\"Generates sine waves with random phases.\"\"\"\n",
    "  # Random phases for each batch element.\n",
    "  phases = np.random.uniform(0, 2 * np.pi, size=(batch_size, 1))\n",
    "  time = np.linspace(0, 4 * np.pi, seq_len + 1)  # +1 for target\n",
    "\n",
    "  # Shape: (Batch, Time)\n",
    "  waves = np.sin(time + phases)\n",
    "\n",
    "  # Add channel dimension: (Batch, Time, 1)\n",
    "  waves = waves[..., None]\n",
    "\n",
    "  # Input is t, Target is t+1\n",
    "  inputs = waves[:, :-1]\n",
    "  targets = waves[:, 1:]\n",
    "  return inputs, targets\n",
    "\n",
    "\n",
    "x_viz, y_viz = generate_sine_data(1, 100)\n",
    "plt.plot(x_viz[0, :, 0], label='Input')\n",
    "plt.plot(y_viz[0, :, 0], label='Target (Shifted)')\n",
    "plt.legend()\n",
    "plt.title('Sample Sine Wave Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59009947",
   "metadata": {
    "id": "sine-model"
   },
   "outputs": [],
   "source": [
    "class SineRNN(bx.Module):\n",
    "  \"\"\"RNN that outputs parameters for a Gaussian distribution.\"\"\"\n",
    "\n",
    "  def __init__(self, graph: bx.Graph, hidden_dim: int = 32):\n",
    "    super().__().__init__(graph)\n",
    "    self.lstm = bx.LSTM(graph.child('lstm'), hidden_size=hidden_dim)\n",
    "    # Output 2 values: mean and log_std.\n",
    "    self.head = bx.Linear(graph.child('head'), output_size=2)\n",
    "\n",
    "  def apply(\n",
    "      self,\n",
    "      params: bx.Params,\n",
    "      x: jax.Array,\n",
    "      prev_state: bx.LSTMState | None = None,\n",
    "  ):\n",
    "    # x: [Batch, Time, 1]\n",
    "    (h, final_state), params = self.lstm.apply(params, x, prev_state)\n",
    "\n",
    "    # Project to Gaussian parameters.\n",
    "    out, params = self.head(params, h)\n",
    "    mu, log_scale = jnp.split(out, 2, axis=-1)\n",
    "\n",
    "    # Constrain scale to be positive.\n",
    "    scale = jax.nn.softplus(log_scale) + 1e-3\n",
    "    return (mu, scale), final_state, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea351006",
   "metadata": {
    "id": "train-sine"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step_sine(params, opt_state, x, y, optimizer):\n",
    "  trainable, non_trainable = params.split()\n",
    "\n",
    "  def loss_fn(t_params):\n",
    "    curr_params = t_params.merge(non_trainable)\n",
    "    (mu, scale), _, new_params = sine_model.apply(curr_params, x)\n",
    "\n",
    "    # Maximize Log Likelihood of the Gaussian.\n",
    "    dist = distrax.Normal(loc=mu, scale=scale)\n",
    "    nll = -dist.log_prob(y).mean()\n",
    "\n",
    "    _, new_non_trainable = new_params.split()\n",
    "    return nll, new_non_trainable\n",
    "\n",
    "  (loss, new_non_trainable), grads = jax.grad(loss_fn, has_aux=True)(trainable)\n",
    "  updates, new_opt_state = optimizer.update(grads, opt_state, trainable)\n",
    "  new_trainable = optax.apply_updates(trainable, updates)\n",
    "  return new_trainable.merge(new_non_trainable), new_opt_state, loss\n",
    "\n",
    "\n",
    "# Create model components.\n",
    "graph = bx.Graph('sine_rnn')\n",
    "sine_model = SineRNN(graph)\n",
    "rng = bx.Rng(graph.child('rng'), seed=42)\n",
    "\n",
    "# Initialize with sample data shape.\n",
    "sample_x, _ = generate_sine_data(batch_size=1, seq_len=50)\n",
    "sine_params = bx.Params(rng=rng)\n",
    "_, _, sine_params = sine_model.apply(sine_params, sample_x)\n",
    "sine_params = sine_params.finalized()\n",
    "\n",
    "# Train.\n",
    "optimizer = optax.adam(1e-2)\n",
    "opt_state = optimizer.init(sine_params.split()[0])\n",
    "\n",
    "losses = []\n",
    "for i in range(1000):\n",
    "  x, y = generate_sine_data(batch_size=32, seq_len=50)\n",
    "  sine_params, opt_state, loss = train_step_sine(\n",
    "      sine_params, opt_state, x, y, optimizer\n",
    "  )\n",
    "  losses.append(loss)\n",
    "  if i % 100 == 0:\n",
    "    print(f'Step {i}, NLL: {loss:.4f}')\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title('Sine Wave Training NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c7a9e9",
   "metadata": {
    "id": "viz-sine"
   },
   "outputs": [],
   "source": [
    "# Visualize predictions.\n",
    "x_test, y_test = generate_sine_data(1, 100)\n",
    "(mu, scale), _, _ = sine_model.apply(sine_params, x_test)\n",
    "\n",
    "t = np.arange(100)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(t, x_test[0, :, 0], 'k--', label='Input')\n",
    "plt.plot(t, mu[0, :, 0], 'b-', label='Prediction (Mean)')\n",
    "plt.fill_between(\n",
    "    t,\n",
    "    mu[0, :, 0] - 2 * scale[0, :, 0],\n",
    "    mu[0, :, 0] + 2 * scale[0, :, 0],\n",
    "    color='b',\n",
    "    alpha=0.2,\n",
    "    label='Uncertainty (2 std)',\n",
    ")\n",
    "plt.legend()\n",
    "plt.title('Sine Wave Prediction with Uncertainty')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83a1bdb",
   "metadata": {},
   "source": [
    "## Part 2: Tiny Shakespeare (Generation)\n",
    "\n",
    "Now we apply the same principles to character generation using a Categorical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4a2659",
   "metadata": {
    "id": "char-data"
   },
   "outputs": [],
   "source": [
    "def download_data():\n",
    "  url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "  if not os.path.exists('input.txt'):\n",
    "    data = requests.get(url).text\n",
    "    with open('input.txt', 'w') as f:\n",
    "      f.write(data)\n",
    "  else:\n",
    "    with open('input.txt', 'r') as f:\n",
    "      data = f.read()\n",
    "  return data\n",
    "\n",
    "\n",
    "text = download_data()\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "def encode(s):\n",
    "  return [stoi[c] for c in s]\n",
    "\n",
    "\n",
    "def decode(indices):\n",
    "  return ''.join([itos[i] for i in indices])\n",
    "\n",
    "\n",
    "data = jnp.array(encode(text), dtype=jnp.uint32)\n",
    "train_data = data[: int(0.9 * len(data))]\n",
    "\n",
    "\n",
    "def get_batch(batch_size=64, block_size=128):\n",
    "  ix = np.random.randint(0, len(train_data) - block_size, size=(batch_size,))\n",
    "  x = jnp.stack([train_data[i : i + block_size] for i in ix])\n",
    "  y = jnp.stack([train_data[i + 1 : i + block_size + 1] for i in ix])\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244d334d",
   "metadata": {
    "id": "char-model"
   },
   "outputs": [],
   "source": [
    "class CharRNN(bx.Module):\n",
    "\n",
    "  def __init__(\n",
    "      self, graph: bx.Graph, vocab_size: int, embed_dim: int, hidden_dim: int\n",
    "  ):\n",
    "    super().__().__init__(graph)\n",
    "    self.embed = bx.Embed(\n",
    "        graph.child('embed'),\n",
    "        num_embeddings=vocab_size,\n",
    "        embedding_size=embed_dim,\n",
    "    )\n",
    "    self.lstm = bx.LSTM(graph.child('lstm'), hidden_size=hidden_dim)\n",
    "    self.head = bx.Linear(graph.child('head'), output_size=vocab_size)\n",
    "\n",
    "  def __call__(self, params: bx.Params, x: jax.Array, prev_state: bx.LSTMState):\n",
    "    # Single step for generation.\n",
    "    x_emb, params = self.embed(params, x)\n",
    "    (x_hidden, new_state), params = self.lstm(params, x_emb, prev_state)\n",
    "    logits, params = self.head(params, x_hidden)\n",
    "    return logits, new_state, params\n",
    "\n",
    "  def apply(\n",
    "      self,\n",
    "      params: bx.Params,\n",
    "      x: jax.Array,\n",
    "      prev_state: bx.LSTMState | None = None,\n",
    "  ):\n",
    "    # Sequence processing for training.\n",
    "    x_emb, params = self.embed(params, x)\n",
    "    (x_seq, final_state), params = self.lstm.apply(params, x_emb, prev_state)\n",
    "    logits, params = self.head(params, x_seq)\n",
    "    return logits, final_state, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee1a769",
   "metadata": {
    "id": "train-char"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step_char(params, opt_state, x, y, optimizer):\n",
    "  trainable, non_trainable = params.split()\n",
    "\n",
    "  def loss_fn(t_params):\n",
    "    curr_params = t_params.merge(non_trainable)\n",
    "    logits, _, new_params = char_model.apply(curr_params, x)\n",
    "\n",
    "    # Categorical Log Likelihood.\n",
    "    dist = distrax.Categorical(logits=logits)\n",
    "    nll = -dist.log_prob(y).mean()\n",
    "\n",
    "    _, new_non = new_params.split()\n",
    "    return nll, new_non\n",
    "\n",
    "  (loss, new_non), grads = jax.grad(loss_fn, has_aux=True)(trainable)\n",
    "  updates, new_opt = optimizer.update(grads, opt_state, trainable)\n",
    "  return optax.apply_updates(trainable, updates).merge(new_non), new_opt, loss\n",
    "\n",
    "\n",
    "# Create model components.\n",
    "graph = bx.Graph('char_rnn')\n",
    "char_model = CharRNN(graph, vocab_size=vocab_size, embed_dim=64, hidden_dim=256)\n",
    "char_rng = bx.Rng(graph.child('rng'), seed=42)\n",
    "\n",
    "# Initialize with sample data shape.\n",
    "sample_x, _ = get_batch(batch_size=1, block_size=128)\n",
    "char_params = bx.Params(rng=char_rng)\n",
    "_, _, char_params = char_model.apply(char_params, sample_x)\n",
    "char_params = char_params.finalized()\n",
    "\n",
    "# Train.\n",
    "optimizer = optax.adamw(3e-4)\n",
    "opt_state = optimizer.init(char_params.split()[0])\n",
    "\n",
    "losses = []\n",
    "print('Training CharRNN...')\n",
    "for step in range(1000):\n",
    "  x, y = get_batch()\n",
    "  char_params, opt_state, loss = train_step_char(\n",
    "      char_params, opt_state, x, y, optimizer\n",
    "  )\n",
    "  losses.append(loss)\n",
    "  if step % 100 == 0:\n",
    "    print(f'Step {step}, NLL: {loss:.4f}')\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title('CharRNN NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d021eaa3",
   "metadata": {
    "id": "gen-char"
   },
   "outputs": [],
   "source": [
    "def generate(params, start_str, length=200, temperature=1.0):\n",
    "  context = jnp.array([encode(start_str)], dtype=jnp.int32)\n",
    "  state, params = char_model.lstm.initial_state(params, context)\n",
    "\n",
    "  # Warmup\n",
    "  logits_seq, state, params = char_model.apply(params, context, state)\n",
    "  next_logits = logits_seq[:, -1, :]\n",
    "\n",
    "  generated = []\n",
    "  for _ in range(length):\n",
    "    key, params = params.next_key()\n",
    "    dist = distrax.Categorical(logits=next_logits / temperature)\n",
    "    next_token = dist.sample(seed=key)\n",
    "    generated.append(int(next_token[0]))\n",
    "\n",
    "    next_logits, state, params = char_model(params, next_token, state)\n",
    "\n",
    "  return start_str + decode(generated)\n",
    "\n",
    "\n",
    "print(generate(char_params, 'ROMEO: '))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
