"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from daydream.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from daydream.utils import (
    FieldMetadata,
    PathParamMetadata,
    RequestMetadata,
    get_discriminator,
    validate_const,
)
import pydantic
from pydantic import Discriminator, Tag, model_serializer
from pydantic.functional_validators import AfterValidator
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


UpdateStreamPipelineRequest = Literal["streamdiffusion",]


UpdateStreamModelIDRequestEnum = Literal[
    "Lykon/dreamshaper-8",
    "prompthero/openjourney-v4",
]
r"""Model to use for generation"""


UpdateStreamPromptRequest6TypedDict = TypeAliasType(
    "UpdateStreamPromptRequest6TypedDict", Union[str, float]
)


UpdateStreamPromptRequest6 = TypeAliasType(
    "UpdateStreamPromptRequest6", Union[str, float]
)


UpdateStreamPromptRequest5TypedDict = TypeAliasType(
    "UpdateStreamPromptRequest5TypedDict",
    Union[str, List[List[UpdateStreamPromptRequest6TypedDict]]],
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


UpdateStreamPromptRequest5 = TypeAliasType(
    "UpdateStreamPromptRequest5", Union[str, List[List[UpdateStreamPromptRequest6]]]
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


UpdateStreamPromptInterpolationMethodRequest3 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""


class UpdateStreamLoraDictRequest3TypedDict(TypedDict):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


class UpdateStreamLoraDictRequest3(BaseModel):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


UpdateStreamAccelerationRequest3 = Literal[
    "none",
    "xformers",
    "tensorrt",
]
r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""


UpdateStreamSeedRequest6TypedDict = TypeAliasType(
    "UpdateStreamSeedRequest6TypedDict", Union[int, float]
)


UpdateStreamSeedRequest6 = TypeAliasType("UpdateStreamSeedRequest6", Union[int, float])


UpdateStreamSeedRequest5TypedDict = TypeAliasType(
    "UpdateStreamSeedRequest5TypedDict",
    Union[int, List[List[UpdateStreamSeedRequest6TypedDict]]],
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


UpdateStreamSeedRequest5 = TypeAliasType(
    "UpdateStreamSeedRequest5", Union[int, List[List[UpdateStreamSeedRequest6]]]
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


UpdateStreamSeedInterpolationMethodRequest3 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""


UpdateStreamImagePreprocessingTypeRequest3 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class UpdateStreamImagePreprocessingProcessorRequest3TypedDict(TypedDict):
    type: UpdateStreamImagePreprocessingTypeRequest3
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamImagePreprocessingProcessorRequest3(BaseModel):
    type: UpdateStreamImagePreprocessingTypeRequest3

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamImagePreprocessingRequest3TypedDict(TypedDict):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[UpdateStreamImagePreprocessingProcessorRequest3TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamImagePreprocessingRequest3(BaseModel):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[UpdateStreamImagePreprocessingProcessorRequest3]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamImagePostprocessingTypeRequest3 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class UpdateStreamImagePostprocessingProcessorRequest3TypedDict(TypedDict):
    type: UpdateStreamImagePostprocessingTypeRequest3
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamImagePostprocessingProcessorRequest3(BaseModel):
    type: UpdateStreamImagePostprocessingTypeRequest3

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamImagePostprocessingRequest3TypedDict(TypedDict):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[UpdateStreamImagePostprocessingProcessorRequest3TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamImagePostprocessingRequest3(BaseModel):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[UpdateStreamImagePostprocessingProcessorRequest3]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamLatentPreprocessingTypeRequest3 = Literal["latent_feedback",]


class UpdateStreamLatentPreprocessingProcessorRequest3TypedDict(TypedDict):
    type: UpdateStreamLatentPreprocessingTypeRequest3
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamLatentPreprocessingProcessorRequest3(BaseModel):
    type: UpdateStreamLatentPreprocessingTypeRequest3

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamLatentPreprocessingRequest3TypedDict(TypedDict):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPreprocessingProcessorRequest3TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamLatentPreprocessingRequest3(BaseModel):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPreprocessingProcessorRequest3]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamLatentPostprocessingTypeRequest3 = Literal["latent_feedback",]


class UpdateStreamLatentPostprocessingProcessorRequest3TypedDict(TypedDict):
    type: UpdateStreamLatentPostprocessingTypeRequest3
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamLatentPostprocessingProcessorRequest3(BaseModel):
    type: UpdateStreamLatentPostprocessingTypeRequest3

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamLatentPostprocessingRequest3TypedDict(TypedDict):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPostprocessingProcessorRequest3TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamLatentPostprocessingRequest3(BaseModel):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPostprocessingProcessorRequest3]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamControlnetModelIDRequest3 = Literal[
    "lllyasviel/control_v11f1p_sd15_depth",
    "lllyasviel/control_v11f1e_sd15_tile",
    "lllyasviel/control_v11p_sd15_canny",
    "daydreamlive/TemporalNet2-stable-diffusion-v1-5",
]
r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""


UpdateStreamPreprocessorRequest3 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]
r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""


class UpdateStreamControlnetRequest3TypedDict(TypedDict):
    model_id: UpdateStreamControlnetModelIDRequest3
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""
    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""
    preprocessor: UpdateStreamPreprocessorRequest3
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""
    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""
    preprocessor_params: NotRequired[Dict[str, Any]]
    r"""Preprocessor parameters"""
    control_guidance_start: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""
    control_guidance_end: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


class UpdateStreamControlnetRequest3(BaseModel):
    model_id: UpdateStreamControlnetModelIDRequest3
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""

    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""

    preprocessor: UpdateStreamPreprocessorRequest3
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""

    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""

    preprocessor_params: Optional[Dict[str, Any]] = None
    r"""Preprocessor parameters"""

    control_guidance_start: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""

    control_guidance_end: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


UpdateStreamTypeRegularRequest = Literal["regular",]
r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""


UpdateStreamWeightTypeRegularRequest = Literal[
    "linear",
    "ease in",
    "ease out",
    "ease in-out",
    "reverse in-out",
    "weak input",
    "weak output",
    "weak middle",
    "strong middle",
    "style transfer",
    "composition",
    "strong style transfer",
    "style and composition",
    "style transfer precise",
    "composition precise",
]
r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class UpdateStreamIPAdapterRegularRequestTypedDict(TypedDict):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    type: NotRequired[UpdateStreamTypeRegularRequest]
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""
    enabled: NotRequired[bool]
    r"""Whether IP adapter is enabled"""
    scale: NotRequired[float]
    r"""Strength of IP adapter style conditioning"""
    weight_type: NotRequired[UpdateStreamWeightTypeRegularRequest]
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class UpdateStreamIPAdapterRegularRequest(BaseModel):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    type: Optional[UpdateStreamTypeRegularRequest] = "regular"
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""

    enabled: Optional[bool] = None
    r"""Whether IP adapter is enabled"""

    scale: Optional[float] = None
    r"""Strength of IP adapter style conditioning"""

    weight_type: Optional[UpdateStreamWeightTypeRegularRequest] = None
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class UpdateStreamCachedAttentionRequest2TypedDict(TypedDict):
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    enabled: bool
    r"""Whether this cached attention is active. Enable cached attention to reuse key/value tensors across frames."""
    max_frames: int
    r"""Number of frames retained in the attention cache. Number of historical K/V frames to retain. Limited by TensorRT engine exports."""
    interval: int
    r"""Cadence (number of frames) for refreshing cached key/value tensors i.e How often (in number of frames) to refresh the cache.
    It is now **frame-based** (not seconds). It accepts integers 1-240, representing how many frames elapse between cache refreshes.

    """


class UpdateStreamCachedAttentionRequest2(BaseModel):
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    enabled: bool
    r"""Whether this cached attention is active. Enable cached attention to reuse key/value tensors across frames."""

    max_frames: int
    r"""Number of frames retained in the attention cache. Number of historical K/V frames to retain. Limited by TensorRT engine exports."""

    interval: int
    r"""Cadence (number of frames) for refreshing cached key/value tensors i.e How often (in number of frames) to refresh the cache.
    It is now **frame-based** (not seconds). It accepts integers 1-240, representing how many frames elapse between cache refreshes.

    """


class UpdateStreamSd15RequestTypedDict(TypedDict):
    model_id: UpdateStreamModelIDRequestEnum
    r"""Model to use for generation"""
    prompt: NotRequired[UpdateStreamPromptRequest5TypedDict]
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""
    prompt_interpolation_method: NotRequired[
        UpdateStreamPromptInterpolationMethodRequest3
    ]
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""
    normalize_prompt_weights: NotRequired[bool]
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""
    normalize_seed_weights: NotRequired[bool]
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""
    negative_prompt: NotRequired[str]
    r"""Text describing what to avoid in the generated image."""
    guidance_scale: NotRequired[float]
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""
    delta: NotRequired[float]
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""
    num_inference_steps: NotRequired[int]
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""
    t_index_list: NotRequired[List[int]]
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """
    use_safety_checker: NotRequired[bool]
    r"""Whether to use safety checker for content filtering"""
    width: NotRequired[int]
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""
    height: NotRequired[int]
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""
    lora_dict: NotRequired[Nullable[UpdateStreamLoraDictRequest3TypedDict]]
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""
    use_lcm_lora: NotRequired[bool]
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""
    lcm_lora_id: NotRequired[str]
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """
    acceleration: NotRequired[UpdateStreamAccelerationRequest3]
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""
    use_denoising_batch: NotRequired[bool]
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""
    do_add_noise: NotRequired[bool]
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""
    seed: NotRequired[UpdateStreamSeedRequest5TypedDict]
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""
    seed_interpolation_method: NotRequired[UpdateStreamSeedInterpolationMethodRequest3]
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""
    enable_similar_image_filter: NotRequired[bool]
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""
    similar_image_filter_threshold: NotRequired[float]
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""
    similar_image_filter_max_skip_frame: NotRequired[int]
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""
    skip_diffusion: NotRequired[bool]
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """
    image_preprocessing: NotRequired[UpdateStreamImagePreprocessingRequest3TypedDict]
    r"""List of image preprocessor configurations for image processing"""
    image_postprocessing: NotRequired[UpdateStreamImagePostprocessingRequest3TypedDict]
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """
    latent_preprocessing: NotRequired[UpdateStreamLatentPreprocessingRequest3TypedDict]
    r"""List of latent preprocessor configurations for latent processing"""
    latent_postprocessing: NotRequired[
        UpdateStreamLatentPostprocessingRequest3TypedDict
    ]
    r"""List of latent postprocessor configurations for latent processing"""
    controlnets: NotRequired[List[UpdateStreamControlnetRequest3TypedDict]]
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""
    ip_adapter: NotRequired[UpdateStreamIPAdapterRegularRequestTypedDict]
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""
    ip_adapter_style_image_url: NotRequired[str]
    r"""HTTPS URL or base64-encoded data URI (data:image/[type];base64,...) of the style image to use. Base64 images are automatically cached and uploaded to storage with content-based deduplication. Maximum size: 5MB. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""
    cached_attention: NotRequired[UpdateStreamCachedAttentionRequest2TypedDict]
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""


class UpdateStreamSd15Request(BaseModel):
    model_id: UpdateStreamModelIDRequestEnum
    r"""Model to use for generation"""

    prompt: Optional[UpdateStreamPromptRequest5] = None
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""

    prompt_interpolation_method: Optional[
        UpdateStreamPromptInterpolationMethodRequest3
    ] = None
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""

    normalize_prompt_weights: Optional[bool] = None
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""

    normalize_seed_weights: Optional[bool] = None
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""

    negative_prompt: Optional[str] = None
    r"""Text describing what to avoid in the generated image."""

    guidance_scale: Optional[float] = None
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""

    delta: Optional[float] = None
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""

    num_inference_steps: Optional[int] = None
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""

    t_index_list: Optional[List[int]] = None
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """

    use_safety_checker: Optional[bool] = None
    r"""Whether to use safety checker for content filtering"""

    width: Optional[int] = None
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""

    height: Optional[int] = None
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""

    lora_dict: OptionalNullable[UpdateStreamLoraDictRequest3] = UNSET
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""

    use_lcm_lora: Optional[bool] = None
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""

    lcm_lora_id: Optional[str] = None
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """

    acceleration: Optional[UpdateStreamAccelerationRequest3] = None
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""

    use_denoising_batch: Optional[bool] = None
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""

    do_add_noise: Optional[bool] = None
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""

    seed: Optional[UpdateStreamSeedRequest5] = None
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""

    seed_interpolation_method: Optional[UpdateStreamSeedInterpolationMethodRequest3] = (
        None
    )
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""

    enable_similar_image_filter: Optional[bool] = None
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""

    similar_image_filter_threshold: Optional[float] = None
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""

    similar_image_filter_max_skip_frame: Optional[int] = None
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""

    skip_diffusion: Optional[bool] = None
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """

    image_preprocessing: Optional[UpdateStreamImagePreprocessingRequest3] = None
    r"""List of image preprocessor configurations for image processing"""

    image_postprocessing: Optional[UpdateStreamImagePostprocessingRequest3] = None
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    latent_preprocessing: Optional[UpdateStreamLatentPreprocessingRequest3] = None
    r"""List of latent preprocessor configurations for latent processing"""

    latent_postprocessing: Optional[UpdateStreamLatentPostprocessingRequest3] = None
    r"""List of latent postprocessor configurations for latent processing"""

    controlnets: Optional[List[UpdateStreamControlnetRequest3]] = None
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""

    ip_adapter: Optional[UpdateStreamIPAdapterRegularRequest] = None
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    ip_adapter_style_image_url: Optional[str] = None
    r"""HTTPS URL or base64-encoded data URI (data:image/[type];base64,...) of the style image to use. Base64 images are automatically cached and uploaded to storage with content-based deduplication. Maximum size: 5MB. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""

    cached_attention: Optional[UpdateStreamCachedAttentionRequest2] = None
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "prompt",
            "prompt_interpolation_method",
            "normalize_prompt_weights",
            "normalize_seed_weights",
            "negative_prompt",
            "guidance_scale",
            "delta",
            "num_inference_steps",
            "t_index_list",
            "use_safety_checker",
            "width",
            "height",
            "lora_dict",
            "use_lcm_lora",
            "lcm_lora_id",
            "acceleration",
            "use_denoising_batch",
            "do_add_noise",
            "seed",
            "seed_interpolation_method",
            "enable_similar_image_filter",
            "similar_image_filter_threshold",
            "similar_image_filter_max_skip_frame",
            "skip_diffusion",
            "image_preprocessing",
            "image_postprocessing",
            "latent_preprocessing",
            "latent_postprocessing",
            "controlnets",
            "ip_adapter",
            "ip_adapter_style_image_url",
            "cached_attention",
        ]
        nullable_fields = ["lora_dict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


UpdateStreamPromptRequest4TypedDict = TypeAliasType(
    "UpdateStreamPromptRequest4TypedDict", Union[str, float]
)


UpdateStreamPromptRequest4 = TypeAliasType(
    "UpdateStreamPromptRequest4", Union[str, float]
)


UpdateStreamPromptRequest3TypedDict = TypeAliasType(
    "UpdateStreamPromptRequest3TypedDict",
    Union[str, List[List[UpdateStreamPromptRequest4TypedDict]]],
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


UpdateStreamPromptRequest3 = TypeAliasType(
    "UpdateStreamPromptRequest3", Union[str, List[List[UpdateStreamPromptRequest4]]]
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


UpdateStreamPromptInterpolationMethodRequest2 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""


class UpdateStreamLoraDictRequest2TypedDict(TypedDict):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


class UpdateStreamLoraDictRequest2(BaseModel):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


UpdateStreamAccelerationRequest2 = Literal[
    "none",
    "xformers",
    "tensorrt",
]
r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""


UpdateStreamSeedRequest4TypedDict = TypeAliasType(
    "UpdateStreamSeedRequest4TypedDict", Union[int, float]
)


UpdateStreamSeedRequest4 = TypeAliasType("UpdateStreamSeedRequest4", Union[int, float])


UpdateStreamSeedRequest3TypedDict = TypeAliasType(
    "UpdateStreamSeedRequest3TypedDict",
    Union[int, List[List[UpdateStreamSeedRequest4TypedDict]]],
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


UpdateStreamSeedRequest3 = TypeAliasType(
    "UpdateStreamSeedRequest3", Union[int, List[List[UpdateStreamSeedRequest4]]]
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


UpdateStreamSeedInterpolationMethodRequest2 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""


UpdateStreamImagePreprocessingTypeRequest2 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class UpdateStreamImagePreprocessingProcessorRequest2TypedDict(TypedDict):
    type: UpdateStreamImagePreprocessingTypeRequest2
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamImagePreprocessingProcessorRequest2(BaseModel):
    type: UpdateStreamImagePreprocessingTypeRequest2

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamImagePreprocessingRequest2TypedDict(TypedDict):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[UpdateStreamImagePreprocessingProcessorRequest2TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamImagePreprocessingRequest2(BaseModel):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[UpdateStreamImagePreprocessingProcessorRequest2]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamImagePostprocessingTypeRequest2 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class UpdateStreamImagePostprocessingProcessorRequest2TypedDict(TypedDict):
    type: UpdateStreamImagePostprocessingTypeRequest2
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamImagePostprocessingProcessorRequest2(BaseModel):
    type: UpdateStreamImagePostprocessingTypeRequest2

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamImagePostprocessingRequest2TypedDict(TypedDict):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[UpdateStreamImagePostprocessingProcessorRequest2TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamImagePostprocessingRequest2(BaseModel):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[UpdateStreamImagePostprocessingProcessorRequest2]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamLatentPreprocessingTypeRequest2 = Literal["latent_feedback",]


class UpdateStreamLatentPreprocessingProcessorRequest2TypedDict(TypedDict):
    type: UpdateStreamLatentPreprocessingTypeRequest2
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamLatentPreprocessingProcessorRequest2(BaseModel):
    type: UpdateStreamLatentPreprocessingTypeRequest2

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamLatentPreprocessingRequest2TypedDict(TypedDict):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPreprocessingProcessorRequest2TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamLatentPreprocessingRequest2(BaseModel):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPreprocessingProcessorRequest2]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamLatentPostprocessingTypeRequest2 = Literal["latent_feedback",]


class UpdateStreamLatentPostprocessingProcessorRequest2TypedDict(TypedDict):
    type: UpdateStreamLatentPostprocessingTypeRequest2
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamLatentPostprocessingProcessorRequest2(BaseModel):
    type: UpdateStreamLatentPostprocessingTypeRequest2

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamLatentPostprocessingRequest2TypedDict(TypedDict):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPostprocessingProcessorRequest2TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamLatentPostprocessingRequest2(BaseModel):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPostprocessingProcessorRequest2]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamControlnetModelIDRequest2 = Literal[
    "xinsir/controlnet-depth-sdxl-1.0",
    "xinsir/controlnet-canny-sdxl-1.0",
    "xinsir/controlnet-tile-sdxl-1.0",
]
r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""


UpdateStreamPreprocessorRequest2 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]
r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""


class UpdateStreamControlnetRequest2TypedDict(TypedDict):
    model_id: UpdateStreamControlnetModelIDRequest2
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""
    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""
    preprocessor: UpdateStreamPreprocessorRequest2
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""
    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""
    preprocessor_params: NotRequired[Dict[str, Any]]
    r"""Preprocessor parameters"""
    control_guidance_start: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""
    control_guidance_end: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


class UpdateStreamControlnetRequest2(BaseModel):
    model_id: UpdateStreamControlnetModelIDRequest2
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""

    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""

    preprocessor: UpdateStreamPreprocessorRequest2
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""

    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""

    preprocessor_params: Optional[Dict[str, Any]] = None
    r"""Preprocessor parameters"""

    control_guidance_start: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""

    control_guidance_end: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


UpdateStreamIPAdapterTypeRequest = Literal[
    "regular",
    "faceid",
]
r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""


UpdateStreamIPAdapterWeightTypeRequest = Literal[
    "linear",
    "ease in",
    "ease out",
    "ease in-out",
    "reverse in-out",
    "weak input",
    "weak output",
    "weak middle",
    "strong middle",
    "style transfer",
    "composition",
    "strong style transfer",
    "style and composition",
    "style transfer precise",
    "composition precise",
]
r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class UpdateStreamIPAdapterRequestTypedDict(TypedDict):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    type: NotRequired[UpdateStreamIPAdapterTypeRequest]
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""
    enabled: NotRequired[bool]
    r"""Whether IP adapter is enabled"""
    scale: NotRequired[float]
    r"""Strength of IP adapter style conditioning"""
    weight_type: NotRequired[UpdateStreamIPAdapterWeightTypeRequest]
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class UpdateStreamIPAdapterRequest(BaseModel):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    type: Optional[UpdateStreamIPAdapterTypeRequest] = "regular"
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""

    enabled: Optional[bool] = None
    r"""Whether IP adapter is enabled"""

    scale: Optional[float] = None
    r"""Strength of IP adapter style conditioning"""

    weight_type: Optional[UpdateStreamIPAdapterWeightTypeRequest] = None
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class UpdateStreamCachedAttentionRequest1TypedDict(TypedDict):
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    enabled: bool
    r"""Whether this cached attention is active. Enable cached attention to reuse key/value tensors across frames."""
    max_frames: int
    r"""Number of frames retained in the attention cache. Number of historical K/V frames to retain. Limited by TensorRT engine exports."""
    interval: int
    r"""Cadence (number of frames) for refreshing cached key/value tensors i.e How often (in number of frames) to refresh the cache.
    It is now **frame-based** (not seconds). It accepts integers 1-240, representing how many frames elapse between cache refreshes.

    """


class UpdateStreamCachedAttentionRequest1(BaseModel):
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    enabled: bool
    r"""Whether this cached attention is active. Enable cached attention to reuse key/value tensors across frames."""

    max_frames: int
    r"""Number of frames retained in the attention cache. Number of historical K/V frames to retain. Limited by TensorRT engine exports."""

    interval: int
    r"""Cadence (number of frames) for refreshing cached key/value tensors i.e How often (in number of frames) to refresh the cache.
    It is now **frame-based** (not seconds). It accepts integers 1-240, representing how many frames elapse between cache refreshes.

    """


class UpdateStreamSDXLRequestTypedDict(TypedDict):
    model_id: Literal["stabilityai/sdxl-turbo"]
    prompt: NotRequired[UpdateStreamPromptRequest3TypedDict]
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""
    prompt_interpolation_method: NotRequired[
        UpdateStreamPromptInterpolationMethodRequest2
    ]
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""
    normalize_prompt_weights: NotRequired[bool]
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""
    normalize_seed_weights: NotRequired[bool]
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""
    negative_prompt: NotRequired[str]
    r"""Text describing what to avoid in the generated image."""
    guidance_scale: NotRequired[float]
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""
    delta: NotRequired[float]
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""
    num_inference_steps: NotRequired[int]
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""
    t_index_list: NotRequired[List[int]]
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """
    use_safety_checker: NotRequired[bool]
    r"""Whether to use safety checker for content filtering"""
    width: NotRequired[int]
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""
    height: NotRequired[int]
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""
    lora_dict: NotRequired[Nullable[UpdateStreamLoraDictRequest2TypedDict]]
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""
    use_lcm_lora: NotRequired[bool]
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""
    lcm_lora_id: NotRequired[str]
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """
    acceleration: NotRequired[UpdateStreamAccelerationRequest2]
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""
    use_denoising_batch: NotRequired[bool]
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""
    do_add_noise: NotRequired[bool]
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""
    seed: NotRequired[UpdateStreamSeedRequest3TypedDict]
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""
    seed_interpolation_method: NotRequired[UpdateStreamSeedInterpolationMethodRequest2]
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""
    enable_similar_image_filter: NotRequired[bool]
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""
    similar_image_filter_threshold: NotRequired[float]
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""
    similar_image_filter_max_skip_frame: NotRequired[int]
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""
    skip_diffusion: NotRequired[bool]
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """
    image_preprocessing: NotRequired[UpdateStreamImagePreprocessingRequest2TypedDict]
    r"""List of image preprocessor configurations for image processing"""
    image_postprocessing: NotRequired[UpdateStreamImagePostprocessingRequest2TypedDict]
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """
    latent_preprocessing: NotRequired[UpdateStreamLatentPreprocessingRequest2TypedDict]
    r"""List of latent preprocessor configurations for latent processing"""
    latent_postprocessing: NotRequired[
        UpdateStreamLatentPostprocessingRequest2TypedDict
    ]
    r"""List of latent postprocessor configurations for latent processing"""
    controlnets: NotRequired[List[UpdateStreamControlnetRequest2TypedDict]]
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""
    ip_adapter: NotRequired[UpdateStreamIPAdapterRequestTypedDict]
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""
    ip_adapter_style_image_url: NotRequired[str]
    r"""HTTPS URL or base64-encoded data URI (data:image/[type];base64,...) of the style image to use. Base64 images are automatically cached and uploaded to storage with content-based deduplication. Maximum size: 5MB. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""
    cached_attention: NotRequired[UpdateStreamCachedAttentionRequest1TypedDict]
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""


class UpdateStreamSDXLRequest(BaseModel):
    model_id: Annotated[
        Annotated[
            Literal["stabilityai/sdxl-turbo"],
            AfterValidator(validate_const("stabilityai/sdxl-turbo")),
        ],
        pydantic.Field(alias="model_id"),
    ] = "stabilityai/sdxl-turbo"

    prompt: Optional[UpdateStreamPromptRequest3] = None
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""

    prompt_interpolation_method: Optional[
        UpdateStreamPromptInterpolationMethodRequest2
    ] = None
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""

    normalize_prompt_weights: Optional[bool] = None
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""

    normalize_seed_weights: Optional[bool] = None
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""

    negative_prompt: Optional[str] = None
    r"""Text describing what to avoid in the generated image."""

    guidance_scale: Optional[float] = None
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""

    delta: Optional[float] = None
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""

    num_inference_steps: Optional[int] = None
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""

    t_index_list: Optional[List[int]] = None
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """

    use_safety_checker: Optional[bool] = None
    r"""Whether to use safety checker for content filtering"""

    width: Optional[int] = None
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""

    height: Optional[int] = None
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""

    lora_dict: OptionalNullable[UpdateStreamLoraDictRequest2] = UNSET
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""

    use_lcm_lora: Optional[bool] = None
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""

    lcm_lora_id: Optional[str] = None
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """

    acceleration: Optional[UpdateStreamAccelerationRequest2] = None
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""

    use_denoising_batch: Optional[bool] = None
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""

    do_add_noise: Optional[bool] = None
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""

    seed: Optional[UpdateStreamSeedRequest3] = None
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""

    seed_interpolation_method: Optional[UpdateStreamSeedInterpolationMethodRequest2] = (
        None
    )
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""

    enable_similar_image_filter: Optional[bool] = None
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""

    similar_image_filter_threshold: Optional[float] = None
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""

    similar_image_filter_max_skip_frame: Optional[int] = None
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""

    skip_diffusion: Optional[bool] = None
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """

    image_preprocessing: Optional[UpdateStreamImagePreprocessingRequest2] = None
    r"""List of image preprocessor configurations for image processing"""

    image_postprocessing: Optional[UpdateStreamImagePostprocessingRequest2] = None
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    latent_preprocessing: Optional[UpdateStreamLatentPreprocessingRequest2] = None
    r"""List of latent preprocessor configurations for latent processing"""

    latent_postprocessing: Optional[UpdateStreamLatentPostprocessingRequest2] = None
    r"""List of latent postprocessor configurations for latent processing"""

    controlnets: Optional[List[UpdateStreamControlnetRequest2]] = None
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""

    ip_adapter: Optional[UpdateStreamIPAdapterRequest] = None
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    ip_adapter_style_image_url: Optional[str] = None
    r"""HTTPS URL or base64-encoded data URI (data:image/[type];base64,...) of the style image to use. Base64 images are automatically cached and uploaded to storage with content-based deduplication. Maximum size: 5MB. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""

    cached_attention: Optional[UpdateStreamCachedAttentionRequest1] = None
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "prompt",
            "prompt_interpolation_method",
            "normalize_prompt_weights",
            "normalize_seed_weights",
            "negative_prompt",
            "guidance_scale",
            "delta",
            "num_inference_steps",
            "t_index_list",
            "use_safety_checker",
            "width",
            "height",
            "lora_dict",
            "use_lcm_lora",
            "lcm_lora_id",
            "acceleration",
            "use_denoising_batch",
            "do_add_noise",
            "seed",
            "seed_interpolation_method",
            "enable_similar_image_filter",
            "similar_image_filter_threshold",
            "similar_image_filter_max_skip_frame",
            "skip_diffusion",
            "image_preprocessing",
            "image_postprocessing",
            "latent_preprocessing",
            "latent_postprocessing",
            "controlnets",
            "ip_adapter",
            "ip_adapter_style_image_url",
            "cached_attention",
        ]
        nullable_fields = ["lora_dict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


UpdateStreamPromptRequest2TypedDict = TypeAliasType(
    "UpdateStreamPromptRequest2TypedDict", Union[str, float]
)


UpdateStreamPromptRequest2 = TypeAliasType(
    "UpdateStreamPromptRequest2", Union[str, float]
)


UpdateStreamPromptRequest1TypedDict = TypeAliasType(
    "UpdateStreamPromptRequest1TypedDict",
    Union[str, List[List[UpdateStreamPromptRequest2TypedDict]]],
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


UpdateStreamPromptRequest1 = TypeAliasType(
    "UpdateStreamPromptRequest1", Union[str, List[List[UpdateStreamPromptRequest2]]]
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


UpdateStreamPromptInterpolationMethodRequest1 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""


class UpdateStreamLoraDictRequest1TypedDict(TypedDict):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


class UpdateStreamLoraDictRequest1(BaseModel):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


UpdateStreamAccelerationRequest1 = Literal[
    "none",
    "xformers",
    "tensorrt",
]
r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""


UpdateStreamSeedRequest2TypedDict = TypeAliasType(
    "UpdateStreamSeedRequest2TypedDict", Union[int, float]
)


UpdateStreamSeedRequest2 = TypeAliasType("UpdateStreamSeedRequest2", Union[int, float])


UpdateStreamSeedRequest1TypedDict = TypeAliasType(
    "UpdateStreamSeedRequest1TypedDict",
    Union[int, List[List[UpdateStreamSeedRequest2TypedDict]]],
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


UpdateStreamSeedRequest1 = TypeAliasType(
    "UpdateStreamSeedRequest1", Union[int, List[List[UpdateStreamSeedRequest2]]]
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


UpdateStreamSeedInterpolationMethodRequest1 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""


UpdateStreamImagePreprocessingTypeRequest1 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class UpdateStreamImagePreprocessingProcessorRequest1TypedDict(TypedDict):
    type: UpdateStreamImagePreprocessingTypeRequest1
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamImagePreprocessingProcessorRequest1(BaseModel):
    type: UpdateStreamImagePreprocessingTypeRequest1

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamImagePreprocessingRequest1TypedDict(TypedDict):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[UpdateStreamImagePreprocessingProcessorRequest1TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamImagePreprocessingRequest1(BaseModel):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[UpdateStreamImagePreprocessingProcessorRequest1]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamImagePostprocessingTypeRequest1 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class UpdateStreamImagePostprocessingProcessorRequest1TypedDict(TypedDict):
    type: UpdateStreamImagePostprocessingTypeRequest1
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamImagePostprocessingProcessorRequest1(BaseModel):
    type: UpdateStreamImagePostprocessingTypeRequest1

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamImagePostprocessingRequest1TypedDict(TypedDict):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[UpdateStreamImagePostprocessingProcessorRequest1TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamImagePostprocessingRequest1(BaseModel):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[UpdateStreamImagePostprocessingProcessorRequest1]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamLatentPreprocessingTypeRequest1 = Literal["latent_feedback",]


class UpdateStreamLatentPreprocessingProcessorRequest1TypedDict(TypedDict):
    type: UpdateStreamLatentPreprocessingTypeRequest1
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamLatentPreprocessingProcessorRequest1(BaseModel):
    type: UpdateStreamLatentPreprocessingTypeRequest1

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamLatentPreprocessingRequest1TypedDict(TypedDict):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPreprocessingProcessorRequest1TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamLatentPreprocessingRequest1(BaseModel):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPreprocessingProcessorRequest1]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamLatentPostprocessingTypeRequest1 = Literal["latent_feedback",]


class UpdateStreamLatentPostprocessingProcessorRequest1TypedDict(TypedDict):
    type: UpdateStreamLatentPostprocessingTypeRequest1
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamLatentPostprocessingProcessorRequest1(BaseModel):
    type: UpdateStreamLatentPostprocessingTypeRequest1

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamLatentPostprocessingRequest1TypedDict(TypedDict):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPostprocessingProcessorRequest1TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamLatentPostprocessingRequest1(BaseModel):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPostprocessingProcessorRequest1]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamControlnetModelIDRequest1 = Literal[
    "thibaud/controlnet-sd21-openpose-diffusers",
    "thibaud/controlnet-sd21-hed-diffusers",
    "thibaud/controlnet-sd21-canny-diffusers",
    "thibaud/controlnet-sd21-depth-diffusers",
    "thibaud/controlnet-sd21-color-diffusers",
    "daydreamlive/TemporalNet2-stable-diffusion-2-1",
]
r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""


UpdateStreamPreprocessorRequest1 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]
r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""


class UpdateStreamControlnetRequest1TypedDict(TypedDict):
    model_id: UpdateStreamControlnetModelIDRequest1
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""
    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""
    preprocessor: UpdateStreamPreprocessorRequest1
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""
    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""
    preprocessor_params: NotRequired[Dict[str, Any]]
    r"""Additional parameters for the preprocessor. For example, canny edge detection uses 'low_threshold' and 'high_threshold' values."""
    control_guidance_start: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""
    control_guidance_end: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


class UpdateStreamControlnetRequest1(BaseModel):
    model_id: UpdateStreamControlnetModelIDRequest1
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""

    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""

    preprocessor: UpdateStreamPreprocessorRequest1
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""

    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""

    preprocessor_params: Optional[Dict[str, Any]] = None
    r"""Additional parameters for the preprocessor. For example, canny edge detection uses 'low_threshold' and 'high_threshold' values."""

    control_guidance_start: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""

    control_guidance_end: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


class UpdateStreamSDTurboRequestTypedDict(TypedDict):
    model_id: Literal["stabilityai/sd-turbo"]
    prompt: NotRequired[UpdateStreamPromptRequest1TypedDict]
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""
    prompt_interpolation_method: NotRequired[
        UpdateStreamPromptInterpolationMethodRequest1
    ]
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""
    normalize_prompt_weights: NotRequired[bool]
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""
    normalize_seed_weights: NotRequired[bool]
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""
    negative_prompt: NotRequired[str]
    r"""Text describing what to avoid in the generated image."""
    guidance_scale: NotRequired[float]
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""
    delta: NotRequired[float]
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""
    num_inference_steps: NotRequired[int]
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""
    t_index_list: NotRequired[List[int]]
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """
    use_safety_checker: NotRequired[bool]
    r"""Whether to use safety checker for content filtering"""
    width: NotRequired[int]
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""
    height: NotRequired[int]
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""
    lora_dict: NotRequired[Nullable[UpdateStreamLoraDictRequest1TypedDict]]
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""
    use_lcm_lora: NotRequired[bool]
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""
    lcm_lora_id: NotRequired[str]
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """
    acceleration: NotRequired[UpdateStreamAccelerationRequest1]
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""
    use_denoising_batch: NotRequired[bool]
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""
    do_add_noise: NotRequired[bool]
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""
    seed: NotRequired[UpdateStreamSeedRequest1TypedDict]
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""
    seed_interpolation_method: NotRequired[UpdateStreamSeedInterpolationMethodRequest1]
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""
    enable_similar_image_filter: NotRequired[bool]
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""
    similar_image_filter_threshold: NotRequired[float]
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""
    similar_image_filter_max_skip_frame: NotRequired[int]
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""
    skip_diffusion: NotRequired[bool]
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """
    image_preprocessing: NotRequired[UpdateStreamImagePreprocessingRequest1TypedDict]
    r"""List of image preprocessor configurations for image processing"""
    image_postprocessing: NotRequired[UpdateStreamImagePostprocessingRequest1TypedDict]
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """
    latent_preprocessing: NotRequired[UpdateStreamLatentPreprocessingRequest1TypedDict]
    r"""List of latent preprocessor configurations for latent processing"""
    latent_postprocessing: NotRequired[
        UpdateStreamLatentPostprocessingRequest1TypedDict
    ]
    r"""List of latent postprocessor configurations for latent processing"""
    controlnets: NotRequired[List[UpdateStreamControlnetRequest1TypedDict]]
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""


class UpdateStreamSDTurboRequest(BaseModel):
    model_id: Annotated[
        Annotated[
            Literal["stabilityai/sd-turbo"],
            AfterValidator(validate_const("stabilityai/sd-turbo")),
        ],
        pydantic.Field(alias="model_id"),
    ] = "stabilityai/sd-turbo"

    prompt: Optional[UpdateStreamPromptRequest1] = None
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""

    prompt_interpolation_method: Optional[
        UpdateStreamPromptInterpolationMethodRequest1
    ] = None
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""

    normalize_prompt_weights: Optional[bool] = None
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""

    normalize_seed_weights: Optional[bool] = None
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""

    negative_prompt: Optional[str] = None
    r"""Text describing what to avoid in the generated image."""

    guidance_scale: Optional[float] = None
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""

    delta: Optional[float] = None
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""

    num_inference_steps: Optional[int] = None
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""

    t_index_list: Optional[List[int]] = None
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """

    use_safety_checker: Optional[bool] = None
    r"""Whether to use safety checker for content filtering"""

    width: Optional[int] = None
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""

    height: Optional[int] = None
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""

    lora_dict: OptionalNullable[UpdateStreamLoraDictRequest1] = UNSET
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""

    use_lcm_lora: Optional[bool] = None
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""

    lcm_lora_id: Optional[str] = None
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """

    acceleration: Optional[UpdateStreamAccelerationRequest1] = None
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""

    use_denoising_batch: Optional[bool] = None
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""

    do_add_noise: Optional[bool] = None
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""

    seed: Optional[UpdateStreamSeedRequest1] = None
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""

    seed_interpolation_method: Optional[UpdateStreamSeedInterpolationMethodRequest1] = (
        None
    )
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""

    enable_similar_image_filter: Optional[bool] = None
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""

    similar_image_filter_threshold: Optional[float] = None
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""

    similar_image_filter_max_skip_frame: Optional[int] = None
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""

    skip_diffusion: Optional[bool] = None
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """

    image_preprocessing: Optional[UpdateStreamImagePreprocessingRequest1] = None
    r"""List of image preprocessor configurations for image processing"""

    image_postprocessing: Optional[UpdateStreamImagePostprocessingRequest1] = None
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    latent_preprocessing: Optional[UpdateStreamLatentPreprocessingRequest1] = None
    r"""List of latent preprocessor configurations for latent processing"""

    latent_postprocessing: Optional[UpdateStreamLatentPostprocessingRequest1] = None
    r"""List of latent postprocessor configurations for latent processing"""

    controlnets: Optional[List[UpdateStreamControlnetRequest1]] = None
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "prompt",
            "prompt_interpolation_method",
            "normalize_prompt_weights",
            "normalize_seed_weights",
            "negative_prompt",
            "guidance_scale",
            "delta",
            "num_inference_steps",
            "t_index_list",
            "use_safety_checker",
            "width",
            "height",
            "lora_dict",
            "use_lcm_lora",
            "lcm_lora_id",
            "acceleration",
            "use_denoising_batch",
            "do_add_noise",
            "seed",
            "seed_interpolation_method",
            "enable_similar_image_filter",
            "similar_image_filter_threshold",
            "similar_image_filter_max_skip_frame",
            "skip_diffusion",
            "image_preprocessing",
            "image_postprocessing",
            "latent_preprocessing",
            "latent_postprocessing",
            "controlnets",
        ]
        nullable_fields = ["lora_dict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


UpdateStreamParamsRequestTypedDict = TypeAliasType(
    "UpdateStreamParamsRequestTypedDict",
    Union[
        UpdateStreamSDTurboRequestTypedDict,
        UpdateStreamSDXLRequestTypedDict,
        UpdateStreamSd15RequestTypedDict,
    ],
)


UpdateStreamParamsRequest = Annotated[
    Union[
        Annotated[UpdateStreamSDTurboRequest, Tag("stabilityai/sd-turbo")],
        Annotated[UpdateStreamSDXLRequest, Tag("stabilityai/sdxl-turbo")],
        Annotated[UpdateStreamSd15Request, Tag("Lykon/dreamshaper-8")],
        Annotated[UpdateStreamSd15Request, Tag("prompthero/openjourney-v4")],
    ],
    Discriminator(lambda m: get_discriminator(m, "model_id", "model_id")),
]


class UpdateStreamStreamDiffusionTypedDict(TypedDict):
    pipeline: UpdateStreamPipelineRequest
    params: UpdateStreamParamsRequestTypedDict


class UpdateStreamStreamDiffusion(BaseModel):
    pipeline: UpdateStreamPipelineRequest

    params: UpdateStreamParamsRequest


UpdateStreamRequestBodyTypedDict = UpdateStreamStreamDiffusionTypedDict


UpdateStreamRequestBody = UpdateStreamStreamDiffusion


class UpdateStreamRequestTypedDict(TypedDict):
    id: str
    r"""ID of the stream to update"""
    body: NotRequired[UpdateStreamRequestBodyTypedDict]


class UpdateStreamRequest(BaseModel):
    id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
    r"""ID of the stream to update"""

    body: Annotated[
        Optional[UpdateStreamRequestBody],
        FieldMetadata(request=RequestMetadata(media_type="application/json")),
    ] = None


UpdateStreamPipelineResponse = Literal["streamdiffusion",]


UpdateStreamModelIDResponseEnum = Literal[
    "Lykon/dreamshaper-8",
    "prompthero/openjourney-v4",
]
r"""Model to use for generation"""


UpdateStreamPromptResponse5TypedDict = TypeAliasType(
    "UpdateStreamPromptResponse5TypedDict", Union[str, float]
)


UpdateStreamPromptResponse5 = TypeAliasType(
    "UpdateStreamPromptResponse5", Union[str, float]
)


UpdateStreamPromptResponse6TypedDict = TypeAliasType(
    "UpdateStreamPromptResponse6TypedDict",
    Union[str, List[List[UpdateStreamPromptResponse5TypedDict]]],
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


UpdateStreamPromptResponse6 = TypeAliasType(
    "UpdateStreamPromptResponse6", Union[str, List[List[UpdateStreamPromptResponse5]]]
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


UpdateStreamPromptInterpolationMethodResponse3 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""


class UpdateStreamLoraDictResponse3TypedDict(TypedDict):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


class UpdateStreamLoraDictResponse3(BaseModel):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


UpdateStreamAccelerationResponse3 = Literal[
    "none",
    "xformers",
    "tensorrt",
]
r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""


UpdateStreamSeedResponse5TypedDict = TypeAliasType(
    "UpdateStreamSeedResponse5TypedDict", Union[int, float]
)


UpdateStreamSeedResponse5 = TypeAliasType(
    "UpdateStreamSeedResponse5", Union[int, float]
)


UpdateStreamSeedResponse6TypedDict = TypeAliasType(
    "UpdateStreamSeedResponse6TypedDict",
    Union[int, List[List[UpdateStreamSeedResponse5TypedDict]]],
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


UpdateStreamSeedResponse6 = TypeAliasType(
    "UpdateStreamSeedResponse6", Union[int, List[List[UpdateStreamSeedResponse5]]]
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


UpdateStreamSeedInterpolationMethodResponse3 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""


UpdateStreamImagePreprocessingTypeResponse3 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class UpdateStreamImagePreprocessingProcessorResponse3TypedDict(TypedDict):
    type: UpdateStreamImagePreprocessingTypeResponse3
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamImagePreprocessingProcessorResponse3(BaseModel):
    type: UpdateStreamImagePreprocessingTypeResponse3

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamImagePreprocessingResponse3TypedDict(TypedDict):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[UpdateStreamImagePreprocessingProcessorResponse3TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamImagePreprocessingResponse3(BaseModel):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[UpdateStreamImagePreprocessingProcessorResponse3]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamImagePostprocessingTypeResponse3 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class UpdateStreamImagePostprocessingProcessorResponse3TypedDict(TypedDict):
    type: UpdateStreamImagePostprocessingTypeResponse3
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamImagePostprocessingProcessorResponse3(BaseModel):
    type: UpdateStreamImagePostprocessingTypeResponse3

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamImagePostprocessingResponse3TypedDict(TypedDict):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[UpdateStreamImagePostprocessingProcessorResponse3TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamImagePostprocessingResponse3(BaseModel):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[UpdateStreamImagePostprocessingProcessorResponse3]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamLatentPreprocessingTypeResponse3 = Literal["latent_feedback",]


class UpdateStreamLatentPreprocessingProcessorResponse3TypedDict(TypedDict):
    type: UpdateStreamLatentPreprocessingTypeResponse3
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamLatentPreprocessingProcessorResponse3(BaseModel):
    type: UpdateStreamLatentPreprocessingTypeResponse3

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamLatentPreprocessingResponse3TypedDict(TypedDict):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPreprocessingProcessorResponse3TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamLatentPreprocessingResponse3(BaseModel):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPreprocessingProcessorResponse3]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamLatentPostprocessingTypeResponse3 = Literal["latent_feedback",]


class UpdateStreamLatentPostprocessingProcessorResponse3TypedDict(TypedDict):
    type: UpdateStreamLatentPostprocessingTypeResponse3
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamLatentPostprocessingProcessorResponse3(BaseModel):
    type: UpdateStreamLatentPostprocessingTypeResponse3

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamLatentPostprocessingResponse3TypedDict(TypedDict):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPostprocessingProcessorResponse3TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamLatentPostprocessingResponse3(BaseModel):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPostprocessingProcessorResponse3]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamControlnetModelIDResponse3 = Literal[
    "lllyasviel/control_v11f1p_sd15_depth",
    "lllyasviel/control_v11f1e_sd15_tile",
    "lllyasviel/control_v11p_sd15_canny",
    "daydreamlive/TemporalNet2-stable-diffusion-v1-5",
]
r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""


UpdateStreamPreprocessorResponse3 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]
r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""


class UpdateStreamControlnetResponse3TypedDict(TypedDict):
    model_id: UpdateStreamControlnetModelIDResponse3
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""
    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""
    preprocessor: UpdateStreamPreprocessorResponse3
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""
    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""
    preprocessor_params: NotRequired[Dict[str, Any]]
    r"""Preprocessor parameters"""
    control_guidance_start: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""
    control_guidance_end: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


class UpdateStreamControlnetResponse3(BaseModel):
    model_id: UpdateStreamControlnetModelIDResponse3
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""

    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""

    preprocessor: UpdateStreamPreprocessorResponse3
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""

    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""

    preprocessor_params: Optional[Dict[str, Any]] = None
    r"""Preprocessor parameters"""

    control_guidance_start: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""

    control_guidance_end: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


UpdateStreamTypeRegularResponse = Literal["regular",]
r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""


UpdateStreamWeightTypeRegularResponse = Literal[
    "linear",
    "ease in",
    "ease out",
    "ease in-out",
    "reverse in-out",
    "weak input",
    "weak output",
    "weak middle",
    "strong middle",
    "style transfer",
    "composition",
    "strong style transfer",
    "style and composition",
    "style transfer precise",
    "composition precise",
]
r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class UpdateStreamIPAdapterRegularResponseTypedDict(TypedDict):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    type: NotRequired[UpdateStreamTypeRegularResponse]
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""
    enabled: NotRequired[bool]
    r"""Whether IP adapter is enabled"""
    scale: NotRequired[float]
    r"""Strength of IP adapter style conditioning"""
    weight_type: NotRequired[UpdateStreamWeightTypeRegularResponse]
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class UpdateStreamIPAdapterRegularResponse(BaseModel):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    type: Optional[UpdateStreamTypeRegularResponse] = "regular"
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""

    enabled: Optional[bool] = None
    r"""Whether IP adapter is enabled"""

    scale: Optional[float] = None
    r"""Strength of IP adapter style conditioning"""

    weight_type: Optional[UpdateStreamWeightTypeRegularResponse] = None
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class UpdateStreamCachedAttentionResponse2TypedDict(TypedDict):
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    enabled: bool
    r"""Whether this cached attention is active. Enable cached attention to reuse key/value tensors across frames."""
    max_frames: int
    r"""Number of frames retained in the attention cache. Number of historical K/V frames to retain. Limited by TensorRT engine exports."""
    interval: int
    r"""Cadence (number of frames) for refreshing cached key/value tensors i.e How often (in number of frames) to refresh the cache.
    It is now **frame-based** (not seconds). It accepts integers 1-240, representing how many frames elapse between cache refreshes.

    """


class UpdateStreamCachedAttentionResponse2(BaseModel):
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    enabled: bool
    r"""Whether this cached attention is active. Enable cached attention to reuse key/value tensors across frames."""

    max_frames: int
    r"""Number of frames retained in the attention cache. Number of historical K/V frames to retain. Limited by TensorRT engine exports."""

    interval: int
    r"""Cadence (number of frames) for refreshing cached key/value tensors i.e How often (in number of frames) to refresh the cache.
    It is now **frame-based** (not seconds). It accepts integers 1-240, representing how many frames elapse between cache refreshes.

    """


class UpdateStreamSd15ResponseTypedDict(TypedDict):
    model_id: UpdateStreamModelIDResponseEnum
    r"""Model to use for generation"""
    prompt: NotRequired[UpdateStreamPromptResponse6TypedDict]
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""
    prompt_interpolation_method: NotRequired[
        UpdateStreamPromptInterpolationMethodResponse3
    ]
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""
    normalize_prompt_weights: NotRequired[bool]
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""
    normalize_seed_weights: NotRequired[bool]
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""
    negative_prompt: NotRequired[str]
    r"""Text describing what to avoid in the generated image."""
    guidance_scale: NotRequired[float]
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""
    delta: NotRequired[float]
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""
    num_inference_steps: NotRequired[int]
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""
    t_index_list: NotRequired[List[int]]
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """
    use_safety_checker: NotRequired[bool]
    r"""Whether to use safety checker for content filtering"""
    width: NotRequired[int]
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""
    height: NotRequired[int]
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""
    lora_dict: NotRequired[Nullable[UpdateStreamLoraDictResponse3TypedDict]]
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""
    use_lcm_lora: NotRequired[bool]
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""
    lcm_lora_id: NotRequired[str]
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """
    acceleration: NotRequired[UpdateStreamAccelerationResponse3]
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""
    use_denoising_batch: NotRequired[bool]
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""
    do_add_noise: NotRequired[bool]
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""
    seed: NotRequired[UpdateStreamSeedResponse6TypedDict]
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""
    seed_interpolation_method: NotRequired[UpdateStreamSeedInterpolationMethodResponse3]
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""
    enable_similar_image_filter: NotRequired[bool]
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""
    similar_image_filter_threshold: NotRequired[float]
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""
    similar_image_filter_max_skip_frame: NotRequired[int]
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""
    skip_diffusion: NotRequired[bool]
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """
    image_preprocessing: NotRequired[UpdateStreamImagePreprocessingResponse3TypedDict]
    r"""List of image preprocessor configurations for image processing"""
    image_postprocessing: NotRequired[UpdateStreamImagePostprocessingResponse3TypedDict]
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """
    latent_preprocessing: NotRequired[UpdateStreamLatentPreprocessingResponse3TypedDict]
    r"""List of latent preprocessor configurations for latent processing"""
    latent_postprocessing: NotRequired[
        UpdateStreamLatentPostprocessingResponse3TypedDict
    ]
    r"""List of latent postprocessor configurations for latent processing"""
    controlnets: NotRequired[List[UpdateStreamControlnetResponse3TypedDict]]
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""
    ip_adapter: NotRequired[UpdateStreamIPAdapterRegularResponseTypedDict]
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""
    ip_adapter_style_image_url: NotRequired[str]
    r"""HTTPS URL or base64-encoded data URI (data:image/[type];base64,...) of the style image to use. Base64 images are automatically cached and uploaded to storage with content-based deduplication. Maximum size: 5MB. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""
    cached_attention: NotRequired[UpdateStreamCachedAttentionResponse2TypedDict]
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""


class UpdateStreamSd15Response(BaseModel):
    model_id: UpdateStreamModelIDResponseEnum
    r"""Model to use for generation"""

    prompt: Optional[UpdateStreamPromptResponse6] = None
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""

    prompt_interpolation_method: Optional[
        UpdateStreamPromptInterpolationMethodResponse3
    ] = None
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""

    normalize_prompt_weights: Optional[bool] = None
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""

    normalize_seed_weights: Optional[bool] = None
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""

    negative_prompt: Optional[str] = None
    r"""Text describing what to avoid in the generated image."""

    guidance_scale: Optional[float] = None
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""

    delta: Optional[float] = None
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""

    num_inference_steps: Optional[int] = None
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""

    t_index_list: Optional[List[int]] = None
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """

    use_safety_checker: Optional[bool] = None
    r"""Whether to use safety checker for content filtering"""

    width: Optional[int] = None
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""

    height: Optional[int] = None
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""

    lora_dict: OptionalNullable[UpdateStreamLoraDictResponse3] = UNSET
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""

    use_lcm_lora: Optional[bool] = None
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""

    lcm_lora_id: Optional[str] = None
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """

    acceleration: Optional[UpdateStreamAccelerationResponse3] = None
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""

    use_denoising_batch: Optional[bool] = None
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""

    do_add_noise: Optional[bool] = None
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""

    seed: Optional[UpdateStreamSeedResponse6] = None
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""

    seed_interpolation_method: Optional[
        UpdateStreamSeedInterpolationMethodResponse3
    ] = None
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""

    enable_similar_image_filter: Optional[bool] = None
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""

    similar_image_filter_threshold: Optional[float] = None
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""

    similar_image_filter_max_skip_frame: Optional[int] = None
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""

    skip_diffusion: Optional[bool] = None
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """

    image_preprocessing: Optional[UpdateStreamImagePreprocessingResponse3] = None
    r"""List of image preprocessor configurations for image processing"""

    image_postprocessing: Optional[UpdateStreamImagePostprocessingResponse3] = None
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    latent_preprocessing: Optional[UpdateStreamLatentPreprocessingResponse3] = None
    r"""List of latent preprocessor configurations for latent processing"""

    latent_postprocessing: Optional[UpdateStreamLatentPostprocessingResponse3] = None
    r"""List of latent postprocessor configurations for latent processing"""

    controlnets: Optional[List[UpdateStreamControlnetResponse3]] = None
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""

    ip_adapter: Optional[UpdateStreamIPAdapterRegularResponse] = None
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    ip_adapter_style_image_url: Optional[str] = None
    r"""HTTPS URL or base64-encoded data URI (data:image/[type];base64,...) of the style image to use. Base64 images are automatically cached and uploaded to storage with content-based deduplication. Maximum size: 5MB. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""

    cached_attention: Optional[UpdateStreamCachedAttentionResponse2] = None
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "prompt",
            "prompt_interpolation_method",
            "normalize_prompt_weights",
            "normalize_seed_weights",
            "negative_prompt",
            "guidance_scale",
            "delta",
            "num_inference_steps",
            "t_index_list",
            "use_safety_checker",
            "width",
            "height",
            "lora_dict",
            "use_lcm_lora",
            "lcm_lora_id",
            "acceleration",
            "use_denoising_batch",
            "do_add_noise",
            "seed",
            "seed_interpolation_method",
            "enable_similar_image_filter",
            "similar_image_filter_threshold",
            "similar_image_filter_max_skip_frame",
            "skip_diffusion",
            "image_preprocessing",
            "image_postprocessing",
            "latent_preprocessing",
            "latent_postprocessing",
            "controlnets",
            "ip_adapter",
            "ip_adapter_style_image_url",
            "cached_attention",
        ]
        nullable_fields = ["lora_dict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


UpdateStreamPromptResponse3TypedDict = TypeAliasType(
    "UpdateStreamPromptResponse3TypedDict", Union[str, float]
)


UpdateStreamPromptResponse3 = TypeAliasType(
    "UpdateStreamPromptResponse3", Union[str, float]
)


UpdateStreamPromptResponse4TypedDict = TypeAliasType(
    "UpdateStreamPromptResponse4TypedDict",
    Union[str, List[List[UpdateStreamPromptResponse3TypedDict]]],
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


UpdateStreamPromptResponse4 = TypeAliasType(
    "UpdateStreamPromptResponse4", Union[str, List[List[UpdateStreamPromptResponse3]]]
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


UpdateStreamPromptInterpolationMethodResponse2 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""


class UpdateStreamLoraDictResponse2TypedDict(TypedDict):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


class UpdateStreamLoraDictResponse2(BaseModel):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


UpdateStreamAccelerationResponse2 = Literal[
    "none",
    "xformers",
    "tensorrt",
]
r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""


UpdateStreamSeedResponse3TypedDict = TypeAliasType(
    "UpdateStreamSeedResponse3TypedDict", Union[int, float]
)


UpdateStreamSeedResponse3 = TypeAliasType(
    "UpdateStreamSeedResponse3", Union[int, float]
)


UpdateStreamSeedResponse4TypedDict = TypeAliasType(
    "UpdateStreamSeedResponse4TypedDict",
    Union[int, List[List[UpdateStreamSeedResponse3TypedDict]]],
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


UpdateStreamSeedResponse4 = TypeAliasType(
    "UpdateStreamSeedResponse4", Union[int, List[List[UpdateStreamSeedResponse3]]]
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


UpdateStreamSeedInterpolationMethodResponse2 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""


UpdateStreamImagePreprocessingTypeResponse2 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class UpdateStreamImagePreprocessingProcessorResponse2TypedDict(TypedDict):
    type: UpdateStreamImagePreprocessingTypeResponse2
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamImagePreprocessingProcessorResponse2(BaseModel):
    type: UpdateStreamImagePreprocessingTypeResponse2

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamImagePreprocessingResponse2TypedDict(TypedDict):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[UpdateStreamImagePreprocessingProcessorResponse2TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamImagePreprocessingResponse2(BaseModel):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[UpdateStreamImagePreprocessingProcessorResponse2]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamImagePostprocessingTypeResponse2 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class UpdateStreamImagePostprocessingProcessorResponse2TypedDict(TypedDict):
    type: UpdateStreamImagePostprocessingTypeResponse2
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamImagePostprocessingProcessorResponse2(BaseModel):
    type: UpdateStreamImagePostprocessingTypeResponse2

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamImagePostprocessingResponse2TypedDict(TypedDict):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[UpdateStreamImagePostprocessingProcessorResponse2TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamImagePostprocessingResponse2(BaseModel):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[UpdateStreamImagePostprocessingProcessorResponse2]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamLatentPreprocessingTypeResponse2 = Literal["latent_feedback",]


class UpdateStreamLatentPreprocessingProcessorResponse2TypedDict(TypedDict):
    type: UpdateStreamLatentPreprocessingTypeResponse2
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamLatentPreprocessingProcessorResponse2(BaseModel):
    type: UpdateStreamLatentPreprocessingTypeResponse2

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamLatentPreprocessingResponse2TypedDict(TypedDict):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPreprocessingProcessorResponse2TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamLatentPreprocessingResponse2(BaseModel):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPreprocessingProcessorResponse2]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamLatentPostprocessingTypeResponse2 = Literal["latent_feedback",]


class UpdateStreamLatentPostprocessingProcessorResponse2TypedDict(TypedDict):
    type: UpdateStreamLatentPostprocessingTypeResponse2
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamLatentPostprocessingProcessorResponse2(BaseModel):
    type: UpdateStreamLatentPostprocessingTypeResponse2

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamLatentPostprocessingResponse2TypedDict(TypedDict):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPostprocessingProcessorResponse2TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamLatentPostprocessingResponse2(BaseModel):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPostprocessingProcessorResponse2]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamControlnetModelIDResponse2 = Literal[
    "xinsir/controlnet-depth-sdxl-1.0",
    "xinsir/controlnet-canny-sdxl-1.0",
    "xinsir/controlnet-tile-sdxl-1.0",
]
r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""


UpdateStreamPreprocessorResponse2 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]
r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""


class UpdateStreamControlnetResponse2TypedDict(TypedDict):
    model_id: UpdateStreamControlnetModelIDResponse2
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""
    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""
    preprocessor: UpdateStreamPreprocessorResponse2
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""
    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""
    preprocessor_params: NotRequired[Dict[str, Any]]
    r"""Preprocessor parameters"""
    control_guidance_start: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""
    control_guidance_end: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


class UpdateStreamControlnetResponse2(BaseModel):
    model_id: UpdateStreamControlnetModelIDResponse2
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""

    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""

    preprocessor: UpdateStreamPreprocessorResponse2
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""

    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""

    preprocessor_params: Optional[Dict[str, Any]] = None
    r"""Preprocessor parameters"""

    control_guidance_start: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""

    control_guidance_end: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


UpdateStreamIPAdapterTypeResponse = Literal[
    "regular",
    "faceid",
]
r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""


UpdateStreamIPAdapterWeightTypeResponse = Literal[
    "linear",
    "ease in",
    "ease out",
    "ease in-out",
    "reverse in-out",
    "weak input",
    "weak output",
    "weak middle",
    "strong middle",
    "style transfer",
    "composition",
    "strong style transfer",
    "style and composition",
    "style transfer precise",
    "composition precise",
]
r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class UpdateStreamIPAdapterResponseTypedDict(TypedDict):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    type: NotRequired[UpdateStreamIPAdapterTypeResponse]
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""
    enabled: NotRequired[bool]
    r"""Whether IP adapter is enabled"""
    scale: NotRequired[float]
    r"""Strength of IP adapter style conditioning"""
    weight_type: NotRequired[UpdateStreamIPAdapterWeightTypeResponse]
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class UpdateStreamIPAdapterResponse(BaseModel):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    type: Optional[UpdateStreamIPAdapterTypeResponse] = "regular"
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""

    enabled: Optional[bool] = None
    r"""Whether IP adapter is enabled"""

    scale: Optional[float] = None
    r"""Strength of IP adapter style conditioning"""

    weight_type: Optional[UpdateStreamIPAdapterWeightTypeResponse] = None
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class UpdateStreamCachedAttentionResponse1TypedDict(TypedDict):
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    enabled: bool
    r"""Whether this cached attention is active. Enable cached attention to reuse key/value tensors across frames."""
    max_frames: int
    r"""Number of frames retained in the attention cache. Number of historical K/V frames to retain. Limited by TensorRT engine exports."""
    interval: int
    r"""Cadence (number of frames) for refreshing cached key/value tensors i.e How often (in number of frames) to refresh the cache.
    It is now **frame-based** (not seconds). It accepts integers 1-240, representing how many frames elapse between cache refreshes.

    """


class UpdateStreamCachedAttentionResponse1(BaseModel):
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    enabled: bool
    r"""Whether this cached attention is active. Enable cached attention to reuse key/value tensors across frames."""

    max_frames: int
    r"""Number of frames retained in the attention cache. Number of historical K/V frames to retain. Limited by TensorRT engine exports."""

    interval: int
    r"""Cadence (number of frames) for refreshing cached key/value tensors i.e How often (in number of frames) to refresh the cache.
    It is now **frame-based** (not seconds). It accepts integers 1-240, representing how many frames elapse between cache refreshes.

    """


class UpdateStreamSDXLResponseTypedDict(TypedDict):
    model_id: Literal["stabilityai/sdxl-turbo"]
    prompt: NotRequired[UpdateStreamPromptResponse4TypedDict]
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""
    prompt_interpolation_method: NotRequired[
        UpdateStreamPromptInterpolationMethodResponse2
    ]
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""
    normalize_prompt_weights: NotRequired[bool]
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""
    normalize_seed_weights: NotRequired[bool]
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""
    negative_prompt: NotRequired[str]
    r"""Text describing what to avoid in the generated image."""
    guidance_scale: NotRequired[float]
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""
    delta: NotRequired[float]
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""
    num_inference_steps: NotRequired[int]
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""
    t_index_list: NotRequired[List[int]]
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """
    use_safety_checker: NotRequired[bool]
    r"""Whether to use safety checker for content filtering"""
    width: NotRequired[int]
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""
    height: NotRequired[int]
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""
    lora_dict: NotRequired[Nullable[UpdateStreamLoraDictResponse2TypedDict]]
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""
    use_lcm_lora: NotRequired[bool]
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""
    lcm_lora_id: NotRequired[str]
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """
    acceleration: NotRequired[UpdateStreamAccelerationResponse2]
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""
    use_denoising_batch: NotRequired[bool]
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""
    do_add_noise: NotRequired[bool]
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""
    seed: NotRequired[UpdateStreamSeedResponse4TypedDict]
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""
    seed_interpolation_method: NotRequired[UpdateStreamSeedInterpolationMethodResponse2]
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""
    enable_similar_image_filter: NotRequired[bool]
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""
    similar_image_filter_threshold: NotRequired[float]
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""
    similar_image_filter_max_skip_frame: NotRequired[int]
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""
    skip_diffusion: NotRequired[bool]
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """
    image_preprocessing: NotRequired[UpdateStreamImagePreprocessingResponse2TypedDict]
    r"""List of image preprocessor configurations for image processing"""
    image_postprocessing: NotRequired[UpdateStreamImagePostprocessingResponse2TypedDict]
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """
    latent_preprocessing: NotRequired[UpdateStreamLatentPreprocessingResponse2TypedDict]
    r"""List of latent preprocessor configurations for latent processing"""
    latent_postprocessing: NotRequired[
        UpdateStreamLatentPostprocessingResponse2TypedDict
    ]
    r"""List of latent postprocessor configurations for latent processing"""
    controlnets: NotRequired[List[UpdateStreamControlnetResponse2TypedDict]]
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""
    ip_adapter: NotRequired[UpdateStreamIPAdapterResponseTypedDict]
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""
    ip_adapter_style_image_url: NotRequired[str]
    r"""HTTPS URL or base64-encoded data URI (data:image/[type];base64,...) of the style image to use. Base64 images are automatically cached and uploaded to storage with content-based deduplication. Maximum size: 5MB. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""
    cached_attention: NotRequired[UpdateStreamCachedAttentionResponse1TypedDict]
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""


class UpdateStreamSDXLResponse(BaseModel):
    model_id: Annotated[
        Annotated[
            Literal["stabilityai/sdxl-turbo"],
            AfterValidator(validate_const("stabilityai/sdxl-turbo")),
        ],
        pydantic.Field(alias="model_id"),
    ] = "stabilityai/sdxl-turbo"

    prompt: Optional[UpdateStreamPromptResponse4] = None
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""

    prompt_interpolation_method: Optional[
        UpdateStreamPromptInterpolationMethodResponse2
    ] = None
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""

    normalize_prompt_weights: Optional[bool] = None
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""

    normalize_seed_weights: Optional[bool] = None
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""

    negative_prompt: Optional[str] = None
    r"""Text describing what to avoid in the generated image."""

    guidance_scale: Optional[float] = None
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""

    delta: Optional[float] = None
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""

    num_inference_steps: Optional[int] = None
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""

    t_index_list: Optional[List[int]] = None
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """

    use_safety_checker: Optional[bool] = None
    r"""Whether to use safety checker for content filtering"""

    width: Optional[int] = None
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""

    height: Optional[int] = None
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""

    lora_dict: OptionalNullable[UpdateStreamLoraDictResponse2] = UNSET
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""

    use_lcm_lora: Optional[bool] = None
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""

    lcm_lora_id: Optional[str] = None
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """

    acceleration: Optional[UpdateStreamAccelerationResponse2] = None
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""

    use_denoising_batch: Optional[bool] = None
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""

    do_add_noise: Optional[bool] = None
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""

    seed: Optional[UpdateStreamSeedResponse4] = None
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""

    seed_interpolation_method: Optional[
        UpdateStreamSeedInterpolationMethodResponse2
    ] = None
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""

    enable_similar_image_filter: Optional[bool] = None
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""

    similar_image_filter_threshold: Optional[float] = None
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""

    similar_image_filter_max_skip_frame: Optional[int] = None
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""

    skip_diffusion: Optional[bool] = None
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """

    image_preprocessing: Optional[UpdateStreamImagePreprocessingResponse2] = None
    r"""List of image preprocessor configurations for image processing"""

    image_postprocessing: Optional[UpdateStreamImagePostprocessingResponse2] = None
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    latent_preprocessing: Optional[UpdateStreamLatentPreprocessingResponse2] = None
    r"""List of latent preprocessor configurations for latent processing"""

    latent_postprocessing: Optional[UpdateStreamLatentPostprocessingResponse2] = None
    r"""List of latent postprocessor configurations for latent processing"""

    controlnets: Optional[List[UpdateStreamControlnetResponse2]] = None
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""

    ip_adapter: Optional[UpdateStreamIPAdapterResponse] = None
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    ip_adapter_style_image_url: Optional[str] = None
    r"""HTTPS URL or base64-encoded data URI (data:image/[type];base64,...) of the style image to use. Base64 images are automatically cached and uploaded to storage with content-based deduplication. Maximum size: 5MB. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""

    cached_attention: Optional[UpdateStreamCachedAttentionResponse1] = None
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "prompt",
            "prompt_interpolation_method",
            "normalize_prompt_weights",
            "normalize_seed_weights",
            "negative_prompt",
            "guidance_scale",
            "delta",
            "num_inference_steps",
            "t_index_list",
            "use_safety_checker",
            "width",
            "height",
            "lora_dict",
            "use_lcm_lora",
            "lcm_lora_id",
            "acceleration",
            "use_denoising_batch",
            "do_add_noise",
            "seed",
            "seed_interpolation_method",
            "enable_similar_image_filter",
            "similar_image_filter_threshold",
            "similar_image_filter_max_skip_frame",
            "skip_diffusion",
            "image_preprocessing",
            "image_postprocessing",
            "latent_preprocessing",
            "latent_postprocessing",
            "controlnets",
            "ip_adapter",
            "ip_adapter_style_image_url",
            "cached_attention",
        ]
        nullable_fields = ["lora_dict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


UpdateStreamPromptResponse1TypedDict = TypeAliasType(
    "UpdateStreamPromptResponse1TypedDict", Union[str, float]
)


UpdateStreamPromptResponse1 = TypeAliasType(
    "UpdateStreamPromptResponse1", Union[str, float]
)


UpdateStreamPromptResponse2TypedDict = TypeAliasType(
    "UpdateStreamPromptResponse2TypedDict",
    Union[str, List[List[UpdateStreamPromptResponse1TypedDict]]],
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


UpdateStreamPromptResponse2 = TypeAliasType(
    "UpdateStreamPromptResponse2", Union[str, List[List[UpdateStreamPromptResponse1]]]
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


UpdateStreamPromptInterpolationMethodResponse1 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""


class UpdateStreamLoraDictResponse1TypedDict(TypedDict):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


class UpdateStreamLoraDictResponse1(BaseModel):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


UpdateStreamAccelerationResponse1 = Literal[
    "none",
    "xformers",
    "tensorrt",
]
r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""


UpdateStreamSeedResponse1TypedDict = TypeAliasType(
    "UpdateStreamSeedResponse1TypedDict", Union[int, float]
)


UpdateStreamSeedResponse1 = TypeAliasType(
    "UpdateStreamSeedResponse1", Union[int, float]
)


UpdateStreamSeedResponse2TypedDict = TypeAliasType(
    "UpdateStreamSeedResponse2TypedDict",
    Union[int, List[List[UpdateStreamSeedResponse1TypedDict]]],
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


UpdateStreamSeedResponse2 = TypeAliasType(
    "UpdateStreamSeedResponse2", Union[int, List[List[UpdateStreamSeedResponse1]]]
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


UpdateStreamSeedInterpolationMethodResponse1 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""


UpdateStreamImagePreprocessingTypeResponse1 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class UpdateStreamImagePreprocessingProcessorResponse1TypedDict(TypedDict):
    type: UpdateStreamImagePreprocessingTypeResponse1
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamImagePreprocessingProcessorResponse1(BaseModel):
    type: UpdateStreamImagePreprocessingTypeResponse1

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamImagePreprocessingResponse1TypedDict(TypedDict):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[UpdateStreamImagePreprocessingProcessorResponse1TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamImagePreprocessingResponse1(BaseModel):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[UpdateStreamImagePreprocessingProcessorResponse1]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamImagePostprocessingTypeResponse1 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class UpdateStreamImagePostprocessingProcessorResponse1TypedDict(TypedDict):
    type: UpdateStreamImagePostprocessingTypeResponse1
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamImagePostprocessingProcessorResponse1(BaseModel):
    type: UpdateStreamImagePostprocessingTypeResponse1

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamImagePostprocessingResponse1TypedDict(TypedDict):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[UpdateStreamImagePostprocessingProcessorResponse1TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamImagePostprocessingResponse1(BaseModel):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[UpdateStreamImagePostprocessingProcessorResponse1]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamLatentPreprocessingTypeResponse1 = Literal["latent_feedback",]


class UpdateStreamLatentPreprocessingProcessorResponse1TypedDict(TypedDict):
    type: UpdateStreamLatentPreprocessingTypeResponse1
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamLatentPreprocessingProcessorResponse1(BaseModel):
    type: UpdateStreamLatentPreprocessingTypeResponse1

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamLatentPreprocessingResponse1TypedDict(TypedDict):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPreprocessingProcessorResponse1TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamLatentPreprocessingResponse1(BaseModel):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPreprocessingProcessorResponse1]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamLatentPostprocessingTypeResponse1 = Literal["latent_feedback",]


class UpdateStreamLatentPostprocessingProcessorResponse1TypedDict(TypedDict):
    type: UpdateStreamLatentPostprocessingTypeResponse1
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class UpdateStreamLatentPostprocessingProcessorResponse1(BaseModel):
    type: UpdateStreamLatentPostprocessingTypeResponse1

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class UpdateStreamLatentPostprocessingResponse1TypedDict(TypedDict):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPostprocessingProcessorResponse1TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class UpdateStreamLatentPostprocessingResponse1(BaseModel):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[UpdateStreamLatentPostprocessingProcessorResponse1]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


UpdateStreamControlnetModelIDResponse1 = Literal[
    "thibaud/controlnet-sd21-openpose-diffusers",
    "thibaud/controlnet-sd21-hed-diffusers",
    "thibaud/controlnet-sd21-canny-diffusers",
    "thibaud/controlnet-sd21-depth-diffusers",
    "thibaud/controlnet-sd21-color-diffusers",
    "daydreamlive/TemporalNet2-stable-diffusion-2-1",
]
r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""


UpdateStreamPreprocessorResponse1 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]
r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""


class UpdateStreamControlnetResponse1TypedDict(TypedDict):
    model_id: UpdateStreamControlnetModelIDResponse1
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""
    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""
    preprocessor: UpdateStreamPreprocessorResponse1
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""
    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""
    preprocessor_params: NotRequired[Dict[str, Any]]
    r"""Additional parameters for the preprocessor. For example, canny edge detection uses 'low_threshold' and 'high_threshold' values."""
    control_guidance_start: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""
    control_guidance_end: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


class UpdateStreamControlnetResponse1(BaseModel):
    model_id: UpdateStreamControlnetModelIDResponse1
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""

    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""

    preprocessor: UpdateStreamPreprocessorResponse1
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""

    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""

    preprocessor_params: Optional[Dict[str, Any]] = None
    r"""Additional parameters for the preprocessor. For example, canny edge detection uses 'low_threshold' and 'high_threshold' values."""

    control_guidance_start: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""

    control_guidance_end: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


class UpdateStreamSDTurboResponseTypedDict(TypedDict):
    model_id: Literal["stabilityai/sd-turbo"]
    prompt: NotRequired[UpdateStreamPromptResponse2TypedDict]
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""
    prompt_interpolation_method: NotRequired[
        UpdateStreamPromptInterpolationMethodResponse1
    ]
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""
    normalize_prompt_weights: NotRequired[bool]
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""
    normalize_seed_weights: NotRequired[bool]
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""
    negative_prompt: NotRequired[str]
    r"""Text describing what to avoid in the generated image."""
    guidance_scale: NotRequired[float]
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""
    delta: NotRequired[float]
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""
    num_inference_steps: NotRequired[int]
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""
    t_index_list: NotRequired[List[int]]
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """
    use_safety_checker: NotRequired[bool]
    r"""Whether to use safety checker for content filtering"""
    width: NotRequired[int]
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""
    height: NotRequired[int]
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""
    lora_dict: NotRequired[Nullable[UpdateStreamLoraDictResponse1TypedDict]]
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""
    use_lcm_lora: NotRequired[bool]
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""
    lcm_lora_id: NotRequired[str]
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """
    acceleration: NotRequired[UpdateStreamAccelerationResponse1]
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""
    use_denoising_batch: NotRequired[bool]
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""
    do_add_noise: NotRequired[bool]
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""
    seed: NotRequired[UpdateStreamSeedResponse2TypedDict]
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""
    seed_interpolation_method: NotRequired[UpdateStreamSeedInterpolationMethodResponse1]
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""
    enable_similar_image_filter: NotRequired[bool]
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""
    similar_image_filter_threshold: NotRequired[float]
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""
    similar_image_filter_max_skip_frame: NotRequired[int]
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""
    skip_diffusion: NotRequired[bool]
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """
    image_preprocessing: NotRequired[UpdateStreamImagePreprocessingResponse1TypedDict]
    r"""List of image preprocessor configurations for image processing"""
    image_postprocessing: NotRequired[UpdateStreamImagePostprocessingResponse1TypedDict]
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """
    latent_preprocessing: NotRequired[UpdateStreamLatentPreprocessingResponse1TypedDict]
    r"""List of latent preprocessor configurations for latent processing"""
    latent_postprocessing: NotRequired[
        UpdateStreamLatentPostprocessingResponse1TypedDict
    ]
    r"""List of latent postprocessor configurations for latent processing"""
    controlnets: NotRequired[List[UpdateStreamControlnetResponse1TypedDict]]
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""


class UpdateStreamSDTurboResponse(BaseModel):
    model_id: Annotated[
        Annotated[
            Literal["stabilityai/sd-turbo"],
            AfterValidator(validate_const("stabilityai/sd-turbo")),
        ],
        pydantic.Field(alias="model_id"),
    ] = "stabilityai/sd-turbo"

    prompt: Optional[UpdateStreamPromptResponse2] = None
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""

    prompt_interpolation_method: Optional[
        UpdateStreamPromptInterpolationMethodResponse1
    ] = None
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""

    normalize_prompt_weights: Optional[bool] = None
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""

    normalize_seed_weights: Optional[bool] = None
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""

    negative_prompt: Optional[str] = None
    r"""Text describing what to avoid in the generated image."""

    guidance_scale: Optional[float] = None
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""

    delta: Optional[float] = None
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""

    num_inference_steps: Optional[int] = None
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""

    t_index_list: Optional[List[int]] = None
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """

    use_safety_checker: Optional[bool] = None
    r"""Whether to use safety checker for content filtering"""

    width: Optional[int] = None
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""

    height: Optional[int] = None
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""

    lora_dict: OptionalNullable[UpdateStreamLoraDictResponse1] = UNSET
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""

    use_lcm_lora: Optional[bool] = None
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""

    lcm_lora_id: Optional[str] = None
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """

    acceleration: Optional[UpdateStreamAccelerationResponse1] = None
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""

    use_denoising_batch: Optional[bool] = None
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""

    do_add_noise: Optional[bool] = None
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""

    seed: Optional[UpdateStreamSeedResponse2] = None
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""

    seed_interpolation_method: Optional[
        UpdateStreamSeedInterpolationMethodResponse1
    ] = None
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""

    enable_similar_image_filter: Optional[bool] = None
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""

    similar_image_filter_threshold: Optional[float] = None
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""

    similar_image_filter_max_skip_frame: Optional[int] = None
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""

    skip_diffusion: Optional[bool] = None
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """

    image_preprocessing: Optional[UpdateStreamImagePreprocessingResponse1] = None
    r"""List of image preprocessor configurations for image processing"""

    image_postprocessing: Optional[UpdateStreamImagePostprocessingResponse1] = None
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    latent_preprocessing: Optional[UpdateStreamLatentPreprocessingResponse1] = None
    r"""List of latent preprocessor configurations for latent processing"""

    latent_postprocessing: Optional[UpdateStreamLatentPostprocessingResponse1] = None
    r"""List of latent postprocessor configurations for latent processing"""

    controlnets: Optional[List[UpdateStreamControlnetResponse1]] = None
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "prompt",
            "prompt_interpolation_method",
            "normalize_prompt_weights",
            "normalize_seed_weights",
            "negative_prompt",
            "guidance_scale",
            "delta",
            "num_inference_steps",
            "t_index_list",
            "use_safety_checker",
            "width",
            "height",
            "lora_dict",
            "use_lcm_lora",
            "lcm_lora_id",
            "acceleration",
            "use_denoising_batch",
            "do_add_noise",
            "seed",
            "seed_interpolation_method",
            "enable_similar_image_filter",
            "similar_image_filter_threshold",
            "similar_image_filter_max_skip_frame",
            "skip_diffusion",
            "image_preprocessing",
            "image_postprocessing",
            "latent_preprocessing",
            "latent_postprocessing",
            "controlnets",
        ]
        nullable_fields = ["lora_dict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


UpdateStreamParamsResponseTypedDict = TypeAliasType(
    "UpdateStreamParamsResponseTypedDict",
    Union[
        UpdateStreamSDTurboResponseTypedDict,
        UpdateStreamSDXLResponseTypedDict,
        UpdateStreamSd15ResponseTypedDict,
    ],
)


UpdateStreamParamsResponse = Annotated[
    Union[
        Annotated[UpdateStreamSDTurboResponse, Tag("stabilityai/sd-turbo")],
        Annotated[UpdateStreamSDXLResponse, Tag("stabilityai/sdxl-turbo")],
        Annotated[UpdateStreamSd15Response, Tag("Lykon/dreamshaper-8")],
        Annotated[UpdateStreamSd15Response, Tag("prompthero/openjourney-v4")],
    ],
    Discriminator(lambda m: get_discriminator(m, "model_id", "model_id")),
]


class UpdateStreamResponseTypedDict(TypedDict):
    r"""Default Response"""

    pipeline: UpdateStreamPipelineResponse
    params: UpdateStreamParamsResponseTypedDict
    id: str
    r"""Unique identifier for the stream"""
    stream_key: str
    r"""Unique key used for streaming to this endpoint"""
    created_at: str
    r"""ISO timestamp when the stream was created"""
    output_playback_id: str
    r"""Playback ID for accessing the stream output"""
    name: str
    r"""Human-readable name of the stream"""
    author: str
    r"""ID of the user who created this stream"""
    from_playground: bool
    r"""Whether this stream was created from the playground interface"""
    gateway_host: str
    r"""Gateway server hostname handling this stream"""
    is_smoke_test: bool
    r"""Whether this is a smoke test stream"""
    whip_url: str
    r"""WebRTC WHIP URL for stream ingestion"""
    output_stream_url: NotRequired[str]
    r"""URL where the processed stream output can be accessed"""


class UpdateStreamResponse(BaseModel):
    r"""Default Response"""

    pipeline: UpdateStreamPipelineResponse

    params: UpdateStreamParamsResponse

    id: str
    r"""Unique identifier for the stream"""

    stream_key: str
    r"""Unique key used for streaming to this endpoint"""

    created_at: str
    r"""ISO timestamp when the stream was created"""

    output_playback_id: str
    r"""Playback ID for accessing the stream output"""

    name: str
    r"""Human-readable name of the stream"""

    author: str
    r"""ID of the user who created this stream"""

    from_playground: bool
    r"""Whether this stream was created from the playground interface"""

    gateway_host: str
    r"""Gateway server hostname handling this stream"""

    is_smoke_test: bool
    r"""Whether this is a smoke test stream"""

    whip_url: str
    r"""WebRTC WHIP URL for stream ingestion"""

    output_stream_url: Optional[str] = None
    r"""URL where the processed stream output can be accessed"""
