"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from daydream.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from daydream.utils import (
    FieldMetadata,
    PathParamMetadata,
    get_discriminator,
    validate_const,
)
import pydantic
from pydantic import Discriminator, Tag, model_serializer
from pydantic.functional_validators import AfterValidator
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


class GetStreamByIDRequestTypedDict(TypedDict):
    id: str
    r"""ID of the stream to retrieve"""


class GetStreamByIDRequest(BaseModel):
    id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
    r"""ID of the stream to retrieve"""


GetStreamByIDPipeline = Literal["streamdiffusion",]


GetStreamByIDModelIDEnum = Literal[
    "Lykon/dreamshaper-8",
    "prompthero/openjourney-v4",
]
r"""Model to use for generation"""


GetStreamByIDPrompt5TypedDict = TypeAliasType(
    "GetStreamByIDPrompt5TypedDict", Union[str, float]
)


GetStreamByIDPrompt5 = TypeAliasType("GetStreamByIDPrompt5", Union[str, float])


GetStreamByIDPrompt6TypedDict = TypeAliasType(
    "GetStreamByIDPrompt6TypedDict",
    Union[str, List[List[GetStreamByIDPrompt5TypedDict]]],
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


GetStreamByIDPrompt6 = TypeAliasType(
    "GetStreamByIDPrompt6", Union[str, List[List[GetStreamByIDPrompt5]]]
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


GetStreamByIDPromptInterpolationMethod3 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""


class GetStreamByIDLoraDict3TypedDict(TypedDict):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


class GetStreamByIDLoraDict3(BaseModel):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


GetStreamByIDAcceleration3 = Literal[
    "none",
    "xformers",
    "tensorrt",
]
r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""


GetStreamByIDSeed5TypedDict = TypeAliasType(
    "GetStreamByIDSeed5TypedDict", Union[int, float]
)


GetStreamByIDSeed5 = TypeAliasType("GetStreamByIDSeed5", Union[int, float])


GetStreamByIDSeed6TypedDict = TypeAliasType(
    "GetStreamByIDSeed6TypedDict", Union[int, List[List[GetStreamByIDSeed5TypedDict]]]
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


GetStreamByIDSeed6 = TypeAliasType(
    "GetStreamByIDSeed6", Union[int, List[List[GetStreamByIDSeed5]]]
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


GetStreamByIDSeedInterpolationMethod3 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""


GetStreamByIDImagePreprocessingType3 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class GetStreamByIDImagePreprocessingProcessor3TypedDict(TypedDict):
    type: GetStreamByIDImagePreprocessingType3
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class GetStreamByIDImagePreprocessingProcessor3(BaseModel):
    type: GetStreamByIDImagePreprocessingType3

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class GetStreamByIDImagePreprocessing3TypedDict(TypedDict):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[GetStreamByIDImagePreprocessingProcessor3TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class GetStreamByIDImagePreprocessing3(BaseModel):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[GetStreamByIDImagePreprocessingProcessor3]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


GetStreamByIDImagePostprocessingType3 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class GetStreamByIDImagePostprocessingProcessor3TypedDict(TypedDict):
    type: GetStreamByIDImagePostprocessingType3
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class GetStreamByIDImagePostprocessingProcessor3(BaseModel):
    type: GetStreamByIDImagePostprocessingType3

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class GetStreamByIDImagePostprocessing3TypedDict(TypedDict):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[GetStreamByIDImagePostprocessingProcessor3TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class GetStreamByIDImagePostprocessing3(BaseModel):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[GetStreamByIDImagePostprocessingProcessor3]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


GetStreamByIDLatentPreprocessingType3 = Literal["latent_feedback",]


class GetStreamByIDLatentPreprocessingProcessor3TypedDict(TypedDict):
    type: GetStreamByIDLatentPreprocessingType3
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class GetStreamByIDLatentPreprocessingProcessor3(BaseModel):
    type: GetStreamByIDLatentPreprocessingType3

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class GetStreamByIDLatentPreprocessing3TypedDict(TypedDict):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[GetStreamByIDLatentPreprocessingProcessor3TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class GetStreamByIDLatentPreprocessing3(BaseModel):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[GetStreamByIDLatentPreprocessingProcessor3]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


GetStreamByIDLatentPostprocessingType3 = Literal["latent_feedback",]


class GetStreamByIDLatentPostprocessingProcessor3TypedDict(TypedDict):
    type: GetStreamByIDLatentPostprocessingType3
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class GetStreamByIDLatentPostprocessingProcessor3(BaseModel):
    type: GetStreamByIDLatentPostprocessingType3

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class GetStreamByIDLatentPostprocessing3TypedDict(TypedDict):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[GetStreamByIDLatentPostprocessingProcessor3TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class GetStreamByIDLatentPostprocessing3(BaseModel):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[GetStreamByIDLatentPostprocessingProcessor3]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


GetStreamByIDControlnetModelID3 = Literal[
    "lllyasviel/control_v11f1p_sd15_depth",
    "lllyasviel/control_v11f1e_sd15_tile",
    "lllyasviel/control_v11p_sd15_canny",
    "daydreamlive/TemporalNet2-stable-diffusion-v1-5",
]
r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""


GetStreamByIDPreprocessor3 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]
r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""


class GetStreamByIDControlnet3TypedDict(TypedDict):
    model_id: GetStreamByIDControlnetModelID3
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""
    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""
    preprocessor: GetStreamByIDPreprocessor3
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""
    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""
    preprocessor_params: NotRequired[Dict[str, Any]]
    r"""Preprocessor parameters"""
    control_guidance_start: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""
    control_guidance_end: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


class GetStreamByIDControlnet3(BaseModel):
    model_id: GetStreamByIDControlnetModelID3
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""

    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""

    preprocessor: GetStreamByIDPreprocessor3
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""

    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""

    preprocessor_params: Optional[Dict[str, Any]] = None
    r"""Preprocessor parameters"""

    control_guidance_start: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""

    control_guidance_end: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


GetStreamByIDTypeRegular = Literal["regular",]
r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""


GetStreamByIDWeightTypeRegular = Literal[
    "linear",
    "ease in",
    "ease out",
    "ease in-out",
    "reverse in-out",
    "weak input",
    "weak output",
    "weak middle",
    "strong middle",
    "style transfer",
    "composition",
    "strong style transfer",
    "style and composition",
    "style transfer precise",
    "composition precise",
]
r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class GetStreamByIDIPAdapterRegularTypedDict(TypedDict):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    type: NotRequired[GetStreamByIDTypeRegular]
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""
    enabled: NotRequired[bool]
    r"""Whether IP adapter is enabled"""
    scale: NotRequired[float]
    r"""Strength of IP adapter style conditioning"""
    weight_type: NotRequired[GetStreamByIDWeightTypeRegular]
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class GetStreamByIDIPAdapterRegular(BaseModel):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    type: Optional[GetStreamByIDTypeRegular] = "regular"
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""

    enabled: Optional[bool] = None
    r"""Whether IP adapter is enabled"""

    scale: Optional[float] = None
    r"""Strength of IP adapter style conditioning"""

    weight_type: Optional[GetStreamByIDWeightTypeRegular] = None
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class GetStreamByIDCachedAttention2TypedDict(TypedDict):
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    enabled: bool
    r"""Whether this cached attention is active. Enable cached attention to reuse key/value tensors across frames."""
    max_frames: int
    r"""Number of frames retained in the attention cache. Number of historical K/V frames to retain. Limited by TensorRT engine exports."""
    interval: int
    r"""Cadence (number of frames) for refreshing cached key/value tensors i.e How often (in number of frames) to refresh the cache.
    It is now **frame-based** (not seconds). It accepts integers 1-240, representing how many frames elapse between cache refreshes.

    """


class GetStreamByIDCachedAttention2(BaseModel):
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    enabled: bool
    r"""Whether this cached attention is active. Enable cached attention to reuse key/value tensors across frames."""

    max_frames: int
    r"""Number of frames retained in the attention cache. Number of historical K/V frames to retain. Limited by TensorRT engine exports."""

    interval: int
    r"""Cadence (number of frames) for refreshing cached key/value tensors i.e How often (in number of frames) to refresh the cache.
    It is now **frame-based** (not seconds). It accepts integers 1-240, representing how many frames elapse between cache refreshes.

    """


class GetStreamByIDSd15TypedDict(TypedDict):
    model_id: GetStreamByIDModelIDEnum
    r"""Model to use for generation"""
    prompt: NotRequired[GetStreamByIDPrompt6TypedDict]
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""
    prompt_interpolation_method: NotRequired[GetStreamByIDPromptInterpolationMethod3]
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""
    normalize_prompt_weights: NotRequired[bool]
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""
    normalize_seed_weights: NotRequired[bool]
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""
    negative_prompt: NotRequired[str]
    r"""Text describing what to avoid in the generated image."""
    guidance_scale: NotRequired[float]
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""
    delta: NotRequired[float]
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""
    num_inference_steps: NotRequired[int]
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""
    t_index_list: NotRequired[List[int]]
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """
    use_safety_checker: NotRequired[bool]
    r"""Whether to use safety checker for content filtering"""
    width: NotRequired[int]
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""
    height: NotRequired[int]
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""
    lora_dict: NotRequired[Nullable[GetStreamByIDLoraDict3TypedDict]]
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""
    use_lcm_lora: NotRequired[bool]
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""
    lcm_lora_id: NotRequired[str]
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """
    acceleration: NotRequired[GetStreamByIDAcceleration3]
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""
    use_denoising_batch: NotRequired[bool]
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""
    do_add_noise: NotRequired[bool]
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""
    seed: NotRequired[GetStreamByIDSeed6TypedDict]
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""
    seed_interpolation_method: NotRequired[GetStreamByIDSeedInterpolationMethod3]
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""
    enable_similar_image_filter: NotRequired[bool]
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""
    similar_image_filter_threshold: NotRequired[float]
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""
    similar_image_filter_max_skip_frame: NotRequired[int]
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""
    skip_diffusion: NotRequired[bool]
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """
    image_preprocessing: NotRequired[GetStreamByIDImagePreprocessing3TypedDict]
    r"""List of image preprocessor configurations for image processing"""
    image_postprocessing: NotRequired[GetStreamByIDImagePostprocessing3TypedDict]
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """
    latent_preprocessing: NotRequired[GetStreamByIDLatentPreprocessing3TypedDict]
    r"""List of latent preprocessor configurations for latent processing"""
    latent_postprocessing: NotRequired[GetStreamByIDLatentPostprocessing3TypedDict]
    r"""List of latent postprocessor configurations for latent processing"""
    controlnets: NotRequired[List[GetStreamByIDControlnet3TypedDict]]
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""
    ip_adapter: NotRequired[GetStreamByIDIPAdapterRegularTypedDict]
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""
    ip_adapter_style_image_url: NotRequired[str]
    r"""HTTPS URL or base64-encoded data URI (data:image/[type];base64,...) of the style image to use. Base64 images are automatically cached and uploaded to storage with content-based deduplication. Maximum size: 5MB. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""
    cached_attention: NotRequired[GetStreamByIDCachedAttention2TypedDict]
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""


class GetStreamByIDSd15(BaseModel):
    model_id: GetStreamByIDModelIDEnum
    r"""Model to use for generation"""

    prompt: Optional[GetStreamByIDPrompt6] = None
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""

    prompt_interpolation_method: Optional[GetStreamByIDPromptInterpolationMethod3] = (
        None
    )
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""

    normalize_prompt_weights: Optional[bool] = None
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""

    normalize_seed_weights: Optional[bool] = None
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""

    negative_prompt: Optional[str] = None
    r"""Text describing what to avoid in the generated image."""

    guidance_scale: Optional[float] = None
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""

    delta: Optional[float] = None
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""

    num_inference_steps: Optional[int] = None
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""

    t_index_list: Optional[List[int]] = None
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """

    use_safety_checker: Optional[bool] = None
    r"""Whether to use safety checker for content filtering"""

    width: Optional[int] = None
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""

    height: Optional[int] = None
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""

    lora_dict: OptionalNullable[GetStreamByIDLoraDict3] = UNSET
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""

    use_lcm_lora: Optional[bool] = None
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""

    lcm_lora_id: Optional[str] = None
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """

    acceleration: Optional[GetStreamByIDAcceleration3] = None
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""

    use_denoising_batch: Optional[bool] = None
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""

    do_add_noise: Optional[bool] = None
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""

    seed: Optional[GetStreamByIDSeed6] = None
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""

    seed_interpolation_method: Optional[GetStreamByIDSeedInterpolationMethod3] = None
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""

    enable_similar_image_filter: Optional[bool] = None
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""

    similar_image_filter_threshold: Optional[float] = None
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""

    similar_image_filter_max_skip_frame: Optional[int] = None
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""

    skip_diffusion: Optional[bool] = None
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """

    image_preprocessing: Optional[GetStreamByIDImagePreprocessing3] = None
    r"""List of image preprocessor configurations for image processing"""

    image_postprocessing: Optional[GetStreamByIDImagePostprocessing3] = None
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    latent_preprocessing: Optional[GetStreamByIDLatentPreprocessing3] = None
    r"""List of latent preprocessor configurations for latent processing"""

    latent_postprocessing: Optional[GetStreamByIDLatentPostprocessing3] = None
    r"""List of latent postprocessor configurations for latent processing"""

    controlnets: Optional[List[GetStreamByIDControlnet3]] = None
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""

    ip_adapter: Optional[GetStreamByIDIPAdapterRegular] = None
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    ip_adapter_style_image_url: Optional[str] = None
    r"""HTTPS URL or base64-encoded data URI (data:image/[type];base64,...) of the style image to use. Base64 images are automatically cached and uploaded to storage with content-based deduplication. Maximum size: 5MB. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""

    cached_attention: Optional[GetStreamByIDCachedAttention2] = None
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "prompt",
            "prompt_interpolation_method",
            "normalize_prompt_weights",
            "normalize_seed_weights",
            "negative_prompt",
            "guidance_scale",
            "delta",
            "num_inference_steps",
            "t_index_list",
            "use_safety_checker",
            "width",
            "height",
            "lora_dict",
            "use_lcm_lora",
            "lcm_lora_id",
            "acceleration",
            "use_denoising_batch",
            "do_add_noise",
            "seed",
            "seed_interpolation_method",
            "enable_similar_image_filter",
            "similar_image_filter_threshold",
            "similar_image_filter_max_skip_frame",
            "skip_diffusion",
            "image_preprocessing",
            "image_postprocessing",
            "latent_preprocessing",
            "latent_postprocessing",
            "controlnets",
            "ip_adapter",
            "ip_adapter_style_image_url",
            "cached_attention",
        ]
        nullable_fields = ["lora_dict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


GetStreamByIDPrompt3TypedDict = TypeAliasType(
    "GetStreamByIDPrompt3TypedDict", Union[str, float]
)


GetStreamByIDPrompt3 = TypeAliasType("GetStreamByIDPrompt3", Union[str, float])


GetStreamByIDPrompt4TypedDict = TypeAliasType(
    "GetStreamByIDPrompt4TypedDict",
    Union[str, List[List[GetStreamByIDPrompt3TypedDict]]],
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


GetStreamByIDPrompt4 = TypeAliasType(
    "GetStreamByIDPrompt4", Union[str, List[List[GetStreamByIDPrompt3]]]
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


GetStreamByIDPromptInterpolationMethod2 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""


class GetStreamByIDLoraDict2TypedDict(TypedDict):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


class GetStreamByIDLoraDict2(BaseModel):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


GetStreamByIDAcceleration2 = Literal[
    "none",
    "xformers",
    "tensorrt",
]
r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""


GetStreamByIDSeed3TypedDict = TypeAliasType(
    "GetStreamByIDSeed3TypedDict", Union[int, float]
)


GetStreamByIDSeed3 = TypeAliasType("GetStreamByIDSeed3", Union[int, float])


GetStreamByIDSeed4TypedDict = TypeAliasType(
    "GetStreamByIDSeed4TypedDict", Union[int, List[List[GetStreamByIDSeed3TypedDict]]]
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


GetStreamByIDSeed4 = TypeAliasType(
    "GetStreamByIDSeed4", Union[int, List[List[GetStreamByIDSeed3]]]
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


GetStreamByIDSeedInterpolationMethod2 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""


GetStreamByIDImagePreprocessingType2 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class GetStreamByIDImagePreprocessingProcessor2TypedDict(TypedDict):
    type: GetStreamByIDImagePreprocessingType2
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class GetStreamByIDImagePreprocessingProcessor2(BaseModel):
    type: GetStreamByIDImagePreprocessingType2

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class GetStreamByIDImagePreprocessing2TypedDict(TypedDict):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[GetStreamByIDImagePreprocessingProcessor2TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class GetStreamByIDImagePreprocessing2(BaseModel):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[GetStreamByIDImagePreprocessingProcessor2]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


GetStreamByIDImagePostprocessingType2 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class GetStreamByIDImagePostprocessingProcessor2TypedDict(TypedDict):
    type: GetStreamByIDImagePostprocessingType2
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class GetStreamByIDImagePostprocessingProcessor2(BaseModel):
    type: GetStreamByIDImagePostprocessingType2

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class GetStreamByIDImagePostprocessing2TypedDict(TypedDict):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[GetStreamByIDImagePostprocessingProcessor2TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class GetStreamByIDImagePostprocessing2(BaseModel):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[GetStreamByIDImagePostprocessingProcessor2]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


GetStreamByIDLatentPreprocessingType2 = Literal["latent_feedback",]


class GetStreamByIDLatentPreprocessingProcessor2TypedDict(TypedDict):
    type: GetStreamByIDLatentPreprocessingType2
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class GetStreamByIDLatentPreprocessingProcessor2(BaseModel):
    type: GetStreamByIDLatentPreprocessingType2

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class GetStreamByIDLatentPreprocessing2TypedDict(TypedDict):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[GetStreamByIDLatentPreprocessingProcessor2TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class GetStreamByIDLatentPreprocessing2(BaseModel):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[GetStreamByIDLatentPreprocessingProcessor2]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


GetStreamByIDLatentPostprocessingType2 = Literal["latent_feedback",]


class GetStreamByIDLatentPostprocessingProcessor2TypedDict(TypedDict):
    type: GetStreamByIDLatentPostprocessingType2
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class GetStreamByIDLatentPostprocessingProcessor2(BaseModel):
    type: GetStreamByIDLatentPostprocessingType2

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class GetStreamByIDLatentPostprocessing2TypedDict(TypedDict):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[GetStreamByIDLatentPostprocessingProcessor2TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class GetStreamByIDLatentPostprocessing2(BaseModel):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[GetStreamByIDLatentPostprocessingProcessor2]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


GetStreamByIDControlnetModelID2 = Literal[
    "xinsir/controlnet-depth-sdxl-1.0",
    "xinsir/controlnet-canny-sdxl-1.0",
    "xinsir/controlnet-tile-sdxl-1.0",
]
r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""


GetStreamByIDPreprocessor2 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]
r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""


class GetStreamByIDControlnet2TypedDict(TypedDict):
    model_id: GetStreamByIDControlnetModelID2
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""
    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""
    preprocessor: GetStreamByIDPreprocessor2
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""
    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""
    preprocessor_params: NotRequired[Dict[str, Any]]
    r"""Preprocessor parameters"""
    control_guidance_start: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""
    control_guidance_end: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


class GetStreamByIDControlnet2(BaseModel):
    model_id: GetStreamByIDControlnetModelID2
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""

    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""

    preprocessor: GetStreamByIDPreprocessor2
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""

    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""

    preprocessor_params: Optional[Dict[str, Any]] = None
    r"""Preprocessor parameters"""

    control_guidance_start: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""

    control_guidance_end: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


GetStreamByIDIPAdapterType = Literal[
    "regular",
    "faceid",
]
r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""


GetStreamByIDIPAdapterWeightType = Literal[
    "linear",
    "ease in",
    "ease out",
    "ease in-out",
    "reverse in-out",
    "weak input",
    "weak output",
    "weak middle",
    "strong middle",
    "style transfer",
    "composition",
    "strong style transfer",
    "style and composition",
    "style transfer precise",
    "composition precise",
]
r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class GetStreamByIDIPAdapterTypedDict(TypedDict):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    type: NotRequired[GetStreamByIDIPAdapterType]
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""
    enabled: NotRequired[bool]
    r"""Whether IP adapter is enabled"""
    scale: NotRequired[float]
    r"""Strength of IP adapter style conditioning"""
    weight_type: NotRequired[GetStreamByIDIPAdapterWeightType]
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class GetStreamByIDIPAdapter(BaseModel):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    type: Optional[GetStreamByIDIPAdapterType] = "regular"
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""

    enabled: Optional[bool] = None
    r"""Whether IP adapter is enabled"""

    scale: Optional[float] = None
    r"""Strength of IP adapter style conditioning"""

    weight_type: Optional[GetStreamByIDIPAdapterWeightType] = None
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class GetStreamByIDCachedAttention1TypedDict(TypedDict):
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    enabled: bool
    r"""Whether this cached attention is active. Enable cached attention to reuse key/value tensors across frames."""
    max_frames: int
    r"""Number of frames retained in the attention cache. Number of historical K/V frames to retain. Limited by TensorRT engine exports."""
    interval: int
    r"""Cadence (number of frames) for refreshing cached key/value tensors i.e How often (in number of frames) to refresh the cache.
    It is now **frame-based** (not seconds). It accepts integers 1-240, representing how many frames elapse between cache refreshes.

    """


class GetStreamByIDCachedAttention1(BaseModel):
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    enabled: bool
    r"""Whether this cached attention is active. Enable cached attention to reuse key/value tensors across frames."""

    max_frames: int
    r"""Number of frames retained in the attention cache. Number of historical K/V frames to retain. Limited by TensorRT engine exports."""

    interval: int
    r"""Cadence (number of frames) for refreshing cached key/value tensors i.e How often (in number of frames) to refresh the cache.
    It is now **frame-based** (not seconds). It accepts integers 1-240, representing how many frames elapse between cache refreshes.

    """


class GetStreamByIDSDXLTypedDict(TypedDict):
    model_id: Literal["stabilityai/sdxl-turbo"]
    prompt: NotRequired[GetStreamByIDPrompt4TypedDict]
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""
    prompt_interpolation_method: NotRequired[GetStreamByIDPromptInterpolationMethod2]
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""
    normalize_prompt_weights: NotRequired[bool]
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""
    normalize_seed_weights: NotRequired[bool]
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""
    negative_prompt: NotRequired[str]
    r"""Text describing what to avoid in the generated image."""
    guidance_scale: NotRequired[float]
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""
    delta: NotRequired[float]
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""
    num_inference_steps: NotRequired[int]
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""
    t_index_list: NotRequired[List[int]]
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """
    use_safety_checker: NotRequired[bool]
    r"""Whether to use safety checker for content filtering"""
    width: NotRequired[int]
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""
    height: NotRequired[int]
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""
    lora_dict: NotRequired[Nullable[GetStreamByIDLoraDict2TypedDict]]
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""
    use_lcm_lora: NotRequired[bool]
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""
    lcm_lora_id: NotRequired[str]
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """
    acceleration: NotRequired[GetStreamByIDAcceleration2]
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""
    use_denoising_batch: NotRequired[bool]
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""
    do_add_noise: NotRequired[bool]
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""
    seed: NotRequired[GetStreamByIDSeed4TypedDict]
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""
    seed_interpolation_method: NotRequired[GetStreamByIDSeedInterpolationMethod2]
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""
    enable_similar_image_filter: NotRequired[bool]
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""
    similar_image_filter_threshold: NotRequired[float]
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""
    similar_image_filter_max_skip_frame: NotRequired[int]
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""
    skip_diffusion: NotRequired[bool]
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """
    image_preprocessing: NotRequired[GetStreamByIDImagePreprocessing2TypedDict]
    r"""List of image preprocessor configurations for image processing"""
    image_postprocessing: NotRequired[GetStreamByIDImagePostprocessing2TypedDict]
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """
    latent_preprocessing: NotRequired[GetStreamByIDLatentPreprocessing2TypedDict]
    r"""List of latent preprocessor configurations for latent processing"""
    latent_postprocessing: NotRequired[GetStreamByIDLatentPostprocessing2TypedDict]
    r"""List of latent postprocessor configurations for latent processing"""
    controlnets: NotRequired[List[GetStreamByIDControlnet2TypedDict]]
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""
    ip_adapter: NotRequired[GetStreamByIDIPAdapterTypedDict]
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""
    ip_adapter_style_image_url: NotRequired[str]
    r"""HTTPS URL or base64-encoded data URI (data:image/[type];base64,...) of the style image to use. Base64 images are automatically cached and uploaded to storage with content-based deduplication. Maximum size: 5MB. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""
    cached_attention: NotRequired[GetStreamByIDCachedAttention1TypedDict]
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""


class GetStreamByIDSDXL(BaseModel):
    model_id: Annotated[
        Annotated[
            Literal["stabilityai/sdxl-turbo"],
            AfterValidator(validate_const("stabilityai/sdxl-turbo")),
        ],
        pydantic.Field(alias="model_id"),
    ] = "stabilityai/sdxl-turbo"

    prompt: Optional[GetStreamByIDPrompt4] = None
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""

    prompt_interpolation_method: Optional[GetStreamByIDPromptInterpolationMethod2] = (
        None
    )
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""

    normalize_prompt_weights: Optional[bool] = None
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""

    normalize_seed_weights: Optional[bool] = None
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""

    negative_prompt: Optional[str] = None
    r"""Text describing what to avoid in the generated image."""

    guidance_scale: Optional[float] = None
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""

    delta: Optional[float] = None
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""

    num_inference_steps: Optional[int] = None
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""

    t_index_list: Optional[List[int]] = None
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """

    use_safety_checker: Optional[bool] = None
    r"""Whether to use safety checker for content filtering"""

    width: Optional[int] = None
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""

    height: Optional[int] = None
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""

    lora_dict: OptionalNullable[GetStreamByIDLoraDict2] = UNSET
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""

    use_lcm_lora: Optional[bool] = None
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""

    lcm_lora_id: Optional[str] = None
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """

    acceleration: Optional[GetStreamByIDAcceleration2] = None
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""

    use_denoising_batch: Optional[bool] = None
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""

    do_add_noise: Optional[bool] = None
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""

    seed: Optional[GetStreamByIDSeed4] = None
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""

    seed_interpolation_method: Optional[GetStreamByIDSeedInterpolationMethod2] = None
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""

    enable_similar_image_filter: Optional[bool] = None
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""

    similar_image_filter_threshold: Optional[float] = None
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""

    similar_image_filter_max_skip_frame: Optional[int] = None
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""

    skip_diffusion: Optional[bool] = None
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """

    image_preprocessing: Optional[GetStreamByIDImagePreprocessing2] = None
    r"""List of image preprocessor configurations for image processing"""

    image_postprocessing: Optional[GetStreamByIDImagePostprocessing2] = None
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    latent_preprocessing: Optional[GetStreamByIDLatentPreprocessing2] = None
    r"""List of latent preprocessor configurations for latent processing"""

    latent_postprocessing: Optional[GetStreamByIDLatentPostprocessing2] = None
    r"""List of latent postprocessor configurations for latent processing"""

    controlnets: Optional[List[GetStreamByIDControlnet2]] = None
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""

    ip_adapter: Optional[GetStreamByIDIPAdapter] = None
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    ip_adapter_style_image_url: Optional[str] = None
    r"""HTTPS URL or base64-encoded data URI (data:image/[type];base64,...) of the style image to use. Base64 images are automatically cached and uploaded to storage with content-based deduplication. Maximum size: 5MB. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""

    cached_attention: Optional[GetStreamByIDCachedAttention1] = None
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "prompt",
            "prompt_interpolation_method",
            "normalize_prompt_weights",
            "normalize_seed_weights",
            "negative_prompt",
            "guidance_scale",
            "delta",
            "num_inference_steps",
            "t_index_list",
            "use_safety_checker",
            "width",
            "height",
            "lora_dict",
            "use_lcm_lora",
            "lcm_lora_id",
            "acceleration",
            "use_denoising_batch",
            "do_add_noise",
            "seed",
            "seed_interpolation_method",
            "enable_similar_image_filter",
            "similar_image_filter_threshold",
            "similar_image_filter_max_skip_frame",
            "skip_diffusion",
            "image_preprocessing",
            "image_postprocessing",
            "latent_preprocessing",
            "latent_postprocessing",
            "controlnets",
            "ip_adapter",
            "ip_adapter_style_image_url",
            "cached_attention",
        ]
        nullable_fields = ["lora_dict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


GetStreamByIDPrompt1TypedDict = TypeAliasType(
    "GetStreamByIDPrompt1TypedDict", Union[str, float]
)


GetStreamByIDPrompt1 = TypeAliasType("GetStreamByIDPrompt1", Union[str, float])


GetStreamByIDPrompt2TypedDict = TypeAliasType(
    "GetStreamByIDPrompt2TypedDict",
    Union[str, List[List[GetStreamByIDPrompt1TypedDict]]],
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


GetStreamByIDPrompt2 = TypeAliasType(
    "GetStreamByIDPrompt2", Union[str, List[List[GetStreamByIDPrompt1]]]
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


GetStreamByIDPromptInterpolationMethod1 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""


class GetStreamByIDLoraDict1TypedDict(TypedDict):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


class GetStreamByIDLoraDict1(BaseModel):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


GetStreamByIDAcceleration1 = Literal[
    "none",
    "xformers",
    "tensorrt",
]
r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""


GetStreamByIDSeed1TypedDict = TypeAliasType(
    "GetStreamByIDSeed1TypedDict", Union[int, float]
)


GetStreamByIDSeed1 = TypeAliasType("GetStreamByIDSeed1", Union[int, float])


GetStreamByIDSeed2TypedDict = TypeAliasType(
    "GetStreamByIDSeed2TypedDict", Union[int, List[List[GetStreamByIDSeed1TypedDict]]]
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


GetStreamByIDSeed2 = TypeAliasType(
    "GetStreamByIDSeed2", Union[int, List[List[GetStreamByIDSeed1]]]
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


GetStreamByIDSeedInterpolationMethod1 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""


GetStreamByIDImagePreprocessingType1 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class GetStreamByIDImagePreprocessingProcessor1TypedDict(TypedDict):
    type: GetStreamByIDImagePreprocessingType1
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class GetStreamByIDImagePreprocessingProcessor1(BaseModel):
    type: GetStreamByIDImagePreprocessingType1

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class GetStreamByIDImagePreprocessing1TypedDict(TypedDict):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[GetStreamByIDImagePreprocessingProcessor1TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class GetStreamByIDImagePreprocessing1(BaseModel):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[GetStreamByIDImagePreprocessingProcessor1]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


GetStreamByIDImagePostprocessingType1 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class GetStreamByIDImagePostprocessingProcessor1TypedDict(TypedDict):
    type: GetStreamByIDImagePostprocessingType1
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class GetStreamByIDImagePostprocessingProcessor1(BaseModel):
    type: GetStreamByIDImagePostprocessingType1

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class GetStreamByIDImagePostprocessing1TypedDict(TypedDict):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[GetStreamByIDImagePostprocessingProcessor1TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class GetStreamByIDImagePostprocessing1(BaseModel):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[GetStreamByIDImagePostprocessingProcessor1]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


GetStreamByIDLatentPreprocessingType1 = Literal["latent_feedback",]


class GetStreamByIDLatentPreprocessingProcessor1TypedDict(TypedDict):
    type: GetStreamByIDLatentPreprocessingType1
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class GetStreamByIDLatentPreprocessingProcessor1(BaseModel):
    type: GetStreamByIDLatentPreprocessingType1

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class GetStreamByIDLatentPreprocessing1TypedDict(TypedDict):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[GetStreamByIDLatentPreprocessingProcessor1TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class GetStreamByIDLatentPreprocessing1(BaseModel):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[GetStreamByIDLatentPreprocessingProcessor1]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


GetStreamByIDLatentPostprocessingType1 = Literal["latent_feedback",]


class GetStreamByIDLatentPostprocessingProcessor1TypedDict(TypedDict):
    type: GetStreamByIDLatentPostprocessingType1
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class GetStreamByIDLatentPostprocessingProcessor1(BaseModel):
    type: GetStreamByIDLatentPostprocessingType1

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class GetStreamByIDLatentPostprocessing1TypedDict(TypedDict):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[GetStreamByIDLatentPostprocessingProcessor1TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class GetStreamByIDLatentPostprocessing1(BaseModel):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[GetStreamByIDLatentPostprocessingProcessor1]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


GetStreamByIDControlnetModelID1 = Literal[
    "thibaud/controlnet-sd21-openpose-diffusers",
    "thibaud/controlnet-sd21-hed-diffusers",
    "thibaud/controlnet-sd21-canny-diffusers",
    "thibaud/controlnet-sd21-depth-diffusers",
    "thibaud/controlnet-sd21-color-diffusers",
    "daydreamlive/TemporalNet2-stable-diffusion-2-1",
]
r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""


GetStreamByIDPreprocessor1 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]
r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""


class GetStreamByIDControlnet1TypedDict(TypedDict):
    model_id: GetStreamByIDControlnetModelID1
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""
    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""
    preprocessor: GetStreamByIDPreprocessor1
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""
    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""
    preprocessor_params: NotRequired[Dict[str, Any]]
    r"""Additional parameters for the preprocessor. For example, canny edge detection uses 'low_threshold' and 'high_threshold' values."""
    control_guidance_start: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""
    control_guidance_end: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


class GetStreamByIDControlnet1(BaseModel):
    model_id: GetStreamByIDControlnetModelID1
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""

    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""

    preprocessor: GetStreamByIDPreprocessor1
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""

    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""

    preprocessor_params: Optional[Dict[str, Any]] = None
    r"""Additional parameters for the preprocessor. For example, canny edge detection uses 'low_threshold' and 'high_threshold' values."""

    control_guidance_start: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""

    control_guidance_end: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


class GetStreamByIDSDTurboTypedDict(TypedDict):
    model_id: Literal["stabilityai/sd-turbo"]
    prompt: NotRequired[GetStreamByIDPrompt2TypedDict]
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""
    prompt_interpolation_method: NotRequired[GetStreamByIDPromptInterpolationMethod1]
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""
    normalize_prompt_weights: NotRequired[bool]
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""
    normalize_seed_weights: NotRequired[bool]
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""
    negative_prompt: NotRequired[str]
    r"""Text describing what to avoid in the generated image."""
    guidance_scale: NotRequired[float]
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""
    delta: NotRequired[float]
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""
    num_inference_steps: NotRequired[int]
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""
    t_index_list: NotRequired[List[int]]
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """
    use_safety_checker: NotRequired[bool]
    r"""Whether to use safety checker for content filtering"""
    width: NotRequired[int]
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""
    height: NotRequired[int]
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""
    lora_dict: NotRequired[Nullable[GetStreamByIDLoraDict1TypedDict]]
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""
    use_lcm_lora: NotRequired[bool]
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""
    lcm_lora_id: NotRequired[str]
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """
    acceleration: NotRequired[GetStreamByIDAcceleration1]
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""
    use_denoising_batch: NotRequired[bool]
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""
    do_add_noise: NotRequired[bool]
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""
    seed: NotRequired[GetStreamByIDSeed2TypedDict]
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""
    seed_interpolation_method: NotRequired[GetStreamByIDSeedInterpolationMethod1]
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""
    enable_similar_image_filter: NotRequired[bool]
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""
    similar_image_filter_threshold: NotRequired[float]
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""
    similar_image_filter_max_skip_frame: NotRequired[int]
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""
    skip_diffusion: NotRequired[bool]
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """
    image_preprocessing: NotRequired[GetStreamByIDImagePreprocessing1TypedDict]
    r"""List of image preprocessor configurations for image processing"""
    image_postprocessing: NotRequired[GetStreamByIDImagePostprocessing1TypedDict]
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """
    latent_preprocessing: NotRequired[GetStreamByIDLatentPreprocessing1TypedDict]
    r"""List of latent preprocessor configurations for latent processing"""
    latent_postprocessing: NotRequired[GetStreamByIDLatentPostprocessing1TypedDict]
    r"""List of latent postprocessor configurations for latent processing"""
    controlnets: NotRequired[List[GetStreamByIDControlnet1TypedDict]]
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""


class GetStreamByIDSDTurbo(BaseModel):
    model_id: Annotated[
        Annotated[
            Literal["stabilityai/sd-turbo"],
            AfterValidator(validate_const("stabilityai/sd-turbo")),
        ],
        pydantic.Field(alias="model_id"),
    ] = "stabilityai/sd-turbo"

    prompt: Optional[GetStreamByIDPrompt2] = None
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""

    prompt_interpolation_method: Optional[GetStreamByIDPromptInterpolationMethod1] = (
        None
    )
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""

    normalize_prompt_weights: Optional[bool] = None
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""

    normalize_seed_weights: Optional[bool] = None
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""

    negative_prompt: Optional[str] = None
    r"""Text describing what to avoid in the generated image."""

    guidance_scale: Optional[float] = None
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""

    delta: Optional[float] = None
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""

    num_inference_steps: Optional[int] = None
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""

    t_index_list: Optional[List[int]] = None
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """

    use_safety_checker: Optional[bool] = None
    r"""Whether to use safety checker for content filtering"""

    width: Optional[int] = None
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""

    height: Optional[int] = None
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""

    lora_dict: OptionalNullable[GetStreamByIDLoraDict1] = UNSET
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""

    use_lcm_lora: Optional[bool] = None
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""

    lcm_lora_id: Optional[str] = None
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """

    acceleration: Optional[GetStreamByIDAcceleration1] = None
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""

    use_denoising_batch: Optional[bool] = None
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""

    do_add_noise: Optional[bool] = None
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""

    seed: Optional[GetStreamByIDSeed2] = None
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""

    seed_interpolation_method: Optional[GetStreamByIDSeedInterpolationMethod1] = None
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""

    enable_similar_image_filter: Optional[bool] = None
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""

    similar_image_filter_threshold: Optional[float] = None
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""

    similar_image_filter_max_skip_frame: Optional[int] = None
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""

    skip_diffusion: Optional[bool] = None
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """

    image_preprocessing: Optional[GetStreamByIDImagePreprocessing1] = None
    r"""List of image preprocessor configurations for image processing"""

    image_postprocessing: Optional[GetStreamByIDImagePostprocessing1] = None
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    latent_preprocessing: Optional[GetStreamByIDLatentPreprocessing1] = None
    r"""List of latent preprocessor configurations for latent processing"""

    latent_postprocessing: Optional[GetStreamByIDLatentPostprocessing1] = None
    r"""List of latent postprocessor configurations for latent processing"""

    controlnets: Optional[List[GetStreamByIDControlnet1]] = None
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "prompt",
            "prompt_interpolation_method",
            "normalize_prompt_weights",
            "normalize_seed_weights",
            "negative_prompt",
            "guidance_scale",
            "delta",
            "num_inference_steps",
            "t_index_list",
            "use_safety_checker",
            "width",
            "height",
            "lora_dict",
            "use_lcm_lora",
            "lcm_lora_id",
            "acceleration",
            "use_denoising_batch",
            "do_add_noise",
            "seed",
            "seed_interpolation_method",
            "enable_similar_image_filter",
            "similar_image_filter_threshold",
            "similar_image_filter_max_skip_frame",
            "skip_diffusion",
            "image_preprocessing",
            "image_postprocessing",
            "latent_preprocessing",
            "latent_postprocessing",
            "controlnets",
        ]
        nullable_fields = ["lora_dict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


GetStreamByIDParamsTypedDict = TypeAliasType(
    "GetStreamByIDParamsTypedDict",
    Union[
        GetStreamByIDSDTurboTypedDict,
        GetStreamByIDSDXLTypedDict,
        GetStreamByIDSd15TypedDict,
    ],
)


GetStreamByIDParams = Annotated[
    Union[
        Annotated[GetStreamByIDSDTurbo, Tag("stabilityai/sd-turbo")],
        Annotated[GetStreamByIDSDXL, Tag("stabilityai/sdxl-turbo")],
        Annotated[GetStreamByIDSd15, Tag("Lykon/dreamshaper-8")],
        Annotated[GetStreamByIDSd15, Tag("prompthero/openjourney-v4")],
    ],
    Discriminator(lambda m: get_discriminator(m, "model_id", "model_id")),
]


class GetStreamByIDResponseTypedDict(TypedDict):
    r"""Default Response"""

    pipeline: GetStreamByIDPipeline
    params: GetStreamByIDParamsTypedDict
    id: str
    r"""Unique identifier for the stream"""
    stream_key: str
    r"""Unique key used for streaming to this endpoint"""
    created_at: str
    r"""ISO timestamp when the stream was created"""
    output_playback_id: str
    r"""Playback ID for accessing the stream output"""
    name: str
    r"""Human-readable name of the stream"""
    author: str
    r"""ID of the user who created this stream"""
    from_playground: bool
    r"""Whether this stream was created from the playground interface"""
    gateway_host: str
    r"""Gateway server hostname handling this stream"""
    is_smoke_test: bool
    r"""Whether this is a smoke test stream"""
    whip_url: str
    r"""WebRTC WHIP URL for stream ingestion"""
    output_stream_url: NotRequired[str]
    r"""URL where the processed stream output can be accessed"""


class GetStreamByIDResponse(BaseModel):
    r"""Default Response"""

    pipeline: GetStreamByIDPipeline

    params: GetStreamByIDParams

    id: str
    r"""Unique identifier for the stream"""

    stream_key: str
    r"""Unique key used for streaming to this endpoint"""

    created_at: str
    r"""ISO timestamp when the stream was created"""

    output_playback_id: str
    r"""Playback ID for accessing the stream output"""

    name: str
    r"""Human-readable name of the stream"""

    author: str
    r"""ID of the user who created this stream"""

    from_playground: bool
    r"""Whether this stream was created from the playground interface"""

    gateway_host: str
    r"""Gateway server hostname handling this stream"""

    is_smoke_test: bool
    r"""Whether this is a smoke test stream"""

    whip_url: str
    r"""WebRTC WHIP URL for stream ingestion"""

    output_stream_url: Optional[str] = None
    r"""URL where the processed stream output can be accessed"""
