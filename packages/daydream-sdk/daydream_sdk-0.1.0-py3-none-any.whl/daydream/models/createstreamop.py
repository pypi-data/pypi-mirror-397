"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from daydream.types import BaseModel, Nullable, OptionalNullable, UNSET, UNSET_SENTINEL
from daydream.utils import get_discriminator, validate_const
import pydantic
from pydantic import Discriminator, Tag, model_serializer
from pydantic.functional_validators import AfterValidator
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


CreateStreamPipelineRequest = Literal["streamdiffusion",]


CreateStreamModelIDRequestEnum = Literal[
    "Lykon/dreamshaper-8",
    "prompthero/openjourney-v4",
]
r"""Model to use for generation"""


CreateStreamPromptRequest6TypedDict = TypeAliasType(
    "CreateStreamPromptRequest6TypedDict", Union[str, float]
)


CreateStreamPromptRequest6 = TypeAliasType(
    "CreateStreamPromptRequest6", Union[str, float]
)


CreateStreamPromptRequest5TypedDict = TypeAliasType(
    "CreateStreamPromptRequest5TypedDict",
    Union[str, List[List[CreateStreamPromptRequest6TypedDict]]],
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


CreateStreamPromptRequest5 = TypeAliasType(
    "CreateStreamPromptRequest5", Union[str, List[List[CreateStreamPromptRequest6]]]
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


CreateStreamPromptInterpolationMethodRequest3 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""


class CreateStreamLoraDictRequest3TypedDict(TypedDict):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


class CreateStreamLoraDictRequest3(BaseModel):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


CreateStreamAccelerationRequest3 = Literal[
    "none",
    "xformers",
    "tensorrt",
]
r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""


CreateStreamSeedRequest6TypedDict = TypeAliasType(
    "CreateStreamSeedRequest6TypedDict", Union[int, float]
)


CreateStreamSeedRequest6 = TypeAliasType("CreateStreamSeedRequest6", Union[int, float])


CreateStreamSeedRequest5TypedDict = TypeAliasType(
    "CreateStreamSeedRequest5TypedDict",
    Union[int, List[List[CreateStreamSeedRequest6TypedDict]]],
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


CreateStreamSeedRequest5 = TypeAliasType(
    "CreateStreamSeedRequest5", Union[int, List[List[CreateStreamSeedRequest6]]]
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


CreateStreamSeedInterpolationMethodRequest3 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""


CreateStreamImagePreprocessingTypeRequest3 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class CreateStreamImagePreprocessingProcessorRequest3TypedDict(TypedDict):
    type: CreateStreamImagePreprocessingTypeRequest3
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamImagePreprocessingProcessorRequest3(BaseModel):
    type: CreateStreamImagePreprocessingTypeRequest3

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamImagePreprocessingRequest3TypedDict(TypedDict):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[CreateStreamImagePreprocessingProcessorRequest3TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamImagePreprocessingRequest3(BaseModel):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[CreateStreamImagePreprocessingProcessorRequest3]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamImagePostprocessingTypeRequest3 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class CreateStreamImagePostprocessingProcessorRequest3TypedDict(TypedDict):
    type: CreateStreamImagePostprocessingTypeRequest3
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamImagePostprocessingProcessorRequest3(BaseModel):
    type: CreateStreamImagePostprocessingTypeRequest3

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamImagePostprocessingRequest3TypedDict(TypedDict):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[CreateStreamImagePostprocessingProcessorRequest3TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamImagePostprocessingRequest3(BaseModel):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[CreateStreamImagePostprocessingProcessorRequest3]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamLatentPreprocessingTypeRequest3 = Literal["latent_feedback",]


class CreateStreamLatentPreprocessingProcessorRequest3TypedDict(TypedDict):
    type: CreateStreamLatentPreprocessingTypeRequest3
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamLatentPreprocessingProcessorRequest3(BaseModel):
    type: CreateStreamLatentPreprocessingTypeRequest3

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamLatentPreprocessingRequest3TypedDict(TypedDict):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPreprocessingProcessorRequest3TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamLatentPreprocessingRequest3(BaseModel):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPreprocessingProcessorRequest3]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamLatentPostprocessingTypeRequest3 = Literal["latent_feedback",]


class CreateStreamLatentPostprocessingProcessorRequest3TypedDict(TypedDict):
    type: CreateStreamLatentPostprocessingTypeRequest3
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamLatentPostprocessingProcessorRequest3(BaseModel):
    type: CreateStreamLatentPostprocessingTypeRequest3

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamLatentPostprocessingRequest3TypedDict(TypedDict):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPostprocessingProcessorRequest3TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamLatentPostprocessingRequest3(BaseModel):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPostprocessingProcessorRequest3]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamControlnetModelIDRequest3 = Literal[
    "lllyasviel/control_v11f1p_sd15_depth",
    "lllyasviel/control_v11f1e_sd15_tile",
    "lllyasviel/control_v11p_sd15_canny",
    "daydreamlive/TemporalNet2-stable-diffusion-v1-5",
]
r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""


CreateStreamPreprocessorRequest3 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]
r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""


class CreateStreamControlnetRequest3TypedDict(TypedDict):
    model_id: CreateStreamControlnetModelIDRequest3
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""
    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""
    preprocessor: CreateStreamPreprocessorRequest3
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""
    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""
    preprocessor_params: NotRequired[Dict[str, Any]]
    r"""Preprocessor parameters"""
    control_guidance_start: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""
    control_guidance_end: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


class CreateStreamControlnetRequest3(BaseModel):
    model_id: CreateStreamControlnetModelIDRequest3
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""

    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""

    preprocessor: CreateStreamPreprocessorRequest3
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""

    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""

    preprocessor_params: Optional[Dict[str, Any]] = None
    r"""Preprocessor parameters"""

    control_guidance_start: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""

    control_guidance_end: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


CreateStreamTypeRegularRequest = Literal["regular",]
r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""


CreateStreamWeightTypeRegularRequest = Literal[
    "linear",
    "ease in",
    "ease out",
    "ease in-out",
    "reverse in-out",
    "weak input",
    "weak output",
    "weak middle",
    "strong middle",
    "style transfer",
    "composition",
    "strong style transfer",
    "style and composition",
    "style transfer precise",
    "composition precise",
]
r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class CreateStreamIPAdapterRegularRequestTypedDict(TypedDict):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    type: NotRequired[CreateStreamTypeRegularRequest]
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""
    enabled: NotRequired[bool]
    r"""Whether IP adapter is enabled"""
    scale: NotRequired[float]
    r"""Strength of IP adapter style conditioning"""
    weight_type: NotRequired[CreateStreamWeightTypeRegularRequest]
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class CreateStreamIPAdapterRegularRequest(BaseModel):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    type: Optional[CreateStreamTypeRegularRequest] = "regular"
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""

    enabled: Optional[bool] = None
    r"""Whether IP adapter is enabled"""

    scale: Optional[float] = None
    r"""Strength of IP adapter style conditioning"""

    weight_type: Optional[CreateStreamWeightTypeRegularRequest] = None
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class CreateStreamCachedAttentionRequest2TypedDict(TypedDict):
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    enabled: bool
    r"""Whether this cached attention is active. Enable cached attention to reuse key/value tensors across frames."""
    max_frames: int
    r"""Number of frames retained in the attention cache. Number of historical K/V frames to retain. Limited by TensorRT engine exports."""
    interval: int
    r"""Cadence (number of frames) for refreshing cached key/value tensors i.e How often (in number of frames) to refresh the cache.
    It is now **frame-based** (not seconds). It accepts integers 1-240, representing how many frames elapse between cache refreshes.

    """


class CreateStreamCachedAttentionRequest2(BaseModel):
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    enabled: bool
    r"""Whether this cached attention is active. Enable cached attention to reuse key/value tensors across frames."""

    max_frames: int
    r"""Number of frames retained in the attention cache. Number of historical K/V frames to retain. Limited by TensorRT engine exports."""

    interval: int
    r"""Cadence (number of frames) for refreshing cached key/value tensors i.e How often (in number of frames) to refresh the cache.
    It is now **frame-based** (not seconds). It accepts integers 1-240, representing how many frames elapse between cache refreshes.

    """


class CreateStreamSd15RequestTypedDict(TypedDict):
    model_id: CreateStreamModelIDRequestEnum
    r"""Model to use for generation"""
    prompt: NotRequired[CreateStreamPromptRequest5TypedDict]
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""
    prompt_interpolation_method: NotRequired[
        CreateStreamPromptInterpolationMethodRequest3
    ]
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""
    normalize_prompt_weights: NotRequired[bool]
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""
    normalize_seed_weights: NotRequired[bool]
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""
    negative_prompt: NotRequired[str]
    r"""Text describing what to avoid in the generated image."""
    guidance_scale: NotRequired[float]
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""
    delta: NotRequired[float]
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""
    num_inference_steps: NotRequired[int]
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""
    t_index_list: NotRequired[List[int]]
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """
    use_safety_checker: NotRequired[bool]
    r"""Whether to use safety checker for content filtering"""
    width: NotRequired[int]
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""
    height: NotRequired[int]
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""
    lora_dict: NotRequired[Nullable[CreateStreamLoraDictRequest3TypedDict]]
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""
    use_lcm_lora: NotRequired[bool]
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""
    lcm_lora_id: NotRequired[str]
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """
    acceleration: NotRequired[CreateStreamAccelerationRequest3]
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""
    use_denoising_batch: NotRequired[bool]
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""
    do_add_noise: NotRequired[bool]
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""
    seed: NotRequired[CreateStreamSeedRequest5TypedDict]
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""
    seed_interpolation_method: NotRequired[CreateStreamSeedInterpolationMethodRequest3]
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""
    enable_similar_image_filter: NotRequired[bool]
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""
    similar_image_filter_threshold: NotRequired[float]
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""
    similar_image_filter_max_skip_frame: NotRequired[int]
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""
    skip_diffusion: NotRequired[bool]
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """
    image_preprocessing: NotRequired[CreateStreamImagePreprocessingRequest3TypedDict]
    r"""List of image preprocessor configurations for image processing"""
    image_postprocessing: NotRequired[CreateStreamImagePostprocessingRequest3TypedDict]
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """
    latent_preprocessing: NotRequired[CreateStreamLatentPreprocessingRequest3TypedDict]
    r"""List of latent preprocessor configurations for latent processing"""
    latent_postprocessing: NotRequired[
        CreateStreamLatentPostprocessingRequest3TypedDict
    ]
    r"""List of latent postprocessor configurations for latent processing"""
    controlnets: NotRequired[List[CreateStreamControlnetRequest3TypedDict]]
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""
    ip_adapter: NotRequired[CreateStreamIPAdapterRegularRequestTypedDict]
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""
    ip_adapter_style_image_url: NotRequired[str]
    r"""HTTPS URL or base64-encoded data URI (data:image/[type];base64,...) of the style image to use. Base64 images are automatically cached and uploaded to storage with content-based deduplication. Maximum size: 5MB. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""
    cached_attention: NotRequired[CreateStreamCachedAttentionRequest2TypedDict]
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""


class CreateStreamSd15Request(BaseModel):
    model_id: CreateStreamModelIDRequestEnum
    r"""Model to use for generation"""

    prompt: Optional[CreateStreamPromptRequest5] = None
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""

    prompt_interpolation_method: Optional[
        CreateStreamPromptInterpolationMethodRequest3
    ] = None
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""

    normalize_prompt_weights: Optional[bool] = None
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""

    normalize_seed_weights: Optional[bool] = None
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""

    negative_prompt: Optional[str] = None
    r"""Text describing what to avoid in the generated image."""

    guidance_scale: Optional[float] = None
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""

    delta: Optional[float] = None
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""

    num_inference_steps: Optional[int] = None
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""

    t_index_list: Optional[List[int]] = None
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """

    use_safety_checker: Optional[bool] = None
    r"""Whether to use safety checker for content filtering"""

    width: Optional[int] = None
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""

    height: Optional[int] = None
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""

    lora_dict: OptionalNullable[CreateStreamLoraDictRequest3] = UNSET
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""

    use_lcm_lora: Optional[bool] = None
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""

    lcm_lora_id: Optional[str] = None
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """

    acceleration: Optional[CreateStreamAccelerationRequest3] = None
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""

    use_denoising_batch: Optional[bool] = None
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""

    do_add_noise: Optional[bool] = None
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""

    seed: Optional[CreateStreamSeedRequest5] = None
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""

    seed_interpolation_method: Optional[CreateStreamSeedInterpolationMethodRequest3] = (
        None
    )
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""

    enable_similar_image_filter: Optional[bool] = None
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""

    similar_image_filter_threshold: Optional[float] = None
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""

    similar_image_filter_max_skip_frame: Optional[int] = None
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""

    skip_diffusion: Optional[bool] = None
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """

    image_preprocessing: Optional[CreateStreamImagePreprocessingRequest3] = None
    r"""List of image preprocessor configurations for image processing"""

    image_postprocessing: Optional[CreateStreamImagePostprocessingRequest3] = None
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    latent_preprocessing: Optional[CreateStreamLatentPreprocessingRequest3] = None
    r"""List of latent preprocessor configurations for latent processing"""

    latent_postprocessing: Optional[CreateStreamLatentPostprocessingRequest3] = None
    r"""List of latent postprocessor configurations for latent processing"""

    controlnets: Optional[List[CreateStreamControlnetRequest3]] = None
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""

    ip_adapter: Optional[CreateStreamIPAdapterRegularRequest] = None
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    ip_adapter_style_image_url: Optional[str] = None
    r"""HTTPS URL or base64-encoded data URI (data:image/[type];base64,...) of the style image to use. Base64 images are automatically cached and uploaded to storage with content-based deduplication. Maximum size: 5MB. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""

    cached_attention: Optional[CreateStreamCachedAttentionRequest2] = None
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "prompt",
            "prompt_interpolation_method",
            "normalize_prompt_weights",
            "normalize_seed_weights",
            "negative_prompt",
            "guidance_scale",
            "delta",
            "num_inference_steps",
            "t_index_list",
            "use_safety_checker",
            "width",
            "height",
            "lora_dict",
            "use_lcm_lora",
            "lcm_lora_id",
            "acceleration",
            "use_denoising_batch",
            "do_add_noise",
            "seed",
            "seed_interpolation_method",
            "enable_similar_image_filter",
            "similar_image_filter_threshold",
            "similar_image_filter_max_skip_frame",
            "skip_diffusion",
            "image_preprocessing",
            "image_postprocessing",
            "latent_preprocessing",
            "latent_postprocessing",
            "controlnets",
            "ip_adapter",
            "ip_adapter_style_image_url",
            "cached_attention",
        ]
        nullable_fields = ["lora_dict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreateStreamPromptRequest4TypedDict = TypeAliasType(
    "CreateStreamPromptRequest4TypedDict", Union[str, float]
)


CreateStreamPromptRequest4 = TypeAliasType(
    "CreateStreamPromptRequest4", Union[str, float]
)


CreateStreamPromptRequest3TypedDict = TypeAliasType(
    "CreateStreamPromptRequest3TypedDict",
    Union[str, List[List[CreateStreamPromptRequest4TypedDict]]],
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


CreateStreamPromptRequest3 = TypeAliasType(
    "CreateStreamPromptRequest3", Union[str, List[List[CreateStreamPromptRequest4]]]
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


CreateStreamPromptInterpolationMethodRequest2 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""


class CreateStreamLoraDictRequest2TypedDict(TypedDict):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


class CreateStreamLoraDictRequest2(BaseModel):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


CreateStreamAccelerationRequest2 = Literal[
    "none",
    "xformers",
    "tensorrt",
]
r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""


CreateStreamSeedRequest4TypedDict = TypeAliasType(
    "CreateStreamSeedRequest4TypedDict", Union[int, float]
)


CreateStreamSeedRequest4 = TypeAliasType("CreateStreamSeedRequest4", Union[int, float])


CreateStreamSeedRequest3TypedDict = TypeAliasType(
    "CreateStreamSeedRequest3TypedDict",
    Union[int, List[List[CreateStreamSeedRequest4TypedDict]]],
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


CreateStreamSeedRequest3 = TypeAliasType(
    "CreateStreamSeedRequest3", Union[int, List[List[CreateStreamSeedRequest4]]]
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


CreateStreamSeedInterpolationMethodRequest2 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""


CreateStreamImagePreprocessingTypeRequest2 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class CreateStreamImagePreprocessingProcessorRequest2TypedDict(TypedDict):
    type: CreateStreamImagePreprocessingTypeRequest2
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamImagePreprocessingProcessorRequest2(BaseModel):
    type: CreateStreamImagePreprocessingTypeRequest2

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamImagePreprocessingRequest2TypedDict(TypedDict):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[CreateStreamImagePreprocessingProcessorRequest2TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamImagePreprocessingRequest2(BaseModel):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[CreateStreamImagePreprocessingProcessorRequest2]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamImagePostprocessingTypeRequest2 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class CreateStreamImagePostprocessingProcessorRequest2TypedDict(TypedDict):
    type: CreateStreamImagePostprocessingTypeRequest2
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamImagePostprocessingProcessorRequest2(BaseModel):
    type: CreateStreamImagePostprocessingTypeRequest2

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamImagePostprocessingRequest2TypedDict(TypedDict):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[CreateStreamImagePostprocessingProcessorRequest2TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamImagePostprocessingRequest2(BaseModel):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[CreateStreamImagePostprocessingProcessorRequest2]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamLatentPreprocessingTypeRequest2 = Literal["latent_feedback",]


class CreateStreamLatentPreprocessingProcessorRequest2TypedDict(TypedDict):
    type: CreateStreamLatentPreprocessingTypeRequest2
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamLatentPreprocessingProcessorRequest2(BaseModel):
    type: CreateStreamLatentPreprocessingTypeRequest2

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamLatentPreprocessingRequest2TypedDict(TypedDict):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPreprocessingProcessorRequest2TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamLatentPreprocessingRequest2(BaseModel):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPreprocessingProcessorRequest2]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamLatentPostprocessingTypeRequest2 = Literal["latent_feedback",]


class CreateStreamLatentPostprocessingProcessorRequest2TypedDict(TypedDict):
    type: CreateStreamLatentPostprocessingTypeRequest2
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamLatentPostprocessingProcessorRequest2(BaseModel):
    type: CreateStreamLatentPostprocessingTypeRequest2

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamLatentPostprocessingRequest2TypedDict(TypedDict):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPostprocessingProcessorRequest2TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamLatentPostprocessingRequest2(BaseModel):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPostprocessingProcessorRequest2]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamControlnetModelIDRequest2 = Literal[
    "xinsir/controlnet-depth-sdxl-1.0",
    "xinsir/controlnet-canny-sdxl-1.0",
    "xinsir/controlnet-tile-sdxl-1.0",
]
r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""


CreateStreamPreprocessorRequest2 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]
r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""


class CreateStreamControlnetRequest2TypedDict(TypedDict):
    model_id: CreateStreamControlnetModelIDRequest2
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""
    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""
    preprocessor: CreateStreamPreprocessorRequest2
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""
    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""
    preprocessor_params: NotRequired[Dict[str, Any]]
    r"""Preprocessor parameters"""
    control_guidance_start: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""
    control_guidance_end: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


class CreateStreamControlnetRequest2(BaseModel):
    model_id: CreateStreamControlnetModelIDRequest2
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""

    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""

    preprocessor: CreateStreamPreprocessorRequest2
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""

    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""

    preprocessor_params: Optional[Dict[str, Any]] = None
    r"""Preprocessor parameters"""

    control_guidance_start: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""

    control_guidance_end: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


CreateStreamIPAdapterTypeRequest = Literal[
    "regular",
    "faceid",
]
r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""


CreateStreamIPAdapterWeightTypeRequest = Literal[
    "linear",
    "ease in",
    "ease out",
    "ease in-out",
    "reverse in-out",
    "weak input",
    "weak output",
    "weak middle",
    "strong middle",
    "style transfer",
    "composition",
    "strong style transfer",
    "style and composition",
    "style transfer precise",
    "composition precise",
]
r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class CreateStreamIPAdapterRequestTypedDict(TypedDict):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    type: NotRequired[CreateStreamIPAdapterTypeRequest]
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""
    enabled: NotRequired[bool]
    r"""Whether IP adapter is enabled"""
    scale: NotRequired[float]
    r"""Strength of IP adapter style conditioning"""
    weight_type: NotRequired[CreateStreamIPAdapterWeightTypeRequest]
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class CreateStreamIPAdapterRequest(BaseModel):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    type: Optional[CreateStreamIPAdapterTypeRequest] = "regular"
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""

    enabled: Optional[bool] = None
    r"""Whether IP adapter is enabled"""

    scale: Optional[float] = None
    r"""Strength of IP adapter style conditioning"""

    weight_type: Optional[CreateStreamIPAdapterWeightTypeRequest] = None
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class CreateStreamCachedAttentionRequest1TypedDict(TypedDict):
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    enabled: bool
    r"""Whether this cached attention is active. Enable cached attention to reuse key/value tensors across frames."""
    max_frames: int
    r"""Number of frames retained in the attention cache. Number of historical K/V frames to retain. Limited by TensorRT engine exports."""
    interval: int
    r"""Cadence (number of frames) for refreshing cached key/value tensors i.e How often (in number of frames) to refresh the cache.
    It is now **frame-based** (not seconds). It accepts integers 1-240, representing how many frames elapse between cache refreshes.

    """


class CreateStreamCachedAttentionRequest1(BaseModel):
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    enabled: bool
    r"""Whether this cached attention is active. Enable cached attention to reuse key/value tensors across frames."""

    max_frames: int
    r"""Number of frames retained in the attention cache. Number of historical K/V frames to retain. Limited by TensorRT engine exports."""

    interval: int
    r"""Cadence (number of frames) for refreshing cached key/value tensors i.e How often (in number of frames) to refresh the cache.
    It is now **frame-based** (not seconds). It accepts integers 1-240, representing how many frames elapse between cache refreshes.

    """


class CreateStreamSDXLRequestTypedDict(TypedDict):
    model_id: Literal["stabilityai/sdxl-turbo"]
    prompt: NotRequired[CreateStreamPromptRequest3TypedDict]
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""
    prompt_interpolation_method: NotRequired[
        CreateStreamPromptInterpolationMethodRequest2
    ]
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""
    normalize_prompt_weights: NotRequired[bool]
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""
    normalize_seed_weights: NotRequired[bool]
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""
    negative_prompt: NotRequired[str]
    r"""Text describing what to avoid in the generated image."""
    guidance_scale: NotRequired[float]
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""
    delta: NotRequired[float]
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""
    num_inference_steps: NotRequired[int]
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""
    t_index_list: NotRequired[List[int]]
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """
    use_safety_checker: NotRequired[bool]
    r"""Whether to use safety checker for content filtering"""
    width: NotRequired[int]
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""
    height: NotRequired[int]
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""
    lora_dict: NotRequired[Nullable[CreateStreamLoraDictRequest2TypedDict]]
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""
    use_lcm_lora: NotRequired[bool]
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""
    lcm_lora_id: NotRequired[str]
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """
    acceleration: NotRequired[CreateStreamAccelerationRequest2]
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""
    use_denoising_batch: NotRequired[bool]
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""
    do_add_noise: NotRequired[bool]
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""
    seed: NotRequired[CreateStreamSeedRequest3TypedDict]
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""
    seed_interpolation_method: NotRequired[CreateStreamSeedInterpolationMethodRequest2]
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""
    enable_similar_image_filter: NotRequired[bool]
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""
    similar_image_filter_threshold: NotRequired[float]
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""
    similar_image_filter_max_skip_frame: NotRequired[int]
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""
    skip_diffusion: NotRequired[bool]
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """
    image_preprocessing: NotRequired[CreateStreamImagePreprocessingRequest2TypedDict]
    r"""List of image preprocessor configurations for image processing"""
    image_postprocessing: NotRequired[CreateStreamImagePostprocessingRequest2TypedDict]
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """
    latent_preprocessing: NotRequired[CreateStreamLatentPreprocessingRequest2TypedDict]
    r"""List of latent preprocessor configurations for latent processing"""
    latent_postprocessing: NotRequired[
        CreateStreamLatentPostprocessingRequest2TypedDict
    ]
    r"""List of latent postprocessor configurations for latent processing"""
    controlnets: NotRequired[List[CreateStreamControlnetRequest2TypedDict]]
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""
    ip_adapter: NotRequired[CreateStreamIPAdapterRequestTypedDict]
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""
    ip_adapter_style_image_url: NotRequired[str]
    r"""HTTPS URL or base64-encoded data URI (data:image/[type];base64,...) of the style image to use. Base64 images are automatically cached and uploaded to storage with content-based deduplication. Maximum size: 5MB. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""
    cached_attention: NotRequired[CreateStreamCachedAttentionRequest1TypedDict]
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""


class CreateStreamSDXLRequest(BaseModel):
    model_id: Annotated[
        Annotated[
            Literal["stabilityai/sdxl-turbo"],
            AfterValidator(validate_const("stabilityai/sdxl-turbo")),
        ],
        pydantic.Field(alias="model_id"),
    ] = "stabilityai/sdxl-turbo"

    prompt: Optional[CreateStreamPromptRequest3] = None
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""

    prompt_interpolation_method: Optional[
        CreateStreamPromptInterpolationMethodRequest2
    ] = None
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""

    normalize_prompt_weights: Optional[bool] = None
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""

    normalize_seed_weights: Optional[bool] = None
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""

    negative_prompt: Optional[str] = None
    r"""Text describing what to avoid in the generated image."""

    guidance_scale: Optional[float] = None
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""

    delta: Optional[float] = None
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""

    num_inference_steps: Optional[int] = None
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""

    t_index_list: Optional[List[int]] = None
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """

    use_safety_checker: Optional[bool] = None
    r"""Whether to use safety checker for content filtering"""

    width: Optional[int] = None
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""

    height: Optional[int] = None
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""

    lora_dict: OptionalNullable[CreateStreamLoraDictRequest2] = UNSET
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""

    use_lcm_lora: Optional[bool] = None
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""

    lcm_lora_id: Optional[str] = None
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """

    acceleration: Optional[CreateStreamAccelerationRequest2] = None
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""

    use_denoising_batch: Optional[bool] = None
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""

    do_add_noise: Optional[bool] = None
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""

    seed: Optional[CreateStreamSeedRequest3] = None
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""

    seed_interpolation_method: Optional[CreateStreamSeedInterpolationMethodRequest2] = (
        None
    )
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""

    enable_similar_image_filter: Optional[bool] = None
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""

    similar_image_filter_threshold: Optional[float] = None
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""

    similar_image_filter_max_skip_frame: Optional[int] = None
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""

    skip_diffusion: Optional[bool] = None
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """

    image_preprocessing: Optional[CreateStreamImagePreprocessingRequest2] = None
    r"""List of image preprocessor configurations for image processing"""

    image_postprocessing: Optional[CreateStreamImagePostprocessingRequest2] = None
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    latent_preprocessing: Optional[CreateStreamLatentPreprocessingRequest2] = None
    r"""List of latent preprocessor configurations for latent processing"""

    latent_postprocessing: Optional[CreateStreamLatentPostprocessingRequest2] = None
    r"""List of latent postprocessor configurations for latent processing"""

    controlnets: Optional[List[CreateStreamControlnetRequest2]] = None
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""

    ip_adapter: Optional[CreateStreamIPAdapterRequest] = None
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    ip_adapter_style_image_url: Optional[str] = None
    r"""HTTPS URL or base64-encoded data URI (data:image/[type];base64,...) of the style image to use. Base64 images are automatically cached and uploaded to storage with content-based deduplication. Maximum size: 5MB. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""

    cached_attention: Optional[CreateStreamCachedAttentionRequest1] = None
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "prompt",
            "prompt_interpolation_method",
            "normalize_prompt_weights",
            "normalize_seed_weights",
            "negative_prompt",
            "guidance_scale",
            "delta",
            "num_inference_steps",
            "t_index_list",
            "use_safety_checker",
            "width",
            "height",
            "lora_dict",
            "use_lcm_lora",
            "lcm_lora_id",
            "acceleration",
            "use_denoising_batch",
            "do_add_noise",
            "seed",
            "seed_interpolation_method",
            "enable_similar_image_filter",
            "similar_image_filter_threshold",
            "similar_image_filter_max_skip_frame",
            "skip_diffusion",
            "image_preprocessing",
            "image_postprocessing",
            "latent_preprocessing",
            "latent_postprocessing",
            "controlnets",
            "ip_adapter",
            "ip_adapter_style_image_url",
            "cached_attention",
        ]
        nullable_fields = ["lora_dict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreateStreamPromptRequest2TypedDict = TypeAliasType(
    "CreateStreamPromptRequest2TypedDict", Union[str, float]
)


CreateStreamPromptRequest2 = TypeAliasType(
    "CreateStreamPromptRequest2", Union[str, float]
)


CreateStreamPromptRequest1TypedDict = TypeAliasType(
    "CreateStreamPromptRequest1TypedDict",
    Union[str, List[List[CreateStreamPromptRequest2TypedDict]]],
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


CreateStreamPromptRequest1 = TypeAliasType(
    "CreateStreamPromptRequest1", Union[str, List[List[CreateStreamPromptRequest2]]]
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


CreateStreamPromptInterpolationMethodRequest1 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""


class CreateStreamLoraDictRequest1TypedDict(TypedDict):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


class CreateStreamLoraDictRequest1(BaseModel):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


CreateStreamAccelerationRequest1 = Literal[
    "none",
    "xformers",
    "tensorrt",
]
r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""


CreateStreamSeedRequest2TypedDict = TypeAliasType(
    "CreateStreamSeedRequest2TypedDict", Union[int, float]
)


CreateStreamSeedRequest2 = TypeAliasType("CreateStreamSeedRequest2", Union[int, float])


CreateStreamSeedRequest1TypedDict = TypeAliasType(
    "CreateStreamSeedRequest1TypedDict",
    Union[int, List[List[CreateStreamSeedRequest2TypedDict]]],
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


CreateStreamSeedRequest1 = TypeAliasType(
    "CreateStreamSeedRequest1", Union[int, List[List[CreateStreamSeedRequest2]]]
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


CreateStreamSeedInterpolationMethodRequest1 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""


CreateStreamImagePreprocessingTypeRequest1 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class CreateStreamImagePreprocessingProcessorRequest1TypedDict(TypedDict):
    type: CreateStreamImagePreprocessingTypeRequest1
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamImagePreprocessingProcessorRequest1(BaseModel):
    type: CreateStreamImagePreprocessingTypeRequest1

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamImagePreprocessingRequest1TypedDict(TypedDict):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[CreateStreamImagePreprocessingProcessorRequest1TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamImagePreprocessingRequest1(BaseModel):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[CreateStreamImagePreprocessingProcessorRequest1]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamImagePostprocessingTypeRequest1 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class CreateStreamImagePostprocessingProcessorRequest1TypedDict(TypedDict):
    type: CreateStreamImagePostprocessingTypeRequest1
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamImagePostprocessingProcessorRequest1(BaseModel):
    type: CreateStreamImagePostprocessingTypeRequest1

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamImagePostprocessingRequest1TypedDict(TypedDict):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[CreateStreamImagePostprocessingProcessorRequest1TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamImagePostprocessingRequest1(BaseModel):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[CreateStreamImagePostprocessingProcessorRequest1]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamLatentPreprocessingTypeRequest1 = Literal["latent_feedback",]


class CreateStreamLatentPreprocessingProcessorRequest1TypedDict(TypedDict):
    type: CreateStreamLatentPreprocessingTypeRequest1
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamLatentPreprocessingProcessorRequest1(BaseModel):
    type: CreateStreamLatentPreprocessingTypeRequest1

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamLatentPreprocessingRequest1TypedDict(TypedDict):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPreprocessingProcessorRequest1TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamLatentPreprocessingRequest1(BaseModel):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPreprocessingProcessorRequest1]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamLatentPostprocessingTypeRequest1 = Literal["latent_feedback",]


class CreateStreamLatentPostprocessingProcessorRequest1TypedDict(TypedDict):
    type: CreateStreamLatentPostprocessingTypeRequest1
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamLatentPostprocessingProcessorRequest1(BaseModel):
    type: CreateStreamLatentPostprocessingTypeRequest1

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamLatentPostprocessingRequest1TypedDict(TypedDict):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPostprocessingProcessorRequest1TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamLatentPostprocessingRequest1(BaseModel):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPostprocessingProcessorRequest1]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamControlnetModelIDRequest1 = Literal[
    "thibaud/controlnet-sd21-openpose-diffusers",
    "thibaud/controlnet-sd21-hed-diffusers",
    "thibaud/controlnet-sd21-canny-diffusers",
    "thibaud/controlnet-sd21-depth-diffusers",
    "thibaud/controlnet-sd21-color-diffusers",
    "daydreamlive/TemporalNet2-stable-diffusion-2-1",
]
r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""


CreateStreamPreprocessorRequest1 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]
r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""


class CreateStreamControlnetRequest1TypedDict(TypedDict):
    model_id: CreateStreamControlnetModelIDRequest1
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""
    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""
    preprocessor: CreateStreamPreprocessorRequest1
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""
    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""
    preprocessor_params: NotRequired[Dict[str, Any]]
    r"""Additional parameters for the preprocessor. For example, canny edge detection uses 'low_threshold' and 'high_threshold' values."""
    control_guidance_start: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""
    control_guidance_end: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


class CreateStreamControlnetRequest1(BaseModel):
    model_id: CreateStreamControlnetModelIDRequest1
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""

    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""

    preprocessor: CreateStreamPreprocessorRequest1
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""

    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""

    preprocessor_params: Optional[Dict[str, Any]] = None
    r"""Additional parameters for the preprocessor. For example, canny edge detection uses 'low_threshold' and 'high_threshold' values."""

    control_guidance_start: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""

    control_guidance_end: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


class CreateStreamSDTurboRequestTypedDict(TypedDict):
    model_id: Literal["stabilityai/sd-turbo"]
    prompt: NotRequired[CreateStreamPromptRequest1TypedDict]
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""
    prompt_interpolation_method: NotRequired[
        CreateStreamPromptInterpolationMethodRequest1
    ]
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""
    normalize_prompt_weights: NotRequired[bool]
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""
    normalize_seed_weights: NotRequired[bool]
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""
    negative_prompt: NotRequired[str]
    r"""Text describing what to avoid in the generated image."""
    guidance_scale: NotRequired[float]
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""
    delta: NotRequired[float]
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""
    num_inference_steps: NotRequired[int]
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""
    t_index_list: NotRequired[List[int]]
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """
    use_safety_checker: NotRequired[bool]
    r"""Whether to use safety checker for content filtering"""
    width: NotRequired[int]
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""
    height: NotRequired[int]
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""
    lora_dict: NotRequired[Nullable[CreateStreamLoraDictRequest1TypedDict]]
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""
    use_lcm_lora: NotRequired[bool]
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""
    lcm_lora_id: NotRequired[str]
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """
    acceleration: NotRequired[CreateStreamAccelerationRequest1]
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""
    use_denoising_batch: NotRequired[bool]
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""
    do_add_noise: NotRequired[bool]
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""
    seed: NotRequired[CreateStreamSeedRequest1TypedDict]
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""
    seed_interpolation_method: NotRequired[CreateStreamSeedInterpolationMethodRequest1]
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""
    enable_similar_image_filter: NotRequired[bool]
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""
    similar_image_filter_threshold: NotRequired[float]
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""
    similar_image_filter_max_skip_frame: NotRequired[int]
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""
    skip_diffusion: NotRequired[bool]
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """
    image_preprocessing: NotRequired[CreateStreamImagePreprocessingRequest1TypedDict]
    r"""List of image preprocessor configurations for image processing"""
    image_postprocessing: NotRequired[CreateStreamImagePostprocessingRequest1TypedDict]
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """
    latent_preprocessing: NotRequired[CreateStreamLatentPreprocessingRequest1TypedDict]
    r"""List of latent preprocessor configurations for latent processing"""
    latent_postprocessing: NotRequired[
        CreateStreamLatentPostprocessingRequest1TypedDict
    ]
    r"""List of latent postprocessor configurations for latent processing"""
    controlnets: NotRequired[List[CreateStreamControlnetRequest1TypedDict]]
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""


class CreateStreamSDTurboRequest(BaseModel):
    model_id: Annotated[
        Annotated[
            Literal["stabilityai/sd-turbo"],
            AfterValidator(validate_const("stabilityai/sd-turbo")),
        ],
        pydantic.Field(alias="model_id"),
    ] = "stabilityai/sd-turbo"

    prompt: Optional[CreateStreamPromptRequest1] = None
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""

    prompt_interpolation_method: Optional[
        CreateStreamPromptInterpolationMethodRequest1
    ] = None
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""

    normalize_prompt_weights: Optional[bool] = None
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""

    normalize_seed_weights: Optional[bool] = None
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""

    negative_prompt: Optional[str] = None
    r"""Text describing what to avoid in the generated image."""

    guidance_scale: Optional[float] = None
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""

    delta: Optional[float] = None
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""

    num_inference_steps: Optional[int] = None
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""

    t_index_list: Optional[List[int]] = None
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """

    use_safety_checker: Optional[bool] = None
    r"""Whether to use safety checker for content filtering"""

    width: Optional[int] = None
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""

    height: Optional[int] = None
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""

    lora_dict: OptionalNullable[CreateStreamLoraDictRequest1] = UNSET
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""

    use_lcm_lora: Optional[bool] = None
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""

    lcm_lora_id: Optional[str] = None
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """

    acceleration: Optional[CreateStreamAccelerationRequest1] = None
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""

    use_denoising_batch: Optional[bool] = None
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""

    do_add_noise: Optional[bool] = None
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""

    seed: Optional[CreateStreamSeedRequest1] = None
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""

    seed_interpolation_method: Optional[CreateStreamSeedInterpolationMethodRequest1] = (
        None
    )
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""

    enable_similar_image_filter: Optional[bool] = None
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""

    similar_image_filter_threshold: Optional[float] = None
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""

    similar_image_filter_max_skip_frame: Optional[int] = None
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""

    skip_diffusion: Optional[bool] = None
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """

    image_preprocessing: Optional[CreateStreamImagePreprocessingRequest1] = None
    r"""List of image preprocessor configurations for image processing"""

    image_postprocessing: Optional[CreateStreamImagePostprocessingRequest1] = None
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    latent_preprocessing: Optional[CreateStreamLatentPreprocessingRequest1] = None
    r"""List of latent preprocessor configurations for latent processing"""

    latent_postprocessing: Optional[CreateStreamLatentPostprocessingRequest1] = None
    r"""List of latent postprocessor configurations for latent processing"""

    controlnets: Optional[List[CreateStreamControlnetRequest1]] = None
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "prompt",
            "prompt_interpolation_method",
            "normalize_prompt_weights",
            "normalize_seed_weights",
            "negative_prompt",
            "guidance_scale",
            "delta",
            "num_inference_steps",
            "t_index_list",
            "use_safety_checker",
            "width",
            "height",
            "lora_dict",
            "use_lcm_lora",
            "lcm_lora_id",
            "acceleration",
            "use_denoising_batch",
            "do_add_noise",
            "seed",
            "seed_interpolation_method",
            "enable_similar_image_filter",
            "similar_image_filter_threshold",
            "similar_image_filter_max_skip_frame",
            "skip_diffusion",
            "image_preprocessing",
            "image_postprocessing",
            "latent_preprocessing",
            "latent_postprocessing",
            "controlnets",
        ]
        nullable_fields = ["lora_dict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreateStreamParamsRequestTypedDict = TypeAliasType(
    "CreateStreamParamsRequestTypedDict",
    Union[
        CreateStreamSDTurboRequestTypedDict,
        CreateStreamSDXLRequestTypedDict,
        CreateStreamSd15RequestTypedDict,
    ],
)


CreateStreamParamsRequest = Annotated[
    Union[
        Annotated[CreateStreamSDTurboRequest, Tag("stabilityai/sd-turbo")],
        Annotated[CreateStreamSDXLRequest, Tag("stabilityai/sdxl-turbo")],
        Annotated[CreateStreamSd15Request, Tag("Lykon/dreamshaper-8")],
        Annotated[CreateStreamSd15Request, Tag("prompthero/openjourney-v4")],
    ],
    Discriminator(lambda m: get_discriminator(m, "model_id", "model_id")),
]


class CreateStreamStreamDiffusionTypedDict(TypedDict):
    pipeline: CreateStreamPipelineRequest
    params: CreateStreamParamsRequestTypedDict
    name: NotRequired[str]
    r"""Human-readable name for the stream"""
    output_rtmp_url: NotRequired[str]
    r"""Custom RTMP URL for stream output destination"""


class CreateStreamStreamDiffusion(BaseModel):
    pipeline: CreateStreamPipelineRequest

    params: CreateStreamParamsRequest

    name: Optional[str] = None
    r"""Human-readable name for the stream"""

    output_rtmp_url: Optional[str] = None
    r"""Custom RTMP URL for stream output destination"""


CreateStreamRequestTypedDict = CreateStreamStreamDiffusionTypedDict


CreateStreamRequest = CreateStreamStreamDiffusion


CreateStreamPipelineResponse = Literal["streamdiffusion",]


CreateStreamModelIDResponseEnum = Literal[
    "Lykon/dreamshaper-8",
    "prompthero/openjourney-v4",
]
r"""Model to use for generation"""


CreateStreamPromptResponse5TypedDict = TypeAliasType(
    "CreateStreamPromptResponse5TypedDict", Union[str, float]
)


CreateStreamPromptResponse5 = TypeAliasType(
    "CreateStreamPromptResponse5", Union[str, float]
)


CreateStreamPromptResponse6TypedDict = TypeAliasType(
    "CreateStreamPromptResponse6TypedDict",
    Union[str, List[List[CreateStreamPromptResponse5TypedDict]]],
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


CreateStreamPromptResponse6 = TypeAliasType(
    "CreateStreamPromptResponse6", Union[str, List[List[CreateStreamPromptResponse5]]]
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


CreateStreamPromptInterpolationMethodResponse3 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""


class CreateStreamLoraDictResponse3TypedDict(TypedDict):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


class CreateStreamLoraDictResponse3(BaseModel):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


CreateStreamAccelerationResponse3 = Literal[
    "none",
    "xformers",
    "tensorrt",
]
r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""


CreateStreamSeedResponse5TypedDict = TypeAliasType(
    "CreateStreamSeedResponse5TypedDict", Union[int, float]
)


CreateStreamSeedResponse5 = TypeAliasType(
    "CreateStreamSeedResponse5", Union[int, float]
)


CreateStreamSeedResponse6TypedDict = TypeAliasType(
    "CreateStreamSeedResponse6TypedDict",
    Union[int, List[List[CreateStreamSeedResponse5TypedDict]]],
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


CreateStreamSeedResponse6 = TypeAliasType(
    "CreateStreamSeedResponse6", Union[int, List[List[CreateStreamSeedResponse5]]]
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


CreateStreamSeedInterpolationMethodResponse3 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""


CreateStreamImagePreprocessingTypeResponse3 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class CreateStreamImagePreprocessingProcessorResponse3TypedDict(TypedDict):
    type: CreateStreamImagePreprocessingTypeResponse3
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamImagePreprocessingProcessorResponse3(BaseModel):
    type: CreateStreamImagePreprocessingTypeResponse3

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamImagePreprocessingResponse3TypedDict(TypedDict):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[CreateStreamImagePreprocessingProcessorResponse3TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamImagePreprocessingResponse3(BaseModel):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[CreateStreamImagePreprocessingProcessorResponse3]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamImagePostprocessingTypeResponse3 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class CreateStreamImagePostprocessingProcessorResponse3TypedDict(TypedDict):
    type: CreateStreamImagePostprocessingTypeResponse3
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamImagePostprocessingProcessorResponse3(BaseModel):
    type: CreateStreamImagePostprocessingTypeResponse3

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamImagePostprocessingResponse3TypedDict(TypedDict):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[CreateStreamImagePostprocessingProcessorResponse3TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamImagePostprocessingResponse3(BaseModel):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[CreateStreamImagePostprocessingProcessorResponse3]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamLatentPreprocessingTypeResponse3 = Literal["latent_feedback",]


class CreateStreamLatentPreprocessingProcessorResponse3TypedDict(TypedDict):
    type: CreateStreamLatentPreprocessingTypeResponse3
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamLatentPreprocessingProcessorResponse3(BaseModel):
    type: CreateStreamLatentPreprocessingTypeResponse3

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamLatentPreprocessingResponse3TypedDict(TypedDict):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPreprocessingProcessorResponse3TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamLatentPreprocessingResponse3(BaseModel):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPreprocessingProcessorResponse3]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamLatentPostprocessingTypeResponse3 = Literal["latent_feedback",]


class CreateStreamLatentPostprocessingProcessorResponse3TypedDict(TypedDict):
    type: CreateStreamLatentPostprocessingTypeResponse3
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamLatentPostprocessingProcessorResponse3(BaseModel):
    type: CreateStreamLatentPostprocessingTypeResponse3

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamLatentPostprocessingResponse3TypedDict(TypedDict):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPostprocessingProcessorResponse3TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamLatentPostprocessingResponse3(BaseModel):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPostprocessingProcessorResponse3]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamControlnetModelIDResponse3 = Literal[
    "lllyasviel/control_v11f1p_sd15_depth",
    "lllyasviel/control_v11f1e_sd15_tile",
    "lllyasviel/control_v11p_sd15_canny",
    "daydreamlive/TemporalNet2-stable-diffusion-v1-5",
]
r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""


CreateStreamPreprocessorResponse3 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]
r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""


class CreateStreamControlnetResponse3TypedDict(TypedDict):
    model_id: CreateStreamControlnetModelIDResponse3
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""
    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""
    preprocessor: CreateStreamPreprocessorResponse3
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""
    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""
    preprocessor_params: NotRequired[Dict[str, Any]]
    r"""Preprocessor parameters"""
    control_guidance_start: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""
    control_guidance_end: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


class CreateStreamControlnetResponse3(BaseModel):
    model_id: CreateStreamControlnetModelIDResponse3
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""

    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""

    preprocessor: CreateStreamPreprocessorResponse3
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""

    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""

    preprocessor_params: Optional[Dict[str, Any]] = None
    r"""Preprocessor parameters"""

    control_guidance_start: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""

    control_guidance_end: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


CreateStreamTypeRegularResponse = Literal["regular",]
r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""


CreateStreamWeightTypeRegularResponse = Literal[
    "linear",
    "ease in",
    "ease out",
    "ease in-out",
    "reverse in-out",
    "weak input",
    "weak output",
    "weak middle",
    "strong middle",
    "style transfer",
    "composition",
    "strong style transfer",
    "style and composition",
    "style transfer precise",
    "composition precise",
]
r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class CreateStreamIPAdapterRegularResponseTypedDict(TypedDict):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    type: NotRequired[CreateStreamTypeRegularResponse]
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""
    enabled: NotRequired[bool]
    r"""Whether IP adapter is enabled"""
    scale: NotRequired[float]
    r"""Strength of IP adapter style conditioning"""
    weight_type: NotRequired[CreateStreamWeightTypeRegularResponse]
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class CreateStreamIPAdapterRegularResponse(BaseModel):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    type: Optional[CreateStreamTypeRegularResponse] = "regular"
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""

    enabled: Optional[bool] = None
    r"""Whether IP adapter is enabled"""

    scale: Optional[float] = None
    r"""Strength of IP adapter style conditioning"""

    weight_type: Optional[CreateStreamWeightTypeRegularResponse] = None
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class CreateStreamCachedAttentionResponse2TypedDict(TypedDict):
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    enabled: bool
    r"""Whether this cached attention is active. Enable cached attention to reuse key/value tensors across frames."""
    max_frames: int
    r"""Number of frames retained in the attention cache. Number of historical K/V frames to retain. Limited by TensorRT engine exports."""
    interval: int
    r"""Cadence (number of frames) for refreshing cached key/value tensors i.e How often (in number of frames) to refresh the cache.
    It is now **frame-based** (not seconds). It accepts integers 1-240, representing how many frames elapse between cache refreshes.

    """


class CreateStreamCachedAttentionResponse2(BaseModel):
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    enabled: bool
    r"""Whether this cached attention is active. Enable cached attention to reuse key/value tensors across frames."""

    max_frames: int
    r"""Number of frames retained in the attention cache. Number of historical K/V frames to retain. Limited by TensorRT engine exports."""

    interval: int
    r"""Cadence (number of frames) for refreshing cached key/value tensors i.e How often (in number of frames) to refresh the cache.
    It is now **frame-based** (not seconds). It accepts integers 1-240, representing how many frames elapse between cache refreshes.

    """


class CreateStreamSd15ResponseTypedDict(TypedDict):
    model_id: CreateStreamModelIDResponseEnum
    r"""Model to use for generation"""
    prompt: NotRequired[CreateStreamPromptResponse6TypedDict]
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""
    prompt_interpolation_method: NotRequired[
        CreateStreamPromptInterpolationMethodResponse3
    ]
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""
    normalize_prompt_weights: NotRequired[bool]
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""
    normalize_seed_weights: NotRequired[bool]
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""
    negative_prompt: NotRequired[str]
    r"""Text describing what to avoid in the generated image."""
    guidance_scale: NotRequired[float]
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""
    delta: NotRequired[float]
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""
    num_inference_steps: NotRequired[int]
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""
    t_index_list: NotRequired[List[int]]
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """
    use_safety_checker: NotRequired[bool]
    r"""Whether to use safety checker for content filtering"""
    width: NotRequired[int]
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""
    height: NotRequired[int]
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""
    lora_dict: NotRequired[Nullable[CreateStreamLoraDictResponse3TypedDict]]
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""
    use_lcm_lora: NotRequired[bool]
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""
    lcm_lora_id: NotRequired[str]
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """
    acceleration: NotRequired[CreateStreamAccelerationResponse3]
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""
    use_denoising_batch: NotRequired[bool]
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""
    do_add_noise: NotRequired[bool]
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""
    seed: NotRequired[CreateStreamSeedResponse6TypedDict]
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""
    seed_interpolation_method: NotRequired[CreateStreamSeedInterpolationMethodResponse3]
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""
    enable_similar_image_filter: NotRequired[bool]
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""
    similar_image_filter_threshold: NotRequired[float]
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""
    similar_image_filter_max_skip_frame: NotRequired[int]
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""
    skip_diffusion: NotRequired[bool]
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """
    image_preprocessing: NotRequired[CreateStreamImagePreprocessingResponse3TypedDict]
    r"""List of image preprocessor configurations for image processing"""
    image_postprocessing: NotRequired[CreateStreamImagePostprocessingResponse3TypedDict]
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """
    latent_preprocessing: NotRequired[CreateStreamLatentPreprocessingResponse3TypedDict]
    r"""List of latent preprocessor configurations for latent processing"""
    latent_postprocessing: NotRequired[
        CreateStreamLatentPostprocessingResponse3TypedDict
    ]
    r"""List of latent postprocessor configurations for latent processing"""
    controlnets: NotRequired[List[CreateStreamControlnetResponse3TypedDict]]
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""
    ip_adapter: NotRequired[CreateStreamIPAdapterRegularResponseTypedDict]
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""
    ip_adapter_style_image_url: NotRequired[str]
    r"""HTTPS URL or base64-encoded data URI (data:image/[type];base64,...) of the style image to use. Base64 images are automatically cached and uploaded to storage with content-based deduplication. Maximum size: 5MB. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""
    cached_attention: NotRequired[CreateStreamCachedAttentionResponse2TypedDict]
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""


class CreateStreamSd15Response(BaseModel):
    model_id: CreateStreamModelIDResponseEnum
    r"""Model to use for generation"""

    prompt: Optional[CreateStreamPromptResponse6] = None
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""

    prompt_interpolation_method: Optional[
        CreateStreamPromptInterpolationMethodResponse3
    ] = None
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""

    normalize_prompt_weights: Optional[bool] = None
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""

    normalize_seed_weights: Optional[bool] = None
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""

    negative_prompt: Optional[str] = None
    r"""Text describing what to avoid in the generated image."""

    guidance_scale: Optional[float] = None
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""

    delta: Optional[float] = None
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""

    num_inference_steps: Optional[int] = None
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""

    t_index_list: Optional[List[int]] = None
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """

    use_safety_checker: Optional[bool] = None
    r"""Whether to use safety checker for content filtering"""

    width: Optional[int] = None
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""

    height: Optional[int] = None
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""

    lora_dict: OptionalNullable[CreateStreamLoraDictResponse3] = UNSET
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""

    use_lcm_lora: Optional[bool] = None
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""

    lcm_lora_id: Optional[str] = None
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """

    acceleration: Optional[CreateStreamAccelerationResponse3] = None
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""

    use_denoising_batch: Optional[bool] = None
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""

    do_add_noise: Optional[bool] = None
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""

    seed: Optional[CreateStreamSeedResponse6] = None
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""

    seed_interpolation_method: Optional[
        CreateStreamSeedInterpolationMethodResponse3
    ] = None
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""

    enable_similar_image_filter: Optional[bool] = None
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""

    similar_image_filter_threshold: Optional[float] = None
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""

    similar_image_filter_max_skip_frame: Optional[int] = None
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""

    skip_diffusion: Optional[bool] = None
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """

    image_preprocessing: Optional[CreateStreamImagePreprocessingResponse3] = None
    r"""List of image preprocessor configurations for image processing"""

    image_postprocessing: Optional[CreateStreamImagePostprocessingResponse3] = None
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    latent_preprocessing: Optional[CreateStreamLatentPreprocessingResponse3] = None
    r"""List of latent preprocessor configurations for latent processing"""

    latent_postprocessing: Optional[CreateStreamLatentPostprocessingResponse3] = None
    r"""List of latent postprocessor configurations for latent processing"""

    controlnets: Optional[List[CreateStreamControlnetResponse3]] = None
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""

    ip_adapter: Optional[CreateStreamIPAdapterRegularResponse] = None
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    ip_adapter_style_image_url: Optional[str] = None
    r"""HTTPS URL or base64-encoded data URI (data:image/[type];base64,...) of the style image to use. Base64 images are automatically cached and uploaded to storage with content-based deduplication. Maximum size: 5MB. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""

    cached_attention: Optional[CreateStreamCachedAttentionResponse2] = None
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "prompt",
            "prompt_interpolation_method",
            "normalize_prompt_weights",
            "normalize_seed_weights",
            "negative_prompt",
            "guidance_scale",
            "delta",
            "num_inference_steps",
            "t_index_list",
            "use_safety_checker",
            "width",
            "height",
            "lora_dict",
            "use_lcm_lora",
            "lcm_lora_id",
            "acceleration",
            "use_denoising_batch",
            "do_add_noise",
            "seed",
            "seed_interpolation_method",
            "enable_similar_image_filter",
            "similar_image_filter_threshold",
            "similar_image_filter_max_skip_frame",
            "skip_diffusion",
            "image_preprocessing",
            "image_postprocessing",
            "latent_preprocessing",
            "latent_postprocessing",
            "controlnets",
            "ip_adapter",
            "ip_adapter_style_image_url",
            "cached_attention",
        ]
        nullable_fields = ["lora_dict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreateStreamPromptResponse3TypedDict = TypeAliasType(
    "CreateStreamPromptResponse3TypedDict", Union[str, float]
)


CreateStreamPromptResponse3 = TypeAliasType(
    "CreateStreamPromptResponse3", Union[str, float]
)


CreateStreamPromptResponse4TypedDict = TypeAliasType(
    "CreateStreamPromptResponse4TypedDict",
    Union[str, List[List[CreateStreamPromptResponse3TypedDict]]],
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


CreateStreamPromptResponse4 = TypeAliasType(
    "CreateStreamPromptResponse4", Union[str, List[List[CreateStreamPromptResponse3]]]
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


CreateStreamPromptInterpolationMethodResponse2 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""


class CreateStreamLoraDictResponse2TypedDict(TypedDict):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


class CreateStreamLoraDictResponse2(BaseModel):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


CreateStreamAccelerationResponse2 = Literal[
    "none",
    "xformers",
    "tensorrt",
]
r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""


CreateStreamSeedResponse3TypedDict = TypeAliasType(
    "CreateStreamSeedResponse3TypedDict", Union[int, float]
)


CreateStreamSeedResponse3 = TypeAliasType(
    "CreateStreamSeedResponse3", Union[int, float]
)


CreateStreamSeedResponse4TypedDict = TypeAliasType(
    "CreateStreamSeedResponse4TypedDict",
    Union[int, List[List[CreateStreamSeedResponse3TypedDict]]],
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


CreateStreamSeedResponse4 = TypeAliasType(
    "CreateStreamSeedResponse4", Union[int, List[List[CreateStreamSeedResponse3]]]
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


CreateStreamSeedInterpolationMethodResponse2 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""


CreateStreamImagePreprocessingTypeResponse2 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class CreateStreamImagePreprocessingProcessorResponse2TypedDict(TypedDict):
    type: CreateStreamImagePreprocessingTypeResponse2
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamImagePreprocessingProcessorResponse2(BaseModel):
    type: CreateStreamImagePreprocessingTypeResponse2

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamImagePreprocessingResponse2TypedDict(TypedDict):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[CreateStreamImagePreprocessingProcessorResponse2TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamImagePreprocessingResponse2(BaseModel):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[CreateStreamImagePreprocessingProcessorResponse2]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamImagePostprocessingTypeResponse2 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class CreateStreamImagePostprocessingProcessorResponse2TypedDict(TypedDict):
    type: CreateStreamImagePostprocessingTypeResponse2
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamImagePostprocessingProcessorResponse2(BaseModel):
    type: CreateStreamImagePostprocessingTypeResponse2

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamImagePostprocessingResponse2TypedDict(TypedDict):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[CreateStreamImagePostprocessingProcessorResponse2TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamImagePostprocessingResponse2(BaseModel):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[CreateStreamImagePostprocessingProcessorResponse2]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamLatentPreprocessingTypeResponse2 = Literal["latent_feedback",]


class CreateStreamLatentPreprocessingProcessorResponse2TypedDict(TypedDict):
    type: CreateStreamLatentPreprocessingTypeResponse2
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamLatentPreprocessingProcessorResponse2(BaseModel):
    type: CreateStreamLatentPreprocessingTypeResponse2

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamLatentPreprocessingResponse2TypedDict(TypedDict):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPreprocessingProcessorResponse2TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamLatentPreprocessingResponse2(BaseModel):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPreprocessingProcessorResponse2]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamLatentPostprocessingTypeResponse2 = Literal["latent_feedback",]


class CreateStreamLatentPostprocessingProcessorResponse2TypedDict(TypedDict):
    type: CreateStreamLatentPostprocessingTypeResponse2
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamLatentPostprocessingProcessorResponse2(BaseModel):
    type: CreateStreamLatentPostprocessingTypeResponse2

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamLatentPostprocessingResponse2TypedDict(TypedDict):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPostprocessingProcessorResponse2TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamLatentPostprocessingResponse2(BaseModel):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPostprocessingProcessorResponse2]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamControlnetModelIDResponse2 = Literal[
    "xinsir/controlnet-depth-sdxl-1.0",
    "xinsir/controlnet-canny-sdxl-1.0",
    "xinsir/controlnet-tile-sdxl-1.0",
]
r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""


CreateStreamPreprocessorResponse2 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]
r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""


class CreateStreamControlnetResponse2TypedDict(TypedDict):
    model_id: CreateStreamControlnetModelIDResponse2
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""
    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""
    preprocessor: CreateStreamPreprocessorResponse2
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""
    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""
    preprocessor_params: NotRequired[Dict[str, Any]]
    r"""Preprocessor parameters"""
    control_guidance_start: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""
    control_guidance_end: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


class CreateStreamControlnetResponse2(BaseModel):
    model_id: CreateStreamControlnetModelIDResponse2
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""

    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""

    preprocessor: CreateStreamPreprocessorResponse2
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""

    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""

    preprocessor_params: Optional[Dict[str, Any]] = None
    r"""Preprocessor parameters"""

    control_guidance_start: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""

    control_guidance_end: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


CreateStreamIPAdapterTypeResponse = Literal[
    "regular",
    "faceid",
]
r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""


CreateStreamIPAdapterWeightTypeResponse = Literal[
    "linear",
    "ease in",
    "ease out",
    "ease in-out",
    "reverse in-out",
    "weak input",
    "weak output",
    "weak middle",
    "strong middle",
    "style transfer",
    "composition",
    "strong style transfer",
    "style and composition",
    "style transfer precise",
    "composition precise",
]
r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class CreateStreamIPAdapterResponseTypedDict(TypedDict):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    type: NotRequired[CreateStreamIPAdapterTypeResponse]
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""
    enabled: NotRequired[bool]
    r"""Whether IP adapter is enabled"""
    scale: NotRequired[float]
    r"""Strength of IP adapter style conditioning"""
    weight_type: NotRequired[CreateStreamIPAdapterWeightTypeResponse]
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class CreateStreamIPAdapterResponse(BaseModel):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    type: Optional[CreateStreamIPAdapterTypeResponse] = "regular"
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""

    enabled: Optional[bool] = None
    r"""Whether IP adapter is enabled"""

    scale: Optional[float] = None
    r"""Strength of IP adapter style conditioning"""

    weight_type: Optional[CreateStreamIPAdapterWeightTypeResponse] = None
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class CreateStreamCachedAttentionResponse1TypedDict(TypedDict):
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    enabled: bool
    r"""Whether this cached attention is active. Enable cached attention to reuse key/value tensors across frames."""
    max_frames: int
    r"""Number of frames retained in the attention cache. Number of historical K/V frames to retain. Limited by TensorRT engine exports."""
    interval: int
    r"""Cadence (number of frames) for refreshing cached key/value tensors i.e How often (in number of frames) to refresh the cache.
    It is now **frame-based** (not seconds). It accepts integers 1-240, representing how many frames elapse between cache refreshes.

    """


class CreateStreamCachedAttentionResponse1(BaseModel):
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    enabled: bool
    r"""Whether this cached attention is active. Enable cached attention to reuse key/value tensors across frames."""

    max_frames: int
    r"""Number of frames retained in the attention cache. Number of historical K/V frames to retain. Limited by TensorRT engine exports."""

    interval: int
    r"""Cadence (number of frames) for refreshing cached key/value tensors i.e How often (in number of frames) to refresh the cache.
    It is now **frame-based** (not seconds). It accepts integers 1-240, representing how many frames elapse between cache refreshes.

    """


class CreateStreamSDXLResponseTypedDict(TypedDict):
    model_id: Literal["stabilityai/sdxl-turbo"]
    prompt: NotRequired[CreateStreamPromptResponse4TypedDict]
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""
    prompt_interpolation_method: NotRequired[
        CreateStreamPromptInterpolationMethodResponse2
    ]
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""
    normalize_prompt_weights: NotRequired[bool]
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""
    normalize_seed_weights: NotRequired[bool]
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""
    negative_prompt: NotRequired[str]
    r"""Text describing what to avoid in the generated image."""
    guidance_scale: NotRequired[float]
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""
    delta: NotRequired[float]
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""
    num_inference_steps: NotRequired[int]
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""
    t_index_list: NotRequired[List[int]]
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """
    use_safety_checker: NotRequired[bool]
    r"""Whether to use safety checker for content filtering"""
    width: NotRequired[int]
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""
    height: NotRequired[int]
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""
    lora_dict: NotRequired[Nullable[CreateStreamLoraDictResponse2TypedDict]]
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""
    use_lcm_lora: NotRequired[bool]
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""
    lcm_lora_id: NotRequired[str]
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """
    acceleration: NotRequired[CreateStreamAccelerationResponse2]
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""
    use_denoising_batch: NotRequired[bool]
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""
    do_add_noise: NotRequired[bool]
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""
    seed: NotRequired[CreateStreamSeedResponse4TypedDict]
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""
    seed_interpolation_method: NotRequired[CreateStreamSeedInterpolationMethodResponse2]
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""
    enable_similar_image_filter: NotRequired[bool]
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""
    similar_image_filter_threshold: NotRequired[float]
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""
    similar_image_filter_max_skip_frame: NotRequired[int]
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""
    skip_diffusion: NotRequired[bool]
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """
    image_preprocessing: NotRequired[CreateStreamImagePreprocessingResponse2TypedDict]
    r"""List of image preprocessor configurations for image processing"""
    image_postprocessing: NotRequired[CreateStreamImagePostprocessingResponse2TypedDict]
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """
    latent_preprocessing: NotRequired[CreateStreamLatentPreprocessingResponse2TypedDict]
    r"""List of latent preprocessor configurations for latent processing"""
    latent_postprocessing: NotRequired[
        CreateStreamLatentPostprocessingResponse2TypedDict
    ]
    r"""List of latent postprocessor configurations for latent processing"""
    controlnets: NotRequired[List[CreateStreamControlnetResponse2TypedDict]]
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""
    ip_adapter: NotRequired[CreateStreamIPAdapterResponseTypedDict]
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""
    ip_adapter_style_image_url: NotRequired[str]
    r"""HTTPS URL or base64-encoded data URI (data:image/[type];base64,...) of the style image to use. Base64 images are automatically cached and uploaded to storage with content-based deduplication. Maximum size: 5MB. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""
    cached_attention: NotRequired[CreateStreamCachedAttentionResponse1TypedDict]
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""


class CreateStreamSDXLResponse(BaseModel):
    model_id: Annotated[
        Annotated[
            Literal["stabilityai/sdxl-turbo"],
            AfterValidator(validate_const("stabilityai/sdxl-turbo")),
        ],
        pydantic.Field(alias="model_id"),
    ] = "stabilityai/sdxl-turbo"

    prompt: Optional[CreateStreamPromptResponse4] = None
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""

    prompt_interpolation_method: Optional[
        CreateStreamPromptInterpolationMethodResponse2
    ] = None
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""

    normalize_prompt_weights: Optional[bool] = None
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""

    normalize_seed_weights: Optional[bool] = None
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""

    negative_prompt: Optional[str] = None
    r"""Text describing what to avoid in the generated image."""

    guidance_scale: Optional[float] = None
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""

    delta: Optional[float] = None
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""

    num_inference_steps: Optional[int] = None
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""

    t_index_list: Optional[List[int]] = None
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """

    use_safety_checker: Optional[bool] = None
    r"""Whether to use safety checker for content filtering"""

    width: Optional[int] = None
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""

    height: Optional[int] = None
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""

    lora_dict: OptionalNullable[CreateStreamLoraDictResponse2] = UNSET
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""

    use_lcm_lora: Optional[bool] = None
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""

    lcm_lora_id: Optional[str] = None
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """

    acceleration: Optional[CreateStreamAccelerationResponse2] = None
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""

    use_denoising_batch: Optional[bool] = None
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""

    do_add_noise: Optional[bool] = None
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""

    seed: Optional[CreateStreamSeedResponse4] = None
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""

    seed_interpolation_method: Optional[
        CreateStreamSeedInterpolationMethodResponse2
    ] = None
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""

    enable_similar_image_filter: Optional[bool] = None
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""

    similar_image_filter_threshold: Optional[float] = None
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""

    similar_image_filter_max_skip_frame: Optional[int] = None
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""

    skip_diffusion: Optional[bool] = None
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """

    image_preprocessing: Optional[CreateStreamImagePreprocessingResponse2] = None
    r"""List of image preprocessor configurations for image processing"""

    image_postprocessing: Optional[CreateStreamImagePostprocessingResponse2] = None
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    latent_preprocessing: Optional[CreateStreamLatentPreprocessingResponse2] = None
    r"""List of latent preprocessor configurations for latent processing"""

    latent_postprocessing: Optional[CreateStreamLatentPostprocessingResponse2] = None
    r"""List of latent postprocessor configurations for latent processing"""

    controlnets: Optional[List[CreateStreamControlnetResponse2]] = None
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""

    ip_adapter: Optional[CreateStreamIPAdapterResponse] = None
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    ip_adapter_style_image_url: Optional[str] = None
    r"""HTTPS URL or base64-encoded data URI (data:image/[type];base64,...) of the style image to use. Base64 images are automatically cached and uploaded to storage with content-based deduplication. Maximum size: 5MB. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""

    cached_attention: Optional[CreateStreamCachedAttentionResponse1] = None
    r"""Cached attention (StreamV2V) configuration. Enable cached attention to reuse key/value tensors across frames. ⚠️ NOTE: Enabling or disabling cached_attention requires a pipeline reload as it uses a different pipeline."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "prompt",
            "prompt_interpolation_method",
            "normalize_prompt_weights",
            "normalize_seed_weights",
            "negative_prompt",
            "guidance_scale",
            "delta",
            "num_inference_steps",
            "t_index_list",
            "use_safety_checker",
            "width",
            "height",
            "lora_dict",
            "use_lcm_lora",
            "lcm_lora_id",
            "acceleration",
            "use_denoising_batch",
            "do_add_noise",
            "seed",
            "seed_interpolation_method",
            "enable_similar_image_filter",
            "similar_image_filter_threshold",
            "similar_image_filter_max_skip_frame",
            "skip_diffusion",
            "image_preprocessing",
            "image_postprocessing",
            "latent_preprocessing",
            "latent_postprocessing",
            "controlnets",
            "ip_adapter",
            "ip_adapter_style_image_url",
            "cached_attention",
        ]
        nullable_fields = ["lora_dict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreateStreamPromptResponse1TypedDict = TypeAliasType(
    "CreateStreamPromptResponse1TypedDict", Union[str, float]
)


CreateStreamPromptResponse1 = TypeAliasType(
    "CreateStreamPromptResponse1", Union[str, float]
)


CreateStreamPromptResponse2TypedDict = TypeAliasType(
    "CreateStreamPromptResponse2TypedDict",
    Union[str, List[List[CreateStreamPromptResponse1TypedDict]]],
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


CreateStreamPromptResponse2 = TypeAliasType(
    "CreateStreamPromptResponse2", Union[str, List[List[CreateStreamPromptResponse1]]]
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


CreateStreamPromptInterpolationMethodResponse1 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""


class CreateStreamLoraDictResponse1TypedDict(TypedDict):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


class CreateStreamLoraDictResponse1(BaseModel):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


CreateStreamAccelerationResponse1 = Literal[
    "none",
    "xformers",
    "tensorrt",
]
r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""


CreateStreamSeedResponse1TypedDict = TypeAliasType(
    "CreateStreamSeedResponse1TypedDict", Union[int, float]
)


CreateStreamSeedResponse1 = TypeAliasType(
    "CreateStreamSeedResponse1", Union[int, float]
)


CreateStreamSeedResponse2TypedDict = TypeAliasType(
    "CreateStreamSeedResponse2TypedDict",
    Union[int, List[List[CreateStreamSeedResponse1TypedDict]]],
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


CreateStreamSeedResponse2 = TypeAliasType(
    "CreateStreamSeedResponse2", Union[int, List[List[CreateStreamSeedResponse1]]]
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


CreateStreamSeedInterpolationMethodResponse1 = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""


CreateStreamImagePreprocessingTypeResponse1 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class CreateStreamImagePreprocessingProcessorResponse1TypedDict(TypedDict):
    type: CreateStreamImagePreprocessingTypeResponse1
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamImagePreprocessingProcessorResponse1(BaseModel):
    type: CreateStreamImagePreprocessingTypeResponse1

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamImagePreprocessingResponse1TypedDict(TypedDict):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[CreateStreamImagePreprocessingProcessorResponse1TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamImagePreprocessingResponse1(BaseModel):
    r"""List of image preprocessor configurations for image processing"""

    processors: List[CreateStreamImagePreprocessingProcessorResponse1]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamImagePostprocessingTypeResponse1 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]


class CreateStreamImagePostprocessingProcessorResponse1TypedDict(TypedDict):
    type: CreateStreamImagePostprocessingTypeResponse1
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamImagePostprocessingProcessorResponse1(BaseModel):
    type: CreateStreamImagePostprocessingTypeResponse1

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamImagePostprocessingResponse1TypedDict(TypedDict):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[CreateStreamImagePostprocessingProcessorResponse1TypedDict]
    r"""List of image processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamImagePostprocessingResponse1(BaseModel):
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    processors: List[CreateStreamImagePostprocessingProcessorResponse1]
    r"""List of image processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamLatentPreprocessingTypeResponse1 = Literal["latent_feedback",]


class CreateStreamLatentPreprocessingProcessorResponse1TypedDict(TypedDict):
    type: CreateStreamLatentPreprocessingTypeResponse1
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamLatentPreprocessingProcessorResponse1(BaseModel):
    type: CreateStreamLatentPreprocessingTypeResponse1

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamLatentPreprocessingResponse1TypedDict(TypedDict):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPreprocessingProcessorResponse1TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamLatentPreprocessingResponse1(BaseModel):
    r"""List of latent preprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPreprocessingProcessorResponse1]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamLatentPostprocessingTypeResponse1 = Literal["latent_feedback",]


class CreateStreamLatentPostprocessingProcessorResponse1TypedDict(TypedDict):
    type: CreateStreamLatentPostprocessingTypeResponse1
    enabled: NotRequired[bool]
    r"""Whether this processor is active"""
    params: NotRequired[Dict[str, Any]]


class CreateStreamLatentPostprocessingProcessorResponse1(BaseModel):
    type: CreateStreamLatentPostprocessingTypeResponse1

    enabled: Optional[bool] = None
    r"""Whether this processor is active"""

    params: Optional[Dict[str, Any]] = None


class CreateStreamLatentPostprocessingResponse1TypedDict(TypedDict):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPostprocessingProcessorResponse1TypedDict]
    r"""List of latent processors to apply"""
    enabled: NotRequired[bool]
    r"""Whether this processor config is active"""


class CreateStreamLatentPostprocessingResponse1(BaseModel):
    r"""List of latent postprocessor configurations for latent processing"""

    processors: List[CreateStreamLatentPostprocessingProcessorResponse1]
    r"""List of latent processors to apply"""

    enabled: Optional[bool] = None
    r"""Whether this processor config is active"""


CreateStreamControlnetModelIDResponse1 = Literal[
    "thibaud/controlnet-sd21-openpose-diffusers",
    "thibaud/controlnet-sd21-hed-diffusers",
    "thibaud/controlnet-sd21-canny-diffusers",
    "thibaud/controlnet-sd21-depth-diffusers",
    "thibaud/controlnet-sd21-color-diffusers",
    "daydreamlive/TemporalNet2-stable-diffusion-2-1",
]
r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""


CreateStreamPreprocessorResponse1 = Literal[
    "blur",
    "canny",
    "depth",
    "depth_tensorrt",
    "external",
    "feedback",
    "hed",
    "lineart",
    "mediapipe_pose",
    "mediapipe_segmentation",
    "openpose",
    "passthrough",
    "pose_tensorrt",
    "realesrgan_trt",
    "sharpen",
    "soft_edge",
    "standard_lineart",
    "temporal_net_tensorrt",
    "upscale",
]
r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""


class CreateStreamControlnetResponse1TypedDict(TypedDict):
    model_id: CreateStreamControlnetModelIDResponse1
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""
    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""
    preprocessor: CreateStreamPreprocessorResponse1
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""
    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""
    preprocessor_params: NotRequired[Dict[str, Any]]
    r"""Additional parameters for the preprocessor. For example, canny edge detection uses 'low_threshold' and 'high_threshold' values."""
    control_guidance_start: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""
    control_guidance_end: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


class CreateStreamControlnetResponse1(BaseModel):
    model_id: CreateStreamControlnetModelIDResponse1
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""

    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""

    preprocessor: CreateStreamPreprocessorResponse1
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Must be one of the supported preprocessors."""

    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""

    preprocessor_params: Optional[Dict[str, Any]] = None
    r"""Additional parameters for the preprocessor. For example, canny edge detection uses 'low_threshold' and 'high_threshold' values."""

    control_guidance_start: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""

    control_guidance_end: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


class CreateStreamSDTurboResponseTypedDict(TypedDict):
    model_id: Literal["stabilityai/sd-turbo"]
    prompt: NotRequired[CreateStreamPromptResponse2TypedDict]
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""
    prompt_interpolation_method: NotRequired[
        CreateStreamPromptInterpolationMethodResponse1
    ]
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""
    normalize_prompt_weights: NotRequired[bool]
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""
    normalize_seed_weights: NotRequired[bool]
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""
    negative_prompt: NotRequired[str]
    r"""Text describing what to avoid in the generated image."""
    guidance_scale: NotRequired[float]
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""
    delta: NotRequired[float]
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""
    num_inference_steps: NotRequired[int]
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""
    t_index_list: NotRequired[List[int]]
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """
    use_safety_checker: NotRequired[bool]
    r"""Whether to use safety checker for content filtering"""
    width: NotRequired[int]
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""
    height: NotRequired[int]
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""
    lora_dict: NotRequired[Nullable[CreateStreamLoraDictResponse1TypedDict]]
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""
    use_lcm_lora: NotRequired[bool]
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""
    lcm_lora_id: NotRequired[str]
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """
    acceleration: NotRequired[CreateStreamAccelerationResponse1]
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""
    use_denoising_batch: NotRequired[bool]
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""
    do_add_noise: NotRequired[bool]
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""
    seed: NotRequired[CreateStreamSeedResponse2TypedDict]
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""
    seed_interpolation_method: NotRequired[CreateStreamSeedInterpolationMethodResponse1]
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""
    enable_similar_image_filter: NotRequired[bool]
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""
    similar_image_filter_threshold: NotRequired[float]
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""
    similar_image_filter_max_skip_frame: NotRequired[int]
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""
    skip_diffusion: NotRequired[bool]
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """
    image_preprocessing: NotRequired[CreateStreamImagePreprocessingResponse1TypedDict]
    r"""List of image preprocessor configurations for image processing"""
    image_postprocessing: NotRequired[CreateStreamImagePostprocessingResponse1TypedDict]
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """
    latent_preprocessing: NotRequired[CreateStreamLatentPreprocessingResponse1TypedDict]
    r"""List of latent preprocessor configurations for latent processing"""
    latent_postprocessing: NotRequired[
        CreateStreamLatentPostprocessingResponse1TypedDict
    ]
    r"""List of latent postprocessor configurations for latent processing"""
    controlnets: NotRequired[List[CreateStreamControlnetResponse1TypedDict]]
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""


class CreateStreamSDTurboResponse(BaseModel):
    model_id: Annotated[
        Annotated[
            Literal["stabilityai/sd-turbo"],
            AfterValidator(validate_const("stabilityai/sd-turbo")),
        ],
        pydantic.Field(alias="model_id"),
    ] = "stabilityai/sd-turbo"

    prompt: Optional[CreateStreamPromptResponse2] = None
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""

    prompt_interpolation_method: Optional[
        CreateStreamPromptInterpolationMethodResponse1
    ] = None
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""

    normalize_prompt_weights: Optional[bool] = None
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""

    normalize_seed_weights: Optional[bool] = None
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""

    negative_prompt: Optional[str] = None
    r"""Text describing what to avoid in the generated image."""

    guidance_scale: Optional[float] = None
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""

    delta: Optional[float] = None
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""

    num_inference_steps: Optional[int] = None
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10-100 with default being 50."""

    t_index_list: Optional[List[int]] = None
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps]. The value should not go above 50.
    """

    use_safety_checker: Optional[bool] = None
    r"""Whether to use safety checker for content filtering"""

    width: Optional[int] = None
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""

    height: Optional[int] = None
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""

    lora_dict: OptionalNullable[CreateStreamLoraDictResponse1] = UNSET
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""

    use_lcm_lora: Optional[bool] = None
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""

    lcm_lora_id: Optional[str] = None
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """

    acceleration: Optional[CreateStreamAccelerationResponse1] = None
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""

    use_denoising_batch: Optional[bool] = None
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""

    do_add_noise: Optional[bool] = None
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""

    seed: Optional[CreateStreamSeedResponse2] = None
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""

    seed_interpolation_method: Optional[
        CreateStreamSeedInterpolationMethodResponse1
    ] = None
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""

    enable_similar_image_filter: Optional[bool] = None
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""

    similar_image_filter_threshold: Optional[float] = None
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""

    similar_image_filter_max_skip_frame: Optional[int] = None
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""

    skip_diffusion: Optional[bool] = None
    r"""Whether to skip the diffusion process. Any ControlNets or diffusion-only parameters are ignored when enabled.
    Example use cases:
    - Stream the output of a preprocessor (e.g. live depth maps or pose skeletons),
    - Run post-processors like RealESRGAN upscaler on externally provided frames,
    - Warm a pipeline without paying the diffusion cost.
    ⚠️ NOTE: skip_diffusion is evaluated at pipeline creation time; switching mid-stream triggers a pipeline reload.

    """

    image_preprocessing: Optional[CreateStreamImagePreprocessingResponse1] = None
    r"""List of image preprocessor configurations for image processing"""

    image_postprocessing: Optional[CreateStreamImagePostprocessingResponse1] = None
    r"""List of image postprocessor configurations for image processing
    ⚠️ NOTE: realesrgan_trt processor requires a restart to change because it affects resolution.

    """

    latent_preprocessing: Optional[CreateStreamLatentPreprocessingResponse1] = None
    r"""List of latent preprocessor configurations for latent processing"""

    latent_postprocessing: Optional[CreateStreamLatentPostprocessingResponse1] = None
    r"""List of latent postprocessor configurations for latent processing"""

    controlnets: Optional[List[CreateStreamControlnetResponse1]] = None
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "prompt",
            "prompt_interpolation_method",
            "normalize_prompt_weights",
            "normalize_seed_weights",
            "negative_prompt",
            "guidance_scale",
            "delta",
            "num_inference_steps",
            "t_index_list",
            "use_safety_checker",
            "width",
            "height",
            "lora_dict",
            "use_lcm_lora",
            "lcm_lora_id",
            "acceleration",
            "use_denoising_batch",
            "do_add_noise",
            "seed",
            "seed_interpolation_method",
            "enable_similar_image_filter",
            "similar_image_filter_threshold",
            "similar_image_filter_max_skip_frame",
            "skip_diffusion",
            "image_preprocessing",
            "image_postprocessing",
            "latent_preprocessing",
            "latent_postprocessing",
            "controlnets",
        ]
        nullable_fields = ["lora_dict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


CreateStreamParamsResponseTypedDict = TypeAliasType(
    "CreateStreamParamsResponseTypedDict",
    Union[
        CreateStreamSDTurboResponseTypedDict,
        CreateStreamSDXLResponseTypedDict,
        CreateStreamSd15ResponseTypedDict,
    ],
)


CreateStreamParamsResponse = Annotated[
    Union[
        Annotated[CreateStreamSDTurboResponse, Tag("stabilityai/sd-turbo")],
        Annotated[CreateStreamSDXLResponse, Tag("stabilityai/sdxl-turbo")],
        Annotated[CreateStreamSd15Response, Tag("Lykon/dreamshaper-8")],
        Annotated[CreateStreamSd15Response, Tag("prompthero/openjourney-v4")],
    ],
    Discriminator(lambda m: get_discriminator(m, "model_id", "model_id")),
]


class CreateStreamResponseTypedDict(TypedDict):
    r"""Default Response"""

    pipeline: CreateStreamPipelineResponse
    params: CreateStreamParamsResponseTypedDict
    id: str
    r"""Unique identifier for the stream"""
    stream_key: str
    r"""Unique key used for streaming to this endpoint"""
    created_at: str
    r"""ISO timestamp when the stream was created"""
    output_playback_id: str
    r"""Playback ID for accessing the stream output"""
    name: str
    r"""Human-readable name of the stream"""
    author: str
    r"""ID of the user who created this stream"""
    from_playground: bool
    r"""Whether this stream was created from the playground interface"""
    gateway_host: str
    r"""Gateway server hostname handling this stream"""
    is_smoke_test: bool
    r"""Whether this is a smoke test stream"""
    whip_url: str
    r"""WebRTC WHIP URL for stream ingestion"""
    output_stream_url: NotRequired[str]
    r"""URL where the processed stream output can be accessed"""


class CreateStreamResponse(BaseModel):
    r"""Default Response"""

    pipeline: CreateStreamPipelineResponse

    params: CreateStreamParamsResponse

    id: str
    r"""Unique identifier for the stream"""

    stream_key: str
    r"""Unique key used for streaming to this endpoint"""

    created_at: str
    r"""ISO timestamp when the stream was created"""

    output_playback_id: str
    r"""Playback ID for accessing the stream output"""

    name: str
    r"""Human-readable name of the stream"""

    author: str
    r"""ID of the user who created this stream"""

    from_playground: bool
    r"""Whether this stream was created from the playground interface"""

    gateway_host: str
    r"""Gateway server hostname handling this stream"""

    is_smoke_test: bool
    r"""Whether this is a smoke test stream"""

    whip_url: str
    r"""WebRTC WHIP URL for stream ingestion"""

    output_stream_url: Optional[str] = None
    r"""URL where the processed stream output can be accessed"""
