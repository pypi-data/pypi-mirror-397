"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from daydream_sdk.types import (
    BaseModel,
    Nullable,
    OptionalNullable,
    UNSET,
    UNSET_SENTINEL,
)
from daydream_sdk.utils import FieldMetadata, PathParamMetadata, RequestMetadata
from pydantic import model_serializer
from typing import Any, Dict, List, Literal, Optional, Union
from typing_extensions import Annotated, NotRequired, TypeAliasType, TypedDict


UpdateStreamModelID = Literal[
    "stabilityai/sd-turbo",
    "prompthero/openjourney-v4",
    "Lykon/dreamshaper-8",
    "stabilityai/sdxl-turbo",
]
r"""Base U-Net model to use for generation. Default: \"stabilityai/sd-turbo\" """


UpdateStreamPrompt2TypedDict = TypeAliasType(
    "UpdateStreamPrompt2TypedDict", Union[str, float]
)


UpdateStreamPrompt2 = TypeAliasType("UpdateStreamPrompt2", Union[str, float])


UpdateStreamPrompt1TypedDict = TypeAliasType(
    "UpdateStreamPrompt1TypedDict", Union[str, List[List[UpdateStreamPrompt2TypedDict]]]
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


UpdateStreamPrompt1 = TypeAliasType(
    "UpdateStreamPrompt1", Union[str, List[List[UpdateStreamPrompt2]]]
)
r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""


UpdateStreamPromptInterpolationMethod = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""


class UpdateStreamLoraDictTypedDict(TypedDict):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


class UpdateStreamLoraDict(BaseModel):
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""


UpdateStreamSeed2TypedDict = TypeAliasType(
    "UpdateStreamSeed2TypedDict", Union[int, float]
)


UpdateStreamSeed2 = TypeAliasType("UpdateStreamSeed2", Union[int, float])


UpdateStreamSeed1TypedDict = TypeAliasType(
    "UpdateStreamSeed1TypedDict", Union[int, List[List[UpdateStreamSeed2TypedDict]]]
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


UpdateStreamSeed1 = TypeAliasType(
    "UpdateStreamSeed1", Union[int, List[List[UpdateStreamSeed2]]]
)
r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""


UpdateStreamSeedInterpolationMethod = Literal[
    "linear",
    "slerp",
]
r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""


UpdateStreamControlnetModelID = Literal[
    "thibaud/controlnet-sd21-openpose-diffusers",
    "thibaud/controlnet-sd21-hed-diffusers",
    "thibaud/controlnet-sd21-canny-diffusers",
    "thibaud/controlnet-sd21-depth-diffusers",
    "thibaud/controlnet-sd21-color-diffusers",
    "lllyasviel/control_v11f1p_sd15_depth",
    "lllyasviel/control_v11f1e_sd15_tile",
    "lllyasviel/control_v11p_sd15_canny",
    "xinsir/controlnet-depth-sdxl-1.0",
    "xinsir/controlnet-canny-sdxl-1.0",
    "xinsir/controlnet-tile-sdxl-1.0",
]
r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""


class UpdateStreamControlnetTypedDict(TypedDict):
    model_id: UpdateStreamControlnetModelID
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""
    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""
    preprocessor: str
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Common options include 'pose_tensorrt', 'soft_edge', 'canny', 'depth_tensorrt', 'passthrough'. If None, no preprocessing is applied.

    ⚠️ NOTE: ControlNet preprocessor must be in the available list (AVAILABLE_PREPROCESSORS).
    """
    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""
    preprocessor_params: NotRequired[Dict[str, Any]]
    r"""Additional parameters for the preprocessor. For example, canny edge detection uses 'low_threshold' and 'high_threshold' values."""
    control_guidance_start: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""
    control_guidance_end: NotRequired[float]
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


class UpdateStreamControlnet(BaseModel):
    model_id: UpdateStreamControlnetModelID
    r"""⚠️ NOTE: ControlNet model_ids must be unique. Additionally, they must be compatible with the selected base model."""

    conditioning_scale: float
    r"""Strength of the ControlNet's influence on generation. Higher values make the model follow the control signal more strictly. Typical range 0.0-1.0, where 0.0 disables the control and 1.0 applies full control. Default: 1.0"""

    preprocessor: str
    r"""Preprocessor to apply to input frames before feeding to the ControlNet. Common options include 'pose_tensorrt', 'soft_edge', 'canny', 'depth_tensorrt', 'passthrough'. If None, no preprocessing is applied.

    ⚠️ NOTE: ControlNet preprocessor must be in the available list (AVAILABLE_PREPROCESSORS).
    """

    enabled: bool
    r"""Whether this ControlNet is active. Disabled ControlNets are not loaded. Default: true"""

    preprocessor_params: Optional[Dict[str, Any]] = None
    r"""Additional parameters for the preprocessor. For example, canny edge detection uses 'low_threshold' and 'high_threshold' values."""

    control_guidance_start: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance begins. 0.0 means guidance starts from the beginning."""

    control_guidance_end: Optional[float] = None
    r"""Fraction of the denoising process (0.0-1.0) when ControlNet guidance ends. 1.0 means guidance continues until the end."""


UpdateStreamType = Literal[
    "regular",
    "faceid",
]
r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""


UpdateStreamWeightType = Literal[
    "linear",
    "ease in",
    "ease out",
    "ease in-out",
    "reverse in-out",
    "weak input",
    "weak output",
    "weak middle",
    "strong middle",
    "style transfer",
    "composition",
    "strong style transfer",
    "style and composition",
    "style transfer precise",
    "composition precise",
]
r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class UpdateStreamIPAdapterTypedDict(TypedDict):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    scale: float
    enabled: bool
    type: NotRequired[UpdateStreamType]
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""
    weight_type: NotRequired[UpdateStreamWeightType]
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class UpdateStreamIPAdapter(BaseModel):
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    scale: float

    enabled: bool

    type: Optional[UpdateStreamType] = None
    r"""Type of IP adapter. Use 'faceid' for SDXL-faceid models, 'regular' for others"""

    weight_type: Optional[UpdateStreamWeightType] = None
    r"""Weight interpolation method for IP adapter style conditioning. Controls how the style influence changes throughout the generation process."""


class ParamsTypedDict(TypedDict):
    r"""Updated pipeline parameters for the stream"""

    model_id: NotRequired[UpdateStreamModelID]
    r"""Base U-Net model to use for generation. Default: \"stabilityai/sd-turbo\" """
    prompt: NotRequired[UpdateStreamPrompt1TypedDict]
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""
    prompt_interpolation_method: NotRequired[UpdateStreamPromptInterpolationMethod]
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""
    normalize_prompt_weights: NotRequired[bool]
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""
    normalize_seed_weights: NotRequired[bool]
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""
    negative_prompt: NotRequired[str]
    r"""Text describing what to avoid in the generated image."""
    guidance_scale: NotRequired[float]
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""
    delta: NotRequired[float]
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""
    num_inference_steps: NotRequired[int]
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10–200 with default being 50."""
    t_index_list: NotRequired[List[int]]
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps].
    """
    use_safety_checker: NotRequired[bool]
    r"""Whether to use safety checker for content filtering"""
    width: NotRequired[int]
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""
    height: NotRequired[int]
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""
    lora_dict: NotRequired[Nullable[UpdateStreamLoraDictTypedDict]]
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""
    use_lcm_lora: NotRequired[bool]
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""
    lcm_lora_id: NotRequired[str]
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """
    acceleration: NotRequired[str]
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""
    use_denoising_batch: NotRequired[bool]
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""
    do_add_noise: NotRequired[bool]
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""
    seed: NotRequired[UpdateStreamSeed1TypedDict]
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""
    seed_interpolation_method: NotRequired[UpdateStreamSeedInterpolationMethod]
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""
    enable_similar_image_filter: NotRequired[bool]
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""
    similar_image_filter_threshold: NotRequired[float]
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""
    similar_image_filter_max_skip_frame: NotRequired[int]
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""
    controlnets: NotRequired[List[UpdateStreamControlnetTypedDict]]
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""
    ip_adapter: NotRequired[UpdateStreamIPAdapterTypedDict]
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""
    ip_adapter_style_image_url: NotRequired[str]
    r"""HTTPS URL of the style image to use. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""


class Params(BaseModel):
    r"""Updated pipeline parameters for the stream"""

    model_id: Optional[UpdateStreamModelID] = None
    r"""Base U-Net model to use for generation. Default: \"stabilityai/sd-turbo\" """

    prompt: Optional[UpdateStreamPrompt1] = None
    r"""Text prompt describing the desired image. Can be a single string or weighted list of (prompt, weight) tuples."""

    prompt_interpolation_method: Optional[UpdateStreamPromptInterpolationMethod] = None
    r"""Method for interpolating between multiple prompts. Slerp provides smoother transitions than linear."""

    normalize_prompt_weights: Optional[bool] = None
    r"""Whether to normalize prompt weights to sum to 1.0 for consistent generation."""

    normalize_seed_weights: Optional[bool] = None
    r"""Whether to normalize seed weights to sum to 1.0 for consistent generation."""

    negative_prompt: Optional[str] = None
    r"""Text describing what to avoid in the generated image."""

    guidance_scale: Optional[float] = None
    r"""Strength of prompt adherence. Higher values make the model follow the prompt more strictly."""

    delta: Optional[float] = None
    r"""Delta sets per-frame denoising progress: lower delta means steadier, less flicker but slower/softer; higher delta means faster, sharper but more flicker/artifacts (often reduce CFG)."""

    num_inference_steps: Optional[int] = None
    r"""Builds the full denoising schedule (the 'grid' of possible refinement steps). Changing it changes what each step number (t_index_list value) means. Keep it fixed for a session and only adjust if you're deliberately redefining the schedule; if you do, proportionally remap your t_index_list. Typical range 10–200 with default being 50."""

    t_index_list: Optional[List[int]] = None
    r"""The ordered list of step indices from the num_inference_steps schedule to execute per frame. Each index is one model pass, so latency scales with the list length. Higher indices (e.g., 40–49 on a 50-step grid) mainly polish and preserve structure (lower flicker), while lower indices (<20) rewrite structure (more flicker, creative). Values must be non-decreasing, and each between 0 and num_inference_steps.

    ⚠️ NOTE: t_index_list must have 1–4 elements, non-decreasing, and within [0, num_inference_steps].
    """

    use_safety_checker: Optional[bool] = None
    r"""Whether to use safety checker for content filtering"""

    width: Optional[int] = None
    r"""Output image width in pixels. Must be divisible by 64 and between 384-1024."""

    height: Optional[int] = None
    r"""Output image height in pixels. Must be divisible by 64 and between 384-1024."""

    lora_dict: OptionalNullable[UpdateStreamLoraDict] = UNSET
    r"""Dictionary mapping LoRA model paths to their weights for fine-tuning the base model."""

    use_lcm_lora: Optional[bool] = None
    r"""Whether to use Latent Consistency Model LoRA for faster inference."""

    lcm_lora_id: Optional[str] = None
    r"""Identifier for the LCM LoRA model to use. Example: \"latent-consistency/lcm-lora-sdv1-5\" """

    acceleration: Optional[str] = None
    r"""Acceleration method for inference. Options: \"none\", \"xformers\", \"tensorrt\". TensorRT provides the best performance but requires engine compilation."""

    use_denoising_batch: Optional[bool] = None
    r"""Whether to process multiple denoising steps in a single batch for efficiency."""

    do_add_noise: Optional[bool] = None
    r"""Whether to add noise to input frames before processing. Enabling this slightly re-noises each frame to improve temporal stability, reduce ghosting/texture sticking, and prevent drift; disabling can yield sharper, lower-latency results but may increase flicker and artifact accumulation over time."""

    seed: Optional[UpdateStreamSeed1] = None
    r"""Random seed for generation. Can be a single integer or weighted list of (seed, weight) tuples."""

    seed_interpolation_method: Optional[UpdateStreamSeedInterpolationMethod] = None
    r"""Method for interpolating between multiple seeds. Slerp provides smoother transitions than linear."""

    enable_similar_image_filter: Optional[bool] = None
    r"""Whether to skip frames that are too similar to the previous output to reduce flicker."""

    similar_image_filter_threshold: Optional[float] = None
    r"""Similarity threshold for the image filter. Higher values allow more variation between frames."""

    similar_image_filter_max_skip_frame: Optional[int] = None
    r"""Maximum number of consecutive frames that can be skipped by the similarity filter."""

    controlnets: Optional[List[UpdateStreamControlnet]] = None
    r"""List of ControlNet configurations for guided generation. Each ControlNet provides different types of conditioning (pose, edges, depth, etc.). Dynamic updates limited to conditioning_scale changes only; cannot add new ControlNets or change model_id/preprocessor/params without reload."""

    ip_adapter: Optional[UpdateStreamIPAdapter] = None
    r"""IP adapter — Turns on IP-Adapter style conditioning and is fully hot-swappable. Available for SDXL, SDXL-faceid, SD1.5"""

    ip_adapter_style_image_url: Optional[str] = None
    r"""HTTPS URL of the style image to use. When set, the runtime downloads the image and configures the pipeline; if omitted, a default style image is used. For 'faceid' type, the image must contain a clear face. Available for SDXL, SDXL-faceid, SD1.5"""

    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = [
            "model_id",
            "prompt",
            "prompt_interpolation_method",
            "normalize_prompt_weights",
            "normalize_seed_weights",
            "negative_prompt",
            "guidance_scale",
            "delta",
            "num_inference_steps",
            "t_index_list",
            "use_safety_checker",
            "width",
            "height",
            "lora_dict",
            "use_lcm_lora",
            "lcm_lora_id",
            "acceleration",
            "use_denoising_batch",
            "do_add_noise",
            "seed",
            "seed_interpolation_method",
            "enable_similar_image_filter",
            "similar_image_filter_threshold",
            "similar_image_filter_max_skip_frame",
            "controlnets",
            "ip_adapter",
            "ip_adapter_style_image_url",
        ]
        nullable_fields = ["lora_dict"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in type(self).model_fields.items():
            k = f.alias or n
            val = serialized.get(k)
            serialized.pop(k, None)

            optional_nullable = k in optional_fields and k in nullable_fields
            is_set = (
                self.__pydantic_fields_set__.intersection({n})
                or k in null_default_fields
            )  # pylint: disable=no-member

            if val is not None and val != UNSET_SENTINEL:
                m[k] = val
            elif val != UNSET_SENTINEL and (
                not k in optional_fields or (optional_nullable and is_set)
            ):
                m[k] = val

        return m


class UpdateStreamRequestBodyTypedDict(TypedDict):
    params: NotRequired[ParamsTypedDict]
    r"""Updated pipeline parameters for the stream"""
    prompt: NotRequired[str]
    r"""Updated prompt to apply to the stream processing"""


class UpdateStreamRequestBody(BaseModel):
    params: Optional[Params] = None
    r"""Updated pipeline parameters for the stream"""

    prompt: Optional[str] = None
    r"""Updated prompt to apply to the stream processing"""


class UpdateStreamRequestTypedDict(TypedDict):
    id: str
    r"""ID of the stream to update"""
    body: NotRequired[UpdateStreamRequestBodyTypedDict]


class UpdateStreamRequest(BaseModel):
    id: Annotated[
        str, FieldMetadata(path=PathParamMetadata(style="simple", explode=False))
    ]
    r"""ID of the stream to update"""

    body: Annotated[
        Optional[UpdateStreamRequestBody],
        FieldMetadata(request=RequestMetadata(media_type="application/json")),
    ] = None


class UpdateStreamResponseTypedDict(TypedDict):
    r"""Default Response"""

    id: str
    r"""Unique identifier for the stream"""
    stream_key: str
    r"""Unique key used for streaming to this endpoint"""
    output_stream_url: str
    r"""URL where the processed stream output can be accessed"""
    pipeline_params: Dict[str, Any]
    r"""Current configuration parameters for the stream pipeline"""
    created_at: str
    r"""ISO timestamp when the stream was created"""
    pipeline_id: str
    r"""ID of the processing pipeline being used"""
    output_playback_id: str
    r"""Playback ID for accessing the stream output"""
    name: str
    r"""Human-readable name of the stream"""
    author: str
    r"""ID of the user who created this stream"""
    from_playground: bool
    r"""Whether this stream was created from the playground interface"""
    gateway_host: str
    r"""Gateway server hostname handling this stream"""
    is_smoke_test: bool
    r"""Whether this is a smoke test stream"""
    whip_url: str
    r"""WebRTC WHIP URL for stream ingestion"""


class UpdateStreamResponse(BaseModel):
    r"""Default Response"""

    id: str
    r"""Unique identifier for the stream"""

    stream_key: str
    r"""Unique key used for streaming to this endpoint"""

    output_stream_url: str
    r"""URL where the processed stream output can be accessed"""

    pipeline_params: Dict[str, Any]
    r"""Current configuration parameters for the stream pipeline"""

    created_at: str
    r"""ISO timestamp when the stream was created"""

    pipeline_id: str
    r"""ID of the processing pipeline being used"""

    output_playback_id: str
    r"""Playback ID for accessing the stream output"""

    name: str
    r"""Human-readable name of the stream"""

    author: str
    r"""ID of the user who created this stream"""

    from_playground: bool
    r"""Whether this stream was created from the playground interface"""

    gateway_host: str
    r"""Gateway server hostname handling this stream"""

    is_smoke_test: bool
    r"""Whether this is a smoke test stream"""

    whip_url: str
    r"""WebRTC WHIP URL for stream ingestion"""
