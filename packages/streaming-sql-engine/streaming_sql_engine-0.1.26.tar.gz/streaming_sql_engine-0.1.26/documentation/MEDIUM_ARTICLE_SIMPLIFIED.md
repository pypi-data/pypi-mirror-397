# Join Data from Anywhere: The Streaming SQL Engine

## How to Join MySQL, PostgreSQL, MongoDB, APIs, and Files in a Single SQL Query

---

_Have you ever needed to join data from a MySQL database with a PostgreSQL database, a MongoDB collection, and a REST API â€” all in one query? Traditional databases can't do this. That's why I built the Streaming SQL Engine._

---

## The Problem: Data Lives Everywhere

Modern applications don't store all their data in one place. You might have:

- User data in PostgreSQL
- Order data in MySQL
- Product catalog in MongoDB
- Pricing information from a REST API
- Inventory data in CSV files
- Product feeds in XML files

**The challenge:** How do you join all this data together?

Traditional solutions require exporting data, importing into a central database, writing complex ETL pipelines, and maintaining data synchronization.

**There had to be a better way.**

---

## The Solution: Streaming SQL Engine

A lightweight Python library that lets you join data from **any source** using standard SQL syntax â€” without exporting, importing, or setting up infrastructure.

---

## ðŸŽ¯ Start Here: The Most Secure Way

### Step 1: Install and Basic Setup

```bash
pip install streaming-sql-engine
```

### Step 2: Use the Default Configuration (Most Stable)

**This is the safest way to start** â€” it handles all edge cases, works with any data types, and requires no special configuration:

```python
from streaming_sql_engine import Engine

# Default configuration: Most stable and reliable
engine = Engine()  # use_polars=False (default)

# Register your data sources
def postgres_users():
    # Your PostgreSQL connection code
    for row in cursor:
        yield {"id": row[0], "name": row[1]}

def mysql_orders():
    # Your MySQL connection code
    for row in cursor:
        yield {"id": row[0], "user_id": row[1], "total": row[2]}

engine.register("users", postgres_users)
engine.register("orders", mysql_orders)

# Write SQL queries
query = """
    SELECT users.name, orders.total
    FROM users
    JOIN orders ON users.id = orders.user_id
    WHERE orders.total > 100
"""

# Execute and iterate results
for row in engine.query(query):
    print(row)
```

**Why This Configuration is Best to Start:**

âœ… **Most Stable**: Handles mixed data types gracefully  
âœ… **No Schema Errors**: No type inference issues  
âœ… **Works Everywhere**: No external dependencies required  
âœ… **Reliable**: Battle-tested Python code  
âœ… **Fast for Small-Medium Data**: 0.72s for 10K rows

**Use this when:**

- You're just getting started
- Your datasets are < 100K rows
- You have mixed data types
- You need maximum reliability
- Polars is not available

---

## ðŸš€ Experimenting with Options

Once you're comfortable with the basics, you can experiment with different options to optimize for your specific use case.

### Option 1: Enable Debug Mode

See what's happening under the hood:

```python
engine = Engine(debug=True)  # Shows execution details
```

**Output:**

```
============================================================
STREAMING SQL ENGINE - DEBUG MODE
============================================================

[1/3] PARSING SQL QUERY...
âœ“ SQL parsed successfully

[2/3] BUILDING LOGICAL PLAN...
âœ“ Logical plan built

[3/3] EXECUTING QUERY...
  Using LOOKUP JOIN (building index...)
```

---

### Option 2: Enable Polars (For Large Datasets)

**When to use**: Large datasets (> 100K rows), consistent data types

```python
engine = Engine(use_polars=True)  # Enable Polars for speed
```

**Benefits:**

- âš¡ Faster for large datasets (vectorized operations)
- ðŸŽ¯ SIMD acceleration
- ðŸ“Š Better for consistent schemas

**Trade-offs:**

- âš ï¸ Requires data normalization (consistent types)
- âš ï¸ Can fail on mixed types
- âš ï¸ Requires Polars dependency

**Example:**

```python
engine = Engine(use_polars=True)

# Make sure your data has consistent types
def normalized_source():
    for row in raw_source():
        yield {
            "id": int(row.get("id", 0)),
            "price": float(row.get("price", 0.0)),
            "name": str(row.get("name", "")),
        }

engine.register("products", normalized_source)
```

---

### Option 3: Enable MMAP (For Large Files)

**When to use**: Large files (> 100MB), memory-constrained systems

```python
engine = Engine()
engine.register("products", source, filename="products.jsonl")  # MMAP enabled
```

**Benefits:**

- ðŸ’¾ 90-99% memory reduction
- ðŸ“ Works with files larger than RAM
- ðŸ”„ OS-managed memory mapping

**Trade-offs:**

- âš ï¸ Requires file-based sources
- âš ï¸ Slower for small files (overhead)

**Example:**

```python
engine = Engine()

def jsonl_source():
    with open("products.jsonl", "r") as f:
        for line in f:
            if line.strip():
                yield json.loads(line)

engine.register("products", jsonl_source, filename="products.jsonl")
```

---

### Option 4: Enable Merge Join (For Sorted Data)

**When to use**: Pre-sorted data, memory-constrained environments

```python
engine = Engine()
engine.register("products", source, ordered_by="id")  # Merge join enabled
```

**Benefits:**

- ðŸ’¾ Lowest memory usage (no index needed)
- âš¡ Fast for sorted data
- ðŸ”„ Streaming algorithm

**Trade-offs:**

- âš ï¸ Requires pre-sorted data
- âš ï¸ Both tables must be sorted

**Example:**

```python
engine = Engine()

# Data must be sorted by join key
def sorted_users():
    # Users sorted by id
    return iter([
        {"id": 1, "name": "Alice"},
        {"id": 2, "name": "Bob"},
        {"id": 3, "name": "Charlie"},
    ])

def sorted_orders():
    # Orders sorted by user_id
    return iter([
        {"id": 1, "user_id": 1, "total": 100},
        {"id": 2, "user_id": 2, "total": 200},
    ])

engine.register("users", sorted_users, ordered_by="id")
engine.register("orders", sorted_orders, ordered_by="user_id")
```

---

## ðŸ”¬ Advanced: Mixing Options

### Mix 1: Polars + MMAP (Best for Large Files)

**The Ultimate Configuration** for large files with consistent types:

```python
engine = Engine(use_polars=True)  # Polars for speed
engine.register("products", source, filename="products.jsonl")  # MMAP for memory
```

**What You Get:**

- âš¡ Fast (Polars vectorized operations)
- ðŸ’¾ Low memory (MMAP 90-99% reduction)
- ðŸŽ¯ Best balance for large datasets

**Performance:**

- Time: 8-15s for 500MB files
- Memory: 0.01 MB (vs 500MB+ without MMAP)

**When to Use:**

- Large files (> 100MB)
- Consistent data types
- Memory-constrained systems

---

### Mix 2: Polars + Column Pruning (For Wide Tables)

**Optimize for tables with many columns:**

```python
engine = Engine(use_polars=True)

def optimized_source(dynamic_columns=None):
    # Only read requested columns
    if dynamic_columns:
        columns = dynamic_columns
    else:
        columns = ["id", "name", "price", "description", "category", ...]  # All columns

    for row in read_data(columns):
        yield row

engine.register("products", optimized_source)
```

**What You Get:**

- ðŸ“‰ Reduced I/O (only reads needed columns)
- âš¡ Faster queries (less data to process)
- ðŸ’¾ Lower memory usage

---

### Mix 3: Polars + Filter Pushdown (For Selective Queries)

**Optimize when queries filter most rows:**

```python
engine = Engine(use_polars=True)

def optimized_source(dynamic_where=None, dynamic_columns=None):
    # Apply WHERE clause at source level
    query = "SELECT * FROM products"
    if dynamic_where:
        query += f" WHERE {dynamic_where}"

    for row in execute_query(query):
        yield row

engine.register("products", optimized_source)
```

**What You Get:**

- ðŸŽ¯ Early filtering (reduces data volume)
- âš¡ Faster execution (less data to process)
- ðŸ“‰ Lower memory usage

---

### Mix 4: All Optimizations Combined

**The Ultimate Configuration** for maximum performance:

```python
engine = Engine(use_polars=True)

def ultimate_source(dynamic_where=None, dynamic_columns=None):
    """
    Source with all optimizations:
    - Filter pushdown (dynamic_where)
    - Column pruning (dynamic_columns)
    - Data normalization (for Polars)
    """
    # Build optimized query
    query = build_query(dynamic_where, dynamic_columns)

    for row in execute_query(query):
        # Normalize types for Polars stability
        yield normalize_types(row)

engine.register("products", ultimate_source, filename="products.jsonl")
```

**What You Get:**

- âš¡ Polars (speed)
- ðŸ’¾ MMAP (memory)
- ðŸ“‰ Column Pruning (I/O)
- ðŸŽ¯ Filter Pushdown (early filtering)

**Best for:** Very large datasets (> 1M rows)

---

## ðŸ“Š Performance Guide

### By Dataset Size

| Size              | Configuration                  | Why                                |
| ----------------- | ------------------------------ | ---------------------------------- |
| **< 10K rows**    | `use_polars=False` (default)   | Fastest, most stable               |
| **10K-100K rows** | `use_polars=False` (default)   | Still fastest, handles mixed types |
| **100K-1M rows**  | `use_polars=True` + `filename` | Polars + MMAP for best balance     |
| **> 1M rows**     | All optimizations              | Maximum performance                |

### By Priority

**Priority: Stability** â†’ Use default (`use_polars=False`)

```python
engine = Engine()  # Most stable
```

**Priority: Speed** â†’ Use Polars

```python
engine = Engine(use_polars=True)  # Fastest for large datasets
```

**Priority: Memory** â†’ Use MMAP

```python
engine = Engine()
engine.register("table", source, filename="data.jsonl")  # Lowest memory
```

**Priority: Both** â†’ Use Polars + MMAP

```python
engine = Engine(use_polars=True)
engine.register("table", source, filename="data.jsonl")  # Fast + Low memory
```

---

## ðŸŽ“ Learning Path

### Level 1: Beginner (Start Here)

```python
# Most stable configuration
engine = Engine()  # Default: use_polars=False
engine.register("table1", source1)
engine.register("table2", source2)
```

**Learn:**

- Basic source registration
- Simple SQL queries
- How joins work

---

### Level 2: Intermediate

```python
# Add debug mode to see what's happening
engine = Engine(debug=True)

# Experiment with Polars for large datasets
engine = Engine(use_polars=True)
```

**Learn:**

- Debug output
- When to use Polars
- Data normalization

---

### Level 3: Advanced

```python
# Use MMAP for large files
engine = Engine(use_polars=True)
engine.register("table", source, filename="data.jsonl")

# Use Merge Join for sorted data
engine.register("table", source, ordered_by="key")
```

**Learn:**

- MMAP for memory efficiency
- Merge Join for sorted data
- Protocol optimizations

---

### Level 4: Expert

```python
# All optimizations combined
engine = Engine(use_polars=True)

def optimized_source(dynamic_where=None, dynamic_columns=None):
    # Filter pushdown + Column pruning
    pass

engine.register("table", optimized_source, filename="data.jsonl")
```

**Learn:**

- Protocol-based optimizations
- Combining all options
- Maximum performance tuning

---

## âš ï¸ Common Pitfalls

### Pitfall 1: Using Polars Without Normalization

**Problem:**

```python
engine = Engine(use_polars=True)
# Mixed types cause schema inference errors
```

**Solution:**

```python
def normalized_source():
    for row in raw_source():
        yield {
            "id": int(row.get("id", 0)),
            "price": float(row.get("price", 0.0)),
        }
```

---

### Pitfall 2: Using MMAP Without Polars (Very Slow)

**Problem:**

```python
engine = Engine(use_polars=False)
engine.register("table", source, filename="data.jsonl")  # Very slow!
```

**Solution:**

```python
engine = Engine(use_polars=True)  # Polars speeds up MMAP index building
engine.register("table", source, filename="data.jsonl")
```

---

### Pitfall 3: Using MMAP for Small Files

**Problem:**

```python
# MMAP overhead > benefit for small files
engine.register("table", source, filename="small.jsonl")  # Slower!
```

**Solution:**

```python
# No filename for small files
engine.register("table", source)  # Faster for < 100MB
```

---

## ðŸŽ¯ Quick Decision Guide

**Just starting?** â†’ Use default (`Engine()`)

**Have large datasets?** â†’ Use `use_polars=True`

**Memory constrained?** â†’ Use `filename` parameter (MMAP)

**Data is sorted?** â†’ Use `ordered_by` parameter (Merge Join)

**Want maximum performance?** â†’ Combine all: `use_polars=True` + `filename` + protocols

---

## Real-World Example: Complete Workflow

### Step 1: Start Simple (Most Secure)

```python
from streaming_sql_engine import Engine

# Start with default (most stable)
engine = Engine()

def postgres_users():
    # Your PostgreSQL code
    pass

def mysql_orders():
    # Your MySQL code
    pass

engine.register("users", postgres_users)
engine.register("orders", mysql_orders)

results = engine.query("SELECT * FROM users JOIN orders ON users.id = orders.user_id")
```

### Step 2: Add Debug Mode (Understand What's Happening)

```python
engine = Engine(debug=True)  # See execution details
```

### Step 3: Optimize for Your Use Case

**If you have large datasets:**

```python
engine = Engine(use_polars=True)  # Enable Polars
```

**If you have large files:**

```python
engine.register("table", source, filename="data.jsonl")  # Enable MMAP
```

**If you have sorted data:**

```python
engine.register("table", source, ordered_by="key")  # Enable Merge Join
```

### Step 4: Combine Optimizations

```python
# Ultimate configuration for large files
engine = Engine(use_polars=True)

def optimized_source(dynamic_where=None, dynamic_columns=None):
    # Supports all optimizations
    pass

engine.register("table", optimized_source, filename="data.jsonl")
```

---

## Summary

### Start Here (Most Secure)

```python
engine = Engine()  # Default: use_polars=False
engine.register("table1", source1)
engine.register("table2", source2)
```

**Why:** Most stable, handles all edge cases, works with any data types

---

### Then Experiment

1. **Add debug mode**: `Engine(debug=True)` - See what's happening
2. **Try Polars**: `Engine(use_polars=True)` - For large datasets
3. **Try MMAP**: `filename="data.jsonl"` - For large files
4. **Try Merge Join**: `ordered_by="key"` - For sorted data

---

### Advanced: Mix Options

**Best Mix for Large Files:**

```python
engine = Engine(use_polars=True)  # Speed
engine.register("table", source, filename="data.jsonl")  # Memory
```

**Best Mix for Maximum Performance:**

```python
engine = Engine(use_polars=True)

def source(dynamic_where=None, dynamic_columns=None):
    # All optimizations
    pass

engine.register("table", source, filename="data.jsonl")
```

---

## Key Takeaways

1. **Start Simple**: Use default configuration (`Engine()`) - it's the most stable
2. **Experiment Gradually**: Add options one at a time to understand their impact
3. **Mix Wisely**: Combine options (Polars + MMAP) for best results
4. **Know When to Use Each**: Small files â†’ default, Large files â†’ Polars + MMAP

---

## Resources

- **PyPI:** `pip install streaming-sql-engine`
- **GitHub:** [Repository URL]
- **Documentation:** See `documentation/` folder for detailed guides

---

**Remember**: Start with the default configuration, then experiment with options as you understand your data and performance needs better.
