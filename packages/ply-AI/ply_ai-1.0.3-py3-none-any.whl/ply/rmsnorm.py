import torch
import triton
import triton.language as tl

# ==========================================
# æ ¸å¿ƒå¼•æ“: RMSNorm Kernel
# ==========================================
# ä¼˜åŒ–ç‚¹ï¼š
# 1. æ˜¾å­˜è¯»å†™æœ€å°åŒ–ï¼šåªè¯»ä¸€æ¬¡ xï¼Œåªå†™ä¸€æ¬¡ yã€‚
# 2. å¯„å­˜å™¨è®¡ç®—ï¼šå¹³æ–¹ã€æ±‚å’Œã€å¼€æ ¹å·éƒ½åœ¨ SRAM é‡Œå®Œæˆã€‚
@triton.jit
def rmsnorm_kernel(
    x_ptr,      # è¾“å…¥æ•°æ®æŒ‡é’ˆ
    w_ptr,      # æƒé‡æŒ‡é’ˆ (gamma)
    out_ptr,    # è¾“å‡ºæŒ‡é’ˆ
    stride_x_row, # è¾“å…¥æ¯è¡Œçš„ stride
    stride_w,     # æƒé‡çš„ stride
    stride_out_row, # è¾“å‡ºæ¯è¡Œçš„ stride
    N_COLS,       # åˆ—æ•° (Hidden Size)
    eps,          # é˜²æ­¢é™¤é›¶
    BLOCK_SIZE: tl.constexpr # å—å¤§å° (éœ€ >= N_COLS)
):
    # 1. ç¡®å®šå½“å‰å¤„ç†å“ªä¸€è¡Œ
    row_idx = tl.program_id(0)
    
    # 2. å‡†å¤‡æŒ‡é’ˆ
    row_start_ptr = x_ptr + row_idx * stride_x_row
    offsets = tl.arange(0, BLOCK_SIZE)
    mask = offsets < N_COLS

    # 3. åŠ è½½æ•°æ® (Load)
    # å…³é”®ï¼šFP16 è¾“å…¥ -> è½¬ FP32 è®¡ç®— (ä¿è¯ç²¾åº¦)
    x = tl.load(row_start_ptr + offsets, mask=mask, other=0.0).to(tl.float32)
    w = tl.load(w_ptr + offsets * stride_w, mask=mask, other=0.0).to(tl.float32)

    # 4. è®¡ç®— RMS (å‡æ–¹æ ¹)
    # PyTorch éœ€è¦ä¸‰æ­¥: x^2 -> mean -> rsqrtï¼Œè¿™é‡Œä¸€æ­¥åˆ°ä½
    x_sq = x * x
    mean_sq = tl.sum(x_sq, axis=0) / N_COLS
    rsqrt = tl.rsqrt(mean_sq + eps)

    # 5. å½’ä¸€åŒ– & ç¼©æ”¾
    out = x * rsqrt * w

    # 6. å†™å› (Store)
    # è½¬å› FP16 å†™å›æ˜¾å­˜
    out_row_start_ptr = out_ptr + row_idx * stride_out_row
    tl.store(out_row_start_ptr + offsets, out.to(tl.float16), mask=mask)

# ==========================================
# å°è£…å±‚ï¼šPlyRMSNorm
# ==========================================
class PlyRMSNorm(torch.nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        super().__init__()
        self.weight = torch.nn.Parameter(torch.ones(hidden_size))
        self.eps = eps

    def forward(self, x):
        # å±•å¹³ä¸º [Total_Rows, Hidden_Size]
        orig_shape = x.shape
        N = x.shape[-1]
        M = x.numel() // N
        
        x_flat = x.view(M, N)
        y_flat = torch.empty_like(x_flat)
        
        # è‡ªåŠ¨è®¡ç®— Block Size (å¿…é¡»æ˜¯ 2 çš„å¹‚æ¬¡)
        BLOCK_SIZE = triton.next_power_of_2(N)
        
        # Grid: ä¸€è¡Œå¯¹åº”ä¸€ä¸ª Block
        grid = (M, )
        
        rmsnorm_kernel[grid](
            x_flat, self.weight, y_flat,
            x_flat.stride(0),
            self.weight.stride(0),
            y_flat.stride(0),
            N, self.eps,
            BLOCK_SIZE=BLOCK_SIZE
        )
        
        return y_flat.view(*orig_shape)

# ==========================================
# æé™è·‘åˆ†æµ‹è¯•
# ==========================================
if __name__ == "__main__":
    torch.manual_seed(0)
    
    # æ¨¡æ‹Ÿ LLaMA-2-7B çš„è´Ÿè½½
    BATCH_TOKENS = 16 * 1024 
    HIDDEN_SIZE = 4096
    
    print(f"ğŸš€ Benchmarking RMSNorm (Memory Bound Operator)...")
    print(f"    Input: [{BATCH_TOKENS}, {HIDDEN_SIZE}] (FP16)")
    print(f"    Hardware: NVIDIA RTX 4090")
    
    if not torch.cuda.is_available():
        print("âŒ No GPU found")
        exit(1)
    
    # å‡†å¤‡æ•°æ®
    x = torch.randn(BATCH_TOKENS, HIDDEN_SIZE, device='cuda', dtype=torch.float16)
    
    # 1. Ply å®ç°
    ply_norm = PlyRMSNorm(HIDDEN_SIZE).cuda()
    
    # 2. PyTorch åŸç”Ÿå®ç°
    class TorchRMSNorm(torch.nn.Module):
        def __init__(self, dim, eps=1e-6):
            super().__init__()
            self.eps = eps
            self.weight = torch.nn.Parameter(torch.ones(dim))
        def forward(self, x):
            dtype = x.dtype
            x = x.float() # å¼ºåˆ¶ FP32 è®¡ç®—
            variance = x.pow(2).mean(-1, keepdim=True)
            x = x * torch.rsqrt(variance + self.eps)
            # [FIXED] ç¡®ä¿ weight ä¹Ÿæ˜¯ float å‚ä¸è®¡ç®—ï¼Œæœ€åå†è½¬å› FP16
            return (self.weight.float() * x).to(dtype)

    # [FIXED] åŠ ä¸Š .half() ç¡®ä¿æƒé‡æ˜¯ FP16ï¼Œè™½ç„¶ forward é‡Œè½¬äº† floatï¼Œä½†éªŒè¯æ—¶ç±»å‹è¦å¯¹é½
    torch_norm = TorchRMSNorm(HIDDEN_SIZE).cuda().half()
    
    # --- 1. éªŒè¯æ­£ç¡®æ€§ ---
    print("ğŸ” Validating...")
    ply_out = ply_norm(x)
    torch_out = torch_norm(x)
    
    # RMSNorm æ¶‰åŠç´¯åŠ ï¼ŒFP16ä¸‹è¯¯å·®ä¼šæ¯” MatMul å¤§ä¸€ç‚¹ç‚¹ï¼Œæ˜¯æ­£å¸¸çš„
    if torch.allclose(ply_out, torch_out, atol=1e-2, rtol=1e-2):
        print("âœ… Correctness: PASSED")
    else:
        print("âš ï¸ Mismatch")
        print(f"   Max Diff: {(ply_out - torch_out).abs().max().item()}")

    # --- 2. æ€§èƒ½æµ‹è¯• ---
    print("â±ï¸  Speed Test...")
    start = torch.cuda.Event(enable_timing=True)
    end = torch.cuda.Event(enable_timing=True)
    
    # PyTorch
    for _ in range(10): _ = torch_norm(x) # é¢„çƒ­
    start.record()
    for _ in range(200): # è·‘ 200 è½®
        _ = torch_norm(x)
    end.record()
    torch.cuda.synchronize()
    torch_ms = start.elapsed_time(end) / 200
    
    # Ply
    for _ in range(10): _ = ply_norm(x) # é¢„çƒ­
    start.record()
    for _ in range(200):
        _ = ply_norm(x)
    end.record()
    torch.cuda.synchronize()
    ply_ms = start.elapsed_time(end) / 200
    
    print("-" * 60)
    print(f"PyTorch RMSNorm: {torch_ms:.4f} ms")
    print(f"Ply RMSNorm:     {ply_ms:.4f} ms")
    print(f"âš¡ Speedup: {torch_ms/ply_ms:.2f}x")
    
    # æœ‰æ•ˆæ˜¾å­˜å¸¦å®½
    total_bytes = 4 * x.numel() # Read X(2) + Write Y(2)
    gb_s = (total_bytes / 1e9) / (ply_ms / 1000)
    print(f"ğŸ’¾ Effective Bandwidth: {gb_s:.2f} GB/s")
    print("-" * 60)

