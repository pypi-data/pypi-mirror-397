import torch
import triton
import triton.language as tl

# ==========================================
# 0. è¾…åŠ©å·¥å…·: 2-bit æ‰“åŒ…é€»è¾‘ (Pythonç«¯)
# ==========================================
# æ˜ å°„è§„åˆ™: 0->00, +1->01, -1->10
def pack_ternary_weights(w):
    # w: [-1, 0, 1] çš„ float/int å¼ é‡
    assert w.dim() == 2
    N, K = w.shape
    assert K % 4 == 0, "Hidden dim must be divisible by 4"
    
    # æ˜ å°„
    w_mapped = torch.zeros_like(w, dtype=torch.uint8)
    w_mapped[w == 1] = 1
    w_mapped[w == -1] = 2
    
    # Pack
    w_packed = torch.zeros((N, K // 4), dtype=torch.uint8, device=w.device)
    w_packed |= (w_mapped[:, 0::4] << 0)
    w_packed |= (w_mapped[:, 1::4] << 2)
    w_packed |= (w_mapped[:, 2::4] << 4)
    w_packed |= (w_mapped[:, 3::4] << 6)
    
    return w_packed

# ==========================================
# 1. BitNet Kernel (2-bit è§£åŒ… + MatMul)
# ==========================================
@triton.jit
def bitnet_matmul_kernel(
    a_ptr, b_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_M)
    num_pid_n = tl.cdiv(N, BLOCK_N)
    pid_m = pid % num_pid_m
    pid_n = pid // num_pid_m

    offs_am = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M
    offs_bn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N
    offs_k = tl.arange(0, BLOCK_K)
    
    # è®¡ç®— A çš„æŒ‡é’ˆ
    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
    
    # è®¡ç®— B çš„æŒ‡é’ˆ (æ³¨æ„ B æ˜¯å‹ç¼©è¿‡çš„)
    # æˆ‘ä»¬å°†åœ¨ loop å†…éƒ¨åŠ¨æ€è®¡ç®— offsetsï¼Œè¿™é‡Œå…ˆä¸åˆå§‹åŒ– b_ptrs
    
    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    
    for k in range(0, tl.cdiv(K, BLOCK_K)):
        # --- Load A ---
        # è¾¹ç•Œæ£€æŸ¥: kç»´åº¦æ˜¯å¦è¶Šç•Œ
        k_remaining = K - k * BLOCK_K
        a_mask = offs_k[None, :] < k_remaining
        
        a = tl.load(a_ptrs, mask=a_mask, other=0.0)
        
        # --- Load B (Packed) ---
        # 1. è®¡ç®—å½“å‰å—åœ¨ K ç»´åº¦çš„ç»å¯¹åæ ‡
        current_k_start = k * BLOCK_K
        
        # 2. è®¡ç®—å‹ç¼©åçš„ç´¢å¼• (é™¤ä»¥ 4)
        # offs_k æ˜¯ [0, 1, ... BLOCK_K-1]
        # packed_k_idxs å°†ä¼šæ˜¯ [0, 0, 0, 0, 1, 1, 1, 1 ...]
        # è¿™æ„å‘³ç€æˆ‘ä»¬åŒä¸€ä¸ª byte ä¼šè¯»å– 4 æ¬¡ï¼Œè¿™æ˜¯ Triton å‘é‡åŒ–åŠ è½½çš„ç‰¹æ€§
        # è™½ç„¶æœ‰ç‚¹æµªè´¹å¸¦å®½ï¼Œä½†é€»è¾‘ç®€å•ä¸”åˆ©ç”¨äº† L1 Cache
        packed_k_idxs = (current_k_start + offs_k) // 4
        
        # 3. è®¡ç®—ä½ç§»é‡
        shift_amounts = ((current_k_start + offs_k) % 4) * 2
        
        # 4. è®¡ç®— B çš„æŒ‡é’ˆåœ°å€
        # B shape [K/4, N]
        # offs_k æ˜¯è¡Œ(K)ï¼Œoffs_bn æ˜¯åˆ—(N)
        b_ptrs_curr = b_ptr + packed_k_idxs[:, None] * stride_bk + offs_bn[None, :] * stride_bn
        
        # [FIXED] æ·»åŠ  Mask
        # æˆ‘ä»¬éœ€è¦æ£€æŸ¥ K è¾¹ç•Œ (è™½ç„¶æ˜¯å‹ç¼©çš„ï¼Œä½†é€»è¾‘ K ç´¢å¼•å¿…é¡»åœ¨èŒƒå›´å†…)
        # åŒæ—¶ä¹Ÿæ£€æŸ¥ N è¾¹ç•Œ
        b_mask = (offs_k[:, None] < k_remaining) & (offs_bn[None, :] < N)
        
        b_packed = tl.load(b_ptrs_curr, mask=b_mask, other=0)
        
        # --- Unpack (On-the-fly) ---
        # (byte >> shift) & 0x3
        b_2bit = (b_packed >> shift_amounts[:, None]) & 0x3
        
        # Map 0->0, 1->1, 2->-1
        # è¿™æ˜¯ä¸€ä¸ªæ— åˆ†æ”¯çš„è½¬æ¢
        b_fp16 = (b_2bit == 1).to(tl.float16) - (b_2bit == 2).to(tl.float16)
        
        # --- Compute ---
        accumulator += tl.dot(a, b_fp16)
        
        # æ­¥è¿› A
        a_ptrs += BLOCK_K * stride_ak
        
    c = accumulator.to(tl.float16)
    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
    tl.store(c_ptrs, c, mask=c_mask)

# ==========================================
# 2. PlyBitLinear æ¨¡å—
# ==========================================
class PlyBitLinear(torch.nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        
        # éšæœºç”Ÿæˆ [-1, 0, 1] æƒé‡
        raw_w = torch.randint(-1, 2, (out_features, in_features), dtype=torch.float32)
        
        # æ‰“åŒ… (Pack) -> (K/4, N)
        # æ³¨æ„: Triton Kernel æœŸæœ› B æ˜¯ (K, N) å¸ƒå±€ (Col-Major conceptually for weights often)
        # è¿™é‡Œæˆ‘ä»¬æŒ‰ (In, Out) å³ (K, N) å­˜å‚¨
        w_t = raw_w.t().contiguous()
        self.register_buffer('packed_weight', pack_ternary_weights(w_t))
        
        self.scale = torch.nn.Parameter(torch.tensor(1.0 / (in_features ** 0.5)))

    def forward(self, x):
        M, K = x.shape
        N = self.out_features
        
        y = torch.empty((M, N), device=x.device, dtype=torch.float16)
        
        grid = lambda META: (
            triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']),
        )
        
        bitnet_matmul_kernel[grid](
            x, self.packed_weight, y,
            M, N, K,
            x.stride(0), x.stride(1),
            self.packed_weight.stride(0), self.packed_weight.stride(1),
            y.stride(0), y.stride(1),
            BLOCK_M=128, BLOCK_N=128, BLOCK_K=64 # ç¨å¾®åŠ å¤§ K å—å¤§å°ä»¥åˆ©ç”¨å¸¦å®½
        )
        
        return y * self.scale

# ==========================================
# 3. éªŒè¯ä¸å¯¹æ¯”
# ==========================================
if __name__ == "__main__":
    torch.manual_seed(0)
    
    M = 4096 
    K = 4096 
    N = 4096 
    
    print(f"ğŸš€ Benchmarking Ply-BitLinear (1.58-bit / 2-bit Packed)...")
    print(f"    Shape: {M}x{K} @ {K}x{N}")
    
    if not torch.cuda.is_available(): exit(1)
    
    x = torch.randn(M, K, device='cuda', dtype=torch.float16)
    bit_layer = PlyBitLinear(K, N).cuda()
    
    # æ˜¾å­˜å¯¹æ¯”
    print("-" * 40)
    print(f"    FP16 Weight Size:   {2 * K * N / 1024**2:.2f} MB")
    print(f"    BitNet Weight Size: {bit_layer.packed_weight.numel() / 1024**2:.2f} MB (â¬‡ï¸ 87.5% reduction!)")
    print("-" * 40)
    
    print("â±ï¸  Speed Test...")
    start = torch.cuda.Event(enable_timing=True)
    end = torch.cuda.Event(enable_timing=True)
    
    # Ply BitNet
    for _ in range(10): bit_layer(x)
    start.record()
    for _ in range(100):
        bit_layer(x)
    end.record()
    torch.cuda.synchronize()
    ply_ms = start.elapsed_time(end) / 100
    
    # PyTorch FP16
    torch_layer = torch.nn.Linear(K, N, bias=False).cuda().half()
    for _ in range(10): torch_layer(x)
    start.record()
    for _ in range(100):
        torch_layer(x)
    end.record()
    torch.cuda.synchronize()
    torch_ms = start.elapsed_time(end) / 100
    
    print(f"    PyTorch FP16 Linear: {torch_ms:.4f} ms")
    print(f"    Ply BitNet Linear:   {ply_ms:.4f} ms")
    print(f"    âš¡ Speedup: {torch_ms / ply_ms:.2f}x")

