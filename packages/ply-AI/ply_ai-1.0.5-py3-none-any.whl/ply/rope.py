import torch
import triton
import triton.language as tl

# ==========================================
# æ ¸å¿ƒå¼•æ“: RoPE Kernel
# ==========================================
# æ•°å­¦åŸç†:
# å°†å‘é‡ x åˆ†ä¸ºå‰åŠéƒ¨åˆ† x_1 å’ŒååŠéƒ¨åˆ† x_2
# x_new_1 = x_1 * cos - x_2 * sin
# x_new_2 = x_2 * cos + x_1 * sin
@triton.jit
def rope_kernel(
    t_ptr,      # è¾“å…¥å¼ é‡ (Q æˆ– K)
    c_ptr,      # Cos è¡¨æŒ‡é’ˆ
    s_ptr,      # Sin è¡¨æŒ‡é’ˆ
    out_ptr,    # è¾“å‡ºæŒ‡é’ˆ
    # æ­¥é•¿
    stride_batch, stride_seq, stride_head, stride_dim,
    stride_c_seq, stride_c_dim, # Cos/Sin æ­¥é•¿
    # å½¢çŠ¶
    SEQ_LEN, HEAD_DIM,
    # å—å¤§å°
    BLOCK_SIZE: tl.constexpr
):
    # 1. è®¡ç®—ç´¢å¼•
    # RoPE æ˜¯ Element-wise æ“ä½œï¼Œæˆ‘ä»¬æ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸€ä¸ª Head çš„ä¸€éƒ¨åˆ†
    pid = tl.program_id(0)
    
    # è§£æ pid å¯¹åº”çš„ Batch, Seq, Head
    # è¿™é‡Œçš„ Grid æˆ‘ä»¬è®¾ä¸º (Batch * Seq * Head)
    # æˆ‘ä»¬åªå¹¶è¡ŒåŒ–è¿™ä¸‰ä¸ªç»´åº¦ï¼ŒDIM ç»´åº¦åœ¨ Block å†…éƒ¨å¤„ç†
    batch_seq_head_idx = pid
    
    # 2. è®¡ç®—å½“å‰å¤„ç†çš„ Head çš„èµ·å§‹ä½ç½®
    # å‡è®¾è¾“å…¥æ˜¯ [Batch, Seq, Head, Dim] å¸ƒå±€
    # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬åªè®¡ç®—ç›¸å¯¹äº t_ptr çš„æ€»åç§»é‡
    # è¿™ç§å†™æ³•å‡è®¾æˆ‘ä»¬ä»å¤–éƒ¨ä¼ å…¥å±•å¹³åçš„ Grid
    pass 
    # (ç”±äº Triton çš„ Grid æ˜ å°„æ¯”è¾ƒçµæ´»ï¼Œæˆ‘ä»¬åœ¨ Pythonç«¯è®¡ç®— Offset æ›´ç®€å•)
    
    # é‡æ–°è®¾è®¡ï¼šè®©æ¯ä¸ª Block å¤„ç†ä¸€è¡Œ (å³ä¸€ä¸ª Head çš„ Dim ç»´åº¦)
    # Grid = (Total_Tokens * Num_Heads, )
    row_idx = pid 
    
    # 3. è®¡ç®— Cos/Sin çš„ç´¢å¼• (å–å†³äº Seq ç»´åº¦)
    # å‡è®¾è¾“å…¥å±•å¹³ä¸º [Total_Tokens * Num_Heads, Head_Dim]
    # æˆ‘ä»¬éœ€è¦çŸ¥é“å½“å‰æ˜¯ç¬¬å‡ ä¸ª Token (Seq_Idx)
    # Num_Heads æ˜¯å¸¸æ•°å—ï¼Ÿä¸ºäº†é€šç”¨æ€§ï¼Œæˆ‘ä»¬é€šè¿‡ stride åæ¨
    
    # ä¸ºäº†æ€§èƒ½ï¼Œæˆ‘ä»¬ç®€åŒ–åœºæ™¯ï¼š
    # è¾“å…¥å·²ç»è¢« reshape æˆ [Total_Rows, Head_Dim]
    # æˆ‘ä»¬è¿˜éœ€è¦ä¼ å…¥ä¸€ä¸ª seq_ids æ•°ç»„ï¼Œå‘Šè¯‰ Kernelæ¯ä¸€è¡Œå¯¹åº”å“ªä¸ªä½ç½®
    # ä½†ä¸ºäº†å…¼å®¹ PyTorch çš„ RoPE æ¥å£ï¼Œé€šå¸¸æ˜¯ä¼ å…¥ offset
    
    # è¿™é‡Œæ¼”ç¤º LLaMA é£æ ¼çš„ RoPE:
    # æ¯ä¸€ä¸ª Block å¤„ç† Head_Dim çš„ä¸€åŠ (HALF_DIM)
    HALF_DIM = HEAD_DIM // 2
    
    # åç§»é‡ [0, 1, ... HALF_DIM-1]
    offsets = tl.arange(0, BLOCK_SIZE)
    mask = offsets < HALF_DIM

    # è®¡ç®—è¾“å…¥æŒ‡é’ˆ (å‰åŠéƒ¨åˆ† x1)
    # t_ptr å·²ç»åç§»åˆ°äº†å½“å‰ Head çš„èµ·å§‹ä½ç½®
    x1_ptr = t_ptr + row_idx * stride_head + offsets * stride_dim
    # è®¡ç®—è¾“å…¥æŒ‡é’ˆ (ååŠéƒ¨åˆ† x2)
    x2_ptr = t_ptr + row_idx * stride_head + (offsets + HALF_DIM) * stride_dim
    
    # åŠ è½½ x1, x2
    x1 = tl.load(x1_ptr, mask=mask, other=0.0).to(tl.float32)
    x2 = tl.load(x2_ptr, mask=mask, other=0.0).to(tl.float32)
    
    # åŠ è½½ cos, sin
    # éœ€è¦è®¡ç®—å½“å‰ row å¯¹åº”çš„ seq_idxã€‚
    # å‡è®¾ t_ptr å·²ç»æ˜¯ [Batch, Seq, Head, Dim]
    # row_idx = batch_idx * (Seq * Head) + seq_idx * Head + head_idx
    # è¿™ç§é™¤æ³•åœ¨ Kernel é‡Œå¾ˆæ…¢ã€‚
    
    # === æé€Ÿæ–¹æ¡ˆ ===
    # æˆ‘ä»¬è®© Python ç®—å¥½ Cos/Sin çš„æŒ‡é’ˆä¼ è¿›æ¥ï¼
    # å‡è®¾ c_ptr, s_ptr å·²ç»å¹¿æ’­åˆ°äº†è·Ÿ t_ptr ä¸€æ ·çš„å½¢çŠ¶ [Total_Rows, Dim]
    # æˆ–è€…æ›´å¸¸è§çš„ï¼šCos/Sin æ˜¯ [Max_Seq, Dim]ï¼Œæˆ‘ä»¬éœ€è¦æŸ¥è¡¨
    
    # è¿™é‡Œé‡‡ç”¨æŸ¥è¡¨æ³•: ä¼ å…¥ seq_start_idx
    # å®é™…ä¸Šï¼Œä¸ºäº†æè‡´é€Ÿåº¦ï¼Œæˆ‘ä»¬ç›´æ¥è®¡ç®— Cos/Sin æŒ‡é’ˆ
    # æˆ‘ä»¬å‡è®¾ c_ptr æŒ‡å‘çš„æ˜¯å½“å‰ batch/seq å¯¹åº”çš„ cos è¡Œ
    # è¿™éœ€è¦ Python ç«¯é…åˆã€‚è¿™é‡Œä¸ºäº† Demo ç®€å•ï¼Œæˆ‘ä»¬å‡è®¾ c_ptr æ˜¯ [Total_Rows, Dim] å·²ç»å±•å¼€å¥½çš„
    
    c_row_ptr = c_ptr + row_idx * stride_c_seq # è¿™é‡Œçš„ stride å®é™…ä¸Šæ˜¯ Head_Dim
    s_row_ptr = s_ptr + row_idx * stride_c_seq
    
    cos = tl.load(c_row_ptr + offsets * stride_c_dim, mask=mask, other=0.0).to(tl.float32)
    sin = tl.load(s_row_ptr + offsets * stride_c_dim, mask=mask, other=0.0).to(tl.float32)
    
    # 4. æ—‹è½¬è®¡ç®— (Rotation)
    y1 = x1 * cos - x2 * sin
    y2 = x1 * sin + x2 * cos
    
    # 5. å†™å›
    out_x1_ptr = out_ptr + row_idx * stride_head + offsets * stride_dim
    out_x2_ptr = out_ptr + row_idx * stride_head + (offsets + HALF_DIM) * stride_dim
    
    tl.store(out_x1_ptr, y1.to(tl.float16), mask=mask)
    tl.store(out_x2_ptr, y2.to(tl.float16), mask=mask)

# ==========================================
# å°è£…å±‚ï¼šPlyRoPE
# ==========================================
class PlyRoPE(torch.nn.Module):
    def __init__(self, head_dim, max_position_embeddings=2048, base=10000):
        super().__init__()
        self.head_dim = head_dim
        # é¢„è®¡ç®— Cos/Sin è¡¨ (Cache)
        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))
        t = torch.arange(max_position_embeddings).float()
        freqs = torch.outer(t, inv_freq)
        
        # LLaMA é£æ ¼: cat(cos, cos), cat(sin, sin) ä¸å¤ªä¸€æ ·
        # æ ‡å‡† RoPE æ˜¯ interleaved [x1, x2, x3, x4] -> [-x2, x1, -x4, x3]
        # ä½† LLaMA æ˜¯ sliced [x_half_1, x_half_2] -> [-x_half_2, x_half_1]
        # æˆ‘ä»¬è¿™é‡Œå®ç° LLaMA é£æ ¼ (æ›´é€‚åˆ Triton è¿ç»­å†…å­˜è¯»å–)
        
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer("cos_cached", emb.cos().to(torch.float16))
        self.register_buffer("sin_cached", emb.sin().to(torch.float16))

    def forward(self, q, k):
        # q, k shape: [Batch, Seq, Num_Heads, Head_Dim]
        # æˆ‘ä»¬éœ€è¦æŠŠå®ƒå±•å¹³ä¸º [Total_Heads, Head_Dim] æ¥å¹¶è¡Œå¤„ç†
        # Total_Heads = Batch * Seq * Num_Heads
        
        batch, seq_len, n_heads, head_dim = q.shape
        assert head_dim == self.head_dim
        
        # å‡†å¤‡ Cos/Sin
        # æˆªå–å½“å‰ seq_len é•¿åº¦ï¼Œå¹¶å¹¿æ’­åˆ° Batch å’Œ Heads
        # cos: [Seq, Dim] -> [Batch, Seq, Heads, Dim] -> Flatten
        # ä¸ºäº† Triton æ–¹ä¾¿ï¼Œæˆ‘ä»¬åœ¨ Python ç«¯åš expand (é›¶æ‹·è´)
        cos = self.cos_cached[:seq_len].unsqueeze(0).unsqueeze(2) # [1, Seq, 1, Dim]
        sin = self.sin_cached[:seq_len].unsqueeze(0).unsqueeze(2)
        
        cos = cos.expand(batch, seq_len, n_heads, head_dim).contiguous().view(-1, head_dim)
        sin = sin.expand(batch, seq_len, n_heads, head_dim).contiguous().view(-1, head_dim)
        
        # å‡†å¤‡è¾“å…¥è¾“å‡º
        q_flat = q.contiguous().view(-1, head_dim)
        k_flat = k.contiguous().view(-1, head_dim)
        
        q_out = torch.empty_like(q_flat)
        k_out = torch.empty_like(k_flat)
        
        # Grid
        n_rows = q_flat.shape[0]
        # Block Size å¤„ç†ä¸€åŠç»´åº¦ (Head_Dim // 2)
        # å¿…é¡»æ˜¯ 2 çš„å¹‚
        BLOCK_SIZE = triton.next_power_of_2(head_dim // 2)
        
        # å¯åŠ¨ Kernel (Q)
        rope_kernel[(n_rows,)](
            q_flat, cos, sin, q_out,
            0, 0, head_dim, 1, # q strides (è§†ä¸º 1D array of rows)
            head_dim, 1,       # cos strides
            n_rows, head_dim,
            BLOCK_SIZE=BLOCK_SIZE
        )
        
        # å¯åŠ¨ Kernel (K)
        rope_kernel[(n_rows,)](
            k_flat, cos, sin, k_out,
            0, 0, head_dim, 1,
            head_dim, 1,
            n_rows, head_dim,
            BLOCK_SIZE=BLOCK_SIZE
        )
        
        return q_out.view(batch, seq_len, n_heads, head_dim), k_out.view(batch, seq_len, n_heads, head_dim)

# ==========================================
# æé™è·‘åˆ†
# ==========================================
if __name__ == "__main__":
    torch.manual_seed(0)
    
    # æ¨¡æ‹Ÿ Orion-0.1B é…ç½®
    BATCH = 4
    SEQ = 2048
    HEADS = 16
    DIM = 64
    
    print(f"ğŸš€ Benchmarking RoPE (Rotary Positional Embeddings)...")
    print(f"    Input: [{BATCH}, {SEQ}, {HEADS}, {DIM}]")
    
    if not torch.cuda.is_available(): exit(1)

    # æ•°æ®
    q = torch.randn(BATCH, SEQ, HEADS, DIM, device='cuda', dtype=torch.float16)
    k = torch.randn(BATCH, SEQ, HEADS, DIM, device='cuda', dtype=torch.float16)
    
    # 1. Ply RoPE
    ply_rope = PlyRoPE(DIM).cuda()
    
    # 2. PyTorch åŸç”Ÿå®ç° (LLaMA é£æ ¼)
    def apply_rotary_pos_emb(q, k, cos, sin):
        # cos, sin: [1, Seq, 1, Dim]
        q_embed = (q * cos) + (rotate_half(q) * sin)
        k_embed = (k * cos) + (rotate_half(k) * sin)
        return q_embed, k_embed

    def rotate_half(x):
        x1 = x[..., : x.shape[-1] // 2]
        x2 = x[..., x.shape[-1] // 2 :]
        return torch.cat((-x2, x1), dim=-1)

    # é¢„å¤‡ PyTorch çš„ cos/sin
    cos_torch = ply_rope.cos_cached[:SEQ].unsqueeze(0).unsqueeze(2)
    sin_torch = ply_rope.sin_cached[:SEQ].unsqueeze(0).unsqueeze(2)
    
    # --- éªŒè¯ ---
    print("ğŸ” Validating...")
    pq, pk = ply_rope(q, k)
    tq, tk = apply_rotary_pos_emb(q, k, cos_torch, sin_torch)
    
    if torch.allclose(pq, tq, atol=1e-2, rtol=1e-2):
        print("âœ… Correctness: PASSED")
    else:
        print("âš ï¸ Mismatch")
        print(f"   Max Diff: {(pq - tq).abs().max().item()}")

    # --- æ€§èƒ½ ---
    print("â±ï¸  Speed Test...")
    start = torch.cuda.Event(enable_timing=True)
    end = torch.cuda.Event(enable_timing=True)
    
    # PyTorch
    for _ in range(10): apply_rotary_pos_emb(q, k, cos_torch, sin_torch)
    start.record()
    for _ in range(100):
        apply_rotary_pos_emb(q, k, cos_torch, sin_torch)
    end.record()
    torch.cuda.synchronize()
    torch_ms = start.elapsed_time(end) / 100
    
    # Ply
    for _ in range(10): ply_rope(q, k)
    start.record()
    for _ in range(100):
        ply_rope(q, k)
    end.record()
    torch.cuda.synchronize()
    ply_ms = start.elapsed_time(end) / 100
    
    print("-" * 60)
    print(f"PyTorch RoPE: {torch_ms:.4f} ms")
    print(f"Ply RoPE:     {ply_ms:.4f} ms")
    print(f"âš¡ Speedup: {torch_ms/ply_ms:.2f}x")
    print("-" * 60)

