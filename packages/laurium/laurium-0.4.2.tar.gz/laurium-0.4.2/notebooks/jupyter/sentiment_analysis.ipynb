{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Laurium\n",
    "\n",
    "This notebook demonstrates how to perform sentiment analysis using the laurium library with:\n",
    "- Ollama as the LLM platform (Qwen2.5:7b model)\n",
    "- Pydantic for structured output parsing\n",
    "- Custom prompts for sentiment classification\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll build a sentiment classifier that:\n",
    "1. Takes text input\n",
    "2. Returns structured JSON with sentiment labels (1=positive, 0=negative)\n",
    "3. Processes data in batches using pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary modules from laurium and supporting libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "import pandas as pd\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "from laurium.decoder_models import extract, llm, prompts, pydantic_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1. Create LLM Instance\n",
    "\n",
    "We'll use Ollama with the Qwen2.5:7b model for our sentiment analysis. Setting temperature to 0.0 ensures consistent, deterministic outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLM instance with Ollama platform\n",
    "sentiment_llm = llm.create_llm(\n",
    "    llm_platform=\"ollama\", model_name=\"qwen2.5:7b\", temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. Define Schema and Build Prompt\n",
    "The prompt is crucial for getting structured output.\n",
    "We define our output schema once and use it in both the prompt and Pydantic\n",
    "model. This eliminates the need to manually specify JSON format in the prompt.\n",
    "\n",
    "**Key Points:**\n",
    "- Clear instructions for sentiment classification\n",
    "- Define schema once, use everywhere\n",
    "- Automatic JSON format generation\n",
    "- No manual format specification needed\n",
    "- Ensures consistency between prompt and Pydantic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\"ai_label\": Literal[0, 1]}  # 1 for positive, 0 for negative\n",
    "descriptions = {\n",
    "    \"ai_label\": \"Sentiment classification (1=positive, 0=negative)\"\n",
    "}\n",
    "\n",
    "# 3. Build prompt with automatic schema integration\n",
    "system_message = prompts.create_system_message(\n",
    "    base_message=\"You are a sentiment analysis assistant.\"\n",
    "    \"Use 1 for positive sentiment, 0 for negative sentiment.\",\n",
    "    keywords=[\"positive\", \"negative\"],\n",
    ")\n",
    "\n",
    "extraction_prompt = prompts.create_prompt(\n",
    "    system_message=system_message,\n",
    "    examples=None,\n",
    "    example_human_template=None,\n",
    "    example_assistant_template=None,\n",
    "    final_query=\"Analyze this text: {text}\",\n",
    "    schema=schema,  # Automatically formats JSON structure in prompt\n",
    "    descriptions=descriptions,  # Provides field context to LLM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### inspect prompt\n",
    "print(\"Generated system message:\")\n",
    "print(extraction_prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3. Define Output Schema with Pydantic\n",
    "\n",
    "We create a dynamic Pydantic model using the exact same schema and\n",
    "descriptions we defined above. This ensures perfect consistency between what\n",
    "the LLM is instructed to output and what our parser expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputModel = pydantic_models.make_dynamic_example_model(\n",
    "    schema=schema, descriptions=descriptions, model_name=\"SentimentOutput\"\n",
    ")\n",
    "\n",
    "# Create Pydantic output parser\n",
    "parser = PydanticOutputParser(pydantic_object=OutputModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4. Create Batch Extractor\n",
    "\n",
    "The BatchExtractor combines our LLM, prompt, and parser to process multiple texts efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the batch extractor\n",
    "extractor = extract.BatchExtractor(\n",
    "    llm=sentiment_llm, prompt=extraction_prompt, parser=parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 5. Prepare Test Data\n",
    "\n",
    "Let's create a small dataset with examples of positive and negative sentiment to test our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data with clear positive and negative examples\n",
    "data = pd.DataFrame(\n",
    "    {\n",
    "        \"text\": [\n",
    "            \"I absolutely love this product!\",\n",
    "            \"This is terrible, worst purchase ever.\",\n",
    "            \"Great value for money, highly recommend!\",\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 6. Process Data and View Results\n",
    "\n",
    "Now we'll run our sentiment analysis on the test data and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.process_chunk(data, text_column=\"text\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
