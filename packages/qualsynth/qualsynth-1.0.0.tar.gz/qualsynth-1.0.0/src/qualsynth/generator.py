"""
QualSynthGenerator: Simple API for Quality-Driven Synthetic Data Generation

This module provides a user-friendly interface for generating synthetic samples
using the QualSynth methodology. It wraps the more complex IterativeWorkflow
with sensible defaults for common use cases.

Example usage:
    from qualsynth import QualSynthGenerator
    
    generator = QualSynthGenerator(
        model_name="gemma3:12b",
        temperature=0.7,
        max_iterations=20
    )
    
    X_synthetic, y_synthetic = generator.fit_generate(X_train, y_train)
"""

import pandas as pd
import numpy as np
from typing import Optional, Tuple, List, Dict, Any, Union
from dataclasses import dataclass

from .core.iterative_workflow import IterativeRefinementWorkflow as IterativeWorkflow, WorkflowConfig


@dataclass
class GeneratorConfig:
    """Configuration for QualSynthGenerator with sensible defaults."""
    # LLM settings
    model_name: str = "gemma3:12b"
    temperature: float = 0.7
    top_p: float = 0.95
    
    # Generation settings
    max_iterations: int = 20
    batch_size: int = 20
    target_ratio: float = 1.0  # Target class ratio (1.0 = balanced)
    
    # Validation settings
    validation_threshold: float = 4.5  # Statistical validation (Ïƒ)
    duplicate_threshold: float = 0.10  # Near-duplicate detection
    
    # Optimization weights
    fairness_weight: float = 0.60
    diversity_weight: float = 0.20
    quality_weight: float = 0.20


class QualSynthGenerator:
    """
    Quality-Driven Synthetic Data Generator using LLM-Guided Oversampling.
    
    QualSynthGenerator provides a simple, scikit-learn-style API for generating
    high-quality synthetic samples for imbalanced classification. It uses Large
    Language Models (LLMs) with iterative refinement and multi-stage validation
    to ensure all generated samples are realistic and statistically plausible.
    
    Parameters
    ----------
    model_name : str, default="gemma3:12b"
        Name of the LLM model to use. Supports:
        - OpenAI models: "gpt-4", "gpt-3.5-turbo"
        - Ollama models: "gemma3:12b", "llama3:8b"
        - Custom endpoints via api_base parameter
        
    temperature : float, default=0.7
        Controls generation consistency. Lower values (0.5-0.7) produce more
        consistent, distribution-matching samples. Higher values increase diversity.
        
    max_iterations : int, default=20
        Maximum number of refinement iterations. Generation continues until
        target samples are reached or this limit is hit.
        
    batch_size : int, default=20
        Number of samples to generate per LLM call. Smaller batches allow
        more detailed per-sample instructions.
        
    target_ratio : float, default=1.0
        Target class ratio after oversampling. 1.0 means perfect balance
        (minority class equals majority class).
        
    validation_threshold : float, default=4.5
        Statistical validation threshold in standard deviations. Samples
        with z-scores above this are rejected.
        
    api_base : str, optional
        Custom API base URL for self-hosted models or alternative providers.
        
    api_key : str, optional
        API key for cloud providers (OpenAI, OpenRouter). Can also be set
        via OPENAI_API_KEY environment variable.
        
    sensitive_attributes : list of str, optional
        Column names of sensitive attributes for fairness-aware generation.
        When provided, the generator prioritizes samples that reduce
        demographic disparity.
        
    verbose : bool, default=True
        Whether to print progress information during generation.
    
    Attributes
    ----------
    config_ : GeneratorConfig
        Configuration object with all parameters.
        
    workflow_ : IterativeWorkflow
        Underlying workflow object (available after fit_generate).
        
    n_samples_generated_ : int
        Number of synthetic samples generated in last fit_generate call.
        
    validation_rate_ : float
        Percentage of generated samples that passed validation.
    
    Examples
    --------
    Basic usage with default settings:
    
    >>> from qualsynth import QualSynthGenerator
    >>> import pandas as pd
    >>> 
    >>> # Load your imbalanced dataset
    >>> X_train = pd.read_csv("train_features.csv")
    >>> y_train = pd.read_csv("train_labels.csv")["target"]
    >>> 
    >>> # Initialize and generate
    >>> generator = QualSynthGenerator(model_name="gpt-4")
    >>> X_syn, y_syn = generator.fit_generate(X_train, y_train)
    >>> 
    >>> # Combine with original data for training
    >>> X_balanced = pd.concat([X_train, X_syn])
    >>> y_balanced = pd.concat([y_train, y_syn])
    
    Using local Ollama model:
    
    >>> generator = QualSynthGenerator(
    ...     model_name="gemma3:12b",
    ...     api_base="http://localhost:11434/v1"
    ... )
    >>> X_syn, y_syn = generator.fit_generate(X_train, y_train)
    
    Fairness-aware generation:
    
    >>> generator = QualSynthGenerator(
    ...     model_name="gpt-4",
    ...     sensitive_attributes=["gender", "race"]
    ... )
    >>> X_syn, y_syn = generator.fit_generate(X_train, y_train)
    
    See Also
    --------
    IterativeWorkflow : Lower-level API with more configuration options.
    
    Notes
    -----
    QualSynthGenerator achieves 100% validation pass rate by filtering samples
    during generation. All returned samples are guaranteed to pass:
    - Exact duplicate detection (hash-based)
    - Schema validation (correct types and ranges)
    - Statistical validation (within training distribution)
    """
    
    def __init__(
        self,
        model_name: str = "gemma3:12b",
        temperature: float = 0.7,
        max_iterations: int = 20,
        batch_size: int = 20,
        target_ratio: float = 1.0,
        validation_threshold: float = 4.5,
        api_base: Optional[str] = None,
        api_key: Optional[str] = None,
        sensitive_attributes: Optional[List[str]] = None,
        verbose: bool = True
    ):
        self.model_name = model_name
        self.temperature = temperature
        self.max_iterations = max_iterations
        self.batch_size = batch_size
        self.target_ratio = target_ratio
        self.validation_threshold = validation_threshold
        self.api_base = api_base
        self.api_key = api_key
        self.sensitive_attributes = sensitive_attributes or []
        self.verbose = verbose
        
        # Attributes set after fit_generate
        self.config_: Optional[GeneratorConfig] = None
        self.workflow_: Optional[IterativeWorkflow] = None
        self.n_samples_generated_: int = 0
        self.validation_rate_: float = 0.0
        
    def fit_generate(
        self,
        X: pd.DataFrame,
        y: Union[pd.Series, np.ndarray],
        n_samples: Optional[int] = None
    ) -> Tuple[pd.DataFrame, pd.Series]:
        """
        Generate synthetic samples for the minority class.
        
        This method analyzes the input data, determines the minority class,
        and generates synthetic samples to balance the class distribution.
        
        Parameters
        ----------
        X : pd.DataFrame
            Training features. Must be a pandas DataFrame with column names.
            
        y : pd.Series or np.ndarray
            Training labels. Binary classification (0/1) is expected.
            
        n_samples : int, optional
            Number of synthetic samples to generate. If not provided,
            generates enough samples to achieve target_ratio.
            
        Returns
        -------
        X_synthetic : pd.DataFrame
            Generated synthetic features with same columns as X.
            
        y_synthetic : pd.Series
            Labels for synthetic samples (all minority class).
            
        Raises
        ------
        ValueError
            If X and y have different lengths, or if y is not binary.
        """
        # Validate inputs
        if len(X) != len(y):
            raise ValueError(f"X and y must have same length. Got {len(X)} and {len(y)}.")
        
        y = pd.Series(y) if isinstance(y, np.ndarray) else y
        unique_classes = y.unique()
        
        if len(unique_classes) != 2:
            raise ValueError(f"Expected binary classification. Got {len(unique_classes)} classes.")
        
        # Identify minority class
        class_counts = y.value_counts()
        minority_class = class_counts.idxmin()
        majority_class = class_counts.idxmax()
        
        n_minority = class_counts[minority_class]
        n_majority = class_counts[majority_class]
        
        # Calculate target samples
        if n_samples is None:
            target_minority = int(n_majority * self.target_ratio)
            n_samples = max(0, target_minority - n_minority)
        
        if n_samples == 0:
            if self.verbose:
                print("Dataset is already balanced. No samples to generate.")
            return pd.DataFrame(columns=X.columns), pd.Series(dtype=y.dtype)
        
        if self.verbose:
            print(f"\n{'='*60}")
            print("QualSynth Generator")
            print(f"{'='*60}")
            print(f"Minority class: {minority_class} ({n_minority} samples)")
            print(f"Majority class: {majority_class} ({n_majority} samples)")
            print(f"Imbalance ratio: {n_majority/n_minority:.2f}:1")
            print(f"Target samples: {n_samples}")
            print(f"Model: {self.model_name}")
            print(f"{'='*60}\n")
        
        # Build workflow config
        workflow_config = WorkflowConfig(
            model_name=self.model_name,
            temperature=self.temperature,
            max_iterations=self.max_iterations,
            batch_size=self.batch_size,
            target_samples=n_samples,
            fairness_weight=0.60,
            diversity_weight=0.20,
            performance_weight=0.20,
        )
        
        # Set API configuration via environment if provided
        import os
        if self.api_base:
            os.environ['OPENAI_API_BASE'] = self.api_base
        if self.api_key:
            os.environ['OPENAI_API_KEY'] = self.api_key
        
        # Filter to minority class
        X_minority = X[y == minority_class].copy()
        y_minority = y[y == minority_class].copy()
        
        # Handle sensitive attributes
        sensitive_features = None
        if self.sensitive_attributes:
            available_attrs = [a for a in self.sensitive_attributes if a in X.columns]
            if available_attrs:
                sensitive_features = X_minority[available_attrs].copy()
        
        # Create and run workflow
        self.workflow_ = IterativeWorkflow(workflow_config)
        
        result = self.workflow_.run(
            X_train=X_minority,
            y_train=y_minority,
            sensitive_features=sensitive_features
        )
        
        # Extract results
        X_synthetic = result.X_generated
        self.n_samples_generated_ = len(X_synthetic) if X_synthetic is not None else 0
        self.validation_rate_ = 100.0  # All returned samples passed validation
        
        if X_synthetic is None or len(X_synthetic) == 0:
            if self.verbose:
                print("\nWarning: No samples generated.")
            return pd.DataFrame(columns=X.columns), pd.Series(dtype=y.dtype)
        
        # Create labels for synthetic samples
        y_synthetic = pd.Series([minority_class] * len(X_synthetic), name=y.name)
        
        if self.verbose:
            print(f"\n{'='*60}")
            print("Generation Complete")
            print(f"{'='*60}")
            print(f"Samples generated: {self.n_samples_generated_}")
            print(f"Validation rate: {self.validation_rate_:.1f}%")
            print(f"New minority count: {n_minority + self.n_samples_generated_}")
            new_ratio = n_majority / (n_minority + self.n_samples_generated_)
            print(f"New imbalance ratio: {new_ratio:.2f}:1")
            print(f"{'='*60}\n")
        
        return X_synthetic, y_synthetic
    
    def get_params(self) -> Dict[str, Any]:
        """Get generator parameters."""
        return {
            'model_name': self.model_name,
            'temperature': self.temperature,
            'max_iterations': self.max_iterations,
            'batch_size': self.batch_size,
            'target_ratio': self.target_ratio,
            'validation_threshold': self.validation_threshold,
            'api_base': self.api_base,
            'sensitive_attributes': self.sensitive_attributes,
            'verbose': self.verbose
        }
    
    def set_params(self, **params) -> 'QualSynthGenerator':
        """Set generator parameters."""
        for key, value in params.items():
            if hasattr(self, key):
                setattr(self, key, value)
            else:
                raise ValueError(f"Unknown parameter: {key}")
        return self
    
    def __repr__(self) -> str:
        return (
            f"QualSynthGenerator(model_name='{self.model_name}', "
            f"temperature={self.temperature}, "
            f"max_iterations={self.max_iterations}, "
            f"batch_size={self.batch_size})"
        )
