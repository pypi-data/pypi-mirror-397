"""Markdown formatter for folder contents."""

import datetime
import re
from pathlib import Path

from pygments.lexers import guess_lexer_for_filename
from pygments.util import ClassNotFound

from ..__version__ import __version__
from ..utils.file_utils import get_language_from_extension
from ..utils.token_utils import estimate_tokens_from_text


class MarkdownFormatter:
    """Formats folder contents as markdown."""

    def __init__(
        self,
        include_tree: bool = True,
        include_stats: bool = True,
        include_preamble: bool = True,
        token_limit: int | None = None,
        char_limit: int | None = None,
        token_estimation_method: str = "average",
        smart_engine_active: bool = False,
    ):
        self.include_tree = include_tree
        self.include_stats = include_stats
        self.include_preamble = include_preamble
        self.token_limit = token_limit
        self.char_limit = char_limit
        self.token_estimation_method = token_estimation_method
        self.smart_engine_active = smart_engine_active
        # We don't need pygments formatter for markdown output - we'll format manually

    def _generate_preamble(
        self, folder_path: Path, processing_stats: dict | None = None
    ) -> str:
        """Generate preamble explaining what the output file contains."""
        folder_name = folder_path.name
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # Get processing stats with defaults
        file_count = processing_stats.get("file_count", 0) if processing_stats else 0
        token_count = processing_stats.get("token_count", 0) if processing_stats else 0

        # Format stats line with condensing info if smart engine was used
        if processing_stats and processing_stats.get("smart_engine_active", False):
            original_tokens = processing_stats.get("original_tokens", 0)
            condensed_tokens = processing_stats.get("condensed_tokens", 0)

            # Calculate condensing percentage
            if original_tokens > 0:
                condensing_percentage = (
                    (original_tokens - condensed_tokens) / original_tokens
                ) * 100
                stats_line = f"**Stats:** {file_count:,} files â€¢ {condensed_tokens:,}/{original_tokens:,} tokens ({condensing_percentage:.1f}% condensed)"
            else:
                stats_line = f"**Stats:** {file_count:,} files â€¢ {condensed_tokens:,}/{original_tokens:,} tokens (0% condensed)"
        else:
            stats_line = f"**Stats:** {file_count:,} files â€¢ {token_count:,} tokens"

        preamble = f"""# ðŸ“ {folder_name}

> **Generated by [folder2md4llms](https://github.com/henriqueslab/folder2md4llms) v{__version__}** - Structured folder analysis for LLM consumption
> Source: `{folder_path}` â€¢ Generated: {timestamp}

{stats_line}

---

"""
        return preamble

    def format_repository(
        self,
        repo_path: Path,
        tree_structure: str | None = None,
        file_contents: dict[str, str] | None = None,
        file_stats: dict | None = None,
        binary_descriptions: dict[str, str] | None = None,
        converted_docs: dict[str, str] | None = None,
        processing_stats: dict | None = None,
    ) -> str:
        """Format the complete folder as markdown."""
        # Clean all input strings of surrogates before processing
        # This prevents encoding errors during markdown generation
        if tree_structure:
            try:
                tree_structure.encode("utf-8")
            except UnicodeEncodeError:
                tree_structure = tree_structure.encode(
                    "utf-8", errors="replace"
                ).decode("utf-8")

        if file_contents:
            cleaned_contents = {}
            for path, content in file_contents.items():
                try:
                    content.encode("utf-8")
                    cleaned_contents[path] = content
                except UnicodeEncodeError:
                    cleaned_contents[path] = content.encode(
                        "utf-8", errors="replace"
                    ).decode("utf-8")
            file_contents = cleaned_contents

        if binary_descriptions:
            cleaned_descriptions = {}
            for path, desc in binary_descriptions.items():
                try:
                    desc.encode("utf-8")
                    cleaned_descriptions[path] = desc
                except UnicodeEncodeError:
                    cleaned_descriptions[path] = desc.encode(
                        "utf-8", errors="replace"
                    ).decode("utf-8")
            binary_descriptions = cleaned_descriptions

        if converted_docs:
            cleaned_docs = {}
            for path, doc in converted_docs.items():
                try:
                    doc.encode("utf-8")
                    cleaned_docs[path] = doc
                except UnicodeEncodeError:
                    cleaned_docs[path] = doc.encode("utf-8", errors="replace").decode(
                        "utf-8"
                    )
            converted_docs = cleaned_docs

        sections = []

        # Add preamble if enabled
        if self.include_preamble:
            sections.append(self._generate_preamble(repo_path, processing_stats))

        # Header (only if preamble is disabled)
        if not self.include_preamble:
            folder_name = repo_path.name
            sections.append(f"# Folder: {folder_name}")
            sections.append("")

        # Table of Contents
        sections.append("## ðŸ“‘ Table of Contents")
        if self.include_tree:
            sections.append("- [ðŸ“ Folder Structure](#-folder-structure)")
        if self.include_stats:
            sections.append("- [ðŸ“Š Folder Statistics](#-folder-statistics)")
        if file_contents:
            sections.append("- [ðŸ“„ Source Code](#-source-code)")
        if converted_docs:
            sections.append("- [ðŸ“‹ Documents](#-documents)")
        if binary_descriptions:
            sections.append("- [ðŸ”§ Binary Files & Assets](#-binary-files--assets)")
        sections.append("")

        # Folder Structure
        if self.include_tree and tree_structure:
            sections.append("## ðŸ“ Folder Structure")
            sections.append("```")
            sections.append(tree_structure)
            sections.append("```")
            sections.append("")

        # Folder Statistics
        if self.include_stats and file_stats:
            sections.append("## ðŸ“Š Folder Statistics")
            sections.append(self._format_stats(file_stats))
            sections.append("")

        # Source Code Files
        if file_contents:
            sections.append("## ðŸ“„ Source Code")
            sections.append("")
            for file_path, content in file_contents.items():
                sections.append(self._format_file_content(file_path, content))
                sections.append("")

        # Converted Documents
        if converted_docs:
            sections.append("## ðŸ“‹ Documents")
            sections.append("")
            for file_path, content in converted_docs.items():
                sections.append(self._format_document_content(file_path, content))
                sections.append("")

        # Binary Files
        if binary_descriptions:
            sections.append("## ðŸ”§ Binary Files & Assets")
            sections.append("")
            for file_path, description in binary_descriptions.items():
                sections.append(self._format_binary_description(file_path, description))
                sections.append("")

        # Clean each section of surrogates before joining
        # This prevents encoding errors during string concatenation
        cleaned_sections = []
        for section in sections:
            if isinstance(section, str):
                try:
                    # Try to encode/decode to detect surrogates
                    section.encode("utf-8")
                    cleaned_sections.append(section)
                except UnicodeEncodeError:
                    # Clean any surrogate characters if encoding fails
                    cleaned_section = section.encode(
                        "utf-8", errors="surrogateescape"
                    ).decode("utf-8", errors="replace")
                    cleaned_sections.append(cleaned_section)
            else:
                cleaned_sections.append(section)

        # Apply token/character limits if specified
        try:
            output = "\n".join(cleaned_sections)
        except UnicodeEncodeError:
            # Last resort: force clean all sections
            cleaned_sections = [
                s.encode("utf-8", errors="replace").decode("utf-8")
                if isinstance(s, str)
                else str(s)
                for s in cleaned_sections
            ]
            output = "\n".join(cleaned_sections)

        output, truncation_info = self._apply_limits(output)

        # Update preamble with truncation info if needed
        if truncation_info:
            output = self._update_preamble_with_truncation(
                output, truncation_info, processing_stats
            )

        # Final safety: Clean any surrogate characters that might have slipped through
        # This prevents encoding errors when writing to file or clipboard
        output = output.encode("utf-8", errors="replace").decode("utf-8")

        return output

    def _apply_limits(self, content: str) -> tuple[str, dict | None]:
        """Apply token or character limits to the content."""
        # Clean surrogates from content first
        try:
            content.encode("utf-8")
        except UnicodeEncodeError:
            content = content.encode("utf-8", errors="replace").decode("utf-8")

        if not self.token_limit and not self.char_limit:
            return content, None

        # Skip truncation if smart engine is active - it has already handled token limits
        if self.smart_engine_active:
            return content, None

        # Check character limit first (simpler)
        if self.char_limit and len(content) > self.char_limit:
            truncated = content[: self.char_limit]
            # Try to truncate at a reasonable boundary (end of line or section)
            last_newline = truncated.rfind("\n\n")
            if last_newline > self.char_limit * 0.9:  # Keep if we're close to limit
                truncated = truncated[:last_newline]

            truncation_info = {
                "type": "character",
                "original_chars": len(content),
                "truncated_chars": len(truncated),
                "limit": self.char_limit,
            }
            return (
                truncated
                + f"\n\n**[Content truncated at {self.char_limit:,} characters]**",
                truncation_info,
            )

        # Check token limit
        if self.token_limit:
            estimated_tokens = estimate_tokens_from_text(
                content, self.token_estimation_method
            )
            if estimated_tokens > self.token_limit:
                # Calculate approximate character limit based on token ratio
                char_to_token_ratio = (
                    len(content) / estimated_tokens if estimated_tokens > 0 else 4.0
                )
                target_chars = int(
                    self.token_limit * char_to_token_ratio * 0.9
                )  # Leave some buffer

                if target_chars < len(content):
                    truncated = content[:target_chars]
                    # Try to truncate at a reasonable boundary
                    last_newline = truncated.rfind("\n\n")
                    if last_newline > target_chars * 0.9:
                        truncated = truncated[:last_newline]

                    final_tokens = estimate_tokens_from_text(
                        truncated, self.token_estimation_method
                    )
                    truncation_info = {
                        "type": "token",
                        "original_tokens": estimated_tokens,
                        "truncated_tokens": final_tokens,
                        "limit": self.token_limit,
                    }
                    return (
                        truncated
                        + f"\n\n**[Content truncated at ~{final_tokens:,} tokens (limit: {self.token_limit:,})]**",
                        truncation_info,
                    )

        return content, None

    def _update_preamble_with_truncation(
        self, content: str, truncation_info: dict, processing_stats: dict | None
    ) -> str:
        """Update the preamble with truncation information."""
        if not truncation_info:
            return content

        # Get original token count from processing stats
        original_tokens = (
            processing_stats.get("token_count", 0) if processing_stats else 0
        )

        if truncation_info["type"] == "token":
            # Replace the original stats line with truncation info
            # Handle both old format and new smart condensing format
            stats_pattern = (
                r"\*\*Stats:\*\* ([\d,]+) files â€¢ ([\d,]+(?:/[\d,]+)?) tokens.*"
            )
            file_count = (
                processing_stats.get("file_count", 0) if processing_stats else 0
            )
            new_stats = f"**Stats:** {file_count:,} files â€¢ {truncation_info['truncated_tokens']:,}/{original_tokens:,} tokens"
            content = re.sub(stats_pattern, new_stats, content, count=1)
        elif truncation_info["type"] == "character":
            # For character limits, we don't have truncated token info, so just show original
            pass

        return content

    def _format_stats(self, stats: dict) -> str:
        """Format repository statistics."""
        lines = []

        # File counts
        lines.append("### File Counts")
        lines.append(f"- **Total Files:** {stats.get('total_files', 0)}")
        lines.append(f"- **Text Files:** {stats.get('text_files', 0)}")
        lines.append(f"- **Binary Files:** {stats.get('binary_files', 0)}")
        lines.append(f"- **Converted Documents:** {stats.get('converted_docs', 0)}")
        lines.append("")

        # Size information
        if "total_size" in stats:
            lines.append("### Size Information")
            lines.append(f"- **Total Size:** {self._format_size(stats['total_size'])}")
            lines.append(
                f"- **Text Content:** {self._format_size(stats.get('text_size', 0))}"
            )
            lines.append("")

        # Language breakdown
        if "languages" in stats:
            lines.append("### Languages")
            languages = stats["languages"]
            for lang, count in sorted(
                languages.items(), key=lambda x: x[1], reverse=True
            ):
                lines.append(f"- **{lang}:** {count} files")
            lines.append("")

        # Token estimation
        if "estimated_tokens" in stats:
            lines.append("### Token Estimation")
            lines.append(f"- **Estimated Tokens:** {stats['estimated_tokens']:,}")
            lines.append("")

        return "\n".join(lines)

    def _format_file_content(self, file_path: str, content: str) -> str:
        """Format a single file's content with syntax highlighting."""
        lines = []

        # File header
        lines.append(f"### ðŸ“„ `{file_path}`")
        lines.append("")

        # Detect language for syntax highlighting
        language = self._detect_language(file_path, content)

        # Add syntax-highlighted content
        if language:
            lines.append(f"```{language}")
        else:
            lines.append("```")

        lines.append(content.rstrip())
        lines.append("```")

        return "\n".join(lines)

    def _format_document_content(self, file_path: str, content: str) -> str:
        """Format a converted document's content."""
        lines = []

        # Document header
        lines.append(f"### ðŸ“‹ `{file_path}`")
        lines.append("")

        # Add converted content
        lines.append(content.rstrip())

        return "\n".join(lines)

    def _format_binary_description(self, file_path: str, description: str) -> str:
        """Format a binary file description."""
        lines = []

        # Binary file header
        lines.append(f"### ðŸ”§ `{file_path}`")
        lines.append("")
        lines.append(description.rstrip())

        return "\n".join(lines)

    def _detect_language(self, file_path: str, content: str) -> str | None:
        """Detect the programming language for syntax highlighting."""
        try:
            # Try to guess from filename
            lexer = guess_lexer_for_filename(file_path, content)
            return lexer.aliases[0] if lexer.aliases else None
        except ClassNotFound:
            # Fall back to extension-based detection
            return get_language_from_extension(Path(file_path).suffix.lower())

    def _format_size(self, size_bytes: int) -> str:
        """Format file size in human-readable format."""
        if size_bytes < 1024:
            return f"{size_bytes} B"
        elif size_bytes < 1024 * 1024:
            return f"{size_bytes / 1024:.1f} KB"
        elif size_bytes < 1024 * 1024 * 1024:
            return f"{size_bytes / (1024 * 1024):.1f} MB"
        else:
            return f"{size_bytes / (1024 * 1024 * 1024):.1f} GB"
