# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import json
import logging
import tempfile
from contextlib import contextmanager
from functools import cached_property
from pathlib import Path
from typing import Generator, Optional, Union

import pandas as pd
from nemo_microservices import NeMoMicroservices
from nemo_microservices.types.data_designer import SettingsResponse
from nemo_microservices.types.data_designer.data_designer_job import DataDesignerJob
from nemo_microservices.types.data_designer.data_designer_job_config_param import DataDesignerJobConfigParam

from ..config.analysis.dataset_profiler import DatasetProfilerResults
from ..config.config_builder import DataDesignerConfigBuilder
from ..config.datastore import DatastoreSettings, resolve_datastore_settings, upload_to_hf_hub
from ..config.interface import DataDesignerInterface
from ..config.models import ModelConfig, ModelProvider
from ..config.preview_results import PreviewResults
from ..config.seed import DatastoreSeedDatasetReference
from ..config.utils.info import InterfaceInfo
from ..logging import RandomEmoji
from .errors import DataDesignerClientError, handle_api_exceptions
from .results.jobs import DataDesignerJobResults
from .results.preview import MessageType

logger = logging.getLogger(__name__)

DEFAULT_PREVIEW_TIMEOUT = 120
DEFAULT_NUM_RECORDS_FOR_PREVIEW = 10


class NeMoDataDesignerClient(DataDesignerInterface[DataDesignerJobResults]):
    """Client for interacting with the NeMo Data Designer service.

    The NeMoDataDesignerClient provides a high-level interface for generating synthetic datasets
    using the NeMo Data Designer service. It supports creating batch data generation jobs,
    running data generation previews, and managing datasets through the datastore.

    The client can be initialized with either an existing NeMoMicroservices client or a base URL
    to create a new connection.
    """

    def __init__(self, *, client: Optional[NeMoMicroservices] = None, base_url: Optional[str] = None, **kwargs):
        """Initialize the NeMoDataDesignerClient.

        Args:
            client: An existing NeMoMicroservices client instance. If provided, this will be used
                instead of creating a new client. Mutually exclusive with base_url.
            base_url: The base URL of the NeMo Microservices instance. Used to create a new
                NeMoMicroservices client if no client is provided. Mutually exclusive with client.
            **kwargs: Additional keyword arguments passed to NeMoMicroservices constructor when
                creating a new client. Ignored if client is provided.

        Raises:
            DataDesignerClientError: If neither client nor base_url is provided.

        Note:
            Either client or base_url must be provided, but not both. If both are provided,
            the client parameter takes precedence.
        """
        if client is None and base_url is None:
            raise DataDesignerClientError("ðŸ›‘ Either client or base_url must be provided")

        self._client = client or NeMoMicroservices(base_url=base_url, **kwargs)
        self._data_designer_resource = self._client.data_designer
        self._datastore_settings: DatastoreSettings | None = None

    @cached_property
    def _settings(self) -> SettingsResponse:
        """Get the Data Designer service settings (cached).

        This property fetches the service settings on first access and caches them
        for subsequent calls. It includes connection validation to ensure the service
        is accessible.

        Returns:
            The service settings including default model configs and model providers.

        Raises:
            DataDesignerClientError: If the connection to the service fails or settings
                cannot be retrieved.
        """
        try:
            return self._data_designer_resource.settings()
        except Exception as e:
            error_msg = f"ðŸ›‘ Failed to connect to Data Designer service or retrieve settings: {str(e)}"
            logger.error(error_msg)
            raise DataDesignerClientError(error_msg) from e

    def create(
        self,
        config_builder: DataDesignerConfigBuilder,
        *,
        num_records: int = 100,
        wait_until_done: bool = False,
        name: str = "nemo-data-designer-job",
        project: str = "nemo-data-designer",
    ) -> DataDesignerJobResults:
        """Create a Data Designer generation job.

        Args:
            config_builder: Data Designer configuration builder.
            num_records: The number of records to generate.
            wait_until_done: Whether to halt your program until the job is done.
            name: Name label for the job within the NeMo Microservices project.
            project: Name of the NeMo Microservices project.

        Returns:
            An object with methods for querying the job's status and results.
        """
        logger.info("ðŸŽ¨ Creating Data Designer generation job")
        config = _get_config_for_api_call(config_builder)
        try:
            job = self._data_designer_resource.jobs.create(
                name=name,
                project=project,
                spec=DataDesignerJobConfigParam(
                    num_records=num_records,
                    config=config,
                ),
            )
            logger.info(f"  |-- job_id: {job.id}")
            results = DataDesignerJobResults(job=job, client=self._client)
            if wait_until_done:
                results.wait_until_done()
            return results
        except Exception as e:
            handle_api_exceptions(e)

    def preview(
        self,
        config_builder: DataDesignerConfigBuilder,
        *,
        num_records: int | None = None,
        timeout: int | None = None,
    ) -> PreviewResults:
        """Generate a set of preview records based on your current Data Designer configuration.

        This method is meant for fast iteration on your Data Designer configuration.

        Args:
            config_builder: Data Designer configuration builder.
            num_records: The number of records to generate. Must be equal to or less than the max number of
                preview records set at deploy time.
            timeout: The timeout for the preview in seconds.

        Returns:
            An object containing the preview dataset and tools for inspecting the results.
        """
        try:
            return self._capture_preview_result(config_builder=config_builder, num_records=num_records, timeout=timeout)
        except Exception as e:
            handle_api_exceptions(e)

    def get_datastore_settings(self) -> Optional[DatastoreSettings]:
        """Get the current datastore settings.

        Returns:
            The current datastore settings if it has been set, None otherwise.
        """
        return self._datastore_settings

    def get_job_results(self, job_id: str) -> DataDesignerJobResults:
        """Retrieve results for an existing data generation job.

        Args:
            job_id: The unique identifier of the job to retrieve results for.

        Returns:
            An object containing methods for querying job status,
            retrieving the generated dataset, and accessing job metadata.

        Raises:
            ValueError: If the job ID provided is empty.
        """
        job = self._data_designer_resource.jobs.retrieve(job_id)
        return DataDesignerJobResults(job=job, client=self._client)

    def list_jobs(self, limit: Optional[int] = None) -> list[DataDesignerJob]:
        """List all jobs.

        Args:
            limit: Optionally limit results to this many jobs.

        Returns:
            A list of Data Designer jobs, ordered by descending creation time.
        """
        jobs = []
        page = 1
        while True:
            response = self._data_designer_resource.jobs.list(page=page)
            for job in response.data:
                if limit is None or len(jobs) < limit:
                    jobs.append(job)

            if (pagination := response.pagination) is None:
                break
            if pagination.page == pagination.total_pages:
                break
            page += 1

            if limit and len(jobs) >= limit:
                break

        return jobs

    def upload_seed_dataset(
        self,
        dataset: Union[str, Path, pd.DataFrame],
        repo_id: str,
        datastore_settings: DatastoreSettings,
    ) -> DatastoreSeedDatasetReference:
        """Upload a dataset to the datastore and return the reference for fetching the dataset.

        This function handles different dataset input types and automatically manages temporary files
        for DataFrame uploads. For DataFrame inputs, a temporary parquet file is created and
        automatically cleaned up after upload.

        Args:
            dataset: Dataset to upload. Can be:
                - pandas.DataFrame: Will be saved as a temporary parquet file.
                - str: Path to an existing dataset file.
                - Path: Path object pointing to an existing dataset file.
            repo_id: Repository ID for the datastore where the dataset will be uploaded.
            datastore_settings: Configuration settings for the datastore connection.

        Returns:
            Seed dataset reference returned from the datastore upload.
        """
        self._datastore_settings = resolve_datastore_settings(datastore_settings)
        logger.info("ðŸ”„ Uploading seed dataset to datastore")
        with _dataset_filename_and_path(dataset) as file_info:
            dataset_id = upload_to_hf_hub(
                dataset_path=file_info["dataset_path"],
                filename=file_info["filename"],
                repo_id=repo_id,
                datastore_settings=self._datastore_settings,
            )
        return DatastoreSeedDatasetReference(dataset=dataset_id, datastore_settings=self._datastore_settings)

    def _capture_preview_result(
        self,
        config_builder: DataDesignerConfigBuilder,
        num_records: int | None,
        timeout: int | None,
    ) -> PreviewResults:
        """Capture the results (including logs) of a workflow preview.

        Args:
            config_builder: The data designer configuration builder containing the generation
                parameters and column definitions.
            num_records: The number of records to generate for the preview. Must be equal to or less than the max number of
                preview records set at deploy time. If None, uses the default number of records for preview.
            timeout: The timeout in seconds for the preview operation.

        Returns:
            An object containing the generated dataset, analysis results, and the original configuration builder.
        """
        config = _get_config_for_api_call(config_builder)

        dataset = None
        analysis = None
        log_levels_seen = set()

        logger.info("ðŸš€ Starting preview generation")
        for response in self._data_designer_resource.preview(
            config=config, num_records=num_records, timeout=timeout or DEFAULT_PREVIEW_TIMEOUT
        ):
            if response.message_type == MessageType.HEARTBEAT:
                continue
            if response.message_type == MessageType.LOG:
                level = response.extra["level"].lower()
                log_levels_seen.add(level)
                if level == "info":
                    logger.info(response.message)
                elif level in {"warning", "warn"}:
                    logger.warning(response.message)
                elif level == "error":
                    logger.error(response.message)
            elif response.message_type == MessageType.DATASET:
                try:
                    dataset = pd.DataFrame.from_dict(json.loads(response.message))
                except Exception as e:
                    logger.error(f"ðŸ›‘ Error loading dataset: {e}")
                    log_levels_seen.add("error")
            elif response.message_type == MessageType.ANALYSIS:
                try:
                    analysis = DatasetProfilerResults.model_validate_json(response.message)
                except Exception as e:
                    logger.error(f"ðŸ›‘ Error loading analysis: {e}")
                    log_levels_seen.add("error")

        if "error" in log_levels_seen:
            logger.error("ðŸ›‘ Preview completed with errors.")
        elif "warning" in log_levels_seen or "warn" in log_levels_seen:
            logger.warning("âš ï¸ Preview completed with warnings.")
        else:
            logger.info(f"{RandomEmoji.success()} Preview complete!")

        return PreviewResults(
            config_builder=config_builder,
            dataset=dataset,
            analysis=analysis,
        )

    def get_default_model_configs(self) -> list[ModelConfig]:
        """Get the default model configurations from the Data Designer service.

        Returns:
            A list of default ModelConfig objects available on the server.
        """
        return self._settings.defaults.model_configs

    def get_default_model_providers(self) -> list[ModelProvider]:
        """Get the available model providers from the Data Designer service.

        Returns:
            A list of ModelProvider objects representing the providers configured on the server.
        """
        return self._settings.model_providers

    @property
    def info(self) -> InterfaceInfo:
        """Get interface information for display purposes.

        Returns:
            An InterfaceInfo object that can be used to display available model providers.
        """
        model_providers = self.get_default_model_providers()
        for model_provider in model_providers:
            model_provider.api_key = "N/A"
            model_provider.endpoint = "N/A"

        return InterfaceInfo(model_providers=model_providers)


def _get_config_for_api_call(config_builder: DataDesignerConfigBuilder) -> dict:
    """Build the config and dump it as a dictionary. This ensures default values on
    Pydantic models are included. If you just pass the pydantic object directly,
    Stainless serializes the object with `exclude_unset=True`, which omits discriminator
    fields, causing the server to be unable to deserialize and respond with a 422.
    """
    return config_builder.build(raise_exceptions=True).model_dump(exclude_unset=False)


@contextmanager
def _dataset_filename_and_path(dataset: Union[str, Path, pd.DataFrame]) -> Generator[dict[str, str], None, None]:
    """Context manager for handling different dataset input types.

    This context manager provides a unified interface for handling different types of
    dataset inputs (DataFrame, file path, or Path object) and ensures proper cleanup
    of temporary files when needed.

    For DataFrame inputs, a temporary parquet file is created and automatically
    cleaned up when the context exits. For file path inputs, the existing file
    is used directly.

    Args:
        dataset: The dataset to process. Can be:
            - pandas.DataFrame: Will be saved as a temporary parquet file that is
              automatically cleaned up when the context exits.
            - str: Path to an existing dataset file as a string.
            - Path: Path object pointing to an existing dataset file.

    Yields:
        A dictionary containing:
            - "filename": The filename to use for the dataset (extracted from path
              or set to a default name for DataFrames).
            - "dataset_path": The actual file path to the dataset (temporary file
              for DataFrames, original path for file inputs).
    """
    if isinstance(dataset, pd.DataFrame):
        with tempfile.NamedTemporaryFile(suffix=".parquet", delete=True) as temp_file:
            dataset.to_parquet(temp_file.name, index=False)
            yield {"filename": "seed-dataset-dataframe.parquet", "dataset_path": temp_file.name}
    else:
        # For Path or str, use the dataset as the path and extract filename
        dataset_path = str(dataset)
        filename = Path(dataset).name
        yield {"filename": filename, "dataset_path": dataset_path}
