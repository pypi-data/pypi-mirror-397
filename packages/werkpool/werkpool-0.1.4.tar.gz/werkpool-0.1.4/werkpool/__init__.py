"""
This module provides tools for managing asynchronous tasks with concurrency and rate limits.
It includes a Task definition for scheduled execution and a WorkerPool to manage multiple tasks using asyncio.

Classes:
    Task: Defines a function to be executed asynchronously, with optional timeout and retry settings.
    WorkerPool: Manages a pool of tasks, enforcing concurrency and rate limits.
"""

__all__ = ["worker_pool", "WorkerPool"]

import asyncio
from collections import deque
from contextlib import asynccontextmanager
from math import ceil
import random
from typing import Any, AsyncGenerator, Awaitable, Callable, Deque, List, NamedTuple, Optional


class Task(NamedTuple):
    fn: Callable[[], Awaitable[Any]]
    timeout: int | None = None
    retries: int = 0
    retryable_exceptions: List[type[Exception]] = [Exception]
    backoff: Callable[[int], float] = lambda attempts: 2**attempts + random.uniform(0, 1)


class DelayedTask:
    task: Task
    future: asyncio.Future

    def __init__(self, task: Task):
        self.task = task
        self.future = asyncio.Future()
        self.attempts = 0

    async def execute(self, callback: Optional[Callable[[], None]] = None):
        try:
            while True:
                try:
                    if self.task.timeout:
                        result = await asyncio.wait_for(self.task.fn(), self.task.timeout)
                    else:
                        result = await self.task.fn()

                    self.future.set_result(result)
                    break
                except Exception as e:
                    if not any(isinstance(e, exc) for exc in self.task.retryable_exceptions):
                        self.future.set_exception(e)
                        break
                    self.attempts += 1
                    if self.attempts >= self.task.retries:
                        self.future.set_exception(e)
                        break
                    await asyncio.sleep(self.task.backoff(self.attempts))
        finally:
            if callback:
                callback()


class Semaphore:
    value: int | None
    semaphore: asyncio.Semaphore | None

    def __init__(self, value: int | None):
        self.value = value
        if value:
            self.semaphore = asyncio.Semaphore(value)

    async def acquire(self):
        if self.value:
            return await self.semaphore.acquire()

    def release(self):
        if self.value:
            return self.semaphore.release()


class WorkerPool:
    size: int | None
    rate: float | None
    count: int

    def __init__(self, size: int | None = None, rate: float | None = None):
        """
        Initializes a new WorkerPool class. Params size, rate or both must be provided.

        :param
            size: specifies the maximum number of concurrent workers that can be executing
                tasks. If None or not provided, no limit will be set.

            rate: specifies the maximum number of executions per second. If None or not
                provided, the rate is unbounded.

        Example usage:
            workers = WorkerPool(size=5, rate=10)
            # This creates a WorkerPool that allows up to 5 concurrent workers,
            # with a maximum rate of 10 executions per second.
        """

        if size and (size <= 0 or round(size) != size):
            raise ValueError("Size must be a positive integer or None.")
        if rate and rate <= 0:
            raise ValueError("Rate must be a positive float or None.")

        self.size = size
        self.rate = rate
        self._executing: List[DelayedTask] = []
        self._queue: Deque[DelayedTask] = deque()
        self._accepting = True
        self._shutdown = False
        self._releasing = False

        # ceil: still need capcaity for partial (e.g. 0.5)
        self._semaphore = Semaphore(ceil(self.rate) if self.rate else None)
        self._wait = 1 / self.rate if self.rate else None

    def _put_task(self, task: DelayedTask):
        self._queue.append(task)

    def _mark_done(self, task: DelayedTask):
        self._executing.remove(task)
        if not self._shutdown:
            asyncio.create_task(self._next())

    async def _release(self):
        if not self._releasing:
            self._releasing = True
            while not self._shutdown and (len(self._executing) > 0 or len(self._queue) > 0):
                await asyncio.sleep(self._wait)
                self._semaphore.release()
            self._releasing = False

    async def _next(self):
        if not self._shutdown and (self.size is None or len(self._executing) < self.size):
            if self._queue:
                task = self._queue.popleft()
                self._executing.append(task)
                await self._semaphore.acquire()
                await task.execute(lambda: self._mark_done(task))

    def run[T](
            self,
            task: Callable[[], Awaitable[T]],
            timeout: int | None = None,
            retries: int = 0,
            retryable_exceptions: List[type[Exception]] = [Exception],
            backoff: Callable[[int], float] = lambda attempts: 2**attempts + random.uniform(0, 1),
            ) -> asyncio.Future[T]:
        """
        Schedules a new task for execution. The task will be executed asynchronously, with optional
        timeout and retry settings.

        :param
            task: the function to be executed asynchronously. This function should be a coroutine, not
                take any arguments, and return the result of the task.
            timeout: the maximum number of seconds to wait for the task to complete. If None or not
                provided, the task will not have a timeout.
            retries: the maximum number of times to retry the task if it fails. If 0 or not provided,
                the task will not be retried.
            retryable_exceptions: a list of exceptions that should trigger a retry. If not provided,
                all exceptions will trigger a retry.
            backoff: a function that takes the number of attempts and returns the number of seconds to
                wait before the next attempt. If not provided, the default backoff strategy will be used:
                2^attempts + random.uniform(0, 1).
        """
        if not self._accepting:
            raise RuntimeError("WorkerPool has shutdown and not accepting new tasks.")

        task = Task(task, timeout, retries, retryable_exceptions, backoff)
        delayed_task = DelayedTask(task)
        self._put_task(delayed_task)
        if self.rate:
            asyncio.create_task(self._release())
        asyncio.create_task(self._next())
        return delayed_task.future

    async def shutdown(self):
        self._accepting = False
        await asyncio.gather(
            *[task.future for task in self._executing],
            *[task.future for task in self._queue],
            return_exceptions=True
        )
        self._shutdown = True
        
        # Wait for background _release() tasks to notice shutdown and exit cleanly
        # This prevents "Task was destroyed but it is pending" warnings
        while self._releasing:
            await asyncio.sleep(0.01)

    async def kill(self):
        self._accepting = False
        self._shutdown = True
        
        # Cancel executing tasks
        for task in self._executing:
            if not task.future.done():
                task.future.set_exception(asyncio.CancelledError("WorkerPool shutdown"))
        
        # Cancel queued tasks
        while self._queue:
            task = self._queue.popleft()
            task.future.set_exception(asyncio.CancelledError("WorkerPool shutdown"))


@asynccontextmanager
async def worker_pool(size: int | None = None, rate: float | None = None) -> AsyncGenerator[WorkerPool, None]:
    try:
        workers = WorkerPool(size, rate)
        yield workers
    finally:
        await workers.shutdown()
