from ...tensor.hooks import RemovableHandle as RemovableHandle
from ...types import _axis_type, _device_type, _dtype_type, _scalar_type, _shape_type, _size_type
from .._tensor_impl import VQNet_Native_Impl_Backend as VQNet_Native_Impl_Backend
from ..utils import maybe_wrap_dim as maybe_wrap_dim
from _typeshed import Incomplete
from pyvqnet.device import DEV_CPU as DEV_CPU, DEV_GPU_0 as DEV_GPU_0
from pyvqnet.dtype import dtype_map as dtype_map, get_default_dtype as get_default_dtype, get_readable_dtype_str as get_readable_dtype_str, kcomplex128 as kcomplex128, kcomplex32 as kcomplex32, kcomplex64 as kcomplex64, kfloat16 as kfloat16, kfloat32 as kfloat32, kfloat64 as kfloat64, kint32 as kint32, valid_param_dtype as valid_param_dtype, vqnet_complex_dtypes as vqnet_complex_dtypes, vqnet_float_dtypes as vqnet_float_dtypes

long = int
integer_types: Incomplete
vqnet_core: Incomplete
vqnet_ad: Incomplete
numeric_types: Incomplete
MIN_FLOAT: Incomplete
MAX_FLOAT: Incomplete

class VQNet_Native_Autograd_Impl_Backend(VQNet_Native_Impl_Backend):
    @staticmethod
    def is_leaf(t): ...
    @staticmethod
    def create_new_qtensor(cls, t): ...
    @staticmethod
    def inplace_requires_grad(self): ...
    @staticmethod
    def toCPU(self): ...
    to_cpu = toCPU
    @staticmethod
    def GPU(self, device=...): ...
    @staticmethod
    def CPU(self): ...
    @staticmethod
    def copy_to_device(self, device): ...
    @staticmethod
    def contiguous(t): ...
    @staticmethod
    def convert_to_device_dtype(t, device, dtype): ...
    @staticmethod
    def rmsprop(params, grads, E, state_steps, lr, beta, epsilon) -> None: ...
    @staticmethod
    def sgd(params, grads, ms, lr, momentum, nesterov) -> None: ...
    @staticmethod
    def adamax(params, grads, ms, vs, state_steps, lr, beta1, beta2, epsilon) -> None: ...
    @staticmethod
    def adamax_init_params(self, group, params_with_grad, grads, ms, vs, state_steps) -> None: ...
    @staticmethod
    def adagrad(params, grads, G, state_steps, lr, epsilon) -> None: ...
    @staticmethod
    def adagrad_init_params(self, group, params_with_grad, grads, G, state_steps) -> None: ...
    @staticmethod
    def adadelta(params, grads, E, A, state_steps, lr, beta, epsilon) -> None: ...
    @staticmethod
    def adadelta_init_params(self, group, params_with_grad, grads, E, A, state_steps) -> None: ...
    @staticmethod
    def adam_init_params(self, group, params_with_grad, grads, ms, vs, max_vcaps, state_steps) -> None: ...
    @staticmethod
    def sgd_init_params(self, group, params_with_grad, grads, ms, state_steps) -> None: ...
    @staticmethod
    def rmsprop_init_params(self, group, params_with_grad, grads, E, state_steps) -> None: ...
    @staticmethod
    def adamw_init_params(self, group, params_with_grad, grads, ms, vs, max_vcaps, state_steps) -> None: ...
    @staticmethod
    def adamw(params, grads, ms, vs, max_vcaps, state_steps, lr, beta1, beta2, epsilon, weight_decay, amsgrad) -> None: ...
    @staticmethod
    def adam(params, grads, ms, vs, max_vcaps, state_steps, weight_decay, lr, beta1, beta2, epsilon, amsgrad) -> None: ...
    @staticmethod
    def item(self_tensor): ...
    @staticmethod
    def getdata(self_tensor): ...
    @staticmethod
    def add_(self_t, t): ...
    @staticmethod
    def sub_(self_t, t): ...
    @staticmethod
    def triu(t, diagonal: int = 0): ...
    @staticmethod
    def tril(t, diagonal: int = 0): ...
    @staticmethod
    def trace(t, k: int = 0): ...
    @staticmethod
    def relu(t): ...
    @staticmethod
    def dense_to_csr(x): ...
    @staticmethod
    def csr_to_dense(x): ...
    @staticmethod
    def as_qtensor(x): ...
    @staticmethod
    def set_random_seed(seed) -> None: ...
    @staticmethod
    def get_random_seed(): ...
    @staticmethod
    def log(t): ...
    @staticmethod
    def frobenius_norm(t, axis: int = None, keepdims: bool = False): ...
    @staticmethod
    def randn(shape: _size_type, mean: float | int = 0.0, std: float | int = 1.0, device: int = 0, dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def randu(shape: _size_type, min: float | int = 0.0, max: float | int = 1.0, device=..., dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def default_if_grad_enabled():
        """
        get default pyvqnet if_grad_enabled
        """
    get_grad_enabled = default_if_grad_enabled
    @staticmethod
    def set_default_if_grad_enabled(flag) -> None:
        """
        set default pyvqnet value of if_grad_enabled
        """
    set_grad_enabled = set_default_if_grad_enabled
    @staticmethod
    def diagonal(t, offset: int = 0, dim1: int = 0, dim2: int = 1): ...
    @staticmethod
    def einsum(equation: str, operands): ...
    @staticmethod
    def square_sum(a, b): ...
    @staticmethod
    def log_softmax(t, axis: int = -1): ...
    @staticmethod
    def concatenate(operands, axis: int = 0): ...
    cat = concatenate
    @staticmethod
    def flatten(t, start: int = 0, end: int = -1): ...
    @staticmethod
    def stack(tensors, axis: int = 0): ...
    @staticmethod
    def tile(t, reps: list[int] | tuple[int, ...]): ...
    @staticmethod
    def grad_setter(t, new_value) -> None: ...
    @staticmethod
    def less_equal(t1, t2): ...
    @staticmethod
    def less(t1, t2): ...
    @staticmethod
    def logical_not(t): ...
    @staticmethod
    def logical_and(t1, t2): ...
    @staticmethod
    def logical_or(t1, t2): ...
    @staticmethod
    def bitwise_and(t1, t2): ...
    @staticmethod
    def logical_xor(t1, t2): ...
    @staticmethod
    def greater(t1, t2): ...
    @staticmethod
    def greater_equal(t1, t2): ...
    @staticmethod
    def conj(t): ...
    @staticmethod
    def adjoint(t): ...
    @staticmethod
    def is_nonzero(t): ...
    @staticmethod
    def nonzero(t): ...
    @staticmethod
    def isfinite(t): ...
    @staticmethod
    def isinf(t): ...
    @staticmethod
    def isnan(t): ...
    @staticmethod
    def isneginf(t): ...
    @staticmethod
    def isposinf(t): ...
    @staticmethod
    def masked_fill(t, mask, value: float | int | complex): ...
    @staticmethod
    def fill_(t, v: _scalar_type): ...
    @staticmethod
    def all(t): ...
    @staticmethod
    def any(t): ...
    @staticmethod
    def reshape_(self_tensor, new_shape: _size_type): ...
    @staticmethod
    def argmax(self_tensor, dim: _axis_type = None, keepdims: bool = None): ...
    @staticmethod
    def argmin(self_tensor, dim: _axis_type = None, keepdims: bool = None): ...
    @staticmethod
    def max(t, axis: list[int] | tuple[int, ...] | int | None = None, keepdims: bool = False): ...
    @staticmethod
    def min(t, axis: list[int] | tuple[int, ...] | int | None = None, keepdims: bool = False): ...
    @staticmethod
    def median(t, axis: _axis_type = None, keepdims: bool = False): ...
    @staticmethod
    def csr_members(data): ...
    @staticmethod
    def mean(t, axis: _axis_type = None, keepdims: bool = False): ...
    @staticmethod
    def var(t, axis: list[int] | tuple[int, ...] | int | None = None, keepdims: bool = False, unbiased: bool = True): ...
    @staticmethod
    def std(t, axis: list[int] | tuple[int, ...] | int | None = None, keepdims: bool = False, unbiased: bool = True): ...
    @staticmethod
    def broadcast_to(t, ref: _size_type): ...
    @staticmethod
    def equal(t1, t2): ...
    @staticmethod
    def maximum(t1, t2): ...
    @staticmethod
    def minimum(t1, t2): ...
    @staticmethod
    def where(condition, t1, t2): ...
    @staticmethod
    def view_as_complex(t): ...
    @staticmethod
    def reciprocal(t): ...
    @staticmethod
    def round(t): ...
    @staticmethod
    def sign(t): ...
    @staticmethod
    def eigh(t): ...
    @staticmethod
    def eigvalsh(t): ...
    @staticmethod
    def ry_matrix(params): ...
    @staticmethod
    def crz_matrix(params): ...
    @staticmethod
    def adaptive_avg_pool2d(t, size): ...
    @staticmethod
    def linear_forward(self, x): ...
    @staticmethod
    def functional_linear(x, weight, bias): ...
    @staticmethod
    def functional_conv2d(x, weight, bias, stride, padding, dilation, groups): ...
    @staticmethod
    def outer(t1, t2): ...
    @staticmethod
    def kron(t1, t2): ...
    @staticmethod
    def neg(t): ...
    @staticmethod
    def real(t): ...
    @staticmethod
    def imag(t): ...
    @staticmethod
    def full(shape: _size_type, value: float | int | complex, device=..., dtype=None): ...
    @staticmethod
    def squeeze(t, axis: list[int] | tuple[int, ...] | int | None = None):
        """
        Remove axes of length one .if `axis` is not specified, remove all single-dimensional axis from the shape of a tensor. 

        :param t: input QTensor
        :param axis: squeeze axis
        :return: A QTensor

        Examples::

            a = np.arange(6).reshape(1,6,1).astype(np.float32)
            A = QTensor(a)
            AA = tensor.squeeze(A,0)

        """
    @staticmethod
    def not_equal(t1, t2): ...
    @staticmethod
    def shallow_copy_raw_data(self, other) -> None:
        """
        self.data will be reference to other.data without autgorad infos.
        """
    @staticmethod
    def pow_scalar(t1, scalar_exp): ...
    @staticmethod
    def power(t1, t2): ...
    pow = power
    @staticmethod
    def floor(t): ...
    @staticmethod
    def ceil(t): ...
    @staticmethod
    def pack_padded_sequence(input, lengths: list[int], batch_first: bool = False, enforce_sorted: bool = True): ...
    @staticmethod
    def pad_packed_sequence(data, batch_sizes, sort_indice, unsorted_indices, batch_first, padding_value, total_length): ...
    @staticmethod
    def zeros_like(t, device=None, dtype=None): ...
    @staticmethod
    def sub(t1, t2): ...
    @staticmethod
    def rsub_scalar(t, scalar: _scalar_type): ...
    @staticmethod
    def add_scalar(t1, val: _scalar_type): ...
    @staticmethod
    def sums(t, axis: _axis_type = None, keepdims: bool = False): ...
    sum = sums
    @staticmethod
    def reshape(t, shape: _size_type): ...
    @staticmethod
    def view(t, shape: _size_type): ...
    @staticmethod
    def grad(t): ...
    @staticmethod
    def create_tensor(data, device, dtype): ...
    @staticmethod
    def backward(self_tensor, grad=None, retain_graph: bool = False): ...
    @staticmethod
    def astype(self_tensor, dtype: int): ...
    @staticmethod
    def full_like(t, value: float | int | complex, device=None, dtype=None): ...
    @staticmethod
    def mul_scalar(t1, val: _scalar_type): ...
    @staticmethod
    def zero_grad(t) -> None: ...
    @staticmethod
    def init_from_other_tensor(self_tensor, other_tensor) -> None: ...
    @staticmethod
    def clone(t, mem_formate: int = 0):
        """
        mem_formate = 0: contiugous
        mem_formate = 1 :strided
        """
    @staticmethod
    def uniform_(self, min_value: _scalar_type, max_value: _scalar_type): ...
    @staticmethod
    def fill_rand_uniform_(self, v: _scalar_type = 1): ...
    @staticmethod
    def fill_rand_signed_uniform_(self, v: _scalar_type = 1): ...
    @staticmethod
    def diag(t, k: int = 0): ...
    @staticmethod
    def mul_(self, t): ...
    @staticmethod
    def div_(self, t): ...
    @staticmethod
    def clamp_(self, min: int | float, max: int | float): ...
    @staticmethod
    def fill_rand_normal_(self, m: _scalar_type = 0, s: _scalar_type = 1, fast_math: bool = True): ...
    @staticmethod
    def fill_rand_binary_(self, v: _scalar_type = 0.5): ...
    @staticmethod
    def detach(t): ...
    @staticmethod
    def is_not_dense(self): ...
    @staticmethod
    def to_numpy(t, copy: bool = True): ...
    @staticmethod
    def swapaxis(t, axis1: int, axis2: int): ...
    @staticmethod
    def transpose(t, dim): ...
    permute = transpose
    @staticmethod
    def zeros(shape: _size_type, device: _device_type = ..., dtype: _dtype_type = None): ...
    @staticmethod
    def is_csr(self): ...
    @staticmethod
    def move_to_device(self, device) -> None: ...
    @staticmethod
    def toGPU(self, device=...): ...
    @staticmethod
    def requires_grad_getter(t): ...
    @staticmethod
    def requires_grad_setter(t, new_value) -> None: ...
    @staticmethod
    def unsqueeze(t, axis: int): ...
    @staticmethod
    def multinomial(t, num_samples: int): ...
    @staticmethod
    def argtopk(t, k: int, axis: int = -1, if_descent: bool = True): ...
    @staticmethod
    def argsort(t, axis: int, descending: bool = False, stable: bool = True): ...
    @staticmethod
    def sort(t, axis: int, descending: bool = False, stable: bool = True): ...
    @staticmethod
    def narrow(input, dim, start, length): ...
    @staticmethod
    def unbind(t, dim: int = 0): ...
    @staticmethod
    def select_1dim(t, dim, index): ...
    @staticmethod
    def select(t, index: list):
        """
        default vqnet impl.
        """
    @staticmethod
    def FLOAT_2_COMPLEX(param): ...
    @staticmethod
    def getitem(self_tensor, item): ...
    @staticmethod
    def index_select(t, dim: int, indice): ...
    @staticmethod
    def set_select(t, index, set_tensor) -> None: ...
    @staticmethod
    def setitem(self_tensor, key, value) -> None: ...
    @staticmethod
    def size(data): ...
    @staticmethod
    def stride(data): ...
    @staticmethod
    def get_vqnet_device(device): ...
    @staticmethod
    def get_vqnet_dtype(dtype): ...
    @staticmethod
    def copy_data_from(dst, src) -> None: ...
    @staticmethod
    def mul(t1, t2): ...
    @staticmethod
    def broadcast(t1, t2): ...
    @staticmethod
    def arange(start: float | int, end: float | int, step: float | int = 1, device=..., dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def empty(shape: _size_type, device: int = 0, dtype=None): ...
    @staticmethod
    def empty_like(t, device=None, dtype=None): ...
    @staticmethod
    def ones_like(input): ...
    @staticmethod
    def ones(shape: _size_type, device=..., dtype=None): ...
    @staticmethod
    def linspace(start: float | int, end: float | int, nums: int, device=..., dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def logspace(start: float | int, end: float | int, nums: int, base: float | int, device=..., dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def eye(size: int, offset: int = ..., device: int = 0, dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def rdivide(t1, t2): ...
    @staticmethod
    def divide(t1, t2): ...
    div = divide
    @staticmethod
    def add(t1, t2): ...
    @staticmethod
    def view_as_real(t): ...
    @staticmethod
    def mv(t1, t2): ...
    @staticmethod
    def bmm(t1, t2): ...
    @staticmethod
    def default_matmul2d(t1, t2): ...
    @staticmethod
    def matmul(t1, t2): ...
    @staticmethod
    def exp(t): ...
    @staticmethod
    def acos(t): ...
    @staticmethod
    def asin(t): ...
    @staticmethod
    def atan(t): ...
    @staticmethod
    def tanh(t): ...
    @staticmethod
    def sinh(t): ...
    @staticmethod
    def cosh(t): ...
    @staticmethod
    def tan(t): ...
    @staticmethod
    def sin(t): ...
    @staticmethod
    def cos(t): ...
    @staticmethod
    def atan2(y, x): ...
    @staticmethod
    def silu(t): ...
    @staticmethod
    def sigmoid(t): ...
    @staticmethod
    def softplus(t, self): ...
    @staticmethod
    def softsign(t, self): ...
    @staticmethod
    def binomial(total_counts, probs): ...
    @staticmethod
    def sqrt(t): ...
    @staticmethod
    def rsqrt(t): ...
    @staticmethod
    def square(t): ...
    @staticmethod
    def clip(t, min_val: int | float | None = None, max_val: int | float | None = None): ...
    @staticmethod
    def abs(t): ...
    @staticmethod
    def softmax(t, axis): ...
    @staticmethod
    def dropout(x, p: float = 0.5, training: bool = True): ...
    @staticmethod
    def hard_sigmoid(t, self): ...
    @staticmethod
    def leakyrelu(t, self): ...
    @staticmethod
    def act_relu(t, self): ...
    @staticmethod
    def tensordot(x, y, dim1: _size_type, dim2: _size_type): ...
    @staticmethod
    def topk(t, k: int, axis: int = -1, if_descent: bool = True): ...
    @staticmethod
    def cumsum(t, axis: int = -1): ...
    @staticmethod
    def flip(t, flip_dims: _size_type): ...
    @staticmethod
    def roll(t, shifts, dims): ...
    @staticmethod
    def pad(t, pad: _shape_type, value: _scalar_type = 0): ...
    @staticmethod
    def gather(t, dim: int, index): ...
    @staticmethod
    def scatter(input, dim: int, indices, src): ...
    @staticmethod
    def expval_pauli(measurements, q_machine, obs): ...
    @staticmethod
    def pad_sequence(qtensor_list, batch_first: bool = False, padding_value: int | float = 0): ...
    @staticmethod
    def moveaxis(t, src: _size_type, dst: _size_type): ...
    @staticmethod
    def marginal_prob(prob, num_wires, wires): ...
    @staticmethod
    def probs(q_state, num_wires, wires): ...
    @staticmethod
    def gelu(x, self): ...
    @staticmethod
    def exp_pauli(reshape_qstates, obs, requires_grad): ...
    @staticmethod
    def bn_forward(self, x): ...
    @staticmethod
    def conv1d_forward(self, x): ...
    @staticmethod
    def conv2d_forward(self, x): ...
    @staticmethod
    def convt2d_forward(self, x): ...
    @staticmethod
    def mp1d_forward(self, x): ...
    @staticmethod
    def mp2d_forward(self, x): ...
    @staticmethod
    def ap1d_forward(self, x): ...
    @staticmethod
    def ap2d_forward(self, x): ...
    @staticmethod
    def embedding(self, x): ...
    @staticmethod
    def gn_forward(self, x): ...
    @staticmethod
    def interpolate(x, scale_factors, mode, align_corners): ...
    @staticmethod
    def lnnd_forward(self, x): ...
    @staticmethod
    def ln2d_forward(self, x): ...
    @staticmethod
    def ln1d_forward(self, x): ...
    @staticmethod
    def pixel_unshuffle(self, x): ...
    @staticmethod
    def pixel_shuffle(self, x): ...
    @staticmethod
    def mse_forward(self, target, output): ...
    @staticmethod
    def cce_forward(self, target, output): ...
    @staticmethod
    def sce_forward(self, target, output): ...
    @staticmethod
    def bce_forward(self, target, output): ...
    @staticmethod
    def nll_loss_forward(self, target, output): ...
    @staticmethod
    def pqc_forward(self, x): ...
    @staticmethod
    def qae_forward(self, x): ...
    @staticmethod
    def quan_forward(self, x): ...
    @staticmethod
    def qcnn_forward(self, x): ...
    @staticmethod
    def qdrl_forward(self, x): ...
    @staticmethod
    def qgan_forward(self, x): ...
    @staticmethod
    def qlinear_forward(self, x): ...
    @staticmethod
    def cirq_forward(self, x): ...
    @staticmethod
    def qiskit_forward(self, x): ...
    @staticmethod
    def pq2_base_default_forward(self, x): ...
    @staticmethod
    def pq2_noise_default_forward(self, x): ...
    @staticmethod
    def pq2_qlayer_forward(self, x): ...
    @staticmethod
    def pq2_qlayerv2_forward(self, x): ...
    @staticmethod
    def pq2_mp_forward(self, x): ...
    @staticmethod
    def qadjoint_forward(self, common_func, x, *args, **kwargs): ...
    @staticmethod
    def find_nodes_without_post(node): ...
    @staticmethod
    def zero_graph_grad(node) -> None: ...
    @staticmethod
    def sdpa_forward(self, query, key, value): ...
    @staticmethod
    def row_paralled_forward(self, x): ...
    @staticmethod
    def col_paralled_forward(self, x): ...
    @staticmethod
    def sync_bn_forward(self, x): ...

def default_matmul2d(t1, t2): ...
def bool_mask_select_set(self, bool_mask, value): ...
def bool_mask_select(self, bool_mask): ...
