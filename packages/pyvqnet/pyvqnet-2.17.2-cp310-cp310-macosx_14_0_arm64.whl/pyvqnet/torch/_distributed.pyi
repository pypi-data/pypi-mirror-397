def init_process_group(backend, rank, world_size) -> None: ...
def get_local_rank(): ...
def get_rank(group=None): ...
def get_size(group=None): ...
get_world_size = get_size

def all_reduce(t, c_op, group=None) -> None: ...
def reduce(t, dst, c_op, group=None) -> None: ...
def send(t, dst, group=None) -> None: ...
def recv(t, src, group=None) -> None: ...
def send_recv(send, dst, recv, src, group=None) -> None: ...
def distributed_broadcast(t, dst, group=None) -> None: ...
def all_gather_to_tensor(t, group=None): ...
def check_comm(comm) -> None: ...
def barrier(group=None) -> None: ...
def init_group(rank_lists): ...
def get_device_num(): ...
def is_initialized(): ...
def is_available(): ...
