from ._tensor import *
from ..device import DEV_CPU as DEV_CPU, DEV_GPU_0 as DEV_GPU_0
from ..dtype import kcomplex128 as kcomplex128, kcomplex32 as kcomplex32, kcomplex64 as kcomplex64, kfloat64 as kfloat64
from ..types import _axis_type, _scalar_type, _shape_type, _size_type
from ._distributed import get_local_rank as get_local_rank, get_rank as get_rank, get_size as get_size, init_process_group as init_process_group
from .initializer import he_normal_ as he_normal_, he_uniform_ as he_uniform_, xavier_normal_ as xavier_normal_, xavier_uniform_ as xavier_uniform_
from .quantum import crz_matrix as crz_matrix, expval_pauli as expval_pauli, probs as probs, ry_matrix as ry_matrix
from .utils import device_map_torch as device_map_torch, dtype_map_torch as dtype_map_torch, get_grad_enabled as get_grad_enabled, get_random_seed as get_random_seed, get_vqnet_device as get_vqnet_device, get_vqnet_dtype as get_vqnet_dtype, requires_grad_getter as requires_grad_getter, requires_grad_setter as requires_grad_setter, set_grad_enabled as set_grad_enabled, set_random_seed as set_random_seed
from _typeshed import Incomplete
from pyvqnet.backends_mock import TorchMock as TorchMock

unary_operators_preprocess: Incomplete

class TorchNativeBackend:
    tc32: Incomplete
    tc64: Incomplete
    tc128: Incomplete
    tf16: Incomplete
    tf32: Incomplete
    tf64: Incomplete
    vqnet_complex_dtypes: Incomplete
    vqnet_float_dtypes: Incomplete
    @classmethod
    def op_class_dict(cls): ...
    @staticmethod
    def check_same_device(t1, t2): ...
    @staticmethod
    def is_leaf(t): ...
    @staticmethod
    def marginal_prob(prob, num_wires, wires): ...
    @staticmethod
    def normal_(w, m: int = 0, s: int = 1) -> None: ...
    @staticmethod
    def xavier_uniform_(w, gain: int = 1) -> None: ...
    @staticmethod
    def xavier_normal_(w, gain: int = 1) -> None: ...
    @staticmethod
    def he_normal_(w, a, mode, nonlinearity) -> None: ...
    @staticmethod
    def he_uniform_(w, a=..., mode: str = 'fan_in', nonlinearity: str = 'leaky_relu') -> None: ...
    @staticmethod
    def zeros_(w) -> None: ...
    @staticmethod
    def ones_(w) -> None: ...
    @staticmethod
    def functional_conv2d(x, weight, bias, stride, padding, dilation, groups): ...
    @staticmethod
    def functional_linear(x, weight, bias): ...
    @staticmethod
    def add_(self_t, t): ...
    @staticmethod
    def sub_(self_t, t): ...
    @staticmethod
    def mul_(self_t, t): ...
    @staticmethod
    def div_(self_t, t): ...
    @staticmethod
    def swapaxis(data, ax1, ax2): ...
    @staticmethod
    def flatten(t, start: int = 0, end: int = -1): ...
    @staticmethod
    def csr_to_dense(x): ...
    @staticmethod
    def dense_to_csr(x): ...
    @staticmethod
    def gather(t, dim: int, index): ...
    @staticmethod
    def scatter(t, dim: int, index, src): ...
    @staticmethod
    def roll(t, shifts, dims): ...
    @staticmethod
    def pad(t, pad_list: _shape_type, value: _scalar_type = 0): ...
    @staticmethod
    def clamp_(self, min: int | float, max: int | float): ...
    @staticmethod
    def stride(data): ...
    @staticmethod
    def initializer_empty(shape, dtype): ...
    @staticmethod
    def initializer_he_normal(shape, dtype, a, mode, nonlinearity): ...
    @staticmethod
    def initializer_he_uniform(shape, dtype, a, mode, nonlinearity): ...
    @staticmethod
    def initializer_xavier_uniform(shape, vqnet_dtype, gain: int = 1): ...
    @staticmethod
    def copy_data_from(dst, src) -> None: ...
    copy_ = copy_data_from
    @staticmethod
    def uniform_(w, a: int = 0, b: int = 1): ...
    @staticmethod
    def initializer_xavier_normal(shape, dtype=None, gain: int = 1): ...
    @staticmethod
    def initializer_normal(shape, dtype=None, m: int = 0, s: int = 1): ...
    @staticmethod
    def floor(t): ...
    @staticmethod
    def add_scalar(t1, val: _scalar_type): ...
    @staticmethod
    def mse_loss(output, target): ...
    @staticmethod
    def argmax(self_tensor, dim: _axis_type = None, keepdims: bool = None): ...
    @staticmethod
    def argmin(self_tensor, dim: _axis_type = None, keepdims: bool = None): ...
    @staticmethod
    def max(self_tensor, dim: _axis_type = None, keepdims: bool = None): ...
    @staticmethod
    def min(self_tensor, dim: _axis_type = None, keepdims: bool = None): ...
    @staticmethod
    def softmax_cross_entropy(output, target): ...
    @staticmethod
    def binary_cross_entropy(output, target): ...
    @staticmethod
    def nll_loss(output, target): ...
    @staticmethod
    def full(shape: _size_type, value: float | int, device=..., dtype=None): ...
    @staticmethod
    def cross_entropy(output, target): ...
    @staticmethod
    def relu(x): ...
    @staticmethod
    def flip(t, flip_dims: _size_type): ...
    @staticmethod
    def default_matmul2d(t1, t2): ...
    @staticmethod
    def matmul(t1, t2): ...
    @staticmethod
    def adjoint(t): ...
    @staticmethod
    def conj(t): ...
    @staticmethod
    def empty(shape, device: int = 0, dtype=None): ...
    @staticmethod
    def is_csr(self): ...
    @staticmethod
    def is_not_dense(self): ...
    @staticmethod
    def kron(t1, t2): ...
    @staticmethod
    def set_grad_enabled(flag): ...
    @staticmethod
    def get_grad_enabled(): ...
    @staticmethod
    def csr_members(data): ...
    @staticmethod
    def linspace(start: float | int, end: float | int, nums: int, device=..., dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def triu(t, diagonal): ...
    @staticmethod
    def tril(t, diagonal): ...
    @staticmethod
    def median(t, axis, keepdims): ...
    @staticmethod
    def multinomial(t, num_samples: int): ...
    @staticmethod
    def cumsum(t, axis: int = -1): ...
    @staticmethod
    def argtopk(t, k: int, axis: int = -1, if_descent: bool = True): ...
    @staticmethod
    def topk(t, k: int, axis: int = -1, if_descent: bool = True): ...
    @staticmethod
    def argsort(t, axis: int, descending: bool = False, stable: bool = True): ...
    @staticmethod
    def sort(t, axis: int, descending: bool = False, stable: bool = True): ...
    @staticmethod
    def round(t): ...
    @staticmethod
    def sign(t): ...
    @staticmethod
    def binomial(total_counts, probs): ...
    @staticmethod
    def logspace(start: float | int, end: float | int, nums: int, base: float | int, device: int = 0, dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def mean(t, axis: _axis_type = None, keepdims: bool = False): ...
    @staticmethod
    def var(t, axis: _axis_type = None, keepdims: bool = False, unbiased: bool = True): ...
    @staticmethod
    def std(t, axis: _axis_type = None, keepdims: bool = False, unbiased: bool = True): ...
    @staticmethod
    def masked_fill(t, mask, value: float | int): ...
    @staticmethod
    def masked_fill_(t, mask, value: float | int): ...
    @staticmethod
    def reciprocal(t): ...
    @staticmethod
    def acos(t): ...
    @staticmethod
    def asin(t): ...
    @staticmethod
    def atan(t): ...
    @staticmethod
    def atan2(y, x): ...
    @staticmethod
    def square(t): ...
    @staticmethod
    def frobenius_norm(t, axis: int = None, keepdims: bool = False): ...
    @staticmethod
    def clone(t): ...
    @staticmethod
    def astype(self_tensor, dtype): ...
    @staticmethod
    def view(self_tensor, shape: _size_type): ...
    @staticmethod
    def maximum(t1, t2): ...
    @staticmethod
    def minimum(t1, t2): ...
    @staticmethod
    def pow(t1, t2): ...
    @staticmethod
    def log(t): ...
    @staticmethod
    def log_softmax(t, axis): ...
    @staticmethod
    def tan(t): ...
    @staticmethod
    def cos(t): ...
    @staticmethod
    def sin(t): ...
    @staticmethod
    def tanh(t): ...
    @staticmethod
    def cosh(t): ...
    @staticmethod
    def sinh(t): ...
    @staticmethod
    def item(self_tensor): ...
    @staticmethod
    def broadcast(t1, t2): ...
    @staticmethod
    def broadcast_to(t, ref): ...
    @staticmethod
    def CPU(t): ...
    @staticmethod
    def GPU(t): ...
    @staticmethod
    def detach(t): ...
    @staticmethod
    def as_qtensor(x): ...
    @staticmethod
    def to_numpy(t, copy: bool = True): ...
    @staticmethod
    def grad(t): ...
    @staticmethod
    def backward(self_tensor, grad=None, retain_graph: bool = False): ...
    @staticmethod
    def sums(t, axis=None, keepdims: bool = False): ...
    @staticmethod
    def mul(t1, t2): ...
    @staticmethod
    def einsum(equation: str, operands): ...
    @staticmethod
    def size(data): ...
    @staticmethod
    def exp(t): ...
    @staticmethod
    def outer(t1, t2): ...
    @staticmethod
    def full_like(data, value, device=None, dtype=None): ...
    @staticmethod
    def get_vqnet_dtype(dtype): ...
    @staticmethod
    def mul_scalar(data1, data2): ...
    @staticmethod
    def stack(tensors, axis: int = 0): ...
    @staticmethod
    def neg(t): ...
    @staticmethod
    def requires_grad_getter(t): ...
    @staticmethod
    def get_vqnet_device(device): ...
    @staticmethod
    def setitem(self_tensor, key, value): ...
    @staticmethod
    def FLOAT_2_COMPLEX(param): ...
    @staticmethod
    def getitem(data, key): ...
    @staticmethod
    def create_new_qtensor(cls, t): ...
    @staticmethod
    def zeros(shape, device, dtype): ...
    @staticmethod
    def tile(data, reps): ...
    @staticmethod
    def arange(start: float | int, end: float | int, step: float | int = 1, device=..., dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def pad_packed_sequence(data, batch_sizes, sort_indice, unsorted_indices, batch_first, padding_value, total_length): ...
    @staticmethod
    def rsub_scalar(t, scalar): ...
    @staticmethod
    def dropout(self, p: float = 0.5, training: bool = True): ...
    @staticmethod
    def sigmoid(input): ...
    @staticmethod
    def index_select(t, dim: int, indice): ...
    @staticmethod
    def fill_(t, v): ...
    @staticmethod
    def pack_padded_sequence(input, lengths: list[int], batch_first: bool = False, enforce_sorted: bool = True): ...
    @staticmethod
    def pad_sequence(qtensor_list, batch_first: bool = False, padding_value: int | float = 0): ...
    @staticmethod
    def initializer_uniform(shape, vqnet_dtype, a: int = 0, b: int = 1): ...
    @staticmethod
    def initializer_ones(shape, vqnet_dtype): ...
    @staticmethod
    def initializer_zeros(shape, vqnet_dtype): ...
    @staticmethod
    def sub(t1, t2): ...
    @staticmethod
    def add(t1, t2): ...
    @staticmethod
    def mul(t1, t2): ...
    @staticmethod
    def divide(t1, t2): ...
    @staticmethod
    def diagonal(t, offset: int = 0, dim1: int = 0, dim2: int = 1): ...
    @staticmethod
    def diag(t, k: int = 0): ...
    @staticmethod
    def eye(size: int, offset: int = ..., device: int = 0, dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def zeros_like(t, device=None, dtype=None): ...
    @staticmethod
    def rdivide(t1, t2): ...
    @staticmethod
    def sqrt(t): ...
    @staticmethod
    def pow_scalar(t1, scalar_exp): ...
    @staticmethod
    def abs(t): ...
    @staticmethod
    def randn(shape, m: float = 0.0, s: float = 1.0, device: int = 0, dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def create_tensor(data, device, dtype): ...
    @staticmethod
    def requires_grad_setter(self, new_value): ...
    @staticmethod
    def clip(t, min_val: int | float | None = None, max_val: int | float | None = None): ...
    @staticmethod
    def less_equal(t1, t2): ...
    @staticmethod
    def less(t1, t2): ...
    @staticmethod
    def logical_not(t): ...
    @staticmethod
    def logical_and(t1, t2): ...
    @staticmethod
    def logical_or(t1, t2): ...
    @staticmethod
    def bitwise_and(t1, t2): ...
    @staticmethod
    def logical_xor(t1, t2): ...
    @staticmethod
    def not_equal(t1, t2): ...
    @staticmethod
    def equal(t1, t2): ...
    @staticmethod
    def greater(t1, t2): ...
    @staticmethod
    def greater_equal(t1, t2): ...
    @staticmethod
    def where(condition, t1, t2): ...
    @staticmethod
    def nonzero(t): ...
    @staticmethod
    def is_nonzero(t): ...
    @staticmethod
    def isfinite(t): ...
    @staticmethod
    def isinf(t): ...
    @staticmethod
    def isnan(t): ...
    @staticmethod
    def isneginf(t): ...
    @staticmethod
    def isposinf(t): ...
    @staticmethod
    def eigh(t): ...
    @staticmethod
    def GPU(self, device=...): ...
    @staticmethod
    def empty_like(t, device=None, dtype=None): ...
    @staticmethod
    def silu(input): ...
    @staticmethod
    def relu(x): ...
    @staticmethod
    def trace(t, k: int = 0): ...
    @staticmethod
    def outer(t1, t2): ...
    @staticmethod
    def kron(t1, t2): ...
    @staticmethod
    def eigvalsh(t): ...
    @staticmethod
    def contiguous(t): ...
    @staticmethod
    def init_from_other_tensor(self_tensor, other_tensor): ...
    @staticmethod
    def view_as_real(t): ...
    @staticmethod
    def view_as_complex(t): ...
    @staticmethod
    def einsum(equation: str, operands): ...
    @staticmethod
    def ones(shape, device=None, dtype=None): ...
    @staticmethod
    def convert_to_device_dtype(t, device, dtype): ...
    @staticmethod
    def inplace_requires_grad(t): ...
    @staticmethod
    def ry_matrix(params): ...
    @staticmethod
    def crz_matrix(params): ...
    @staticmethod
    def expval_pauli(measurements, q_machine, obs): ...
    @staticmethod
    def probs(q_state, num_wires, wires): ...
    @staticmethod
    def reshape(t, shape): ...
    @staticmethod
    def softmax(t, axis): ...
    @staticmethod
    def tensordot(x, y, dim1, dim2): ...
    @staticmethod
    def moveaxis(data, src, dst): ...
    @staticmethod
    def fill_rand_uniform_(self, v: _scalar_type = 1): ...
    @staticmethod
    def fill_rand_signed_uniform_(self, v: _scalar_type = 1): ...
    @staticmethod
    def fill_rand_normal_(self, m: _scalar_type = 0, s: _scalar_type = 1, fast_math: bool = True): ...
    @staticmethod
    def fill_rand_binary_(self, v: _scalar_type = 0.5): ...
    @staticmethod
    def any(t): ...
    @staticmethod
    def all(t): ...
    @staticmethod
    def narrow(input, dim, start, length): ...
    @staticmethod
    def select(t, index: list): ...
    @staticmethod
    def unbind(t, dim: int = 0): ...
    @staticmethod
    def select_1dim(t, dim, index): ...
    @staticmethod
    def cat(tensors, axis): ...
    @staticmethod
    def zero_grad(t): ...
    @staticmethod
    def ceil(t): ...
    @staticmethod
    def copy_to_device(self, device): ...
    @staticmethod
    def bmm(t1, t2): ...
    @staticmethod
    def mv(t1, t2): ...
    @staticmethod
    def set_random_seed(seed) -> None: ...
    @staticmethod
    def unsqueeze(t, axis): ...
    @staticmethod
    def real(t): ...
    @staticmethod
    def imag(t): ...
    @staticmethod
    def grad_setter(t, new_value): ...
    @staticmethod
    def transpose(t, dim): ...
    permute = transpose
    @staticmethod
    def squeeze(t, axis): ...
    @staticmethod
    def get_random_seed(): ...
    @staticmethod
    def toGPU(self, device=...): ...
    @staticmethod
    def toCPU(self): ...
    @staticmethod
    def move_to_device(self, device): ...
    @staticmethod
    def copy_to_device(self, device): ...
    @staticmethod
    def unary_operators_preprocess(t): ...
    @staticmethod
    def randu(shape, min, max, device=..., dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def rmsprop_init_params(self, group, params_with_grad, grads, E, state_steps): ...
    @staticmethod
    def rmsprop(params, grads, E, state_steps, lr, beta, epsilon): ...
    @staticmethod
    def adam_init_params(self, group, params_with_grad, grads, ms, vs, max_vcaps, state_steps): ...
    @staticmethod
    def sgd_init_params(self, group, params_with_grad, grads, ms, state_steps): ...
    @staticmethod
    def sgd(params, grads, ms, lr, momentum, nesterov): ...
    @staticmethod
    def adamax(params, grads, ms, vs, state_steps, lr, beta1, beta2, epsilon): ...
    @staticmethod
    def adamax_init_params(self, group, params_with_grad, grads, ms, vs, state_steps): ...
    @staticmethod
    def adagrad(params, grads, G, state_steps, lr, epsilon): ...
    @staticmethod
    def adagrad_init_params(self, group, params_with_grad, grads, G, state_steps): ...
    @staticmethod
    def adadelta(params, grads, E, A, state_steps, lr, beta, epsilon): ...
    @staticmethod
    def adadelta_init_params(self, group, params_with_grad, grads, E, A, state_steps): ...
    @staticmethod
    def adamw_init_params(self, group, params_with_grad, grads, ms, vs, max_vcaps, state_steps): ...
    @staticmethod
    def adamw(params, grads, ms, vs, max_vcaps, state_steps, lr, beta1, beta2, epsilon, weight_decay, amsgrad): ...
    @staticmethod
    def adam(params, grads, ms, vs, max_vcaps, state_steps, weight_decay, lr, beta1, beta2, epsilon, amsgrad): ...
    @staticmethod
    def find_nodes_without_post(f): ...
    @staticmethod
    def zero_graph_grad(f) -> None: ...
    @staticmethod
    def get_qmachine(): ...
    @staticmethod
    def qadjoint_forward(self, common_func, x, *args, **kwargs): ...
    @staticmethod
    def init_process_group(backend, rank, world_size) -> None: ...
    @staticmethod
    def get_local_rank(): ...
    @staticmethod
    def get_rank(group=None): ...
    @staticmethod
    def get_size(group=None): ...
    get_world_size = get_size

def find_leaf_tensors(tensor, only_leaf: bool = True): ...
