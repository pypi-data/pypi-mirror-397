from ._tensor import *
from ._distributed import *
from ..device import DEV_CPU as DEV_CPU, DEV_GPU_0 as DEV_GPU_0
from ..dtype import kcomplex128 as kcomplex128, kcomplex32 as kcomplex32, kcomplex64 as kcomplex64, kfloat64 as kfloat64
from ..types import _axis_type, _scalar_type, _shape_type, _size_type
from .quantum import crz_matrix as crz_matrix, expval_pauli as expval_pauli, probs as probs, ry_matrix as ry_matrix
from .torch_native_backend import TorchNativeBackend as TorchNativeBackend
from .utils import device_map_torch as device_map_torch, dtype_map_torch as dtype_map_torch, get_grad_enabled as get_grad_enabled, get_random_seed as get_random_seed, get_vqnet_device as get_vqnet_device, get_vqnet_dtype as get_vqnet_dtype, requires_grad_getter as requires_grad_getter, requires_grad_setter as requires_grad_setter, set_grad_enabled as set_grad_enabled, set_random_seed as set_random_seed
from _typeshed import Incomplete
from pyvqnet.backends_mock import TorchMock as TorchMock

unary_operators_preprocess: Incomplete

class TorchWrapperBackend(TorchNativeBackend):
    tc32: Incomplete
    tc64: Incomplete
    tc128: Incomplete
    vqnet_complex_dtypes: Incomplete
    @staticmethod
    def marginal_prob(prob, num_wires, wires): ...
    @staticmethod
    def unary_operators_preprocess(t): ...
    @staticmethod
    def mul(t1, t2): ...
    @staticmethod
    def einsum(equation: str, operands): ...
    @staticmethod
    def size(data): ...
    @staticmethod
    def exp(t): ...
    @staticmethod
    def outer(t1, t2): ...
    @staticmethod
    def narrow(input, dim, start, length): ...
    @staticmethod
    def get_vqnet_dtype(dtype): ...
    @staticmethod
    def stack(tensors, axis: int = 0): ...
    @staticmethod
    def unbind(t, dim): ...
    @staticmethod
    def select_1dim(t, dim, index): ...
    @staticmethod
    def acos(t): ...
    @staticmethod
    def asin(t): ...
    @staticmethod
    def atan(t): ...
    @staticmethod
    def tan(t): ...
    @staticmethod
    def cos(t): ...
    @staticmethod
    def sin(t): ...
    @staticmethod
    def pow(t1, t2): ...
    @staticmethod
    def frobenius_norm(t, axis: int = None, keepdims: bool = False): ...
    @staticmethod
    def view(self_tensor, shape: _size_type): ...
    @staticmethod
    def maximum(t1, t2): ...
    @staticmethod
    def minimum(t1, t2): ...
    @staticmethod
    def square(t): ...
    @staticmethod
    def log(t): ...
    @staticmethod
    def log_softmax(t, axis): ...
    @staticmethod
    def atan2(y, x): ...
    @staticmethod
    def tanh(t): ...
    @staticmethod
    def cosh(t): ...
    @staticmethod
    def sinh(t): ...
    @staticmethod
    def reciprocal(t): ...
    @staticmethod
    def neg(t): ...
    @staticmethod
    def requires_grad_getter(t): ...
    @staticmethod
    def FLOAT_2_COMPLEX(param): ...
    @staticmethod
    def getitem(data, key): ...
    @staticmethod
    def create_new_qtensor(cls, t): ...
    @staticmethod
    def zeros(shape, device, dtype): ...
    @staticmethod
    def tile(data, reps): ...
    @staticmethod
    def randn(shape, m: float = 0.0, s: float = 1.0, device: int = 0, dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def diagonal(t, offset: int = 0, dim1: int = 0, dim2: int = 1): ...
    @staticmethod
    def sub(t1, t2): ...
    @staticmethod
    def add(t1, t2): ...
    @staticmethod
    def mul(t1, t2): ...
    @staticmethod
    def divide(t1, t2): ...
    @staticmethod
    def diag(t, k: int = 0): ...
    @staticmethod
    def eye(size: int, offset: int = ..., device: int = 0, dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def zeros_like(t, device=None, dtype=None): ...
    @staticmethod
    def rdivide(t1, t2): ...
    @staticmethod
    def sqrt(t): ...
    @staticmethod
    def pow_scalar(t1, scalar_exp): ...
    @staticmethod
    def abs(t): ...
    @staticmethod
    def randu(shape, min: float = 0.0, max: float = 1.0, device=..., dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def arange(start: float | int, end: float | int, step: float | int = 1, device=..., dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def pad_sequence(qtensor_list, batch_first: bool = False, padding_value: int | float = 0): ...
    @staticmethod
    def pad_packed_sequence(data, batch_sizes, sort_indice, unsorted_indices, batch_first, padding_value, total_length): ...
    @staticmethod
    def pack_padded_sequence(input, lengths: list[int], batch_first: bool = False, enforce_sorted: bool = True): ...
    @staticmethod
    def sigmoid(input): ...
    @staticmethod
    def rsub_scalar(t, scalar): ...
    @staticmethod
    def index_select(t, dim: int, indice): ...
    @staticmethod
    def create_tensor(data, device, dtype): ...
    @staticmethod
    def requires_grad_setter(self, new_value): ...
    @staticmethod
    def ones(shape, device=None, dtype=None): ...
    @staticmethod
    def convert_to_device_dtype(t, device, dtype): ...
    @staticmethod
    def inplace_requires_grad(t): ...
    @staticmethod
    def ry_matrix(params): ...
    @staticmethod
    def crz_matrix(params): ...
    @staticmethod
    def expval_pauli(measurements, q_machine, obs): ...
    @staticmethod
    def sums(t, axis=None, keepdims: bool = False): ...
    @staticmethod
    def view_as_real(t): ...
    @staticmethod
    def trace(t, k: int = 0): ...
    @staticmethod
    def GPU(self, device=...): ...
    @staticmethod
    def empty_like(t, device=None, dtype=None): ...
    @staticmethod
    def silu(input): ...
    @staticmethod
    def relu(x): ...
    @staticmethod
    def outer(t1, t2): ...
    @staticmethod
    def kron(t1, t2): ...
    @staticmethod
    def eigh(t): ...
    @staticmethod
    def eigvalsh(t): ...
    @staticmethod
    def view_as_complex(t): ...
    @staticmethod
    def CPU(t): ...
    @staticmethod
    def linspace(start: float | int, end: float | int, nums: int, device=..., dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def logspace(start: float | int, end: float | int, nums: int, base: float | int, device: int = 0, dtype=None, requires_grad: bool = False): ...
    @staticmethod
    def cumsum(t, axis: int = -1): ...
    @staticmethod
    def triu(t, diagonal): ...
    @staticmethod
    def tril(t, diagonal): ...
    @staticmethod
    def argtopk(t, k: int, axis: int = -1, if_descent: bool = True): ...
    @staticmethod
    def topk(t, k: int, axis: int = -1, if_descent: bool = True): ...
    @staticmethod
    def argsort(t, axis: int, descending: bool = False, stable: bool = True): ...
    @staticmethod
    def median(t, axis, keepdims): ...
    @staticmethod
    def sort(t, axis: int, descending: bool = False, stable: bool = True): ...
    @staticmethod
    def multinomial(t, num_samples: int): ...
    @staticmethod
    def round(t): ...
    @staticmethod
    def sign(t): ...
    @staticmethod
    def binomial(total_counts, probs): ...
    @staticmethod
    def csr_members(data): ...
    @staticmethod
    def mean(t, axis: _axis_type = None, keepdims: bool = False): ...
    @staticmethod
    def var(t, axis: _axis_type = None, keepdims: bool = False, unbiased: bool = True): ...
    @staticmethod
    def std(t, axis: _axis_type = None, keepdims: bool = False, unbiased: bool = True): ...
    @staticmethod
    def broadcast(t1, t2): ...
    @staticmethod
    def masked_fill(t, mask, value): ...
    @staticmethod
    def broadcast_to(t, ref): ...
    @staticmethod
    def GPU(t): ...
    @staticmethod
    def cross_entropy(output, target): ...
    @staticmethod
    def argmax(self_tensor, dim: _axis_type = None, keepdims: bool = None): ...
    @staticmethod
    def argmin(self_tensor, dim: _axis_type = None, keepdims: bool = None): ...
    @staticmethod
    def max(self_tensor, dim: _axis_type = None, keepdims: bool = None): ...
    @staticmethod
    def min(self_tensor, dim: _axis_type = None, keepdims: bool = None): ...
    @staticmethod
    def softmax_cross_entropy(output, target): ...
    @staticmethod
    def nll_loss(output, target): ...
    @staticmethod
    def binary_cross_entropy(output, target): ...
    @staticmethod
    def full(shape: _size_type, value: float | int, device=..., dtype=None): ...
    @staticmethod
    def relu(x): ...
    @staticmethod
    def flip(t, flip_dims: _size_type): ...
    @staticmethod
    def detach(t): ...
    @staticmethod
    def get_grad_enabled(): ...
    @staticmethod
    def matmul(t1, t2): ...
    @staticmethod
    def adjoint(t): ...
    @staticmethod
    def conj(t): ...
    @staticmethod
    def empty(shape, device: int = 0, dtype=None): ...
    @staticmethod
    def is_not_dense(self): ...
    @staticmethod
    def is_csr(self): ...
    @staticmethod
    def floor(t): ...
    @staticmethod
    def pad(t, pad_list: _shape_type, value: _scalar_type = 0): ...
    @staticmethod
    def roll(t, shifts, dims): ...
    @staticmethod
    def csr_to_dense(x): ...
    @staticmethod
    def swapaxis(data, ax1, ax2): ...
    @staticmethod
    def flatten(t, start: int = 0, end: int = -1): ...
    @staticmethod
    def clip(t, min_val: int | float | None = None, max_val: int | float | None = None): ...
    @staticmethod
    def less_equal(t1, t2): ...
    @staticmethod
    def less(t1, t2): ...
    @staticmethod
    def logical_not(t): ...
    @staticmethod
    def logical_and(t1, t2): ...
    @staticmethod
    def logical_or(t1, t2): ...
    @staticmethod
    def bitwise_and(t1, t2): ...
    @staticmethod
    def logical_xor(t1, t2): ...
    @staticmethod
    def greater(t1, t2): ...
    @staticmethod
    def greater_equal(t1, t2): ...
    @staticmethod
    def where(condition, t1, t2): ...
    @staticmethod
    def nonzero(t): ...
    @staticmethod
    def isfinite(t): ...
    @staticmethod
    def isinf(t): ...
    @staticmethod
    def isnan(t): ...
    @staticmethod
    def isneginf(t): ...
    @staticmethod
    def isposinf(t): ...
    @staticmethod
    def not_equal(t1, t2): ...
    @staticmethod
    def equal(t1, t2): ...
    @staticmethod
    def einsum(equation: str, operands): ...
    @staticmethod
    def dense_to_csr(x): ...
    @staticmethod
    def gather(t, dim: int, index): ...
    @staticmethod
    def scatter(t, dim: int, index, src): ...
    @staticmethod
    def add_scalar(t1, val: _scalar_type): ...
    @staticmethod
    def mse_loss(output, target): ...
    @staticmethod
    def kron(t1, t2): ...
    @staticmethod
    def set_grad_enabled(flag): ...
    @staticmethod
    def as_qtensor(x): ...
    @staticmethod
    def to_numpy(t, copy: bool = True): ...
    @staticmethod
    def grad(t): ...
    @staticmethod
    def backward(self_tensor, grad=None, retain_graph: bool = False): ...
    @staticmethod
    def probs(q_state, num_wires, wires): ...
    @staticmethod
    def reshape(t, shape): ...
    @staticmethod
    def softmax(t, axis): ...
    @staticmethod
    def dropout(self, p: float = 0.5, training: bool = True): ...
    @staticmethod
    def tensordot(x, y, dim1, dim2): ...
    @staticmethod
    def moveaxis(data, src, dst): ...
    @staticmethod
    def astype(self_tensor, dtype): ...
    @staticmethod
    def clone(t): ...
    @staticmethod
    def contiguous(t): ...
    @staticmethod
    def functional_linear(x, weight, bias): ...
    @staticmethod
    def functional_conv2d(x, weight, bias, stride, padding, dilation, groups): ...
    @staticmethod
    def cat(tensors, axis): ...
    @staticmethod
    def copy_to_device(self, device): ...
    @staticmethod
    def full_like(data, value, device=None, dtype=None): ...
    @staticmethod
    def mul_scalar(data1, data2): ...
    @staticmethod
    def ceil(t): ...
    @staticmethod
    def bmm(t1, t2): ...
    @staticmethod
    def unsqueeze(t, axis): ...
    @staticmethod
    def squeeze(t, axis): ...
    @staticmethod
    def mv(t1, t2): ...
    @staticmethod
    def set_random_seed(seed) -> None: ...
    @staticmethod
    def move_to_device(self, device): ...
    @staticmethod
    def copy_to_device(self, device): ...
    @staticmethod
    def real(t): ...
    @staticmethod
    def grad_setter(t, new_value): ...
    @staticmethod
    def imag(t): ...
    @staticmethod
    def transpose(t, dim): ...
    permute = transpose
    @staticmethod
    def setitem(self_tensor, key, value): ...
    @staticmethod
    def get_vqnet_device(device): ...
    @staticmethod
    def all_reduce(t, c_op, group=None) -> None: ...
    @staticmethod
    def reduce(t, dst, c_op, group=None) -> None: ...
    @staticmethod
    def send(t, dst, group=None) -> None: ...
    @staticmethod
    def recv(t, src, group=None) -> None: ...
    @staticmethod
    def send_recv(send, dst, recv, src, group=None) -> None: ...
    @staticmethod
    def distributed_broadcast(t, dst, group=None) -> None: ...
    @staticmethod
    def all_gather_to_tensor(t, group=None): ...
    @staticmethod
    def check_comm(comm) -> None: ...
    @staticmethod
    def barrier(group=None) -> None: ...
    @staticmethod
    def init_group(rank_lists): ...
    @staticmethod
    def get_device_num(): ...
    @staticmethod
    def is_initialized(): ...
    @staticmethod
    def is_available(): ...
    @staticmethod
    def qadjoint_forward(self, common_func, x, *args, **kwargs): ...
