from ..dtype import kcomplex128 as kcomplex128, kcomplex32 as kcomplex32, kcomplex64 as kcomplex64, kfloat64 as kfloat64
from .utils import device_map_torch as device_map_torch, dtype_map_torch as dtype_map_torch
from _typeshed import Incomplete
from functools import lru_cache
from pyvqnet.backends_mock import TorchMock as TorchMock

@lru_cache
def is_torch_version_greater_than(current_version, v): ...

current_version: Incomplete
vqnet_complex_dtypes: Incomplete
vqnet_float_dtypes: Incomplete
unary_operators_preprocess: Incomplete

def complex_to_float_param(param): ...
def float_to_complex_param(param): ...
def uniform(shape, vqnet_dtype, a: int = 0, b: int = 1): ...
def normal_(w, m: int = 0, s: int = 1) -> None: ...
def normal(shape, vqnet_dtype, m: int = 0, s: int = 1): ...
def uniform_(w, a: int = 0, b: int = 1) -> None: ...
def copy_(dst, src) -> None: ...
def copy_data_from(dst, src) -> None: ...
def clone(t): ...
def contiguous(t): ...
def detach(t): ...
def grad_setter(self, new_value) -> None: ...
def check_same_device(torch_device, vqnet_device): ...
def init_from_other_tensor(self_tensor, other_tensor) -> None: ...
def check_same_dtype(torch_dtype, vqnet_dtype): ...
def create_tensor(data, device, dtype):
    """
    create torch tensor from qtensor.
    """
def to_numpy(p, copy: bool = True): ...
def zero_grad(data) -> None: ...
def item(data): ...
def argmax(data, axis, keepdims): ...
def argmin(data, axis, keepdims): ...
def fill_(data, v) -> None: ...
def all(data): ...
def l2_norm(data): ...
def any(data): ...
def fill_rand_uniform_(data, v) -> None: ...
def fill_rand_signed_uniform_(data, v) -> None: ...
def fill_rand_normal_(data, m, s, fast_math) -> None: ...
def fill_rand_binary_(data, v) -> None: ...
def unbind(t, dim: int = 0): ...
def select_1dim(data, dim, index): ...
def view(data, shape): ...
def reshape_(data, new_shape) -> None: ...
def inplace_requires_grad(data): ...
def move_to_device(data, device):
    '''
    convert data.data to device if backend="torch"
    convert data to device if backend="torch-native"
    '''
def CPU(data): ...
def GPU(data, device): ...
def toCPU(data):
    '''
    convert data.data to cpu if backend="torch"
    convert data to cpu if backend="torch-native"
    '''
def toGPU(data, device):
    '''
    convert data.data to gpu if backend="torch"
    convert data to gpu if backend="torch-native"
    '''
def to_device(data, device): ...
def stride(data): ...
def FLOAT_2_COMPLEX(param):
    """
    Convert a PyTorch tensor of float type to its corresponding complex type.
    If the input is already a complex tensor, return it directly.
    Otherwise, raise an error for unsupported types.
    """
def nll_loss(output, target): ...
def binary_cross_entropy(output, target): ...
def softmax_cross_entropy(output, target): ...
def mse_loss(output, target): ...
def cross_entropy(output, target): ...
def create_new_qtensor(cls, t): ...
def is_not_dense(data): ...
def is_csr(data): ...
def csr_members(data): ...
def size(data): ...
def equal(data1, data2): ...
def less(data1, data2): ...
def tile(data, reps): ...
def getitem(data, key): ...
def pad(t, p, v): ...
def setitem(data, key, value): ...
def narrow(t, dim, start, length): ...
def initializer_uniform(shape, vqnet_dtype, a: int = 0, b: int = 1): ...
def initializer_ones(shape, vqnet_dtype): ...
def initializer_xavier_normal(shape, vqnet_dtype, gain): ...
def initializer_normal(shape, vqnet_dtype, m, s): ...
def initializer_xavier_uniform(shape, vqnet_dtype, gain): ...
def initializer_he_normal(shape, vqnet_dtype, a=..., mode: str = 'fan_in', nonlinearity: str = 'leaky_relu'): ...
def initializer_he_uniform(shape, vqnet_dtype, a=..., mode: str = 'fan_in', nonlinearity: str = 'leaky_relu'): ...
def initializer_empty(shape, vqnet_dtype): ...
def initializer_zeros(shape, vqnet_dtype): ...
def pad_sequence(sequences, batch_first: bool = False, padding_value: float = 0.0): ...
def pack_padded_sequence(input, lengths, batch_first: bool = False, enforce_sorted: bool = True): ...
def pad_packed_sequence(data, batch_sizes, sort_indice, unsorted_indice, batch_first, padding_value, total_length): ...
def index_select(t, dim, indice): ...
def dense_to_csr(x): ...
def csr_to_dense(x): ...
def gather(t, dim, index): ...
def set_select(t, index, set_tensor) -> None: ...
def scatter(t, dim, index, src): ...
def flip(data, dims): ...
def reshape(data, shape): ...
def flatten(data, st, ed): ...
def masked_fill(t1, mask1, value): ...
def masked_fill_(t1, mask1, value): ...
def zeros(shape, device, dtype): ...
def broadcast(data1, data2): ...
def broadcast_to(data, shape): ...
def moveaxis(data, src, dst): ...
def squeeze(t, axis): ...
def unsqueeze(data, axis): ...
def swapaxis(data, axis1, axis2): ...
def transpose(data, axis1, axis2): ...
def permute(data, dim): ...
def stack(tensors, axis): ...
def cat(tensors, axis): ...
def zeros_like(data, device=None, dtype=None): ...
def ones(shape, device=None, dtype=None): ...
def ones_like(data, device=None, dtype=None): ...
def empty(shape, device, dtype): ...
def empty_like(data, device, dtype): ...
def full(shape, value, device=None, dtype=None): ...
def full_like(data, value, device=None, dtype=None): ...
def arange(start, end, step, device=None, dtype=None, requires_grad: bool = False): ...
def linspace(start, end, nums, device=None, dtype=None, requires_grad: bool = False): ...
def logspace(start, end, nums, base, device=None, dtype=None, requires_grad: bool = False): ...
def eye(size, offset, device=None, dtype=None, requires_grad: bool = False): ...
def is_nonzero(data): ...
def diagonal(data, offset, dim1, dim2): ...
def diag(data, k): ...
def randu(shape, min, max, device=None, dtype=None, requires_grad: bool = False): ...
def as_qtensor(x): ...
def randn(shape, mean: float = 0.0, std: float = 1.0, device=None, dtype=None, requires_grad: bool = False): ...
def binomial(total_count, probs_data): ...
def triu(data, diagonal): ...
def tril(data, diagonal): ...
def softmax(x, axis): ...
def tensordot(x, y, dim1, dim2): ...
def einsum(equation, operands): ...
def conj(t): ...
def view_as_real(t): ...
def view_as_complex(t): ...
def eigvalsh(t): ...
def eigh(t): ...
def outer(t1, t2): ...
def adjoint(t): ...
def floor(data): ...
def ceil(data): ...
def round(data): ...
def clamp_(self, min_v, max_v): ...
def add_(self, t): ...
def sub_(self, t): ...
def mul_(self, t): ...
def div_(self, t): ...
def add(data1, data2): ...
def rsub_scalar(t, scalar): ...
def sub(data1, data2): ...
def backward(self_tensor, grad=None, retain_graph: bool = False): ...
def roll(t, shifts, dims): ...
def mul_scalar(data1, data2): ...
def mul(data1, data2): ...
def add_scalar(data1, data2): ...
def rdivide(data1, data2): ...
def divide(data1, data2): ...
def sums(data, axis, keepdims): ...
def cumsum(data, axis): ...
def multinomial(data, num_samples): ...
def median(data, axis, keepdims): ...
def std(data, axis, keepdims, unbiased): ...
def var(data, axis, keepdims, unbiased): ...
def mv(data1, data2): ...
def bmm(data1, data2): ...
def matmul(data1, data2): ...
def kron(data1, data2): ...
def reciprocal(data): ...
def sign(data): ...
def neg(data): ...
def trace(data, k): ...
def imag(t): ...
def real(t): ...
def pow_scalar(t1, t2): ...
def pow(data1, data2): ...
def exp(data): ...
def sin(t): ...
def cos(t): ...
def sigmoid(data): ...
def silu(data): ...
def atan2(t1, t2): ...
def tan(data): ...
def asin(data): ...
def acos(data): ...
def atan(data): ...
def sinh(data): ...
def cosh(data): ...
def relu(data): ...
def tanh(data): ...
def abs(data): ...
def log(data): ...
def log_softmax(data, axis): ...
def sqrt(data): ...
def square(data): ...
def frobenius_norm(data, axis, keepdims): ...
def sort(data, axis, descending, stable): ...
def topk(data, k, axis, if_descent): ...
def argsort(data, axis, descending, stable): ...
def argtopk(data, k, axis, if_descent): ...
def not_equal(t1, t2): ...
def nonzero(data): ...
def isfinite(data): ...
def isinf(data): ...
def isnan(data): ...
def isneginf(data): ...
def isposinf(data): ...
def bitwise_and(t1, t2): ...
def logical_and(t1, t2): ...
def logical_or(t1, t2): ...
def logical_not(t1): ...
def logical_xor(t1, t2): ...
def min(data, axis, keepdims): ...
def max(data, axis, keepdims): ...
def maximum(data1, data2): ...
def minimum(data1, data2): ...
def clip(data, min_val, max_val): ...
def where(condition, data1, data2): ...
def grad(self): ...
def astype(data, dtype): ...
def mean(data, axis, keepdims): ...
def greater(data1, data2): ...
def less_equal(data1, data2): ...
def greater_equal(data1, data2): ...
def adagrad_init_params(self, group, params_with_grad, grads, G, state_steps) -> None: ...
def adadelta_init_params(self, group, params_with_grad, grads, E, A, state_steps) -> None: ...
def adam_init_params(self, group, params_with_grad, grads, ms, vs, max_vcaps, state_steps) -> None: ...
def rmsprop(params, grads, E, state_steps, lr, beta, epsilon) -> None: ...
def adamax(params, grads, ms, vs, state_steps, lr, beta1, beta2, epsilon) -> None: ...
def adamw_init_params(self, group, params_with_grad, grads, ms, vs, max_vcaps, state_steps) -> None: ...
def adamw(params, grads, ms, vs, max_vcaps, state_steps, lr, beta1, beta2, epsilon, weight_decay, amsgrad) -> None: ...
def adamax_init_params(self, group, params_with_grad, grads, ms, vs, state_steps) -> None: ...
def adagrad(params, grads, G, state_steps, lr, epsilon) -> None: ...
def adadelta(params, grads, E, A, state_steps, lr, beta, epsilon) -> None: ...
def sgd(params, grads, ms, lr, momentum, nesterov) -> None: ...
def adam(params, grads, ms, vs, max_vCap, state_steps, weight_decay, lr, beta1, beta2, epsilon, amsgrad): ...
def rmsprop_init_params(self, group, params_with_grad, grads, E, state_steps) -> None: ...
def sgd_init_params(self, group, params_with_grad, grads, ms, state_steps) -> None: ...
def convert_to_device_dtype(t, device, dtype): ...
def copy_to_device(self, device): ...
def functional_conv2d(x, weight, bias, stride, padding, dilation, groups): ...
def functional_linear(x, weight, bias): ...
