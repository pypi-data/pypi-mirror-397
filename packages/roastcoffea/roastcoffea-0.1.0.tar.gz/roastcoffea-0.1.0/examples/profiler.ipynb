{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Metrics Demo: Complete Guide to Dask Spans Integration\n",
    "\n",
    "This notebook demonstrates all fine-grained performance metrics available through Dask Spans in roastcoffea.\n",
    "\n",
    "## What are Fine Metrics?\n",
    "\n",
    "Fine metrics provide detailed performance breakdowns beyond wall time:\n",
    "- **CPU vs I/O time**: How much time spent computing vs waiting?\n",
    "- **Compression overhead**: Time spent compressing/decompressing data\n",
    "- **Serialization overhead**: Time spent serializing/deserializing Python objects\n",
    "- **Disk/Memory I/O**: Bytes read from disk or memory\n",
    "- **Real compression ratios**: Actual uncompressed vs compressed bytes\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "Fine metrics come from **Dask Spans** and are available at multiple granularities:\n",
    "1. **Cumulative (workflow-level)**: Aggregated across all tasks and workers\n",
    "2. **Per-task**: Broken down by individual tasks (task prefix)\n",
    "3. **Per-worker**: (Future) Separated by individual workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using test file: /Users/moaly/.local/skhepdata/nanoAOD_2015_CMS_Open_Data_ttbar.root\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "\n",
    "import awkward as ak\n",
    "from coffea import processor\n",
    "from coffea.nanoevents import NanoAODSchema\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Get a test file from scikit-hep-testdata\n",
    "from skhep_testdata import data_path\n",
    "\n",
    "test_file = data_path(\"nanoAOD_2015_CMS_Open_Data_ttbar.root\")\n",
    "print(f\"Using test file: {test_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Simple Coffea Processor\n",
    "\n",
    "We'll create a processor that does some real work: filtering jets, computing masses, and creating histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetAnalysisProcessor(processor.ProcessorABC):\n",
    "    \"\"\"Simple jet analysis processor for testing fine metrics.\"\"\"\n",
    "\n",
    "    def process(self, events):\n",
    "        # Select jets with pT > 30 GeV\n",
    "        jets = events.Jet[events.Jet.pt > 30]\n",
    "\n",
    "        # Select events with at least 2 jets\n",
    "        two_jet_events = events[ak.num(jets) >= 2]\n",
    "        two_jets = jets[ak.num(jets) >= 2]\n",
    "\n",
    "        # Calculate dijet invariant mass for leading two jets\n",
    "        if len(two_jets) > 0:\n",
    "            j1 = two_jets[:, 0]\n",
    "            j2 = two_jets[:, 1]\n",
    "            dijet_mass = (j1 + j2).mass\n",
    "        else:\n",
    "            dijet_mass = ak.Array([])\n",
    "\n",
    "        return {\n",
    "            \"nevents\": len(events),\n",
    "            \"njets_total\": ak.sum(ak.num(jets)),\n",
    "            \"nevents_2jet\": len(two_jet_events),\n",
    "            \"dijet_mass_mean\": ak.mean(dijet_mass) if len(dijet_mass) > 0 else 0,\n",
    "            \"jet_pt_sum\": ak.sum(jets.pt),\n",
    "        }\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Collecting Fine Metrics\n",
    "\n",
    "Let's run the processor with metrics collection enabled. Fine metrics are collected automatically when Dask Spans are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moaly/Work/iris-hep/roastcoffea/.pixi/envs/dev/lib/python3.13/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 60701 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard: http://127.0.0.1:60701/status\n"
     ]
    }
   ],
   "source": [
    "# Create fileset\n",
    "fileset = {\n",
    "    \"DY\": {\n",
    "        \"files\": {test_file: \"Events\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Start Dask cluster\n",
    "cluster = LocalCluster(n_workers=4, threads_per_worker=1, processes=True)\n",
    "client = Client(cluster)\n",
    "\n",
    "print(f\"Dashboard: {client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d31d458b454d5dbe914a97b83bf115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run with metrics collection\n",
    "executor = processor.DaskExecutor(client=client)\n",
    "runner = processor.Runner(\n",
    "    executor=executor,\n",
    "    savemetrics=True,\n",
    "    schema=NanoAODSchema,\n",
    ")\n",
    "\n",
    "output, report = runner(\n",
    "    fileset,\n",
    "    treename=\"Events\",\n",
    "    processor_instance=JetAnalysisProcessor(),\n",
    ")\n",
    "\n",
    "\n",
    "# discover workers + useful metadata\n",
    "sch = client.scheduler_info()\n",
    "workers = list(sch[\"workers\"].keys())  # e.g. ['tcp://10.0.0.5:12345', ...]\n",
    "\n",
    "\n",
    "def worker_meta(addr):\n",
    "    w = sch[\"workers\"][addr]\n",
    "    return {\n",
    "        \"address\": addr,\n",
    "        \"name\": w.get(\"name\"),\n",
    "        \"host\": w.get(\"host\"),\n",
    "        \"nthreads\": w.get(\"nthreads\"),\n",
    "        \"memory_limit\": w.get(\"memory_limit\"),\n",
    "        \"versions\": w.get(\"versions\", {}),\n",
    "    }\n",
    "\n",
    "\n",
    "profiles_by_worker = {\n",
    "    addr: {\n",
    "        \"meta\": worker_meta(addr),\n",
    "        \"profile\": client.profile(workers=[addr]),  # same tree shape you printed\n",
    "    }\n",
    "    for addr in workers\n",
    "}\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"dask_profiles.json\", \"w\") as f:\n",
    "    json.dump(profiles_by_worker, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections.abc import Iterable\n",
    "from typing import Any\n",
    "\n",
    "# ---------- helpers ----------\n",
    "\n",
    "\n",
    "def _iter_children(node: dict[str, Any]) -> Iterable[dict[str, Any]]:\n",
    "    ch = node.get(\"children\") or {}\n",
    "    if isinstance(ch, dict):\n",
    "        return ch.values()\n",
    "    if isinstance(ch, list):\n",
    "        return ch\n",
    "    return ()\n",
    "\n",
    "\n",
    "def _label_from_node(node: dict[str, Any]) -> tuple[str, str, int]:\n",
    "    \"\"\"\n",
    "    Return a normalized (func, file, line) using identifier/description.\n",
    "    \"\"\"\n",
    "    func = file = \"\"\n",
    "    line = 0\n",
    "    ident = node.get(\"identifier\")\n",
    "    if isinstance(ident, str) and \";\" in ident:\n",
    "        try:\n",
    "            f, p, ln = ident.split(\";\", 2)\n",
    "            func, file, line = f, p, int(ln)\n",
    "        except Exception:\n",
    "            pass\n",
    "    desc = node.get(\"description\") or {}\n",
    "    func = (desc.get(\"name\") or func or \"\").strip()\n",
    "    file = (desc.get(\"filename\") or file or \"\").strip()\n",
    "    line = int(desc.get(\"line_number\") or line or 0)\n",
    "    return func, file, line\n",
    "\n",
    "\n",
    "def _node_label_string(func: str, file: str, line: int) -> str:\n",
    "    # concise but unique-ish; keep file basename to shorten long paths if you prefer\n",
    "    return f\"{func}@{file}:{line}\"\n",
    "\n",
    "\n",
    "# ---------- core traversal ----------\n",
    "\n",
    "\n",
    "def _build_tree_and_stacks(\n",
    "    node: dict[str, Any],\n",
    "    *,\n",
    "    interval_ms: float,\n",
    "    path_labels: list[str],\n",
    "    stacks_exclusive: dict[tuple[str, ...], int],\n",
    "    root_total_count: int,\n",
    "    max_depth: int = 2000,\n",
    ") -> tuple[dict[str, Any] | None, int]:\n",
    "    \"\"\"\n",
    "    DFS:\n",
    "      - returns (clean_tree_node, inclusive_count)\n",
    "      - fills stacks_exclusive with exclusive counts per stack path\n",
    "    \"\"\"\n",
    "    if max_depth <= 0:\n",
    "        return None, 0\n",
    "\n",
    "    count_incl = int(node.get(\"count\") or 0)\n",
    "    func, file, line = _label_from_node(node)\n",
    "    here_label = _node_label_string(func, file, line)\n",
    "    new_path = path_labels + [here_label]\n",
    "\n",
    "    # Recurse children\n",
    "    children_clean = []\n",
    "    children_count_sum = 0\n",
    "    for ch in _iter_children(node):\n",
    "        child_clean, child_incl = _build_tree_and_stacks(\n",
    "            ch,\n",
    "            interval_ms=interval_ms,\n",
    "            path_labels=new_path,\n",
    "            stacks_exclusive=stacks_exclusive,\n",
    "            root_total_count=root_total_count,\n",
    "            max_depth=max_depth - 1,\n",
    "        )\n",
    "        if child_clean is not None:\n",
    "            children_clean.append(child_clean)\n",
    "            children_count_sum += child_incl\n",
    "\n",
    "    # Exclusive count for THIS frame\n",
    "    count_excl = max(count_incl - children_count_sum, 0)\n",
    "    if count_excl > 0:\n",
    "        stacks_exclusive[tuple(new_path)] = (\n",
    "            stacks_exclusive.get(tuple(new_path), 0) + count_excl\n",
    "        )\n",
    "\n",
    "    # Build lean tree node\n",
    "    time_ms_incl = count_incl * interval_ms\n",
    "    time_ms_excl = count_excl * interval_ms\n",
    "    pct_incl = (100.0 * count_incl / root_total_count) if root_total_count else 0.0\n",
    "    pct_excl = (100.0 * count_excl / root_total_count) if root_total_count else 0.0\n",
    "\n",
    "    clean = {\n",
    "        \"label\": here_label,\n",
    "        \"func\": func,\n",
    "        \"file\": file,\n",
    "        \"line\": line,\n",
    "        \"count_incl\": count_incl,\n",
    "        \"time_ms_incl\": time_ms_incl,\n",
    "        \"pct_incl\": pct_incl,\n",
    "        \"count_excl\": count_excl,\n",
    "        \"time_ms_excl\": time_ms_excl,\n",
    "        \"pct_excl\": pct_excl,\n",
    "        \"children\": children_clean,\n",
    "    }\n",
    "    return clean, count_incl\n",
    "\n",
    "\n",
    "def _sum_all_counts(node: dict[str, Any]) -> int:\n",
    "    total = int(node.get(\"count\") or 0)\n",
    "    for ch in _iter_children(node):\n",
    "        total += _sum_all_counts(ch)\n",
    "    return total\n",
    "\n",
    "\n",
    "# ---------- public APIs ----------\n",
    "\n",
    "\n",
    "def parse_profile_to_stacks_and_tree(\n",
    "    profile_tree: dict[str, Any],\n",
    "    *,\n",
    "    interval_ms: float = 10.0,\n",
    "    top_n_stacks: int = 50,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convert one Dask profile tree to:\n",
    "      - clean hierarchical tree with incl/excl time\n",
    "      - stack list (exclusive), good for flame graphs\n",
    "    \"\"\"\n",
    "    # Some profiles report root=0; fall back to total over subtree so percentages are meaningful\n",
    "    root_count = int(profile_tree.get(\"count\") or 0)\n",
    "    fallback_total = _sum_all_counts(profile_tree)\n",
    "    total_count_used = root_count if root_count > 0 else fallback_total\n",
    "    total_time_ms = total_count_used * interval_ms\n",
    "\n",
    "    stacks_exclusive: dict[tuple[str, ...], int] = {}\n",
    "    clean_tree, _ = _build_tree_and_stacks(\n",
    "        profile_tree,\n",
    "        interval_ms=interval_ms,\n",
    "        path_labels=[],\n",
    "        stacks_exclusive=stacks_exclusive,\n",
    "        root_total_count=total_count_used,\n",
    "    )\n",
    "\n",
    "    # Build stack rows\n",
    "    stack_rows = []\n",
    "    for stack, cnt in stacks_exclusive.items():\n",
    "        t_ms = cnt * interval_ms\n",
    "        stack_rows.append(\n",
    "            {\n",
    "                \"stack\": list(stack),  # [\"root@...\", \"...\", \"leaf@...\"]\n",
    "                \"depth\": len(stack),\n",
    "                \"count_excl\": cnt,\n",
    "                \"time_ms_excl\": t_ms,\n",
    "                \"time_s_excl\": t_ms / 1000.0,\n",
    "                \"pct_of_total\": (100.0 * cnt / total_count_used)\n",
    "                if total_count_used\n",
    "                else 0.0,\n",
    "            }\n",
    "        )\n",
    "    stack_rows.sort(key=lambda r: r[\"count_excl\"], reverse=True)\n",
    "    top_rows = stack_rows[:top_n_stacks]\n",
    "\n",
    "    # Optional: a ready-to-write flamegraph TSV (exclusive)\n",
    "    flamegraph_tsv = \"\\n\".join(\n",
    "        [\";\".join(row[\"stack\"]) + f\"\\t{row['count_excl']}\" for row in stack_rows]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"meta\": {\n",
    "            \"interval_ms\": interval_ms,\n",
    "            \"total_count_used\": total_count_used,\n",
    "            \"total_time_ms\": total_time_ms,\n",
    "            \"empty_tree\": (total_count_used == 0),\n",
    "        },\n",
    "        \"tree\": clean_tree,\n",
    "        \"stacks\": {\n",
    "            \"rows\": stack_rows,\n",
    "            \"top\": top_rows,\n",
    "            \"flamegraph_tsv\": flamegraph_tsv,  # can be big; drop if you don't need it\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def parse_profiles_by_worker_to_stacks(\n",
    "    profiles_by_worker: dict[str, dict[str, Any]],\n",
    "    *,\n",
    "    interval_ms: float = 10.0,\n",
    "    top_n_stacks: int = 50,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Apply parse_profile_to_stacks_and_tree to each worker and bundle results.\n",
    "    \"\"\"\n",
    "    workers_out = []\n",
    "    for addr, payload in profiles_by_worker.items():\n",
    "        meta = payload.get(\"meta\", {}) or {\"address\": addr}\n",
    "        tree = payload.get(\"profile\", {}) or {}\n",
    "        parsed = parse_profile_to_stacks_and_tree(\n",
    "            tree, interval_ms=interval_ms, top_n_stacks=top_n_stacks\n",
    "        )\n",
    "        workers_out.append(\n",
    "            {\n",
    "                \"worker\": {\n",
    "                    \"address\": meta.get(\"address\", addr),\n",
    "                    \"name\": meta.get(\"name\"),\n",
    "                    \"host\": meta.get(\"host\"),\n",
    "                    \"nthreads\": meta.get(\"nthreads\"),\n",
    "                    \"memory_limit\": meta.get(\"memory_limit\"),\n",
    "                },\n",
    "                \"profile\": parsed,\n",
    "            }\n",
    "        )\n",
    "    return {\n",
    "        \"interval_ms\": interval_ms,\n",
    "        \"workers\": workers_out,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'address': 'tcp://127.0.0.1:60713', 'name': 3, 'host': '127.0.0.1', 'nthreads': 1, 'memory_limit': None}\n"
     ]
    }
   ],
   "source": [
    "# 1) Collect per-worker profiles (you already have this)\n",
    "# profiles_by_worker = { addr: {\"meta\": {...}, \"profile\": client.profile(workers=[addr])} ... }\n",
    "\n",
    "# 2) Parse into clean tree + per-stack (exclusive) aggregation\n",
    "parsed = parse_profiles_by_worker_to_stacks(profiles_by_worker, interval_ms=10.0, top_n_stacks=40)\n",
    "\n",
    "# 3) Inspect a workerâ€™s top stacks\n",
    "w0 = parsed[\"workers\"][0]\n",
    "print(w0[\"worker\"])\n",
    "for row in w0[\"profile\"][\"stacks\"][\"top\"][:10]:\n",
    "    print(f\"{row['pct_of_total']:5.1f}%  {row['time_ms_excl']:8.1f} ms  depth={row['depth']}  {row['stack'][-1]}\")\n",
    "\n",
    "# 4) If you want a flame graph, write the TSV (one per worker)\n",
    "# with open(\"worker0.flame.tsv\", \"w\") as f:\n",
    "#     f.write(w0[\"profile\"][\"stacks\"][\"flamegraph_tsv\"])\n",
    "# Then use flamegraph.pl or speedscope to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster closed\n"
     ]
    }
   ],
   "source": [
    "# Close cluster\n",
    "client.close()\n",
    "cluster.close()\n",
    "print(\"Cluster closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
