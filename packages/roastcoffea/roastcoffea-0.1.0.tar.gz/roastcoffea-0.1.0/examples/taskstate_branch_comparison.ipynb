{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# TaskState Branch Comparison\n",
    "\n",
    "This notebook tests how TaskState.nbytes changes with different numbers of branches read.\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "If we read more branches from ROOT files:\n",
    "- Coffea's `bytesread` should increase (more data read from disk)\n",
    "- TaskState's `nbytes` (output size) should also increase (larger results)\n",
    "\n",
    "## Approach\n",
    "\n",
    "1. Create processors that read 1, 2, 5, and 10 branches\n",
    "2. Monitor TaskState during each run\n",
    "3. Compare task output sizes across runs\n",
    "4. Compare with Coffea's bytesread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025.11.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import awkward as ak\n",
    "import coffea\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import skhep_testdata\n",
    "from coffea import processor\n",
    "from coffea.nanoevents import NanoAODSchema\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "coffea.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Task Monitor (from previous notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskMonitor:\n",
    "    \"\"\"Monitor Dask scheduler tasks in real-time.\"\"\"\n",
    "\n",
    "    def __init__(self, scheduler, interval=0.1):\n",
    "        self.scheduler = scheduler\n",
    "        self.interval = interval\n",
    "        self.snapshots = []\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start monitoring.\"\"\"\n",
    "        self.running = True\n",
    "        self.thread = threading.Thread(target=self._monitor_loop, daemon=True)\n",
    "        self.thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop monitoring.\"\"\"\n",
    "        self.running = False\n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=5)\n",
    "\n",
    "    def _monitor_loop(self):\n",
    "        \"\"\"Background monitoring loop.\"\"\"\n",
    "        while self.running:\n",
    "            try:\n",
    "                snapshot = self._capture_snapshot()\n",
    "                if snapshot[\"tasks\"]:\n",
    "                    self.snapshots.append(snapshot)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in monitoring: {e}\")\n",
    "\n",
    "            time.sleep(self.interval)\n",
    "\n",
    "    def _capture_snapshot(self):\n",
    "        \"\"\"Capture current scheduler state.\"\"\"\n",
    "        timestamp = datetime.datetime.now()\n",
    "\n",
    "        tasks = []\n",
    "        for task_key, task_state in self.scheduler.tasks.items():\n",
    "            worker = None\n",
    "            if hasattr(task_state, \"who_has\") and task_state.who_has:\n",
    "                worker = (\n",
    "                    list(task_state.who_has)[0].address if task_state.who_has else None\n",
    "                )\n",
    "\n",
    "            tasks.append(\n",
    "                {\n",
    "                    \"key\": task_key,\n",
    "                    \"state\": task_state.state,\n",
    "                    \"worker\": worker,\n",
    "                    \"nbytes\": task_state.nbytes if hasattr(task_state, \"nbytes\") else 0,\n",
    "                    \"type\": task_state.type if hasattr(task_state, \"type\") else None,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"num_tasks\": len(tasks),\n",
    "            \"num_workers\": len(self.scheduler.workers),\n",
    "            \"tasks\": tasks,\n",
    "        }\n",
    "\n",
    "    def get_all_tasks(self):\n",
    "        \"\"\"Get all unique tasks seen across all snapshots.\"\"\"\n",
    "        all_tasks = {}\n",
    "\n",
    "        for snapshot in self.snapshots:\n",
    "            for task in snapshot[\"tasks\"]:\n",
    "                key = task[\"key\"]\n",
    "                if key not in all_tasks or task[\"nbytes\"] > all_tasks[key][\"nbytes\"]:\n",
    "                    all_tasks[key] = task\n",
    "\n",
    "        return list(all_tasks.values())\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear snapshots for next run.\"\"\"\n",
    "        self.snapshots = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Create Processors with Different Branch Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_profiler import profile\n",
    "\n",
    "\n",
    "class BranchTestProcessor(processor.ProcessorABC):\n",
    "    \"\"\"Processor that reads a specific number of branches.\"\"\"\n",
    "\n",
    "    def __init__(self, num_branches: int):\n",
    "        self.num_branches = num_branches\n",
    "\n",
    "    @profile\n",
    "    def process(self, events):\n",
    "        results = {}\n",
    "\n",
    "        # Always count events\n",
    "        results[\"nevents\"] = len(events)\n",
    "\n",
    "        # Read increasing numbers of branches\n",
    "        if self.num_branches >= 1:\n",
    "            # Branch 1: Jet pt\n",
    "            jets = events.Jet[events.Jet.pt > 30]\n",
    "            results[\"njets\"] = ak.sum(ak.num(jets))\n",
    "            results[\"mean_jet_pt\"] = ak.mean(jets.pt) if len(jets) > 0 else 0\n",
    "\n",
    "        if self.num_branches >= 2:\n",
    "            # Branch 2: Jet eta\n",
    "            results[\"mean_jet_eta\"] = ak.mean(jets.eta) if len(jets) > 0 else 0\n",
    "\n",
    "        if self.num_branches >= 3:\n",
    "            # Branch 3: Jet phi\n",
    "            results[\"mean_jet_phi\"] = ak.mean(jets.phi) if len(jets) > 0 else 0\n",
    "\n",
    "        if self.num_branches >= 4:\n",
    "            # Branch 4: Jet mass\n",
    "            results[\"mean_jet_mass\"] = ak.mean(jets.mass) if len(jets) > 0 else 0\n",
    "\n",
    "        if self.num_branches >= 5:\n",
    "            # Branch 5: Electron pt\n",
    "            electrons = events.Electron[events.Electron.pt > 20]\n",
    "            results[\"nelectrons\"] = ak.sum(ak.num(electrons))\n",
    "            results[\"mean_electron_pt\"] = (\n",
    "                ak.mean(electrons.pt) if len(electrons) > 0 else 0\n",
    "            )\n",
    "\n",
    "        if self.num_branches >= 6:\n",
    "            # Branch 6: Electron eta\n",
    "            results[\"mean_electron_eta\"] = (\n",
    "                ak.mean(electrons.eta) if len(electrons) > 0 else 0\n",
    "            )\n",
    "\n",
    "        if self.num_branches >= 7:\n",
    "            # Branch 7: Muon pt\n",
    "            muons = events.Muon[events.Muon.pt > 20]\n",
    "            results[\"nmuons\"] = ak.sum(ak.num(muons))\n",
    "            results[\"mean_muon_pt\"] = ak.mean(muons.pt) if len(muons) > 0 else 0\n",
    "\n",
    "        if self.num_branches >= 8:\n",
    "            # Branch 8: Muon eta\n",
    "            results[\"mean_muon_eta\"] = ak.mean(muons.eta) if len(muons) > 0 else 0\n",
    "\n",
    "        if self.num_branches >= 9:\n",
    "            # Branch 9: MET pt\n",
    "            results[\"mean_met_pt\"] = ak.mean(events.MET.pt)\n",
    "\n",
    "        if self.num_branches >= 10:\n",
    "            # Branch 10: MET phi\n",
    "            results[\"mean_met_phi\"] = ak.mean(events.MET.phi)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Start Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moaly/Work/iris-hep/roastcoffea/.pixi/envs/dev/lib/python3.13/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 64127 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard: http://127.0.0.1:64127/status\n",
      "Workers: 2\n"
     ]
    }
   ],
   "source": [
    "# Start cluster\n",
    "cluster = LocalCluster(n_workers=2, threads_per_worker=1, processes=True)\n",
    "client = Client(cluster)\n",
    "\n",
    "print(f\"Dashboard: {client.dashboard_link}\")\n",
    "print(f\"Workers: {len(client.scheduler_info()['workers'])}\")\n",
    "\n",
    "# Get test file\n",
    "test_file = skhep_testdata.data_path(\"nanoAOD_2015_CMS_Open_Data_ttbar.root\")\n",
    "\n",
    "# Create fileset\n",
    "fileset = {\n",
    "    \"ttbar\": {\n",
    "        \"files\": {test_file: \"Events\"},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Run Experiments with Different Branch Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing with 1 branches\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c745d87b7a944525b6836ef754514740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fe55970dc34117b18a5497bb420cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file /var/folders/0v/cvfdg7wn2k59f7d0pp1wdsy80000gn/T/ipykernel_8716/1640942170.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moaly/Work/iris-hep/roastcoffea/.pixi/envs/dev/lib/python3.13/site-packages/coffea/nanoevents/schemas/nanoaod.py:264: RuntimeWarning: Missing cross-reference index for LowPtElectron_electronIdx => Electron\n",
      "  warnings.warn(\n",
      "/Users/moaly/Work/iris-hep/roastcoffea/.pixi/envs/dev/lib/python3.13/site-packages/coffea/nanoevents/schemas/nanoaod.py:264: RuntimeWarning: Missing cross-reference index for LowPtElectron_genPartIdx => GenPart\n",
      "  warnings.warn(\n",
      "/Users/moaly/Work/iris-hep/roastcoffea/.pixi/envs/dev/lib/python3.13/site-packages/coffea/nanoevents/schemas/nanoaod.py:264: RuntimeWarning: Missing cross-reference index for LowPtElectron_photonIdx => Photon\n",
      "  warnings.warn(\n",
      "/Users/moaly/Work/iris-hep/roastcoffea/.pixi/envs/dev/lib/python3.13/site-packages/coffea/nanoevents/schemas/nanoaod.py:264: RuntimeWarning: Missing cross-reference index for FatJet_subJetIdx1 => SubJet\n",
      "  warnings.warn(\n",
      "/Users/moaly/Work/iris-hep/roastcoffea/.pixi/envs/dev/lib/python3.13/site-packages/coffea/nanoevents/schemas/nanoaod.py:264: RuntimeWarning: Missing cross-reference index for FatJet_subJetIdx2 => SubJet\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[Accessed(branch='nJet', buffer_key='d48060b2-6a57-11ed-8e14-0600a8c0beef/%2FEvents%3B1/0-200/offsets/nJet%2C%21load%2C%21counts2offsets%2C%21skip%2C%21offsets'),\n",
      " Accessed(branch='Jet_pt', buffer_key='d48060b2-6a57-11ed-8e14-0600a8c0beef/%2FEvents%3B1/0-200/data/Jet_pt%2C%21load%2C%21content')]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'mem_usage_bytes'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m pprint(\u001b[38;5;28mlen\u001b[39m(report[\u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m     36\u001b[39m pprint(report[\u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m pprint(\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmem_usage_bytes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m/\u001b[32m1e6\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Wait for final tasks\u001b[39;00m\n\u001b[32m     39\u001b[39m time.sleep(\u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'mem_usage_bytes'"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Test with different numbers of branches\n",
    "branch_counts = [1, 2, 3, 5, 10]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Create monitor once\n",
    "monitor = TaskMonitor(client.cluster.scheduler, interval=0.1)\n",
    "\n",
    "for num_branches in branch_counts:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Testing with {num_branches} branches\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    # Reset monitor for this run\n",
    "    monitor.reset()\n",
    "    monitor.start()\n",
    "\n",
    "    # Run processor\n",
    "    proc = BranchTestProcessor(num_branches=num_branches)\n",
    "    executor = processor.DaskExecutor(client=client)\n",
    "    runner = processor.Runner(\n",
    "        executor=executor,\n",
    "        savemetrics=True,\n",
    "        schema=NanoAODSchema,\n",
    "    )\n",
    "\n",
    "    output, report = runner(\n",
    "        fileset,\n",
    "        treename=\"Events\",\n",
    "        processor_instance=proc,\n",
    "    )\n",
    "\n",
    "    pprint(len(report[\"columns\"]))\n",
    "    pprint(report[\"columns\"])\n",
    "    # Wait for final tasks\n",
    "    time.sleep(1)\n",
    "    monitor.stop()\n",
    "\n",
    "    # Get task data\n",
    "    all_tasks = monitor.get_all_tasks()\n",
    "    processor_tasks = [\n",
    "        task\n",
    "        for task in all_tasks\n",
    "        if \"BranchTestProcessor\" in str(task[\"key\"])\n",
    "        or \"process\" in str(task[\"key\"]).lower()\n",
    "    ]\n",
    "\n",
    "    # Calculate statistics\n",
    "    task_sizes = [task[\"nbytes\"] for task in processor_tasks if task[\"nbytes\"]]\n",
    "\n",
    "    result = {\n",
    "        \"num_branches\": num_branches,\n",
    "        \"coffea_bytesread\": report[\"bytesread\"],\n",
    "        \"coffea_bytesread_mb\": report[\"bytesread\"] / 1e6,\n",
    "        \"num_processor_tasks\": len(processor_tasks),\n",
    "        \"total_task_bytes\": sum(task_sizes),\n",
    "        \"total_task_bytes_kb\": sum(task_sizes) / 1e3,\n",
    "        \"mean_task_bytes\": sum(task_sizes) / len(task_sizes) if task_sizes else 0,\n",
    "        \"mean_task_bytes_kb\": (sum(task_sizes) / len(task_sizes) / 1e3)\n",
    "        if task_sizes\n",
    "        else 0,\n",
    "        \"chunks\": report[\"chunks\"],\n",
    "        \"events\": report[\"entries\"],\n",
    "    }\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"  Coffea bytesread: {result['coffea_bytesread_mb']:.2f} MB\")\n",
    "    print(f\"  Processor tasks: {result['num_processor_tasks']}\")\n",
    "    print(f\"  Total task output: {result['total_task_bytes_kb']:.2f} KB\")\n",
    "    print(f\"  Mean task output: {result['mean_task_bytes_kb']:.2f} KB\")\n",
    "    print(f\"  Chunks: {result['chunks']}\")\n",
    "    print(f\"  Events: {result['events']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n=== Summary Table ===\")\n",
    "print(\n",
    "    df[\n",
    "        [\n",
    "            \"num_branches\",\n",
    "            \"coffea_bytesread_mb\",\n",
    "            \"total_task_bytes_kb\",\n",
    "            \"mean_task_bytes_kb\",\n",
    "            \"num_processor_tasks\",\n",
    "            \"chunks\",\n",
    "        ]\n",
    "    ].to_string(index=False)\n",
    ")\n",
    "\n",
    "# Calculate ratios\n",
    "print(\"\\n=== Scaling Analysis ===\")\n",
    "if len(df) > 1:\n",
    "    baseline_bytesread = df.iloc[0][\"coffea_bytesread_mb\"]\n",
    "    baseline_task_bytes = df.iloc[0][\"total_task_bytes_kb\"]\n",
    "\n",
    "    df[\"bytesread_ratio\"] = df[\"coffea_bytesread_mb\"] / baseline_bytesread\n",
    "    df[\"task_bytes_ratio\"] = df[\"total_task_bytes_kb\"] / baseline_task_bytes\n",
    "\n",
    "    print(\n",
    "        df[\n",
    "            [\n",
    "                \"num_branches\",\n",
    "                \"bytesread_ratio\",\n",
    "                \"task_bytes_ratio\",\n",
    "            ]\n",
    "        ].to_string(index=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Coffea bytesread vs branches\n",
    "axes[0, 0].plot(\n",
    "    df[\"num_branches\"], df[\"coffea_bytesread_mb\"], \"o-\", linewidth=2, markersize=8\n",
    ")\n",
    "axes[0, 0].set_xlabel(\"Number of Branches\")\n",
    "axes[0, 0].set_ylabel(\"Coffea Bytesread (MB)\")\n",
    "axes[0, 0].set_title(\"Coffea Bytesread vs Number of Branches\")\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: TaskState total output vs branches\n",
    "axes[0, 1].plot(\n",
    "    df[\"num_branches\"],\n",
    "    df[\"total_task_bytes_kb\"],\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    markersize=8,\n",
    "    color=\"orange\",\n",
    ")\n",
    "axes[0, 1].set_xlabel(\"Number of Branches\")\n",
    "axes[0, 1].set_ylabel(\"Total Task Output (KB)\")\n",
    "axes[0, 1].set_title(\"TaskState Total Output vs Number of Branches\")\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Mean task output vs branches\n",
    "axes[1, 0].plot(\n",
    "    df[\"num_branches\"],\n",
    "    df[\"mean_task_bytes_kb\"],\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    markersize=8,\n",
    "    color=\"green\",\n",
    ")\n",
    "axes[1, 0].set_xlabel(\"Number of Branches\")\n",
    "axes[1, 0].set_ylabel(\"Mean Task Output (KB)\")\n",
    "axes[1, 0].set_title(\"Mean Task Output vs Number of Branches\")\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Scaling ratios\n",
    "if len(df) > 1:\n",
    "    axes[1, 1].plot(\n",
    "        df[\"num_branches\"],\n",
    "        df[\"bytesread_ratio\"],\n",
    "        \"o-\",\n",
    "        linewidth=2,\n",
    "        markersize=8,\n",
    "        label=\"Coffea Bytesread\",\n",
    "    )\n",
    "    axes[1, 1].plot(\n",
    "        df[\"num_branches\"],\n",
    "        df[\"task_bytes_ratio\"],\n",
    "        \"s-\",\n",
    "        linewidth=2,\n",
    "        markersize=8,\n",
    "        label=\"TaskState Output\",\n",
    "    )\n",
    "    axes[1, 1].set_xlabel(\"Number of Branches\")\n",
    "    axes[1, 1].set_ylabel(\"Ratio (relative to 1 branch)\")\n",
    "    axes[1, 1].set_title(\"Scaling Ratios\")\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Analysis and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Key Findings ===\")\n",
    "print()\n",
    "\n",
    "if len(df) > 1:\n",
    "    # Calculate correlation\n",
    "    correlation = df[[\"coffea_bytesread_mb\", \"total_task_bytes_kb\"]].corr().iloc[0, 1]\n",
    "\n",
    "    print(\n",
    "        f\"1. Correlation between Coffea bytesread and TaskState output: {correlation:.3f}\"\n",
    "    )\n",
    "    print()\n",
    "\n",
    "    # Calculate average scaling\n",
    "    avg_bytesread_increase = df[\"bytesread_ratio\"].mean()\n",
    "    avg_task_bytes_increase = df[\"task_bytes_ratio\"].mean()\n",
    "\n",
    "    print(f\"2. Average bytesread scaling factor: {avg_bytesread_increase:.2f}x\")\n",
    "    print(f\"   Average task output scaling factor: {avg_task_bytes_increase:.2f}x\")\n",
    "    print()\n",
    "\n",
    "    # Check if they scale similarly\n",
    "    ratio_difference = abs(df[\"bytesread_ratio\"] - df[\"task_bytes_ratio\"]).mean()\n",
    "\n",
    "    print(f\"3. Mean difference in scaling ratios: {ratio_difference:.3f}\")\n",
    "    if ratio_difference < 0.2:\n",
    "        print(\"   → TaskState output scales similarly to Coffea bytesread ✅\")\n",
    "    else:\n",
    "        print(\"   → TaskState output scales differently from Coffea bytesread ⚠️\")\n",
    "    print()\n",
    "\n",
    "    print(\"4. Interpretation:\")\n",
    "    print(\"   - TaskState.nbytes measures OUTPUT size (pickled result)\")\n",
    "    print(\"   - Coffea bytesread measures INPUT size (compressed ROOT data)\")\n",
    "    if correlation > 0.8:\n",
    "        print(\"   - Strong correlation suggests task output scales with input data\")\n",
    "        print(\"   - TaskState could be useful as a PROXY for relative chunk sizes\")\n",
    "    else:\n",
    "        print(\"   - Weak correlation suggests task output doesn't directly track input\")\n",
    "        print(\"   - TaskState may not be reliable for chunk size estimation\")\n",
    "    print()\n",
    "\n",
    "    print(\"5. Limitations:\")\n",
    "    print(\"   - Absolute values differ greatly (KB vs MB)\")\n",
    "    print(\"   - Cannot calculate actual bytes per chunk from TaskState alone\")\n",
    "    print(\"   - Still need decorator approach for true per-chunk input tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()\n",
    "print(\"Cluster closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
