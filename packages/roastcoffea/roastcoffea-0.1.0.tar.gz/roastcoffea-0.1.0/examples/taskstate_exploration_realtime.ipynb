{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time TaskState Exploration\n",
    "\n",
    "This notebook captures Dask TaskState data **during execution** before tasks are cleaned up.\n",
    "\n",
    "## Approach\n",
    "\n",
    "1. Create a background thread that periodically snapshots scheduler state\n",
    "2. Run Coffea processing\n",
    "3. Analyze captured task data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import threading\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import awkward as ak\n",
    "import pandas as pd\n",
    "import skhep_testdata\n",
    "from coffea import processor\n",
    "from coffea.nanoevents import NanoAODSchema\n",
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Task Monitoring Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskMonitor:\n",
    "    \"\"\"Monitor Dask scheduler tasks in real-time.\"\"\"\n",
    "\n",
    "    def __init__(self, scheduler, interval=0.1):\n",
    "        self.scheduler = scheduler\n",
    "        self.interval = interval\n",
    "        self.snapshots = []\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start monitoring.\"\"\"\n",
    "        self.running = True\n",
    "        self.thread = threading.Thread(target=self._monitor_loop, daemon=True)\n",
    "        self.thread.start()\n",
    "        print(f\"Started task monitoring (interval={self.interval}s)\")\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop monitoring.\"\"\"\n",
    "        self.running = False\n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=5)\n",
    "        print(f\"Stopped monitoring. Captured {len(self.snapshots)} snapshots\")\n",
    "\n",
    "    def _monitor_loop(self):\n",
    "        \"\"\"Background monitoring loop.\"\"\"\n",
    "        while self.running:\n",
    "            try:\n",
    "                snapshot = self._capture_snapshot()\n",
    "                if snapshot[\"tasks\"]:\n",
    "                    self.snapshots.append(snapshot)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in monitoring: {e}\")\n",
    "\n",
    "            time.sleep(self.interval)\n",
    "\n",
    "    def _capture_snapshot(self):\n",
    "        \"\"\"Capture current scheduler state.\"\"\"\n",
    "        timestamp = datetime.datetime.now()\n",
    "\n",
    "        tasks = []\n",
    "        for task_key, task_state in self.scheduler.tasks.items():\n",
    "            # Get worker info\n",
    "            worker = None\n",
    "            if hasattr(task_state, \"who_has\") and task_state.who_has:\n",
    "                worker = (\n",
    "                    list(task_state.who_has)[0].address if task_state.who_has else None\n",
    "                )\n",
    "\n",
    "            tasks.append(\n",
    "                {\n",
    "                    \"key\": task_key,\n",
    "                    \"state\": task_state.state,\n",
    "                    \"worker\": worker,\n",
    "                    \"nbytes\": task_state.nbytes if hasattr(task_state, \"nbytes\") else 0,\n",
    "                    \"type\": task_state.type if hasattr(task_state, \"type\") else None,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"num_tasks\": len(tasks),\n",
    "            \"num_workers\": len(self.scheduler.workers),\n",
    "            \"tasks\": tasks,\n",
    "        }\n",
    "\n",
    "    def get_all_tasks(self):\n",
    "        \"\"\"Get all unique tasks seen across all snapshots.\"\"\"\n",
    "        all_tasks = {}\n",
    "\n",
    "        for snapshot in self.snapshots:\n",
    "            for task in snapshot[\"tasks\"]:\n",
    "                key = task[\"key\"]\n",
    "                # Keep the most complete version of each task\n",
    "                if key not in all_tasks or task[\"nbytes\"] > all_tasks[key][\"nbytes\"]:\n",
    "                    all_tasks[key] = task\n",
    "\n",
    "        return list(all_tasks.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Simple Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleProcessor(processor.ProcessorABC):\n",
    "    \"\"\"Simple processor for testing.\"\"\"\n",
    "\n",
    "    def process(self, events):\n",
    "        # Do some computation\n",
    "        jets = events.Jet[events.Jet.pt > 30]\n",
    "\n",
    "        return {\n",
    "            \"nevents\": len(events),\n",
    "            \"njets\": ak.sum(ak.num(jets)),\n",
    "            \"mean_pt\": ak.mean(jets.pt) if len(jets) > 0 else 0,\n",
    "            \"mean_eta\": ak.mean(jets.eta) if len(jets) > 0 else 0,\n",
    "        }\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Cluster and Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moaly/Work/iris-hep/roastcoffea/.pixi/envs/dev/lib/python3.13/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 61828 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard: http://127.0.0.1:61828/status\n",
      "Workers: 4\n",
      "Started task monitoring (interval=0.01s)\n"
     ]
    }
   ],
   "source": [
    "# Start cluster\n",
    "cluster = LocalCluster(n_workers=4, threads_per_worker=1, processes=True)\n",
    "client = Client(cluster)\n",
    "\n",
    "print(f\"Dashboard: {client.dashboard_link}\")\n",
    "print(f\"Workers: {len(client.scheduler_info()['workers'])}\")\n",
    "\n",
    "# Start monitoring\n",
    "monitor = TaskMonitor(client.cluster.scheduler, interval=0.01)\n",
    "monitor.start()\n",
    "\n",
    "# Give it a moment to start\n",
    "time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Coffea Processing (While Monitoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f8c4254e554fe58115c7aed73359e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Coffea processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moaly/Work/iris-hep/roastcoffea/.pixi/envs/dev/lib/python3.13/site-packages/coffea/nanoevents/schemas/nanoaod.py:264: RuntimeWarning: Missing cross-reference index for LowPtElectron_electronIdx => Electron\n",
      "  warnings.warn(\n",
      "/Users/moaly/Work/iris-hep/roastcoffea/.pixi/envs/dev/lib/python3.13/site-packages/coffea/nanoevents/schemas/nanoaod.py:264: RuntimeWarning: Missing cross-reference index for LowPtElectron_genPartIdx => GenPart\n",
      "  warnings.warn(\n",
      "/Users/moaly/Work/iris-hep/roastcoffea/.pixi/envs/dev/lib/python3.13/site-packages/coffea/nanoevents/schemas/nanoaod.py:264: RuntimeWarning: Missing cross-reference index for LowPtElectron_photonIdx => Photon\n",
      "  warnings.warn(\n",
      "/Users/moaly/Work/iris-hep/roastcoffea/.pixi/envs/dev/lib/python3.13/site-packages/coffea/nanoevents/schemas/nanoaod.py:264: RuntimeWarning: Missing cross-reference index for FatJet_subJetIdx1 => SubJet\n",
      "  warnings.warn(\n",
      "/Users/moaly/Work/iris-hep/roastcoffea/.pixi/envs/dev/lib/python3.13/site-packages/coffea/nanoevents/schemas/nanoaod.py:264: RuntimeWarning: Missing cross-reference index for FatJet_subJetIdx2 => SubJet\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 200 events in 1 chunks\n",
      "Total bytes read: 0.34 MB\n",
      "Stopped monitoring. Captured 67 snapshots\n"
     ]
    }
   ],
   "source": [
    "# Get test file\n",
    "test_file = skhep_testdata.data_path(\"nanoAOD_2015_CMS_Open_Data_ttbar.root\")\n",
    "\n",
    "# Create fileset\n",
    "fileset = {\n",
    "    \"ttbar\": {\n",
    "        \"files\": {test_file: \"Events\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Run processor\n",
    "proc = SimpleProcessor()\n",
    "executor = processor.DaskExecutor(client=client)\n",
    "runner = processor.Runner(\n",
    "    executor=executor,\n",
    "    savemetrics=True,\n",
    "    schema=NanoAODSchema,\n",
    ")\n",
    "\n",
    "print(\"Starting Coffea processing...\")\n",
    "output, report = runner(\n",
    "    fileset,\n",
    "    treename=\"Events\",\n",
    "    processor_instance=proc,\n",
    ")\n",
    "\n",
    "print(f\"\\nProcessed {report['entries']} events in {report['chunks']} chunks\")\n",
    "print(f\"Total bytes read: {report['bytesread'] / 1e6:.2f} MB\")\n",
    "\n",
    "# Wait a bit for final tasks to be captured\n",
    "time.sleep(1)\n",
    "\n",
    "# Stop monitoring\n",
    "monitor.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Captured Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Monitoring Summary ===\n",
      "Total snapshots: 67\n",
      "Duration: 0.76s\n",
      "\n",
      "Task count over time:\n",
      "  0: 2 tasks at 14:11:00.506\n",
      "  1: 2 tasks at 14:11:00.578\n",
      "  2: 2 tasks at 14:11:00.646\n",
      "  3: 2 tasks at 14:11:00.714\n",
      "  4: 2 tasks at 14:11:00.783\n",
      "  5: 2 tasks at 14:11:00.850\n",
      "  6: 2 tasks at 14:11:00.917\n",
      "  7: 2 tasks at 14:11:00.984\n",
      "  8: 2 tasks at 14:11:01.054\n",
      "  9: 2 tasks at 14:11:01.126\n",
      "  10: 2 tasks at 14:11:01.195\n",
      "  11: 2 tasks at 14:11:01.265\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Monitoring Summary ===\")\n",
    "print(f\"Total snapshots: {len(monitor.snapshots)}\")\n",
    "print(\n",
    "    f\"Duration: {(monitor.snapshots[-1]['timestamp'] - monitor.snapshots[0]['timestamp']).total_seconds():.2f}s\"\n",
    ")\n",
    "\n",
    "# Show task counts over time\n",
    "print(\"\\nTask count over time:\")\n",
    "for i, snapshot in enumerate(\n",
    "    monitor.snapshots[:: max(1, len(monitor.snapshots) // 10)]\n",
    "):\n",
    "    print(\n",
    "        f\"  {i}: {snapshot['num_tasks']} tasks at {snapshot['timestamp'].strftime('%H:%M:%S.%f')[:-3]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract All Unique Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total unique tasks captured: 2 ===\n",
      "Processor-related tasks: 2\n",
      "\n",
      "=== Sample Task Details ===\n",
      "\n",
      "Task 1:\n",
      "  Key: lambda-479af8013eef8fd3a0f3694766170521\n",
      "  State: memory\n",
      "  Worker: tcp://127.0.0.1:61841\n",
      "  Size: 1.20 KB\n",
      "  Type: bytes\n",
      "\n",
      "Task 2:\n",
      "  Key: SimpleProcessor-6cc9379618d35fa267d3ddfd2223c89b\n",
      "  State: memory\n",
      "  Worker: tcp://127.0.0.1:61841\n",
      "  Size: 0.71 KB\n",
      "  Type: bytes\n"
     ]
    }
   ],
   "source": [
    "all_tasks = monitor.get_all_tasks()\n",
    "print(f\"=== Total unique tasks captured: {len(all_tasks)} ===\")\n",
    "\n",
    "# Filter for processor-related tasks\n",
    "processor_tasks = [\n",
    "    task\n",
    "    for task in all_tasks\n",
    "    # if 'SimpleProcessor' in str(task['key']) or 'process' in str(task['key']).lower()\n",
    "]\n",
    "\n",
    "print(f\"Processor-related tasks: {len(processor_tasks)}\")\n",
    "\n",
    "# Show sample tasks\n",
    "print(\"\\n=== Sample Task Details ===\")\n",
    "for i, task in enumerate(processor_tasks):\n",
    "    print(f\"\\nTask {i + 1}:\")\n",
    "    print(f\"  Key: {task['key']}\")\n",
    "    print(f\"  State: {task['state']}\")\n",
    "    print(f\"  Worker: {task['worker']}\")\n",
    "    print(f\"  Size: {task['nbytes'] / 1e3:.2f} KB\" if task[\"nbytes\"] else \"  Size: N/A\")\n",
    "    print(f\"  Type: {task['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Key Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Task Key Structure ===\n",
      "\n",
      "1. lambda-479af8013eef8fd3a0f3694766170521\n",
      "   Type: <class 'str'>\n",
      "\n",
      "2. SimpleProcessor-6cc9379618d35fa267d3ddfd2223c89b\n",
      "   Type: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Task Key Structure ===\")\n",
    "\n",
    "for i, task in enumerate(processor_tasks[:10]):\n",
    "    key = task[\"key\"]\n",
    "    print(f\"\\n{i + 1}. {key}\")\n",
    "    print(f\"   Type: {type(key)}\")\n",
    "\n",
    "    if isinstance(key, tuple):\n",
    "        print(f\"   Length: {len(key)}\")\n",
    "        for j, elem in enumerate(key):\n",
    "            print(f\"   [{j}]: {type(elem).__name__} = {elem}\")\n",
    "\n",
    "            # Check for file/dataset info\n",
    "            if isinstance(elem, str) and any(\n",
    "                x in elem for x in [\"ttbar\", \"root\", \"nanoAOD\", \"file\"]\n",
    "            ):\n",
    "                print(\"       ^ Contains file/dataset info!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-Worker Task Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Per-Worker Distribution ===\n",
      "\n",
      "Worker: tcp://127.0.0.1:61841\n",
      "  Tasks: 2\n",
      "  Total bytes: 0.00 MB\n",
      "  Avg bytes/task: 0.95 KB\n",
      "  Sample tasks:\n",
      "    lambda-479af8013eef8fd3a0f3694766170521... : 1.2 KB\n",
      "    SimpleProcessor-6cc9379618d35fa267d3ddfd2223c89b... : 0.7 KB\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Per-Worker Distribution ===\")\n",
    "\n",
    "worker_stats = defaultdict(lambda: {\"count\": 0, \"total_bytes\": 0, \"tasks\": []})\n",
    "\n",
    "for task in processor_tasks:\n",
    "    worker = task[\"worker\"] or \"unknown\"\n",
    "    worker_stats[worker][\"count\"] += 1\n",
    "    worker_stats[worker][\"total_bytes\"] += task[\"nbytes\"] if task[\"nbytes\"] else 0\n",
    "    worker_stats[worker][\"tasks\"].append(task)\n",
    "\n",
    "for worker, stats in worker_stats.items():\n",
    "    print(f\"\\nWorker: {worker}\")\n",
    "    print(f\"  Tasks: {stats['count']}\")\n",
    "    print(f\"  Total bytes: {stats['total_bytes'] / 1e6:.2f} MB\")\n",
    "    print(\n",
    "        f\"  Avg bytes/task: {stats['total_bytes'] / stats['count'] / 1e3:.2f} KB\"\n",
    "        if stats[\"count\"] > 0\n",
    "        else \"  Avg: N/A\"\n",
    "    )\n",
    "\n",
    "    # Show sample task keys\n",
    "    print(\"  Sample tasks:\")\n",
    "    for task in stats[\"tasks\"][:3]:\n",
    "        size = f\"{task['nbytes'] / 1e3:.1f} KB\" if task[\"nbytes\"] else \"N/A\"\n",
    "        print(f\"    {str(task['key'])[:60]}... : {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Task DataFrame ===\n",
      "                                                key                 worker  \\\n",
      "0  SimpleProcessor-6cc9379618d35fa267d3ddfd2223c89b  tcp://127.0.0.1:61763   \n",
      "\n",
      "    state  nbytes_kb   type  \n",
      "0  memory      0.707  bytes  \n",
      "\n",
      "=== Size Statistics ===\n",
      "       nbytes_kb\n",
      "count      1.000\n",
      "mean       0.707\n",
      "std          NaN\n",
      "min        0.707\n",
      "25%        0.707\n",
      "50%        0.707\n",
      "75%        0.707\n",
      "max        0.707\n",
      "\n",
      "=== Per-Worker Summary ===\n",
      "                       count    sum   mean  std\n",
      "worker                                         \n",
      "tcp://127.0.0.1:61763      1  0.707  0.707  NaN\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"key\": str(task[\"key\"])[:60] + \"...\"\n",
    "            if len(str(task[\"key\"])) > 60\n",
    "            else str(task[\"key\"]),\n",
    "            \"worker\": task[\"worker\"],\n",
    "            \"state\": task[\"state\"],\n",
    "            \"nbytes_kb\": task[\"nbytes\"] / 1e3 if task[\"nbytes\"] else 0,\n",
    "            \"type\": task[\"type\"],\n",
    "        }\n",
    "        for task in processor_tasks\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=== Task DataFrame ===\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(\"\\n=== Size Statistics ===\")\n",
    "print(df[[\"nbytes_kb\"]].describe())\n",
    "\n",
    "print(\"\\n=== Per-Worker Summary ===\")\n",
    "if len(df) > 0:\n",
    "    print(df.groupby(\"worker\")[\"nbytes_kb\"].agg([\"count\", \"sum\", \"mean\", \"std\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Coffea Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparison with Coffea Report ===\n",
      "\n",
      "Coffea report:\n",
      "  Chunks: 1\n",
      "  Bytes read: 0.34 MB\n",
      "  Events: 200\n",
      "\n",
      "TaskState captured:\n",
      "  Processor tasks: 1\n",
      "  Total task result sizes: 0.00 MB\n",
      "\n",
      "Note: TaskState.nbytes is OUTPUT size (result), not INPUT bytes read!\n",
      "That's why task sizes (0.00 MB) differ from bytesread (0.34 MB)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Comparison with Coffea Report ===\")\n",
    "print(\"\\nCoffea report:\")\n",
    "print(f\"  Chunks: {report['chunks']}\")\n",
    "print(f\"  Bytes read: {report['bytesread'] / 1e6:.2f} MB\")\n",
    "print(f\"  Events: {report['entries']}\")\n",
    "\n",
    "print(\"\\nTaskState captured:\")\n",
    "print(f\"  Processor tasks: {len(processor_tasks)}\")\n",
    "total_task_bytes = sum(task[\"nbytes\"] for task in processor_tasks if task[\"nbytes\"])\n",
    "print(f\"  Total task result sizes: {total_task_bytes / 1e6:.2f} MB\")\n",
    "\n",
    "print(\"\\nNote: TaskState.nbytes is OUTPUT size (result), not INPUT bytes read!\")\n",
    "print(\n",
    "    f\"That's why task sizes ({total_task_bytes / 1e6:.2f} MB) differ from bytesread ({report['bytesread'] / 1e6:.2f} MB)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "### What We Successfully Captured:\n",
    "\n",
    "1. ✅ **Task result sizes** - `TaskState.nbytes` for each task\n",
    "2. ✅ **Worker attribution** - Which worker processed which task\n",
    "3. ✅ **Task counts per worker** - Load distribution\n",
    "4. ✅ **Task states** - Captured tasks during execution\n",
    "\n",
    "### Critical Limitations:\n",
    "\n",
    "1. ❌ **Input bytes** - Only output/result size, not bytes read from file\n",
    "2. ❌ **Chunk identification** - Task keys are opaque, no file/dataset info\n",
    "3. ❌ **Event counts** - Not available in TaskState\n",
    "4. ⚠️ **Overhead** - Monitoring every 0.1s adds overhead\n",
    "\n",
    "### Can We Use This for Bytes Per Chunk?\n",
    "\n",
    "**Partially, but with major caveats:**\n",
    "\n",
    "- We get **output bytes per task**, not input bytes read\n",
    "- We can see **per-worker distribution** of task sizes\n",
    "- We **cannot** map tasks to files/datasets without additional tracking\n",
    "- We **cannot** calculate bytes per event (no event counts)\n",
    "- We **cannot** calculate throughput per chunk (no input bytes)\n",
    "\n",
    "### Recommendation:\n",
    "\n",
    "**TaskState tracking provides partial data but is insufficient for your requirements.**\n",
    "\n",
    "For complete per-chunk tracking with:\n",
    "- Bytes per chunk (input)\n",
    "- Worker attribution\n",
    "- Bytes per event\n",
    "- Throughput per chunk\n",
    "\n",
    "**You need the `@track_metrics` decorator approach** that captures:\n",
    "- Input metadata (file, dataset, events)\n",
    "- Processing time\n",
    "- Worker ID\n",
    "- Memory/size estimates\n",
    "\n",
    "TaskState could supplement this as automatic fallback, but cannot replace it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()\n",
    "print(\"Cluster closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
