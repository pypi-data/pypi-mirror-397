{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Metrics Demo: Complete Guide to Dask Spans Integration\n",
    "\n",
    "This notebook demonstrates all fine-grained performance metrics available through Dask Spans in roastcoffea.\n",
    "\n",
    "## What are Fine Metrics?\n",
    "\n",
    "Fine metrics provide detailed performance breakdowns beyond wall time:\n",
    "- **CPU vs Non-CPU time**: How much time spent computing vs waiting/coordination?\n",
    "- **Processor vs Overhead**: Separate your processor work from Dask coordination overhead\n",
    "- **Compression overhead**: Time spent compressing/decompressing data\n",
    "- **Serialization overhead**: Time spent serializing/deserializing Python objects\n",
    "- **Disk/Memory I/O**: Bytes read from disk or memory\n",
    "- **Real compression ratios**: Actual uncompressed vs compressed bytes\n",
    "\n",
    "## Important: Processor Instance Parameter\n",
    "\n",
    "To get accurate metrics separating your processor work from Dask overhead, **always pass `processor_instance`** to `MetricsCollector`:\n",
    "\n",
    "```python\n",
    "processor = MyProcessor()\n",
    "with MetricsCollector(client, processor_instance=processor) as collector:\n",
    "    output, report = runner(fileset, processor_instance=processor)\n",
    "```\n",
    "\n",
    "Without it, all metrics (including Dask coordination overhead) are aggregated together.\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "Fine metrics come from **Dask Spans** and are available at multiple granularities:\n",
    "1. **Processor-level**: Your actual processor work (when processor_instance provided)\n",
    "2. **Overhead-level**: Dask coordination tasks (lambdas, etc.)\n",
    "3. **Per-task**: Broken down by individual tasks (task prefix)\n",
    "4. **Per-worker**: (Future) Separated by individual workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using test file: /Users/moaly/.local/skhepdata/nanoAOD_2015_CMS_Open_Data_ttbar.root\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "\n",
    "import awkward as ak\n",
    "\n",
    "# Get a test file from scikit-hep-testdata\n",
    "import skhep_testdata\n",
    "from coffea import processor\n",
    "from coffea.nanoevents import NanoAODSchema\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# roastcoffea imports\n",
    "from roastcoffea import (\n",
    "    MetricsCollector,\n",
    "    plot_per_task_bytes_read,\n",
    "    # Per-task visualization\n",
    "    plot_per_task_cpu_io,\n",
    "    plot_per_task_overhead,\n",
    ")\n",
    "\n",
    "test_file = skhep_testdata.data_path(\"nanoAOD_2015_CMS_Open_Data_ttbar.root\")\n",
    "print(f\"Using test file: {test_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Simple Coffea Processor\n",
    "\n",
    "We'll create a processor that does some real work: filtering jets, computing masses, and creating histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JetAnalysisProcessor(processor.ProcessorABC):\n",
    "    \"\"\"Simple jet analysis processor for testing fine metrics.\"\"\"\n",
    "\n",
    "    def process(self, events):\n",
    "        # Select jets with pT > 30 GeV\n",
    "        jets = events.Jet[events.Jet.pt > 30]\n",
    "\n",
    "        # Select events with at least 2 jets\n",
    "        two_jet_events = events[ak.num(jets) >= 2]\n",
    "        two_jets = jets[ak.num(jets) >= 2]\n",
    "\n",
    "        # Calculate dijet invariant mass for leading two jets\n",
    "        if len(two_jets) > 0:\n",
    "            j1 = two_jets[:, 0]\n",
    "            j2 = two_jets[:, 1]\n",
    "            dijet_mass = (j1 + j2).mass\n",
    "        else:\n",
    "            dijet_mass = ak.Array([])\n",
    "\n",
    "        return {\n",
    "            \"nevents\": len(events),\n",
    "            \"njets_total\": ak.sum(ak.num(jets)),\n",
    "            \"nevents_2jet\": len(two_jet_events),\n",
    "            \"dijet_mass_mean\": ak.mean(dijet_mass) if len(dijet_mass) > 0 else 0,\n",
    "            \"jet_pt_sum\": ak.sum(jets.pt),\n",
    "        }\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Collecting Fine Metrics\n",
    "\n",
    "Let's run the processor with metrics collection enabled. Fine metrics are collected automatically when Dask Spans are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moaly/Work/iris-hep/roastcoffea/.pixi/envs/dev/lib/python3.13/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 65146 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard: http://127.0.0.1:65146/status\n"
     ]
    }
   ],
   "source": [
    "# Create fileset\n",
    "fileset = {\n",
    "    \"DY\": {\n",
    "        \"files\": {test_file: \"Events\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Start Dask cluster\n",
    "cluster = LocalCluster(n_workers=2, threads_per_worker=1, processes=True)\n",
    "client = Client(cluster)\n",
    "\n",
    "print(f\"Dashboard: {client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dask Span completed but no metrics were collected. Fine-grained metrics may not be available.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'JetAnalysisProcessor' object has no attribute 'DaskExecutor'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m processor = JetAnalysisProcessor()\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m MetricsCollector(client, track_workers=\u001b[38;5;28;01mTrue\u001b[39;00m, processor_instance=processor) \u001b[38;5;28;01mas\u001b[39;00m collector:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     executor = \u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDaskExecutor\u001b[49m(client=client)\n\u001b[32m      5\u001b[39m     runner = processor.Runner(\n\u001b[32m      6\u001b[39m         executor=executor,\n\u001b[32m      7\u001b[39m         savemetrics=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      8\u001b[39m         schema=NanoAODSchema,\n\u001b[32m      9\u001b[39m     )\n\u001b[32m     11\u001b[39m     output, report = runner(\n\u001b[32m     12\u001b[39m         fileset,\n\u001b[32m     13\u001b[39m         treename=\u001b[33m\"\u001b[39m\u001b[33mEvents\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m         processor_instance=processor,\n\u001b[32m     15\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: 'JetAnalysisProcessor' object has no attribute 'DaskExecutor'"
     ]
    }
   ],
   "source": [
    "# Run with metrics collection - RECOMMENDED approach\n",
    "instance = JetAnalysisProcessor()\n",
    "with MetricsCollector(\n",
    "    client, track_workers=True, processor_instance=instance\n",
    ") as collector:\n",
    "    executor = instance.DaskExecutor(client=client)\n",
    "    runner = instance.Runner(\n",
    "        executor=executor,\n",
    "        savemetrics=True,\n",
    "        schema=NanoAODSchema,\n",
    "    )\n",
    "\n",
    "    output, report = runner(\n",
    "        fileset,\n",
    "        treename=\"Events\",\n",
    "        processor_instance=instance,\n",
    "    )\n",
    "\n",
    "    # Provide report to collector\n",
    "    collector.set_coffea_report(report)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Analysis complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Viewing Cumulative Fine Metrics\n",
    "\n",
    "Let's see the workflow-level metrics aggregated across all tasks and workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                        Throughput Metrics                        </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Metric            </span>┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Value                                      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Data Rate         </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 0.00 Gbps (0.4 MB/s)                       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Compression Ratio </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 0.00x                                      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Total Data Read   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 328.73 KB compressed, 1.51 KB uncompressed </span>│\n",
       "└───────────────────┴────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                        Throughput Metrics                        \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1;36m \u001b[0m\u001b[1;36mMetric           \u001b[0m\u001b[1;36m \u001b[0m┃\u001b[1;36m \u001b[0m\u001b[1;36mValue                                     \u001b[0m\u001b[1;36m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mData Rate        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m0.00 Gbps (0.4 MB/s)                      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mCompression Ratio\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m0.00x                                     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTotal Data Read  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m328.73 KB compressed, 1.51 KB uncompressed\u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────┴────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">           Event Processing Metrics           </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Metric                     </span>┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Value         </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Total Events               </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 200           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Event Rate (Wall Clock)    </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 0.2 kHz       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Event Rate (Aggregated)    </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 5.0 kHz       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Event Rate (Core-Averaged) </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 120.0 Hz/core </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Efficiency Ratio           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 4.8%          </span>│\n",
       "└────────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m           Event Processing Metrics           \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1;36m \u001b[0m\u001b[1;36mMetric                    \u001b[0m\u001b[1;36m \u001b[0m┃\u001b[1;36m \u001b[0m\u001b[1;36mValue        \u001b[0m\u001b[1;36m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mTotal Events              \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m200          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mEvent Rate (Wall Clock)   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m0.2 kHz      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mEvent Rate (Aggregated)   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m5.0 kHz      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mEvent Rate (Core-Averaged)\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m120.0 Hz/core\u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mEfficiency Ratio          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m4.8%         \u001b[0m\u001b[35m \u001b[0m│\n",
       "└────────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">          Resource Utilization          </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n",
       "┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Metric                   </span>┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Value     </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Workers (Time-Averaged)  </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 2.0       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Peak Workers             </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 2         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Cores per Worker         </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 1.0       </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Total Cores              </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 2         </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Core Efficiency          </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 2.4%      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Speedup Factor           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 0.0x      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Peak Memory (per worker) </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 216.58 MB </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Avg Memory (per worker)  </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 117.72 MB </span>│\n",
       "└──────────────────────────┴───────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m          Resource Utilization          \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n",
       "┃\u001b[1;36m \u001b[0m\u001b[1;36mMetric                  \u001b[0m\u001b[1;36m \u001b[0m┃\u001b[1;36m \u001b[0m\u001b[1;36mValue    \u001b[0m\u001b[1;36m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mWorkers (Time-Averaged) \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m2.0      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mPeak Workers            \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m2        \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mCores per Worker        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m1.0      \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTotal Cores             \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m2        \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mCore Efficiency         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m2.4%     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mSpeedup Factor          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m0.0x     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mPeak Memory (per worker)\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m216.58 MB\u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mAvg Memory (per worker) \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m117.72 MB\u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────────────────────┴───────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">       Timing Breakdown       </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Metric             </span>┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Value </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Wall Time          </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 0.8s  </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Total CPU Time     </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 0.0s  </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Number of Chunks   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 1     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Avg CPU Time/Chunk </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 0.0s  </span>│\n",
       "└────────────────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m       Timing Breakdown       \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;36m \u001b[0m\u001b[1;36mMetric            \u001b[0m\u001b[1;36m \u001b[0m┃\u001b[1;36m \u001b[0m\u001b[1;36mValue\u001b[0m\u001b[1;36m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mWall Time         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m0.8s \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTotal CPU Time    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m0.0s \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mNumber of Chunks  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m1    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mAvg CPU Time/Chunk\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m0.0s \u001b[0m\u001b[35m \u001b[0m│\n",
       "└────────────────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\"> Fine Metrics (from </span>\n",
       "<span style=\"font-style: italic\">    Dask Spans)     </span>\n",
       "┏━━━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Metric   </span>┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Value </span>┃\n",
       "┡━━━━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> CPU Time </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 0.2s  </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> I/O Time </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 0.0s  </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> CPU %    </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 93.3% </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> I/O %    </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 6.7%  </span>│\n",
       "└──────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m Fine Metrics (from \u001b[0m\n",
       "\u001b[3m    Dask Spans)     \u001b[0m\n",
       "┏━━━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;36m \u001b[0m\u001b[1;36mMetric  \u001b[0m\u001b[1;36m \u001b[0m┃\u001b[1;36m \u001b[0m\u001b[1;36mValue\u001b[0m\u001b[1;36m \u001b[0m┃\n",
       "┡━━━━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mCPU Time\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m0.2s \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mI/O Time\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m0.0s \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mCPU %   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m93.3%\u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mI/O %   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m6.7% \u001b[0m\u001b[35m \u001b[0m│\n",
       "└──────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print comprehensive summary (includes fine metrics table)\n",
    "collector.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access fine metrics programmatically\n",
    "metrics = collector.get_metrics()\n",
    "\n",
    "print(\"\\n=== Key Fine Metrics ===\")\n",
    "if metrics.get(\"processor_cpu_time_seconds\") is not None:\n",
    "    print(f\"Processor CPU Time: {metrics['processor_cpu_time_seconds']:.2f} seconds\")\n",
    "    print(\n",
    "        f\"Processor Non-CPU Time: {metrics['processor_noncpu_time_seconds']:.2f} seconds\"\n",
    "    )\n",
    "    print(f\"Processor CPU %: {metrics['processor_cpu_percentage']:.1f}%\")\n",
    "    print(f\"Processor Non-CPU %: {metrics['processor_noncpu_percentage']:.1f}%\")\n",
    "    print()\n",
    "\n",
    "    # Show overhead metrics if available\n",
    "    overhead_cpu = metrics.get(\"overhead_cpu_time_seconds\", 0)\n",
    "    overhead_noncpu = metrics.get(\"overhead_noncpu_time_seconds\", 0)\n",
    "    if overhead_cpu > 0 or overhead_noncpu > 0:\n",
    "        print(\"=== Dask Overhead ===\")\n",
    "        print(f\"Overhead CPU Time: {overhead_cpu:.2f} seconds\")\n",
    "        print(f\"Overhead Non-CPU Time: {overhead_noncpu:.2f} seconds\")\n",
    "        print()\n",
    "\n",
    "    print(f\"Disk Read: {metrics.get('disk_read_bytes', 0) / 1e9:.2f} GB\")\n",
    "    print(f\"Disk Write: {metrics.get('disk_write_bytes', 0) / 1e9:.2f} GB\")\n",
    "    print()\n",
    "\n",
    "    compression_overhead = metrics.get(\"total_compression_overhead_seconds\", 0)\n",
    "    if compression_overhead > 0:\n",
    "        print(f\"Compression Overhead: {compression_overhead:.2f} seconds\")\n",
    "        print(f\"  - Decompress: {metrics.get('decompression_time_seconds', 0):.2f}s\")\n",
    "        print(f\"  - Compress: {metrics.get('compression_time_seconds', 0):.2f}s\")\n",
    "        print()\n",
    "\n",
    "    serialization_overhead = metrics.get(\"total_serialization_overhead_seconds\", 0)\n",
    "    if serialization_overhead > 0:\n",
    "        print(f\"Serialization Overhead: {serialization_overhead:.2f} seconds\")\n",
    "        print(f\"  - Deserialize: {metrics.get('deserialization_time_seconds', 0):.2f}s\")\n",
    "        print(f\"  - Serialize: {metrics.get('serialization_time_seconds', 0):.2f}s\")\n",
    "        print()\n",
    "\n",
    "    if (\n",
    "        metrics.get(\"compression_ratio\") is not None\n",
    "        and metrics[\"compression_ratio\"] > 0\n",
    "    ):\n",
    "        print(f\"Compression Ratio: {metrics['compression_ratio']:.2f}x\")\n",
    "        print(\n",
    "            f\"Uncompressed Bytes: {metrics.get('total_bytes_uncompressed', 0) / 1e9:.2f} GB\"\n",
    "        )\n",
    "else:\n",
    "    print(\"Fine metrics not available (Dask Spans may not be enabled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data Structure\n",
    "\n",
    "Let's look at the raw Span metrics to understand the data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Raw Span Metrics Structure ===\n",
      "Total keys: 11\n",
      "\n",
      "Sample keys (first 10):\n",
      "  ('execute', 'lambda', 'thread-cpu', 'seconds'): 1.7542000000000002e-05\n",
      "  ('execute', 'lambda', 'thread-noncpu', 'seconds'): 1.4644508178710935e-05\n",
      "  ('execute', 'lambda', 'executor', 'seconds'): 0.00034168828278779984\n",
      "  ('execute', 'lambda', 'other', 'seconds'): 0.000291542150080204\n",
      "  ('execute', 'JetAnalysisProcessor', 'memory-read', 'count'): 1\n",
      "  ('execute', 'JetAnalysisProcessor', 'memory-read', 'bytes'): 1546\n",
      "  ('execute', 'JetAnalysisProcessor', 'thread-cpu', 'seconds'): 0.234355125\n",
      "  ('execute', 'JetAnalysisProcessor', 'thread-noncpu', 'seconds'): 0.01683911366271973\n",
      "  ('execute', 'JetAnalysisProcessor', 'executor', 'seconds'): 0.0009408453479409218\n",
      "  ('execute', 'JetAnalysisProcessor', 'other', 'seconds'): 0.00030058296397328377\n",
      "\n",
      "=== Key Format Explanation ===\n",
      "Keys are tuples: (context, task_prefix, activity, unit)\n",
      "  - context: 'execute' for task execution\n",
      "  - task_prefix: Task identifier (e.g., 'process-abc123')\n",
      "  - activity: 'thread-cpu', 'thread-noncpu', 'disk-read', etc.\n",
      "  - unit: 'seconds' for time, 'bytes' for data size\n"
     ]
    }
   ],
   "source": [
    "# Access raw span metrics\n",
    "span_metrics = collector.span_metrics\n",
    "\n",
    "if span_metrics:\n",
    "    print(\"=== Raw Span Metrics Structure ===\")\n",
    "    print(f\"Total keys: {len(span_metrics)}\")\n",
    "    print(\"\\nSample keys (first 10):\")\n",
    "    for i, (key, value) in enumerate(list(span_metrics.items())[:10]):\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    print(\"\\n=== Key Format Explanation ===\")\n",
    "    print(\"Keys are tuples: (context, task_prefix, activity, unit)\")\n",
    "    print(\"  - context: 'execute' for task execution\")\n",
    "    print(\"  - task_prefix: Task identifier (e.g., 'process-abc123')\")\n",
    "    print(\"  - activity: 'thread-cpu', 'thread-noncpu', 'disk-read', etc.\")\n",
    "    print(\"  - unit: 'seconds' for time, 'bytes' for data size\")\n",
    "else:\n",
    "    print(\"No span metrics collected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Per-Task Analysis\n",
    "\n",
    "Fine metrics are broken down by task prefix, allowing us to see which tasks are CPU-bound vs waiting/coordination-bound.\n",
    "\n",
    "**Note:** \"Non-CPU time\" includes waiting time, GIL contention, coordination overhead, and other non-compute activities - not just I/O operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract per-task breakdown\n",
    "from roastcoffea.visualization.plots.per_task import extract_per_task_metrics\n",
    "\n",
    "if span_metrics:\n",
    "    per_task = extract_per_task_metrics(span_metrics)\n",
    "\n",
    "    print(\"=== Per-Task Metrics ===\")\n",
    "    print(f\"Number of unique tasks: {len(per_task)}\")\n",
    "    print(\"\\nTask breakdown:\")\n",
    "\n",
    "    for task_prefix, activities in per_task.items():\n",
    "        print(f\"\\n  Task: {task_prefix}\")\n",
    "        cpu = activities.get(\"thread-cpu\", 0)\n",
    "        noncpu = activities.get(\"thread-noncpu\", 0)\n",
    "        total = cpu + noncpu\n",
    "\n",
    "        if total > 0:\n",
    "            print(f\"    CPU time: {cpu:.3f}s ({cpu / total * 100:.1f}%)\")\n",
    "            print(f\"    Non-CPU time: {noncpu:.3f}s ({noncpu / total * 100:.1f}%)\")\n",
    "\n",
    "        if \"disk-read\" in activities:\n",
    "            print(f\"    Disk read: {activities['disk-read'] / 1e6:.2f} MB\")\n",
    "        if \"memory-read\" in activities:\n",
    "            print(f\"    Memory read: {activities['memory-read'] / 1e6:.2f} MB\")\n",
    "\n",
    "        if \"decompress\" in activities:\n",
    "            print(f\"    Decompress time: {activities['decompress']:.3f}s\")\n",
    "        if \"deserialize\" in activities:\n",
    "            print(f\"    Deserialize time: {activities['deserialize']:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Per-Task Visualizations\n",
    "\n",
    "Let's create plots showing the distribution across tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CPU vs Non-CPU time per task\n",
    "if span_metrics:\n",
    "    try:\n",
    "        fig, ax = plot_per_task_cpu_io(\n",
    "            span_metrics=span_metrics,\n",
    "            title=\"CPU vs Non-CPU Time per Task\",\n",
    "            figsize=(14, 6),\n",
    "        )\n",
    "        print(\"Successfully created CPU/Non-CPU plot\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not create CPU/Non-CPU plot: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not create bytes read plot: No per-task disk-read metrics found in span_metrics\n"
     ]
    }
   ],
   "source": [
    "# Plot bytes read per task (disk-read or memory-read)\n",
    "if span_metrics:\n",
    "    try:\n",
    "        fig, ax = plot_per_task_bytes_read(\n",
    "            span_metrics=span_metrics, title=\"Bytes Read per Task\", figsize=(14, 6)\n",
    "        )\n",
    "        print(\"Successfully created bytes read plot\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not create bytes read plot: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not create overhead plot: No per-task overhead metrics found in span_metrics\n"
     ]
    }
   ],
   "source": [
    "# Plot compression & serialization overhead per task\n",
    "if span_metrics:\n",
    "    try:\n",
    "        fig, ax = plot_per_task_overhead(\n",
    "            span_metrics=span_metrics,\n",
    "            title=\"Compression & Serialization Overhead per Task\",\n",
    "            figsize=(14, 6),\n",
    "        )\n",
    "        print(\"Successfully created overhead plot\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Could not create overhead plot: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Comparing Scenarios\n",
    "\n",
    "Let's run multiple scenarios to see how fine metrics help identify bottlenecks.\n",
    "\n",
    "**Note on Robustness:** roastcoffea automatically handles metric synchronization timing. Dask Spans sync metrics from workers to the scheduler via heartbeats (default: 1s interval). Our implementation uses retry logic with exponential backoff to ensure metrics are available even if tasks complete right after a heartbeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run and collect metrics\n",
    "def run_analysis(processor_class, label):\n",
    "    \"\"\"Run analysis and return fine metrics.\"\"\"\n",
    "    processor = processor_class()\n",
    "    with MetricsCollector(\n",
    "        client, track_workers=False, processor_instance=processor\n",
    "    ) as collector:\n",
    "        executor = processor.DaskExecutor(client=client)\n",
    "        runner = processor.Runner(\n",
    "            executor=executor,\n",
    "            savemetrics=True,\n",
    "            schema=NanoAODSchema,\n",
    "        )\n",
    "\n",
    "        output, report = runner(\n",
    "            fileset,\n",
    "            treename=\"Events\",\n",
    "            processor_instance=processor,\n",
    "        )\n",
    "\n",
    "        collector.set_coffea_report(report)\n",
    "    metrics = collector.get_metrics()\n",
    "\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"cpu_time\": metrics.get(\"processor_cpu_time_seconds\", 0),\n",
    "        \"noncpu_time\": metrics.get(\"processor_noncpu_time_seconds\", 0),\n",
    "        \"cpu_pct\": metrics.get(\"processor_cpu_percentage\", 0),\n",
    "        \"noncpu_pct\": metrics.get(\"processor_noncpu_percentage\", 0),\n",
    "        \"wall_time\": metrics.get(\"wall_time\", 0),\n",
    "        \"overhead_cpu\": metrics.get(\"overhead_cpu_time_seconds\", 0),\n",
    "        \"overhead_noncpu\": metrics.get(\"overhead_noncpu_time_seconds\", 0),\n",
    "    }\n",
    "\n",
    "\n",
    "# Define a CPU-heavy processor\n",
    "class CPUHeavyProcessor(processor.ProcessorABC):\n",
    "    \"\"\"Processor with more CPU-intensive operations.\"\"\"\n",
    "\n",
    "    def process(self, events):\n",
    "        jets = events.Jet\n",
    "        # Compute multiple derived quantities\n",
    "        for _ in range(5):  # Repeat calculations\n",
    "            pt = jets.pt\n",
    "            eta = jets.eta\n",
    "            phi = jets.phi\n",
    "            energy = ak.zip({\"pt\": pt, \"eta\": eta, \"phi\": phi})\n",
    "        return {\"nevents\": len(events)}\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator\n",
    "\n",
    "\n",
    "# Run both scenarios\n",
    "print(\"Running analysis scenarios...\\n\")\n",
    "results = []\n",
    "results.append(run_analysis(JetAnalysisProcessor, \"Standard Analysis\"))\n",
    "results.append(run_analysis(CPUHeavyProcessor, \"CPU-Heavy Analysis\"))\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n=== Scenario Comparison ===\")\n",
    "print(\n",
    "    f\"{'Scenario':<25} {'Wall Time':<12} {'Proc CPU':<12} {'Proc Non-CPU':<14} {'CPU %':<8} {'Non-CPU %':<10}\"\n",
    ")\n",
    "print(\"-\" * 95)\n",
    "for r in results:\n",
    "    print(\n",
    "        f\"{r['label']:<25} {r['wall_time']:<12.2f} {r['cpu_time']:<12.2f} {r['noncpu_time']:<14.2f} {r['cpu_pct']:<8.1f} {r['noncpu_pct']:<10.1f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n=== Dask Overhead (if separated) ===\")\n",
    "print(f\"{'Scenario':<25} {'Overhead CPU':<15} {'Overhead Non-CPU':<18}\")\n",
    "print(\"-\" * 65)\n",
    "for r in results:\n",
    "    if r[\"overhead_cpu\"] > 0 or r[\"overhead_noncpu\"] > 0:\n",
    "        print(\n",
    "            f\"{r['label']:<25} {r['overhead_cpu']:<15.3f} {r['overhead_noncpu']:<18.3f}\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"{r['label']:<25} {'N/A':<15} {'N/A':<18}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Understanding Worker-Level Data (Future)\n",
    "\n",
    "Currently, we collect **cumulative** metrics aggregated across all workers. However, Dask also tracks per-worker metrics in `Worker.digests_total`.\n",
    "\n",
    "### Current Implementation\n",
    "- **`Span.cumulative_worker_metrics`**: Aggregated across all workers\n",
    "- Keys: `('execute', task_prefix, activity, unit)`\n",
    "\n",
    "### Available (Not Yet Implemented)\n",
    "- **`Worker.digests_total`**: Per-worker metrics with span IDs\n",
    "- Keys: `('execute', span_id, task_prefix, activity, unit)`\n",
    "- Would allow per-worker CPU/I/O analysis\n",
    "\n",
    "### What This Means\n",
    "If we access worker digests directly, we could answer questions like:\n",
    "- Which workers are CPU-bound vs I/O-bound?\n",
    "- Is work balanced across workers?\n",
    "- Do some workers have more compression overhead?\n",
    "\n",
    "This is a potential future enhancement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Key Takeaways\n",
    "\n",
    "### What Fine Metrics Tell You\n",
    "\n",
    "1. **CPU % high**: Your analysis is compute-bound\n",
    "   - Optimize algorithms\n",
    "   - Consider vectorization with NumPy/Awkward operations\n",
    "   - May benefit from more cores\n",
    "\n",
    "2. **Non-CPU % high**: Your analysis is waiting/coordination-bound\n",
    "   - Includes I/O, waiting time, GIL contention, task coordination\n",
    "   - Optimize data access patterns\n",
    "   - Consider columnar filtering\n",
    "   - May not benefit from more cores\n",
    "\n",
    "3. **High Dask overhead**: Coordination tasks consuming significant time\n",
    "   - Consider larger chunk sizes\n",
    "   - Reduce task graph complexity\n",
    "   - May indicate too many small tasks\n",
    "\n",
    "4. **High compression overhead**: Decompression is a bottleneck\n",
    "   - Consider different compression codecs\n",
    "   - Pre-decompress if reading same files repeatedly\n",
    "\n",
    "5. **High serialization overhead**: Python object pickling is slow\n",
    "   - Minimize object complexity\n",
    "   - Use simpler data structures\n",
    "   - Consider Dask serialization optimizations\n",
    "\n",
    "6. **Low compression ratio**: Data not compressing well\n",
    "   - Already compressed formats\n",
    "   - Random/entropy-heavy data\n",
    "\n",
    "### Granularity Levels\n",
    "\n",
    "- **Cumulative**: Overall workflow performance\n",
    "- **Processor-separated**: Actual processor work vs Dask overhead (when processor_instance provided)\n",
    "- **Per-task**: Identify outlier tasks or unbalanced work\n",
    "- **Per-worker** (future): Diagnose worker-specific issues\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- No function-level profiling (use `cProfile` or `py-spy` for that)\n",
    "- Activities are predefined by Dask (can't add custom metrics)\n",
    "- Requires recent Dask version with Spans support\n",
    "- \"Non-CPU\" is a catch-all metric - includes many different waiting/coordination scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close cluster\n",
    "client.close()\n",
    "cluster.close()\n",
    "print(\"Cluster closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
