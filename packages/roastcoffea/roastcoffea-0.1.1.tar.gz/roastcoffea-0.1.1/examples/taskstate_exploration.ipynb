{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Dask TaskState for Per-Worker Chunk Tracking\n",
    "\n",
    "This notebook explores what data is available from Dask's internal `TaskState` objects to understand if we can use them for per-worker chunk byte tracking.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Determine if we can extract:\n",
    "- Bytes per chunk (input and/or output)\n",
    "- Which worker processed which chunk\n",
    "- Mapping from task keys to user-visible chunks (files, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "import pandas as pd\n",
    "import skhep_testdata\n",
    "from coffea import processor\n",
    "from coffea.nanoevents import NanoAODSchema\n",
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Simple Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleProcessor(processor.ProcessorABC):\n",
    "    \"\"\"Simple processor for testing.\"\"\"\n",
    "\n",
    "    def process(self, events):\n",
    "        # Do some computation\n",
    "        jets = events.Jet[events.Jet.pt > 30]\n",
    "\n",
    "        return {\n",
    "            \"nevents\": len(events),\n",
    "            \"njets\": ak.sum(ak.num(jets)),\n",
    "            \"dataset\": events.metadata.get(\"dataset\", \"unknown\"),\n",
    "            \"filename\": events.metadata.get(\"filename\", \"unknown\"),\n",
    "        }\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Dask Cluster\n",
    "\n",
    "We'll use 2 workers to see task distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moaly/Work/iris-hep/roastcoffea/.pixi/envs/dev/lib/python3.13/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 55712 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard: http://127.0.0.1:55712/status\n",
      "Workers: 2\n"
     ]
    }
   ],
   "source": [
    "cluster = LocalCluster(n_workers=2, threads_per_worker=1, processes=True)\n",
    "client = Client(cluster)\n",
    "\n",
    "print(f\"Dashboard: {client.dashboard_link}\")\n",
    "print(f\"Workers: {len(client.scheduler_info()['workers'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Coffea Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc4c5a1057c43ebb99fbe7395175c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c09b596b5df430bbf174fe7535df6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moaly/Work/iris-hep/roastcoffea/.pixi/envs/dev/lib/python3.13/site-packages/coffea/nanoevents/schemas/nanoaod.py:264: RuntimeWarning: Missing cross-reference index for LowPtElectron_electronIdx => Electron\n",
      "  warnings.warn(\n",
      "/Users/moaly/Work/iris-hep/roastcoffea/.pixi/envs/dev/lib/python3.13/site-packages/coffea/nanoevents/schemas/nanoaod.py:264: RuntimeWarning: Missing cross-reference index for LowPtElectron_genPartIdx => GenPart\n",
      "  warnings.warn(\n",
      "/Users/moaly/Work/iris-hep/roastcoffea/.pixi/envs/dev/lib/python3.13/site-packages/coffea/nanoevents/schemas/nanoaod.py:264: RuntimeWarning: Missing cross-reference index for LowPtElectron_photonIdx => Photon\n",
      "  warnings.warn(\n",
      "/Users/moaly/Work/iris-hep/roastcoffea/.pixi/envs/dev/lib/python3.13/site-packages/coffea/nanoevents/schemas/nanoaod.py:264: RuntimeWarning: Missing cross-reference index for FatJet_subJetIdx1 => SubJet\n",
      "  warnings.warn(\n",
      "/Users/moaly/Work/iris-hep/roastcoffea/.pixi/envs/dev/lib/python3.13/site-packages/coffea/nanoevents/schemas/nanoaod.py:264: RuntimeWarning: Missing cross-reference index for FatJet_subJetIdx2 => SubJet\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 200 events in 1 chunks\n",
      "Total bytes read: 0.34 MB\n"
     ]
    }
   ],
   "source": [
    "# Get test file\n",
    "test_file = skhep_testdata.data_path(\"nanoAOD_2015_CMS_Open_Data_ttbar.root\")\n",
    "\n",
    "# Create fileset\n",
    "fileset = {\n",
    "    \"ttbar\": {\n",
    "        \"files\": {test_file: \"Events\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Run processor\n",
    "proc = SimpleProcessor()\n",
    "executor = processor.DaskExecutor(client=client)\n",
    "runner = processor.Runner(\n",
    "    executor=executor,\n",
    "    savemetrics=True,\n",
    "    schema=NanoAODSchema,\n",
    ")\n",
    "\n",
    "output, report = runner(\n",
    "    fileset,\n",
    "    treename=\"Events\",\n",
    "    processor_instance=proc,\n",
    ")\n",
    "\n",
    "print(f\"\\nProcessed {report['entries']} events in {report['chunks']} chunks\")\n",
    "print(f\"Total bytes read: {report['bytesread'] / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Access Scheduler and Worker State\n",
    "\n",
    "Let's explore what's available in the Dask scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Scheduler Info ===\n",
      "Scheduler type: <class 'distributed.scheduler.Scheduler'>\n",
      "Number of workers: 2\n",
      "Number of tasks: 0\n",
      "\n",
      "Worker IDs: ['tcp://127.0.0.1:55720', 'tcp://127.0.0.1:55721']\n"
     ]
    }
   ],
   "source": [
    "# Get scheduler from client\n",
    "scheduler = client.cluster.scheduler\n",
    "\n",
    "print(\"=== Scheduler Info ===\")\n",
    "print(f\"Scheduler type: {type(scheduler)}\")\n",
    "print(f\"Number of workers: {len(scheduler.workers)}\")\n",
    "print(f\"Number of tasks: {len(scheduler.tasks)}\")\n",
    "print(f\"\\nWorker IDs: {list(scheduler.workers.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Explore Worker State\n",
    "\n",
    "Let's see what data is available for each worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Worker State Details ===\n",
      "\n",
      "Worker: tcp://127.0.0.1:55720\n",
      "  Address: tcp://127.0.0.1:55720\n",
      "  Threads: 1\n",
      "  Memory limit: 17.18 GB\n",
      "  Memory used: 0.00 GB\n",
      "  Total nbytes: 0.00 MB\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'WorkerState' object has no attribute 'tasks'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Memory used: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworker_state.memory.managed\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m1e9\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m GB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Total nbytes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworker_state.nbytes\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m1e6\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m MB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Number of tasks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mworker_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Currently processing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(worker_state.processing)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Show attributes available\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'WorkerState' object has no attribute 'tasks'"
     ]
    }
   ],
   "source": [
    "print(\"=== Worker State Details ===\")\n",
    "for worker_id, worker_state in scheduler.workers.items():\n",
    "    print(f\"\\nWorker: {worker_id}\")\n",
    "    print(f\"  Address: {worker_state.address}\")\n",
    "    print(f\"  Threads: {worker_state.nthreads}\")\n",
    "    print(f\"  Memory limit: {worker_state.memory_limit / 1e9:.2f} GB\")\n",
    "    print(f\"  Memory used: {worker_state.memory.managed / 1e9:.2f} GB\")\n",
    "    print(f\"  Total nbytes: {worker_state.nbytes / 1e6:.2f} MB\")\n",
    "    print(f\"  Number of tasks: {len(worker_state.tasks)}\")\n",
    "    print(f\"  Currently processing: {len(worker_state.processing)}\")\n",
    "\n",
    "    # Show attributes available\n",
    "    attrs = [a for a in dir(worker_state) if not a.startswith(\"_\")]\n",
    "    print(f\"  Available attributes: {', '.join(attrs[:10])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Explore TaskState Objects\n",
    "\n",
    "This is the critical part - what's in individual task states?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== TaskState Details ===\")\n",
    "\n",
    "# Get all tasks from scheduler\n",
    "all_tasks = list(scheduler.tasks.values())\n",
    "print(f\"Total tasks in scheduler: {len(all_tasks)}\")\n",
    "\n",
    "# Look at first few tasks\n",
    "for i, task in enumerate(all_tasks[:5]):\n",
    "    print(f\"\\n--- Task {i + 1} ---\")\n",
    "    print(f\"Key: {task.key}\")\n",
    "    print(f\"State: {task.state}\")\n",
    "    print(f\"Worker: {task.who_has if hasattr(task, 'who_has') else 'N/A'}\")\n",
    "    print(\n",
    "        f\"nbytes: {task.nbytes / 1e3:.2f} KB\"\n",
    "        if hasattr(task, \"nbytes\") and task.nbytes\n",
    "        else \"nbytes: N/A\"\n",
    "    )\n",
    "    print(f\"Type: {task.type if hasattr(task, 'type') else 'N/A'}\")\n",
    "\n",
    "    # Show available attributes\n",
    "    attrs = [a for a in dir(task) if not a.startswith(\"_\")]\n",
    "    print(f\"Attributes: {', '.join(attrs[:15])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Find Coffea-Related Tasks\n",
    "\n",
    "Let's filter for tasks related to our processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Coffea/Processor Tasks ===\")\n",
    "\n",
    "# Find tasks with 'process' or processor name in key\n",
    "processor_tasks = [\n",
    "    task\n",
    "    for task in all_tasks\n",
    "    if \"SimpleProcessor\" in str(task.key) or \"process\" in str(task.key).lower()\n",
    "]\n",
    "\n",
    "print(f\"Found {len(processor_tasks)} processor-related tasks\\n\")\n",
    "\n",
    "# Show details for processor tasks\n",
    "for i, task in enumerate(processor_tasks[:10]):\n",
    "    print(f\"\\nTask {i + 1}:\")\n",
    "    print(f\"  Key: {task.key}\")\n",
    "    print(f\"  State: {task.state}\")\n",
    "\n",
    "    # Try to get worker who processed it\n",
    "    if hasattr(task, \"who_has\") and task.who_has:\n",
    "        worker_addr = list(task.who_has)[0].address if task.who_has else None\n",
    "        print(f\"  Worker: {worker_addr}\")\n",
    "\n",
    "    # Get size\n",
    "    if hasattr(task, \"nbytes\") and task.nbytes:\n",
    "        print(f\"  Result size: {task.nbytes / 1e3:.2f} KB\")\n",
    "\n",
    "    # Check for any metadata\n",
    "    if hasattr(task, \"annotations\"):\n",
    "        print(f\"  Annotations: {task.annotations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Per-Worker Task Breakdown\n",
    "\n",
    "Let's see which worker processed how many tasks and total bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Per-Worker Task Distribution ===\")\n",
    "\n",
    "worker_stats = {}\n",
    "\n",
    "for worker_id, worker_state in scheduler.workers.items():\n",
    "    worker_tasks = worker_state.tasks\n",
    "\n",
    "    # Calculate stats\n",
    "    total_bytes = sum(\n",
    "        task.nbytes for task in worker_tasks if hasattr(task, \"nbytes\") and task.nbytes\n",
    "    )\n",
    "\n",
    "    processor_related = [\n",
    "        task\n",
    "        for task in worker_tasks\n",
    "        if \"SimpleProcessor\" in str(task.key) or \"process\" in str(task.key).lower()\n",
    "    ]\n",
    "\n",
    "    worker_stats[worker_id] = {\n",
    "        \"total_tasks\": len(worker_tasks),\n",
    "        \"processor_tasks\": len(processor_related),\n",
    "        \"total_bytes\": total_bytes,\n",
    "    }\n",
    "\n",
    "    print(f\"\\nWorker: {worker_id}\")\n",
    "    print(f\"  Total tasks: {len(worker_tasks)}\")\n",
    "    print(f\"  Processor tasks: {len(processor_related)}\")\n",
    "    print(f\"  Total result bytes: {total_bytes / 1e6:.2f} MB\")\n",
    "\n",
    "    # Show sample task keys\n",
    "    if processor_related:\n",
    "        print(\"  Sample task keys:\")\n",
    "        for task in processor_related[:3]:\n",
    "            size = (\n",
    "                f\"{task.nbytes / 1e3:.1f} KB\"\n",
    "                if hasattr(task, \"nbytes\") and task.nbytes\n",
    "                else \"N/A\"\n",
    "            )\n",
    "            print(f\"    {task.key}: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Can We Map Task Keys to Chunks?\n",
    "\n",
    "Let's see if task keys contain any information about files or datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Task Key Analysis ===\")\n",
    "\n",
    "# Examine task key structure\n",
    "print(\"\\nTask key examples:\")\n",
    "for i, task in enumerate(processor_tasks[:10]):\n",
    "    key = task.key\n",
    "    print(f\"\\n{i + 1}. {key}\")\n",
    "    print(f\"   Type: {type(key)}\")\n",
    "\n",
    "    if isinstance(key, tuple):\n",
    "        print(f\"   Length: {len(key)}\")\n",
    "        print(f\"   Elements: {key}\")\n",
    "\n",
    "        # Check if any element contains file/dataset info\n",
    "        for j, elem in enumerate(key):\n",
    "            if isinstance(elem, str):\n",
    "                if \"ttbar\" in elem or \"root\" in elem or \"nanoAOD\" in elem:\n",
    "                    print(f\"   -> Element {j} might contain file/dataset info: {elem}\")\n",
    "\n",
    "print(\"\\n=== Conclusion ===\")\n",
    "print(\"Task keys are typically tuples like ('function-name', 'hash', index)\")\n",
    "print(\"They generally do NOT contain human-readable file/dataset information.\")\n",
    "print(\"Coffea's internal structure may have this mapping, but it's not in task keys.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Create Summary DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a DataFrame of all processor tasks\n",
    "task_data = []\n",
    "\n",
    "for task in processor_tasks:\n",
    "    # Get worker\n",
    "    worker = None\n",
    "    if hasattr(task, \"who_has\") and task.who_has:\n",
    "        worker = list(task.who_has)[0].address if task.who_has else None\n",
    "\n",
    "    task_data.append(\n",
    "        {\n",
    "            \"task_key\": str(task.key)[:50] + \"...\"\n",
    "            if len(str(task.key)) > 50\n",
    "            else str(task.key),\n",
    "            \"worker\": worker,\n",
    "            \"state\": task.state,\n",
    "            \"nbytes\": task.nbytes if hasattr(task, \"nbytes\") and task.nbytes else 0,\n",
    "            \"nbytes_kb\": task.nbytes / 1e3\n",
    "            if hasattr(task, \"nbytes\") and task.nbytes\n",
    "            else 0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(task_data)\n",
    "\n",
    "print(\"=== Task Summary DataFrame ===\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(\"\\n=== Summary Statistics ===\")\n",
    "print(df[[\"nbytes_kb\"]].describe())\n",
    "\n",
    "print(\"\\n=== Per-Worker Summary ===\")\n",
    "print(df.groupby(\"worker\")[\"nbytes_kb\"].agg([\"count\", \"sum\", \"mean\", \"std\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings and Conclusions\n",
    "\n",
    "### What We CAN Get from TaskState:\n",
    "\n",
    "1. ✅ **Result size per task** (`TaskState.nbytes`)\n",
    "2. ✅ **Worker attribution** (which worker processed which task)\n",
    "3. ✅ **Task state** (waiting, executing, finished, etc.)\n",
    "4. ✅ **Number of tasks per worker**\n",
    "\n",
    "### What We CANNOT Get:\n",
    "\n",
    "1. ❌ **Input bytes read** - only output/result size available\n",
    "2. ❌ **File/dataset mapping** - task keys are opaque hashes\n",
    "3. ❌ **Chunk identification** - no way to map task to user-visible \"chunk\"\n",
    "4. ❌ **Event counts per task** - not in TaskState\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "- `nbytes` is the **output** size (accumulator result), not input bytes read from file\n",
    "- Task keys don't contain human-readable information (filename, dataset, etc.)\n",
    "- Would need to maintain separate mapping from task keys to chunk metadata\n",
    "- Snapshot overhead - thousands of tasks to track\n",
    "\n",
    "### Recommendation:\n",
    "\n",
    "**TaskState tracking alone is insufficient** for the requirements (per-chunk with worker attribution, bytes per event, throughput per chunk).\n",
    "\n",
    "**Better approach**: Use `@track_metrics` decorator that:\n",
    "- Captures input metadata (filename, dataset, event count)\n",
    "- Measures processing time directly\n",
    "- Gets worker ID from `get_worker()`\n",
    "- Can estimate bytes if needed\n",
    "- Provides clean, user-visible chunk attribution\n",
    "\n",
    "TaskState could supplement decorator data but cannot replace it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()\n",
    "print(\"Cluster closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
