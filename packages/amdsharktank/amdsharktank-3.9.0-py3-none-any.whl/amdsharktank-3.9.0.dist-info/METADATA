Metadata-Version: 2.4
Name: amdsharktank
Version: 3.9.0
Summary: amdshark layers and inference models for genai
Author: amdshark Authors
License-Expression: Apache-2.0
Project-URL: Repository, https://github.com/nod-ai/amd-shark-ai
Classifier: Development Status :: 3 - Alpha
Classifier: Programming Language :: Python :: 3
Description-Content-Type: text/markdown
Requires-Dist: iree-turbine
Requires-Dist: fire>=0.7
Requires-Dist: gguf>=0.11.0
Requires-Dist: numpy
Requires-Dist: huggingface-hub
Requires-Dist: transformers==4.52.1
Requires-Dist: datasets
Requires-Dist: einops
Requires-Dist: sentencepiece
Requires-Dist: fastapi>=0.112.2
Requires-Dist: uvicorn>=0.30.6
Provides-Extra: dev
Requires-Dist: azure-identity>=1.19; extra == "dev"
Requires-Dist: azure-storage-blob>=12.24; extra == "dev"
Provides-Extra: testing
Requires-Dist: accelerate; extra == "testing"
Requires-Dist: datasets==3.0.0; extra == "testing"
Requires-Dist: diffusers; extra == "testing"
Requires-Dist: parameterized; extra == "testing"
Requires-Dist: protobuf; extra == "testing"
Requires-Dist: pytest==8.0.0; extra == "testing"
Requires-Dist: pytest-html; extra == "testing"
Requires-Dist: pytest-cov; extra == "testing"
Requires-Dist: pytest-timeout; extra == "testing"
Requires-Dist: pytest-xdist==3.5.0; extra == "testing"
Requires-Dist: safetensors>=0.4.5; extra == "testing"

# amdshark Tank

**WARNING: This is an early preview that is in progress. It is not ready for
general use.**

Light weight inference optimized layers and models for popular genai
applications.

This sub-project is a work in progress. It is intended to be a repository of
layers, model recipes, and conversion tools from popular LLM quantization
tooling.

## Project Status

[![CI - amdsharktank nightly](https://github.com/nod-ai/amd-shark-ai/actions/workflows/ci-amdsharktank-nightly.yml/badge.svg?branch=main)](https://github.com/nod-ai/amd-shark-ai/actions/workflows/ci-amdsharktank-nightly.yml)

## Examples

The repository will ultimately grow a curated set of models and tools for
constructing them, but for the moment, it largely contains some CLI examples.
These are all under active development and should not yet be expected to work.


### Perform batched inference in PyTorch on a paged llama derived LLM:

Note: Use `--device='cuda:0'` to run this inference on an AMD GPU.

```shell
python -m amdsharktank.examples.paged_llm_v1 \
  --hf-dataset=open_llama_3b_v2_f16_gguf \
  --device='cuda:0' \
  "Prompt 1" \
  "Prompt 2" ...
```

### Export an IREE compilable batched LLM for serving:

```shell
python -m amdsharktank.examples.export_paged_llm_v1 \
  --hf-dataset=open_llama_3b_v2_f16_gguf \
  --output-mlir=/tmp/open_llama_3b_v2_f16.mlir \
  --output-config=/tmp/open_llama_3b_v2_f16.json
```

### Generate sample input tokens for IREE inference/tracy:Add commentMore actions

```shell
python -m amdsharktank.examples.paged_llm_v1 \
  --irpa-file=open_llama_3b_v2_f16.irpa \
  --tokenizer-config-json=tokenizer_config.json \
  --prompt-seq-len=128 \
  --bs=4 \
  --dump-decode-steps=1 \
  --max-decode-steps=1 \
  --dump-path='/tmp' \
  --device='cuda:0'
```

### Dump parsed information about a model from a gguf file:

```shell
python -m amdsharktank.tools.dump_gguf --hf-dataset=open_llama_3b_v2_f16_gguf
```

## Package Python Release Builds

* To build wheels for Linux:

    ```bash
    ./build_tools/build_linux_package.sh
    ```

    That should produce
    `build_tools/wheelhouse/amdsharktank-{X.Y.Z}.dev0-py3-none-any.whl`, which can
    then be installed with

    ```bash
    python3 -m pip install build_tools/wheelhouse/amdsharktank-{X.Y.Z}.dev0-py3-none-any.whl
    ```

* To build a wheel for your host OS/arch manually:

    ```bash
    # Build amdsharktank.*.whl into the dist/ directory
    #   e.g. `amdsharktank-3.0.0.dev0-py3-none-any.whl`
    python3 -m pip wheel -v -w dist .

    # Install the built wheel.
    python3 -m pip install dist/*.whl
    ```
