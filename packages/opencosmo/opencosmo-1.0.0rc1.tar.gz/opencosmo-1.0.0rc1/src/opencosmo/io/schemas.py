from __future__ import annotations

from collections import defaultdict
from copy import copy
from itertools import chain
from typing import TYPE_CHECKING, Any, Iterable, Optional

import h5py
import hdf5plugin  # type: ignore
import healpy as hp
import numpy as np

import opencosmo.io.writers as iow
from opencosmo.index import ChunkedIndex, concatenate, get_length
from opencosmo.spatial.check import find_coordinates_2d

if TYPE_CHECKING:
    from mpi4py import MPI
    from numpy.typing import DTypeLike, NDArray

    import opencosmo as oc
    import opencosmo.io.protocols as iop
    from opencosmo.header import OpenCosmoHeader
    from opencosmo.index import DataIndex

ColumnShape = tuple[int, ...]

"""
Schemas represent the hierarchy of groups and datasets in a given hdf file.

Schemas may have child schemas, and must be able to allocate themselves
in an open HDF5 file. Allocation may look as simple as creating a particular
group and passing that group to child allocators. In the case of a ColumnSchema,
actual creation of hdf5 datasets is performed, complete with its length, dtype
and compressin.

Any validaty tests across multiple children must be performed by the parent schema.
For example, an OpenCosmoDataset must have columns with compatible shapes. It is the
responsibility of the DatasetSchema to enforce this.


Note that "Dataset" is used in the opencosmo sense here so:

hdf5.Dataset == opencosmo Column
hdf5.Group == opencosmo Dataset (sometimes)

Schemas also must define an into_writer method that creates the object that will
actually put data into the file. As a result, certain schemas must hold references
to the underlying data they reference.

See opencosmo.io.protocols for required methods.
"""

COMPRESSION = hdf5plugin.Blosc2()


class FileSchema:
    """
    Always the top level of schema. Has very few responsibilites
    besides holding children.
    """

    def __init__(self):
        self.children = {}

    def add_child(self, child: iop.DataSchema, name: str):
        if name in self.children:
            raise ValueError(f"File schema already has child named {name}")
        match child:
            case (
                SimCollectionSchema()
                | StructCollectionSchema()
                | DatasetSchema()
                | LightconeSchema()
                | EmptyColumnSchema()
            ):
                self.children[name] = child
            case _:
                raise ValueError(
                    f"File schema cannot have children of type {type(child)}"
                )

    def allocate(self, group: h5py.File | h5py.Group):
        self.verify()
        if not isinstance(group, h5py.File):
            raise ValueError(
                "File Schema allocation must be done at the top level of a h5py file!"
            )
        if len(self.children) == 1:
            ds = next(iter(self.children.values()))
            return ds.allocate(group)

        for ds_name, ds in self.children.items():
            ds_group = group.require_group(ds_name)
            ds.allocate(ds_group)

    def verify(self):
        if len(self.children) == 0:
            raise ValueError("This file schema has no children!")
        for child in self.children.values():
            child.verify()

    def into_writer(self, comm: Optional["MPI.Comm"] = None):
        children = {
            name: schema.into_writer(comm) for name, schema in self.children.items()
        }
        return iow.FileWriter(children)


class SimCollectionSchema:
    """
    A schema for simulation collections. Like FileSchema, it has very few
    responsibilites outside of holding children and making sure they are
    valid types.
    """

    def __init__(self):
        self.children = {}

    def add_child(self, child: iop.DataSchema, name: str):
        if name in self.children:
            raise ValueError(f"SimCollection already has a dataset with name {name}")
        match child:
            case StructCollectionSchema() | DatasetSchema():
                self.children[name] = child
            case _:
                raise ValueError(
                    f"Sim collection cannot take children of type {type(child)}"
                )

    def allocate(self, group: h5py.File | h5py.Group):
        if not isinstance(group, h5py.File):
            raise ValueError(
                "File Schema allocation must be done at the top level of a h5py file!"
            )
        for ds_name, ds in self.children.items():
            ds_group = group.require_group(ds_name)
            ds.allocate(ds_group)

    def verify(self):
        zero_length = set()
        for name, child in self.children.items():
            try:
                child.verify()
            except ZeroLengthError:
                zero_length.add(name)
        if len(zero_length) == len(self.children):
            raise ZeroLengthError

    def into_writer(self, comm: Optional["MPI.Comm"] = None):
        children = {
            name: child.into_writer(comm) for name, child in self.children.items()
        }
        return iow.CollectionWriter(children)


class LightconeSchema:
    def __init__(self) -> None:
        self.children: dict[str, DatasetSchema | StackedLightconeDatasetSchema] = {}

    def verify(self):
        zero_length = set()
        for name, child in self.children.items():
            try:
                child.verify()
            except ZeroLengthError:
                zero_length.add(name)
        if len(zero_length) == len(self.children):
            raise ZeroLengthError

    def add_child(self, child: iop.DataSchema, name: str):
        if name in self.children:
            raise ValueError(f"LightconeSchema already has child with name {name}")
        match child:
            case DatasetSchema() | StackedLightconeDatasetSchema():
                self.children[name] = child
            case _:
                raise ValueError(
                    f"LightconeSchema cannot take children of type {type(child)}"
                )

    def allocate(self, group: h5py.Group):
        if len(self.children) == 1:
            next(iter(self.children.values())).allocate(group)
            return
        for ds_name, ds in self.children.items():
            ds_group = group.require_group(ds_name)
            ds.allocate(ds_group)

    def into_writer(self, comm: Optional["MPI.Comm"] = None):
        dataset_writers = {
            key: val.into_writer(comm) for key, val in self.children.items()
        }
        return iow.CollectionWriter(dataset_writers)


class StructCollectionSchema:
    """
    Schema for structure collections. Actual linking data is held by datasets,
    so the main responsibility of this schema is to ensure that at least one
    link exists across all its datasets.

    Structure collections always have a header, because they always
    contain several datasets from the same simulation.
    """

    def __init__(self) -> None:
        self.children: dict[str, DatasetSchema | StructCollectionSchema] = {}

    def verify(self):
        if len(self.children) < 2:
            raise ValueError("StructCollection must have at least two children!")

        found_links = False
        zero_length = set()
        for name, child in self.children.items():
            try:
                child.verify()
            except ZeroLengthError:
                zero_length.add(name)
                continue
            linked_columns = filter(lambda col: "data_linked" in col, child.children)
            if list(linked_columns):
                found_links = True

        if not found_links:
            raise ValueError("StructCollection must get at least one link!")
        elif len(zero_length) == len(self.children):
            raise ZeroLengthError

    def add_child(self, child: iop.DataSchema, name: str):
        if name in self.children:
            raise ValueError(
                f"StructCollectionSchema already has child with name {name}"
            )
        match child:
            case DatasetSchema():
                self.children[name] = child
            case StructCollectionSchema():
                for key, grandchild in child.children.items():
                    if key in self.children:
                        self.children[f"{name}_{key}"] = grandchild
                    else:
                        self.children[key] = grandchild
            case _:
                raise ValueError(
                    f"StructCollectionSchema cannot take children of type {type(child)}"
                )

    def allocate(self, group: h5py.Group):
        if len(self.children) == 1:
            raise ValueError("A dataset collection cannot have only one member!")
        for ds_name, ds in self.children.items():
            ds_group = group.require_group(ds_name)
            ds.allocate(ds_group)

    def into_writer(self, comm: Optional["MPI.Comm"] = None):
        dataset_writers = {
            key: val.into_writer(comm) for key, val in self.children.items()
        }
        return iow.CollectionWriter(dataset_writers)


class ZeroLengthError(Exception):
    pass


class DatasetSchema:
    """
    A schema for an opencosmo.dataset. This schema may or may not have a header.
    For example, a StructureCollection has a single collection for all datasets,
    so it does not need to be handled by the datasets it holds.
    """

    def __init__(
        self,
        header: Optional[OpenCosmoHeader] = None,
    ):
        self.columns: dict[str, dict[str, ColumnSchema]] = defaultdict(dict)
        self.header = header

    @property
    def children(self):
        return self.columns

    def len(self):
        col = next(iter(self.columns["data"].values()))
        return len(col)

    @classmethod
    def make_schema(
        cls,
        sources: dict[str, h5py.Group],
        columns: Iterable[str],
        index: DataIndex,
        header: Optional[OpenCosmoHeader] = None,
    ):
        schema = DatasetSchema(header)
        columns = set(columns)
        for colname in columns:
            colname_parts = colname.split("/", maxsplit=1)
            if (
                colname_parts[0] not in sources
                or colname_parts[1] not in sources[colname_parts[0]].keys()
            ):
                raise ValueError("Found a column with no source!")

            # Still need this to get metadata if the column is cached
            colsource = sources[colname_parts[0]][colname_parts[1]]

            column_schema = ColumnSchema(
                colname_parts[1], index, colsource, colsource.attrs
            )
            schema.add_child(column_schema, colname)
        return schema

    def add_child(self, child: iop.DataSchema, child_id: str):
        if not isinstance(child, ColumnSchema):
            raise ValueError("Dataset schemas can only take columns as childre")
        colparts = child_id.split("/", maxsplit=1)

        if colparts[1] in self.columns[colparts[0]]:
            raise ValueError(f"DatasetSchema already has a child named {child_id}")
        self.columns[colparts[0]][colparts[1]] = child

    def verify(self):
        if len(self.columns) == 0:
            raise ValueError("Datasets must have at least one column")

        sizes = set(len(col) for col in self.columns["data"].values())

        if len(sizes) == 1 and sizes.pop() == 0:
            raise ZeroLengthError()

        for group in self.columns.values():
            for column in group.values():
                column.verify()

    def allocate(self, group: h5py.File | h5py.Group):
        for groupname, columns in self.columns.items():
            data_group = group.require_group(groupname)
            for colname, column in columns.items():
                column.allocate(data_group)
        if self.header is not None:
            self.header.write(group)

    def into_writer(self, comm: Optional["MPI.Comm"] = None):
        colnames = list(self.columns.keys())
        colnames.sort()

        writers = {}
        for group, columns in self.columns.items():
            writers[group] = {
                name: cs.into_writer(comm) for name, cs in columns.items()
            }
        return iow.DatasetWriter(writers, comm)


def get_stacked_order(datasets: Iterable[oc.Dataset], max_index_depth: int):
    datasets = list(datasets)
    nside = 2**max_index_depth
    coordinates = list(map(find_coordinates_2d, datasets))
    pixels = np.concatenate(
        [
            hp.ang2pix(nside, coords.ra.value, coords.dec.value, lonlat=True, nest=True)
            for coords in coordinates
        ]
    )
    new_order = np.argsort(pixels)
    return np.split(new_order, [len(ds) for ds in datasets[:-1]])


class StackedLightconeDatasetSchema:
    def __init__(
        self,
        datasets: list[oc.Dataset],
        header: OpenCosmoHeader,
        total_length: int,
        offset: int,
        comm: Optional[MPI.Comm] = None,
    ):
        self.children = [ds.make_schema(with_header=True) for ds in datasets]
        self.header = header
        self.total_length = total_length
        self.offset = offset
        self.comm = comm
        max_depth = -1
        for child in self.children:
            if "index" not in child.children:
                continue
            i = 0
            while f"level_{i}/start" in child.children["index"].keys():
                i += 1

            if i - 1 > max_depth:
                max_depth = i - 1
        if max_depth == -1:
            max_depth = 6
        self.__order = get_stacked_order(datasets, max_depth)
        for i, child in enumerate(self.children):
            for group, columns in child.columns.items():
                if "data" not in group:
                    continue
                for column in columns.values():
                    column.output_order = self.__order[i]

    def verify(self):
        zero_length = set()
        for child in self.children:
            try:
                child.verify()
            except ZeroLengthError:
                zero_length.add(name)
        if len(zero_length) == len(self.children):
            raise ZeroLengthError

    def add_child(self, child: iop.DataSchema, name: str):
        raise NotImplementedError(
            "StackedLightconeDatasetSchemas children must be provided as datasets at initialization."
        )

    def allocate(self, group: h5py.File | h5py.Group):
        reference_dataset = self.children[0]

        for groupname, columns in reference_dataset.children.items():
            data_group = group.require_group(groupname)
            for colname, column in columns.items():
                new_column = copy(column)
                if "data" in groupname:
                    new_column.total_length = self.total_length
                new_column.allocate(data_group)

        if self.header is not None:
            self.header.write(group)

    def into_writer(self, comm: Optional["MPI.Comm"] = None):
        children = [child.into_writer() for child in self.children]
        return iow.StackedDatasetWriter(children, self.__order, self.offset, self.comm)


class ColumnSchema:
    """
    This is where the magic actually happens. The ColumnSchema actually allocates
    space in the file holds a reference to the data that will eventually be written.
    It is also used eternally by the link schemas.
    """

    def __init__(
        self,
        name: str,
        index: DataIndex,
        source: h5py.Dataset | NDArray,
        attrs: dict[str, Any],
        total_length: Optional[int] = None,
        output_order: Optional[np.ndarray] = None,
    ):
        self.name = name
        self.index = index
        self.source = source
        self.attrs = attrs
        self.offset = 0
        self.output_order = output_order
        if total_length is None:
            total_length = get_length(index)
        self.total_length = total_length

    def __len__(self):
        return get_length(self.index)

    def concatenate(self, *others: "ColumnSchema"):
        for other in others:
            if other.name != self.name:
                raise ValueError("Tried to combine columns with different names")
            if self.source.shape[1:] != other.source.shape[1:]:
                raise ValueError("Tried to combine columns with incompatible shapes")

        new_index = concatenate(self.index, *[o.index for o in others])
        return ColumnSchema(self.name, new_index, self.source, self.attrs)

    def add_child(self, *args, **kwargs):
        raise TypeError("Columns do not take children!")

    insert = add_child

    def set_offset(self, offset: int):
        self.offset = offset

    def verify(self):
        assert self.output_order is None or self.offset == 0

    def allocate(self, group: h5py.Group):
        shape = (self.total_length,) + self.source.shape[1:]
        group.require_dataset(self.name, shape, self.source.dtype)
        for name, attr in self.attrs.items():
            group[self.name].attrs[name] = attr

    def into_writer(self, comm: Optional["MPI.Comm"] = None):
        return iow.ColumnWriter(self.name, self.index, self.source, self.offset)


class EmptyColumnSchema:
    """
    Represents a column that contains no data. The ONLY time this should be used
    is if we are writing in an MPI context and one or more of the ranks
    does not have data for a given column. However it must still know the column
    exists because structural work must be performed by all ranks in
    parallel HDF5.

    Its associated writer is effectively a no-op.

    """

    def __init__(
        self,
        name: str,
        attrs: dict[str, Any],
        dtype: DTypeLike,
        shape: tuple[int, ...],
    ):
        self.name = name
        self.attrs = attrs
        self.dtype = dtype
        self.shape = shape

    def __len__(self):
        return self.shape[0]

    def add_child(self, *args, **kwargs):
        raise TypeError("Columns do not take children!")

    insert = add_child

    def set_offset(self, offset: int):
        return

    def verify(self):
        return True

    def allocate(self, group: h5py.Group):
        group.create_dataset(self.name, self.shape, self.dtype)
        for name, attr in self.attrs.items():
            group[self.name].attrs[name] = attr

    def into_writer(self, comm: Optional["MPI.Comm"] = None):
        return iow.EmptyColumnWriter(self.name)
