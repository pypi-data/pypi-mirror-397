"""Shared utilities for CLI commands.

This module centralizes small helpers used across multiple CLI commands.
"""

import os
import sys
import time
from typing import Any

from flow.cli.ui.facade import TerminalAdapter
from flow.cli.ui.presentation.table_styles import create_flow_table, wrap_table_in_panel
from flow.cli.utils.theme_manager import theme_manager
from flow.domain.parsers.instance_parser import extract_gpu_info
from flow.domain.pricing import calculator as pricing_core
from flow.errors import TaskNotFoundError
from flow.resources import get_gpu_pricing as _get_pricing_data
from flow.sdk.client import Flow

console = theme_manager.create_console()


def env_items_to_dict(
    items: tuple[tuple[str, str], ...] | list[tuple[str, str]] | None,
) -> dict[str, str]:
    """Convert repeated EnvItem tuples into a dictionary.

    Example: [("FOO", "bar"), ("A", "B")] -> {"FOO": "bar", "A": "B"}
    """
    env: dict[str, str] = {}
    for key, value in items or ():  # type: ignore[assignment]
        env[str(key)] = str(value)
    return env


def display_config(
    config: dict[str, Any],
    compact: bool = False,
    instance_mode: bool = False,
) -> None:
    """Display task configuration in a responsive table."""
    layout = TerminalAdapter.get_responsive_layout()

    table = create_flow_table(
        title=None, show_borders=layout["show_borders"], padding=1, expand=False
    )
    table.show_header = False
    table.add_column("Setting", style=theme_manager.get_color("accent"), no_wrap=True)
    table.add_column("Value", style=theme_manager.get_color("default"))

    # Name
    if "name" in config:
        table.add_row("Name", f"[bold]{config.get('name')}[/bold]")

    # Command (compact if very long) - skip for instance mode
    if not instance_mode:
        command = config.get("command", "N/A")
        if isinstance(command, list):
            command = " ".join(command)
        max_cmd_len = 80 if layout["density"].value != "compact" else 50
        if isinstance(command, str) and len(command) > max_cmd_len:
            command = TerminalAdapter.intelligent_truncate(command, max_cmd_len, "middle")
        table.add_row("Command", f"[dim]{command}[/dim]")

    # Image
    if not compact and not instance_mode:
        image = config.get("image")
        if image:
            table.add_row("Image", image)
        # Show working directory for clarity about code path (host+container)
        wd = config.get("working_dir") or config.get("working_dir")
        if wd:
            table.add_row("Working Dir", str(wd))

    # Instance type and count
    instance_type = config.get("instance_type", "N/A")
    num_instances = int(config.get("num_instances", 1) or 1)

    # Format instance type to show GPU count when it's implied (e.g. "h100" -> "8xh100")
    _gpu_type, gpu_count = extract_gpu_info(instance_type)
    if gpu_count > 1 and "x" not in instance_type.lower():
        display_instance_type = f"{gpu_count}x{instance_type}"
    else:
        display_instance_type = instance_type

    if num_instances > 1:
        table.add_row("Instances", f"{num_instances} × {display_instance_type}")
    else:
        table.add_row("Instance Type", display_instance_type)

    # Region if present
    if not compact:
        region = config.get("region")
        if region:
            table.add_row("Region", region)

    # Priority value needed for pricing calculations
    priority = (config.get("priority") or "med").lower()

    # Priority display - skip for instance mode
    if not instance_mode:
        table.add_row("Priority", priority.capitalize())

    if config.get("k8s"):
        table.add_row("K8s Cluster", config.get("k8s"))

    # Pricing - shown in instance mode for resource allocation context
    if instance_mode:
        if config.get("max_price_per_hour"):
            per_instance_price = float(config["max_price_per_hour"]) or 0.0
            table.add_row("Max Price/Instance", f"${per_instance_price:.2f}/hr")
        else:
            # Priority-based limit pricing summary
            pricing_table = pricing_core.get_pricing_table(overrides=_get_pricing_data())
            gpu_type, gpu_count = extract_gpu_info(instance_type)
            default_table = pricing_table.get("default", {"med": 4.0})
            per_gpu_price = pricing_table.get(gpu_type, pricing_table.get("default", {})).get(
                priority, default_table.get("med", 4.0)
            )
            instance_price = per_gpu_price * max(gpu_count, 1)
            table.add_row(
                "Max Price/Instance",
                f"${instance_price:.2f}/hr ({gpu_count} GPU{'s' if gpu_count > 1 else ''})",
            )

    # Upload strategy/timeout and destination path
    upload_code = config.get("upload_code", True)
    if not compact and upload_code and ("upload_strategy" in config or "upload_timeout" in config):
        strategy = config.get("upload_strategy", "auto")
        timeout = int(config.get("upload_timeout", 600))
        wd = config.get("working_dir") or "/workspace"
        table.add_row("Code Upload", f"{strategy} (timeout {timeout}s) → {wd} (host+container)")

    # SSH keys count
    if not compact:
        ssh_keys = config.get("ssh_keys") or []
        if isinstance(ssh_keys, list | tuple) and len(ssh_keys) > 0:
            shown = ", ".join(ssh_keys[:2]) + (" …" if len(ssh_keys) > 2 else "")
            table.add_row("SSH Keys", shown)

    # Mounts summary (CLI overrides)
    if not compact and "mounts" in config and config["mounts"]:
        mount_strs = []
        for mount in config["mounts"]:
            if isinstance(mount, dict):
                source = mount.get("source", "")
                target = mount.get("target", "")
                mount_strs.append(f"{target} → {source}")
        if mount_strs:
            table.add_row(
                "Mounts", "\n".join(mount_strs[:5] + (["…"] if len(mount_strs) > 5 else []))
            )
    # Data mounts summary (native config mounts like s3:// and volume://)
    if not compact and "data_mounts" in config and config["data_mounts"]:
        dm_strs = []
        for m in config["data_mounts"]:
            if isinstance(m, dict):
                source = m.get("source", "")
                target = m.get("target", "")
                dm_strs.append(f"{target} → {source}")
        if dm_strs:
            table.add_row(
                "Data Mounts", "\n".join(dm_strs[:5] + (["…"] if len(dm_strs) > 5 else []))
            )

    # Resources (if present)
    if not compact and "resources" in config:
        resources = config["resources"] or {}
        vcpus = resources.get("vcpus")
        mem = resources.get("memory")
        gpus = resources.get("gpus")
        if vcpus:
            table.add_row("vCPUs", str(vcpus))
        if mem:
            table.add_row("Memory", f"{mem} GB")
        if gpus:
            table.add_row("GPUs", str(gpus))

    # Print within a panel title (compute-mode aware)
    try:
        from flow.cli.ui.presentation.nomenclature import get_entity_labels as _labels

        wrap_table_in_panel(table, f"{_labels().header} Configuration", console)
    except Exception:  # noqa: BLE001
        wrap_table_in_panel(table, "Task Configuration", console)


def _invalidate_task_cache() -> None:
    """Invalidate HTTP client task cache to ensure fresh status data."""
    from flow.adapters.http.client import HttpClientPool

    for http_client in HttpClientPool._clients.values():
        if hasattr(http_client, "invalidate_task_cache"):
            http_client.invalidate_task_cache()


def wait_for_task(
    flow_client: Flow,
    task_id: str,
    watch: bool = False,
    json_output: bool = False,
    task_name: str | None = None,
    show_submission_message: bool = True,
    *,
    progress_adapter: object | None = None,
) -> str:
    """Wait for a task to reach running state with progress indication.

    Args:
        flow_client: Flow client instance
        task_id: Task ID to wait for
        watch: Whether to watch task progress
        json_output: Whether to output JSON
        task_name: Optional task name for better display
        show_submission_message: Whether to show "Task submitted" message (default: True)

    Returns:
        Final task status
    """
    if json_output:
        # For JSON output, just poll without visual progress
        while True:
            _invalidate_task_cache()
            status = flow_client.status(task_id)
            if status not in ["pending", "preparing"]:
                return status
            time.sleep(2)

    if watch:
        # Use animated progress for watching mode
        if progress_adapter is not None:
            # Adapter-managed loop without local progress UI
            while True:
                _invalidate_task_cache()
                status = flow_client.status(task_id)
                if status == "running":
                    return status
                if status in ["completed", "failed", "cancelled"]:
                    return status
                try:
                    if hasattr(progress_adapter, "tick"):
                        progress_adapter.tick()
                except Exception:  # noqa: BLE001
                    pass
                time.sleep(1)
        else:
            from flow.cli.ui.presentation.animated_progress import AnimatedEllipsisProgress

            if show_submission_message:
                if task_name:
                    console.print(f"Task submitted: [accent]{task_name}[/accent]")
                else:
                    console.print(f"Task submitted with ID: [accent]{task_id}[/accent]")
            console.print("[dim]Watching task progress...[/dim]\n")

            with AnimatedEllipsisProgress(console, "Waiting for task to start", transient=True):
                while True:
                    _invalidate_task_cache()
                    status = flow_client.status(task_id)

                    if status == "running":
                        console.print("[success]✓[/success] Task is running")
                        task_ref = task_name or task_id
                        from flow.cli.commands.messages import print_next_actions as _pna

                        _pna(console, [f"Stream logs: [accent]flow logs {task_ref} -f[/accent]"])
                        return status
                    elif status in ["completed", "failed", "cancelled"]:
                        return status

                    time.sleep(2)
    else:
        # Simple waiting mode with animated progress
        if progress_adapter is not None:
            # Adapter-managed loop without local progress UI
            if show_submission_message and task_name is None:
                # Suppress by design for adapter-driven UX
                pass

            ALLOCATION_TIMEOUT_SECONDS = 120
            start_ts = time.time()
            while True:
                _invalidate_task_cache()
                try:
                    status = flow_client.status(task_id)
                except TaskNotFoundError:
                    # Task may not be immediately queryable after creation (eventual consistency)
                    # Continue waiting - the loop will timeout if the task truly doesn't exist
                    pass
                else:
                    if status not in ["pending", "preparing"]:
                        return status
                    if time.time() - start_ts > ALLOCATION_TIMEOUT_SECONDS:
                        return status

                try:
                    if hasattr(progress_adapter, "tick"):
                        progress_adapter.tick()
                except Exception:  # noqa: BLE001
                    pass
                time.sleep(1)
        else:
            from flow.cli.ui.presentation.animated_progress import AnimatedEllipsisProgress

            if show_submission_message:
                if task_name:
                    console.print(f"Task submitted: [accent]{task_name}[/accent]")
                else:
                    console.print(f"Task submitted with ID: [accent]{task_id}[/accent]")

            # Instance allocation is typically much faster than full provisioning
            # Allocation = getting assigned a GPU (usually <2 minutes)
            # Provisioning = boot + configure + SSH ready (up to 12-20 minutes)
            ALLOCATION_TIMEOUT_SECONDS = 120  # 2 minutes for GPU allocation from pool

            with AnimatedEllipsisProgress(
                console,
                "Waiting for instance allocation",
                transient=True,
                show_progress_bar=True,
                estimated_seconds=ALLOCATION_TIMEOUT_SECONDS,
            ):
                # Soft timeout after allocation window; fall back to background provisioning UX
                start_ts = time.time()
                while True:
                    _invalidate_task_cache()
                    try:
                        status = flow_client.status(task_id)
                    except TaskNotFoundError:
                        # Task may not be immediately queryable after creation (eventual consistency)
                        # Continue waiting - the loop will timeout if the task truly doesn't exist
                        pass
                    else:
                        if status not in ["pending", "preparing"]:
                            return status

                        if time.time() - start_ts > ALLOCATION_TIMEOUT_SECONDS:
                            # Stop waiting; let caller present non-blocking guidance
                            return status

                    time.sleep(2)


def maybe_show_auto_status(
    *,
    focus: str | None = None,
    reason: str | None = None,
    show_all: bool | None = None,
    limit: int = 10,
) -> None:
    """Optionally show a compact status table after a state-changing command.

    Behavior:
    - Enabled by default; disable with env `FLOW_AUTO_STATUS=0`.
    - Suppressed when stdout is not a TTY (unless `FLOW_AUTO_STATUS=1` explicitly).
    - Shows active tasks by default, limited to a small number to avoid noise.

    Args:
        focus: Optional task identifier to mention in a heading.
        reason: Optional reason label to show in a dim header.
        show_all: If True, show recent tasks; default is active-only.
        limit: Maximum number of tasks to display (default: 10).
    """
    try:
        pref = os.environ.get("FLOW_AUTO_STATUS", "").strip().lower()
        if pref in {"0", "false", "no", "off"}:
            return
        # If not an interactive terminal and not explicitly opted in, skip
        if not sys.stdout.isatty() and pref not in {"1", "true", "yes", "on"}:
            return

        # Import lazily to avoid heavyweight imports during CLI bootstrap
        from flow.cli.ui.facade.views import (  # type: ignore
            DisplayOptions,
            TaskPresenter,
        )

        opts = DisplayOptions(
            show_all=bool(show_all) if show_all is not None else False,
            status_filter=None,
            limit=max(1, int(limit or 10)),
            show_details=False,
            # Important: mark as non-caching so we don't overwrite the user's
            # index cache (used by 1, 1-3 selections) after state-changing commands.
            # TaskPresenter.present_task_list() only saves indices when json_output is False.
            json_output=True,
        )

        header_bits: list[str] = []
        if reason:
            header_bits.append(reason)
        header_bits.append("status")
        if focus:
            header_bits.append(f"for {focus}")
        header_text = " ".join(header_bits).strip()
        if header_text:
            console.print(f"[dim]— {header_text} —[/dim]")

        presenter = TaskPresenter(console)
        presenter.present_task_list(opts)
    except Exception:  # noqa: BLE001
        # Never fail a primary command due to status rendering
        return
